\documentclass{article}[11pt]
\usepackage[T1]{fontenc}
\usepackage{titling}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{csquotes}

%\usepackage{biblatex}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\setlength{\droptitle}{-6em}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\title{Application: Doctoral student in Speech-centric Science and Speech Technology}
\author{Jim O'Regan}
\date{June 2021}

\begin{document}

\maketitle

%\section{My journey to this point}

%My background for over a decade was in machine translation; I watched a friend translate the manual for a car engine, word by word, and thought ``surely there must be a way to automate this''. That lead me on a journey through several areas of text processing, culminating in a master's degree at Trinity College, Dublin.

%When I arrived at Dublin, my supervisor had no space for me to work in, and Ailbhe N\'i Chasaide kindly found a space for me in her speech lab, where I was working with three of my classmates. Being surrounded by other students who were, for the most part, working on various topics around speech processing, motivated me to learn about it in more depth than was strictly necessary for the master's programme, if for no other reason than to be able to understand the conversations going on around me. My journey through text processing had brought me in contact with Automatic Speech Recognition (ASR), though lacking data to train a system of my own, I never pursued it in any great depth.

%The assignment for one of Ailbhe's modules, ``Describing the Sounds of Languages'', involved comparing the vowels of two varieties of English, using formant measurements. At the eleventh hour, I got frustrated at mixing up my current measurement set with an older set, and deleted the older set. Except I mixed them up again, and deleted the wrong set of files. As the part I had found the most time consuming had been segmenting the vowels, I automated my way around that portion of recreating the data by writing a script to wrap the PocketSphinx ASR engine, in phoneme recognition mode, and extracted only the vowels. I mentioned this in my assignment essay, and this piqued Ailbhe's interest, as she had an interest in ASR for Irish. As I had never quite made the transition with language processing from a hobby to work, I set about finding data to use for a proof-of-concept ASR system for Irish. Although there was not enough labeled data to build a system that worked well enough for real use, there was enough to demonstrate that it was, at least, feasible.

%The period immediately after completing my dissertation was quite bleak; from the moment I started my studies, I had been receiving regular emails and messages from recruiters. From the moment I started to write my dissertation, I stopped receiving them. After about a year of job hunting, I decided to pursue other forms of employment, and accepted a contract as an English teacher in Poland. A few weeks later, Ailbhe asked me to come work for her on ASR for Irish. 

%~\citep{panayotov2015}.
%\section{Personal Background}

%In the most general of terms, my primary interests lie on the intersection of computing and linguistics. I started learning Polish to try to impress a girl; at the time, the material available on the web fit into one of two categories: A1-beginners' materials, and linguistics papers, so I started learning about linguistics, to find answers to the questions my Polish friends were unable to answer\footnote{Possibly the first was ``why is `two hundred' `dwie\'scie', and not `dwasta'?''--the answer is that is a calcified dual number form.}. Later, the same girl I had tried to impress asked for my help while she pain-stakingly, word by word, translated a manual for a car engine for her father. As a programmer since the age of 7, I naturally thought ``surely there's a way to automate this''--a question that lead me on a ten year journey through machine translation, natural language processing, and to a Speech and Language Processing master's degree at Trinity College, Dublin.

%While there, due to a lack of space elsewhere, Ailbhe N\'i Chasaide kindly offered me a desk in her Phonetics and Speech Laboratory. While writing the assignment for one of her phonetics modules, I accidentally deleted my data at a stage when it seemed like starting from scratch would not leave enough time to submit, so I used ASR to perform the segmentation for me, as this had been the most time-consuming part, and mentioned it in my essay, Ailbhe asked if I thought ASR for Irish was feasible, so I set about collecting data for a proof-of-concept system on the weekends. A year after I submitted my dissertation, I had the opportunity to work for Ailbhe on ASR for Irish, but I had grown frustrated of unemployment before that time, and was contracted to work as an English teacher in Poland from the start of the school year.

%\section{Research Interests}

%\begin{itemize}
    %\item Open Source

    %I'd like to think that the next misguided nerd who thinks he can somehow impress a girl with software will be able to build on something I created.

    %\item Less-resourced/minority languages and dialects
    
    %Having Open Source software is often not enough, without the knowledge and skills to put it to use effectively.
    
    %\item Assistive technologies for language
    
    %Towards the end of his life, my father had limited use of his hands; related to the previous point: despite being a native-speaker of arguably the best-resourced language on Earth, my father was unable to use speech technology because he did not speak a well-resourced \textit{dialect}.
    
    %\item Computer-Assisted Language Learning/Computer-Assisted Pronunciation Training
%\end{itemize}

%\begin{itemize}
    %\item Challenging data
    
    %My work/hobbies over the past 15 years has involved finding data where none seemed to exist; in finding ways to make use of data that seemed unsuitable.
    
%\end{itemize}

I am writing to express my interest in the candidature listed under the title \href{https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:402029/where:4/}{``Doctoral student in Speech-centric Science and Speech Technology''}, with a specific interest in task 2:

\begin{displayquote}
Develop Swedish deep learning based automatic speech recognition (speech-to-text) that can handle long and complex recordings from Swedish media archives.
\end{displayquote}

I have been involved in speech and language processing for approximately 15 years, the first twelve of which were primarily spent working with Machine Translation, the last three working with speech recognition. I quite clearly remember the transition in text processing from web-harvested text being regarded with suspicion, to a grudging acceptance of curated sites (e.g., Project Gutenberg), to achieving validity as a corpus source, to the present, where the bulk of the corpora used in deep learning-based methods for natural language processing have been harvested from the web.

The trends in speech recognition at the moment are somewhat reminiscent of those early days of the web as a corpus, in that large amounts of audio drawn from the web are being used: wav2vec 2.0~\citep{baevski2020wav2vec} uses a large amount of audio to pretrain an acoustic model, using an unsupervised method inspired by BERT~\citep{devlin2019bert}; the XLSR-53~\citep{conneau2020unsupervised} model extends this, by training on 53 different languages, from Mozilla's Common Voice corpus~\citep{ardila-etal-2020-common}. During a recent sprint organised by Hugging Face, volunteers trained ASR models based on XLSR-53 for the languages of Common Voice, many beating the previous state-of-the-art for those languages; more interestingly, similar gains were reported by a handful of volunteers who trained ASR models on languages \textit{not} featured in Common Voice.

More recently still, wav2vec-U, short for wav2vec Unsupervised~\citep{baevski2021unsupervised}, drew on ideas from unsupervised Machine Translation (MT), such as \citet{artetxe-etal-2018-robust}, which use unsupervised methods to align word vectors.

Further work in the area of unsupervised MT concentrates on extracting parallel sentence pairs from unaligned corpora;~\citep{artetxe2019marginbased,artetxe2019massively} the scope for applying similar methods to the task of extracting transcripts from unaligned corpora using embeddings seems quite broad.

Audio embeddings are, by their very nature, much larger than word embeddings. Embeddings-of-embeddings, such as the sentence embeddings used in extracting parallel sentence pairs, are the avenue I propose to explore, which wav2vec-U as well as work on image description generation (e.g., \citet{karpathy2015deep}) suggest are likely to be successful.

The radio recordings from the Royal Library are an ideal dataset for taking the next step into harvesting the web for audio. News bulletins tend to be repeated throughout the day on any particular station, and typically originate from the same news agencies, either verbatim, or lightly paraphrased. Even without pushing the boundaries on harvesting data from parallel sources, using traditional ASR methods across broadcasts from multiple regions can provide data from otherwise under-represented dialects, in a manner somewhat similar to the construction of LibriSpeech~\citep{panayotov2015}, by using higher quality transcripts from, e.g., the Stockholm dialect as a transcript source for others.

\section{Research questions}

\begin{itemize}
    \item What analogue of sentence embeddings can best be used to find unlabeled transcriptions?
    
    \item What methods of segmenting the audio are required for those embeddings?
    
    \item To what extent can ASR output from a higher-resourced dialect be used to increase coverage to lower-resourced dialects?
    
\end{itemize}

\bibliography{main}
%\bibliographystyle{apa}
%\nocite*{}

\end{document}