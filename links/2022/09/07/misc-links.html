<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 07/09/2022 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 07/09/2022" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2022/09/07/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2022/09/07/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-07T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2022/09/07/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 07/09/2022","dateModified":"2022-09-07T00:00:00-05:00","datePublished":"2022-09-07T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2022/09/07/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/notes/">notes</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/notes/about/">About Me</a>
  <a class="nav-item" href="/notes/search/">Search</a>
  <a class="nav-item" href="/notes/categories/">Tags</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 07/09/2022</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-09-07T00:00:00-05:00" itemprop="datePublished">
        Sep 7, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://arxiv.org/abs/2204.02492">Towards End-to-end Unsupervised Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2204.02492</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2204.02492}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2204.02492}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Alexander H. and Hsu, Wei-Ning and Auli, Michael and Baevski, Alexei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards End-to-end Unsupervised Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/1808.02228">Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.1808.02228</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1808.02228}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/1808.02228}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yu-Hsuan and Lee, Hung-yi and Lee, Lin-shan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/zhenghuatan/rVADfast">zhenghuatan/rVADfast</a></p>

<p><a href="https://arxiv.org/abs/2110.07205">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a>,
<a href="https://github.com/microsoft/SpeechT5">microsoft/SpeechT5</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2110.07205</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2110.07205}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2110.07205}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<hr>

<p><a href="https://github.com/microsoft/SpeechT5/issues/3">How to load the pretrained models in pytorch</a></p>

<p><a href="https://confcats-event-sessions.s3.amazonaws.com/lrec22/papers/final/323/323_Paper.pdf">Multilingual and Multimodal Learning for Brazilian Portuguese</a></p>

<p><a href="https://confcats-event-sessions.s3.amazonaws.com/lrec22/papers/final/1041/1041_Paper.pdf">RoomReader: A Multimodal Corpus of Online Multiparty Conversational Interactions</a></p>

<p><a href="https://confcats-event-sessions.s3.amazonaws.com/lrec22/papers/final/687/687_Paper.pdf">Investigating Independence vs. Control: Agenda-Setting in Russian News Coverage on Social Media</a></p>

<p><a href="https://cs.slu.edu/~scannell/pub/dppsi.pdf">Diachronic Parsing of Pre-Standard Irish</a></p>

<p><a href="https://github.com/probabilisticai/probai-2022">probabilisticai/probai-2022</a>, <a href="https://www.youtube.com/channel/UCcMwNzhpePJE3xzOP_3pqsw/videos">videos</a></p>

<hr>

<p><a href="https://ai.facebook.com/blog/ai-speech-brain-activity/">Using AI to decode speech from brain activity</a></p>

<hr>

<p><a href="https://github.com/huggingface/transformers/pull/16782">add wav2vec2_alignment</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/15773">Add fairseq FastSpeech2</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17302">Add Emformer</a></p>

<p><a href="https://github.com/huggingface/transformers/commit/fe785730dcbf3390aa07f667e8d3c4b02d6638e0">data2vec-vision Onnx ready-made configuration</a></p>

<p><a href="https://github.com/huggingface/transformers/commit/ee0d001de71f0da892f86caa3cf2387020ec9696">Add a TF in-graph tokenizer for BERT</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17845">add MobileNetV2 model</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17772">Adding Omnivore Model to HF</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17733">Layoutlmv2 tesseractconfig</a></p>

<p><a href="https://huggingface.co/pyannote/embedding">pyannote/embedding</a></p>

<p><a href="https://huggingface.co/blog/asr-chunking">ASR chunking</a></p>

<hr>

<p><a href="https://lithme.eu/">LITHME</a></p>

<p><a href="https://www.clarin.eu/event/2022/clarin-annual-conference-2022">CLARIN Annual Conference 2022</a></p>

<hr>

<p><a href="https://github.com/google/lyra">google/lyra</a> — A Very Low-Bitrate Codec for Speech Compression</p>

<p><a href="https://github.com/salesforce/awd-lstm-lm">salesforce/awd-lstm-lm</a></p>

<p><a href="https://arxiv.org/abs/1911.03588">MKD: a Multi-Task Knowledge Distillation Approach for Pretrained Language Models</a></p>

<p><a href="https://arxiv.org/abs/2106.13871">Transflower: probabilistic autoregressive dance generation with multimodal attention</a>,
<a href="https://github.com/guillefix/transflower-lightning">code</a></p>

<p><a href="https://arxiv.org/abs/2203.17113">Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data</a></p>

<p><a href="https://arxiv.org/abs/2004.04290">An investigation of phone-based subword units for end-to-end speech recognition</a></p>

<p><a href="https://lorenlugosch.github.io/posts/2020/11/transducer/">Sequence-to-sequence learning with Transducers</a></p>

<p><a href="https://arxiv.org/abs/2010.10504">Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</a></p>

<p><a href="https://pytorch.org/audio/main/tutorials/online_asr_tutorial.html">ONLINE ASR WITH EMFORMER RNN-T</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">We published Tuda german model from <a href="https://t.co/4xPzWgW6fw">https://t.co/4xPzWgW6fw</a><a href="https://t.co/7mdkimirTj">https://t.co/7mdkimirTj</a><br>it is big (4.4G) and slightly more accurate than Vosk on audiobooks and well covers CV test<br><br>9.48 (Tuda-de test), 25.82 (podcast) 4.97 (cv-test) 11.01 (mls) 35.20 (mtedx)</p>— AlphaCephei (@alphacep) <a href="https://twitter.com/alphacep/status/1557445857762578434?ref_src=twsrc%5Etfw">August 10, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
<p><a href="https://github.com/uhh-lt/kaldi-tuda-de">code</a></p>

<p><a href="https://www.faithcomesbyhearing.com/audio-bible-resources/recordings-database">Recordings Database</a></p>

<p><a href="https://huggingface.co/spaces/k2-fsa/automatic-speech-recognition">spaces/k2-fsa/automatic-speech-recognition</a></p>

<p><a href="https://github.com/csukuangfj/optimized_transducer">csukuangfj/optimized_transducer</a></p>

<p><a href="https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/1705.PDF">Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</a></p>

<p><a href="https://arxiv.org/abs/2203.15614">Integrating Lattice-Free MMI into End-to-End Speech Recognition</a></p>

<p><a href="https://github.com/clarin-eric/parla-clarin">clarin-eric/parla-clarin</a></p>

<p><a href="https://github.com/clarin-eric/ParlaMint">clarin-eric/ParlaMint</a></p>

<p><a href="https://osf.io/ag3kj/">MASC-MEG</a></p>

<p><a href="https://www.youtube.com/watch?v=spUNpyF58BY">But what is the Fourier Transform? A visual introduction.</a></p>

<p><a href="https://arxiv.org/abs/2209.03143">AudioLM: a Language Modeling Approach to Audio Generation</a></p>

<hr>

<p><a href="https://arxiv.org/abs/2203.17113">Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data</a></p>

<p><a href="https://homepages.inf.ed.ac.uk/htang2/sigml/mlslp2021/MLSLP2021_paper_15.pdf">Layer-wise analysis of a self-supervised speech representation</a></p>

<hr>

<p><a href="https://psi.engr.tamu.edu/l2-arctic-corpus/">L2-ARCTIC</a></p>


  </div><a class="u-url" href="/notes/links/2022/09/07/misc-links.html" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list">
  <li>
    <a href="https://jimregan.github.io/notes/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
