<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 07/09/2022</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-09-07T00:00:00-05:00" itemprop="datePublished">
        Sep 7, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://arxiv.org/abs/2204.02492">Towards End-to-end Unsupervised Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2204.02492</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2204.02492}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2204.02492}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Alexander H. and Hsu, Wei-Ning and Auli, Michael and Baevski, Alexei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards End-to-end Unsupervised Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/1808.02228">Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.1808.02228</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1808.02228}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/1808.02228}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yu-Hsuan and Lee, Hung-yi and Lee, Lin-shan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Segmental Audio Word2Vec: Representing Utterances as Sequences of Vectors with Applications in Spoken Term Detection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/zhenghuatan/rVADfast">zhenghuatan/rVADfast</a></p>

<p><a href="https://arxiv.org/abs/2110.07205">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a>,
<a href="https://github.com/microsoft/SpeechT5">microsoft/SpeechT5</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2110.07205</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2110.07205}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2110.07205}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<hr>

<p><a href="https://github.com/microsoft/SpeechT5/issues/3">How to load the pretrained models in pytorch</a></p>

<p><a href="https://confcats-event-sessions.s3.amazonaws.com/lrec22/papers/final/323/323_Paper.pdf">Multilingual and Multimodal Learning for Brazilian Portuguese</a></p>

<p><a href="https://confcats-event-sessions.s3.amazonaws.com/lrec22/papers/final/1041/1041_Paper.pdf">RoomReader: A Multimodal Corpus of Online Multiparty Conversational Interactions</a></p>

<p><a href="https://confcats-event-sessions.s3.amazonaws.com/lrec22/papers/final/687/687_Paper.pdf">Investigating Independence vs. Control: Agenda-Setting in Russian News Coverage on Social Media</a></p>

<p><a href="https://cs.slu.edu/~scannell/pub/dppsi.pdf">Diachronic Parsing of Pre-Standard Irish</a></p>

<p><a href="https://github.com/probabilisticai/probai-2022">probabilisticai/probai-2022</a>, <a href="https://www.youtube.com/channel/UCcMwNzhpePJE3xzOP_3pqsw/videos">videos</a></p>

<hr>

<p><a href="https://ai.facebook.com/blog/ai-speech-brain-activity/">Using AI to decode speech from brain activity</a></p>

<hr>

<p><a href="https://github.com/huggingface/transformers/pull/16782">add wav2vec2_alignment</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/15773">Add fairseq FastSpeech2</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17302">Add Emformer</a></p>

<p><a href="https://github.com/huggingface/transformers/commit/fe785730dcbf3390aa07f667e8d3c4b02d6638e0">data2vec-vision Onnx ready-made configuration</a></p>

<p><a href="https://github.com/huggingface/transformers/commit/ee0d001de71f0da892f86caa3cf2387020ec9696">Add a TF in-graph tokenizer for BERT</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17845">add MobileNetV2 model</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17772">Adding Omnivore Model to HF</a></p>

<p><a href="https://github.com/huggingface/transformers/pull/17733">Layoutlmv2 tesseractconfig</a></p>

<p><a href="https://huggingface.co/pyannote/embedding">pyannote/embedding</a></p>

<p><a href="https://huggingface.co/blog/asr-chunking">ASR chunking</a></p>

<hr>

<p><a href="https://lithme.eu/">LITHME</a></p>

<p><a href="https://www.clarin.eu/event/2022/clarin-annual-conference-2022">CLARIN Annual Conference 2022</a></p>

<hr>

<p><a href="https://github.com/google/lyra">google/lyra</a> — A Very Low-Bitrate Codec for Speech Compression</p>

<p><a href="https://github.com/salesforce/awd-lstm-lm">salesforce/awd-lstm-lm</a></p>

<p><a href="https://arxiv.org/abs/1911.03588">MKD: a Multi-Task Knowledge Distillation Approach for Pretrained Language Models</a></p>

<p><a href="https://arxiv.org/abs/2106.13871">Transflower: probabilistic autoregressive dance generation with multimodal attention</a>,
<a href="https://github.com/guillefix/transflower-lightning">code</a></p>

<p><a href="https://arxiv.org/abs/2203.17113">Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data</a></p>

<p><a href="https://arxiv.org/abs/2004.04290">An investigation of phone-based subword units for end-to-end speech recognition</a></p>

<p><a href="https://lorenlugosch.github.io/posts/2020/11/transducer/">Sequence-to-sequence learning with Transducers</a></p>

<p><a href="https://arxiv.org/abs/2010.10504">Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</a></p>

<p><a href="https://pytorch.org/audio/main/tutorials/online_asr_tutorial.html">ONLINE ASR WITH EMFORMER RNN-T</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">We published Tuda german model from <a href="https://t.co/4xPzWgW6fw">https://t.co/4xPzWgW6fw</a><a href="https://t.co/7mdkimirTj">https://t.co/7mdkimirTj</a><br>it is big (4.4G) and slightly more accurate than Vosk on audiobooks and well covers CV test<br><br>9.48 (Tuda-de test), 25.82 (podcast) 4.97 (cv-test) 11.01 (mls) 35.20 (mtedx)</p>— AlphaCephei (@alphacep) <a href="https://twitter.com/alphacep/status/1557445857762578434?ref_src=twsrc%5Etfw">August 10, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<p><a href="https://github.com/uhh-lt/kaldi-tuda-de">code</a></p>

<p><a href="https://www.faithcomesbyhearing.com/audio-bible-resources/recordings-database">Recordings Database</a></p>

<p><a href="https://huggingface.co/spaces/k2-fsa/automatic-speech-recognition">spaces/k2-fsa/automatic-speech-recognition</a></p>

<p><a href="https://github.com/csukuangfj/optimized_transducer">csukuangfj/optimized_transducer</a></p>

<p><a href="https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/1705.PDF">Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</a></p>

<p><a href="https://arxiv.org/abs/2203.15614">Integrating Lattice-Free MMI into End-to-End Speech Recognition</a></p>

<p><a href="https://github.com/clarin-eric/parla-clarin">clarin-eric/parla-clarin</a></p>

<p><a href="https://github.com/clarin-eric/ParlaMint">clarin-eric/ParlaMint</a></p>

<p><a href="https://osf.io/ag3kj/">MASC-MEG</a></p>

<p><a href="https://www.youtube.com/watch?v=spUNpyF58BY">But what is the Fourier Transform? A visual introduction.</a></p>

<p><a href="https://arxiv.org/abs/2209.03143">AudioLM: a Language Modeling Approach to Audio Generation</a></p>

<hr>

<p><a href="https://arxiv.org/abs/2203.17113">Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data</a></p>

<p><a href="https://homepages.inf.ed.ac.uk/htang2/sigml/mlslp2021/MLSLP2021_paper_15.pdf">Layer-wise analysis of a self-supervised speech representation</a></p>

<hr>

<p><a href="https://psi.engr.tamu.edu/l2-arctic-corpus/">L2-ARCTIC</a></p>


  </div><a class="u-url" href="/notes/links/2022/09/07/misc-links.html" hidden></a>
</article>