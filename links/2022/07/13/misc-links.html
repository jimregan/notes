<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 13/07/2022</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-13T00:00:00-05:00" itemprop="datePublished">
        Jul 13, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://github.com/patrick-kidger/equinox">patrick-kidger/equinox</a> â€” Callable PyTrees and filtered transforms =&gt; neural networks in JAX.</p>

<p><a href="https://github.com/patrick-kidger/diffrax">patrick-kidger/diffrax</a> â€” Numerical differential equation solvers in JAX. Autodifferentiable and GPU-capable.</p>

<hr>

<p><a href="https://arxiv.org/abs/2207.00952">M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2207.00952</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2207.00952}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2207.00952}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Jinming and Yang, Hao and Shareghi, Ehsan and Haffari, Gholamreza}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{arXiv.org perpetual, non-exclusive license}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Check out our latest breakthrough in machine translation that Mark Zuckerberg just announced. We built and open sourced a state-of-the-art AI model that now translates between 200 different languages.</p>â€” Meta AI (@MetaAI) <a href="https://twitter.com/MetaAI/status/1544670269469507585?ref_src=twsrc%5Etfw">July 6, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<p><a href="https://github.com/facebookresearch/fairseq/tree/nllb">Code</a> is open source, model <a href="https://github.com/facebookresearch/fairseq/blob/nllb/LICENSE.model.md">is not</a></p>

<hr>

<p><a href="https://github.com/huggingface/transformers/pull/17387">Trillson in transformers</a></p>

<hr>

<p><a href="https://ieeexplore.ieee.org/document/9414560">Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@INPROCEEDINGS</span><span class="p">{</span><span class="nl">9414560</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Shi, Yangyang and Wang, Yongqiang and Wu, Chunyang and Yeh, Ching-Feng and Chan, Julian and Zhang, Frank and Le, Duc and Seltzer, Mike}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{6783-6787}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/ICASSP39728.2021.9414560}</span><span class="p">}</span>
</code></pre></div></div>

<hr>

<p><a href="https://github.com/lumaku/ctc-segmentation">lumaku/ctc-segmentation</a> â€” Segment an audio file and obtain utterance alignments.</p>

<hr>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">This past week I spent some time learning about SentenceTransformers (<a href="https://t.co/5ZAV7lJq7u">https://t.co/5ZAV7lJq7u</a>), and I'm pretty blown away by what sentence embeddings can be used for.<br><br>If you're curious to see what researchers have been getting up to with it, here's a ðŸ§µ with some highlights:</p>â€” Nima Boscarino (@NimaBoscarino) <a href="https://twitter.com/NimaBoscarino/status/1535331680805801984?ref_src=twsrc%5Etfw">June 10, 2022</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<hr>

<p><a href="https://memcauliffe.com/how-much-data-do-you-need-for-a-good-mfa-alignment.html">How much data do you need for a good MFA alignment?</a></p>

<blockquote>
  <ul>
    <li>If you care only about alignments of the training data, 3-5 hours should be enough.
      <ul>
        <li>Caveat: increasing the number of speakers/varieties in the training data will likely need more training data</li>
      </ul>
    </li>
    <li>If you care about generating models for more widespread use, 8-10 should be enough for generalizing to the same variety
      <ul>
        <li>The more speakers the better, but also more speakers should need more data</li>
        <li>I usually recommend about 20 hours for a decently performant model</li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr>

<p><a href="https://github.com/google-research/t5x">google-research/t5x</a> â€” essentially a new and improved implementation of the T5 codebase (based on Mesh TensorFlow) in JAX and Flax.</p>

<p><a href="https://github.com/google/seqio">google/seqio</a> â€” Task-based datasets, preprocessing, and evaluation for sequence models.</p>

<hr>

<p><a href="https://www.reddit.com/r/weirddalle/">r/weirddalle</a></p>

<hr>

<p><a href="https://arxiv.org/abs/2204.02492">Towards End-to-end Unsupervised Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2204.02492</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2204.02492}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2204.02492}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Alexander H. and Hsu, Wei-Ning and Auli, Michael and Baevski, Alexei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards End-to-end Unsupervised Speech Recognition}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr>

<p><a href="https://arxiv.org/abs/2204.05409">Unified Speech-Text Pre-training for Speech Translation and Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">tang2022unified</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Unified Speech-Text Pre-training for Speech Translation and Recognition}</span><span class="p">,</span>
      <span class="na">author</span><span class="p">=</span><span class="s">{Yun Tang and Hongyu Gong and Ning Dong and Changhan Wang and Wei-Ning Hsu and Jiatao Gu and Alexei Baevski and Xian Li and Abdelrahman Mohamed and Michael Auli and Juan Pino}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2204.05409}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr>


  </div><a class="u-url" href="/notes/links/2022/07/13/misc-links.html" hidden></a>
</article>