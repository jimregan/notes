<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 22/01/2025 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 22/01/2025" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2025/01/22/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2025/01/22/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-01-22T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2025/01/22/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 22/01/2025","dateModified":"2025-01-22T00:00:00-06:00","datePublished":"2025-01-22T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2025/01/22/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/notes/">notes</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/notes/about/">About Me</a>
  <a class="nav-item" href="/notes/search/">Search</a>
  <a class="nav-item" href="/notes/categories/">Tags</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 22/01/2025</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2025-01-22T00:00:00-06:00" itemprop="datePublished">
        Jan 22, 2025
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://arxiv.org/abs/2501.12326">UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a>,
<a href="https://github.com/bytedance/UI-TARS">no code yet</a>,
<a href="https://github.com/bytedance/UI-TARS-desktop">desktop app</a>,
<a href="https://huggingface.co/bytedance-research/UI-TARS-72B-DPO">model</a></p>

<p><a href="https://en.wiktionary.org/wiki/Category:Swedish_terms_with_IPA_pronunciation">Swedish terms with IPA pronunciation</a></p>

<p><a href="https://dl.acm.org/doi/10.5555/3540261.3541982">Combiner: full attention transformer with sparse computation cost</a>,
<a href="https://proceedings.neurips.cc/paper/2021/file/bd4a6d0563e0604510989eb8f9ff71f5-Paper.pdf">pdf</a></p>

<p><a href="https://huggingface.co/OuteAI/OuteTTS-0.3-500M">OuteAI/OuteTTS-0.3-500M</a>,
<a href="https://github.com/edwko/OuteTTS">code</a>
(1B model is not open)</p>

<p><a href="https://librivox.org/an-sgealuidhe-gaedhealach-by-douglas-hyde/">An Sgéaluidhe Gaedhealach</a></p>

<p><a href="https://mhsung.github.io/kaist-cs492d-fall-2024/">Diffusion Models and Their Applications</a></p>

<p><a href="https://github.com/HidekiKawahara/SparkNG">HidekiKawahara/SparkNG</a> — MATLAB real-time/interactive speech tools. This series is obsolete. SP3ARK is the up-to-date series (will be).</p>

<p><a href="https://www.vocaltractlab.de/index.php?page=vocaltractlab-download">VocalTractLab</a></p>

<p>Hakarps kyrka: <a href="https://commons.wikimedia.org/wiki/File:Hakarps_kyrka.ogg">audio</a>,
<a href="https://sv.wikipedia.org/w/index.php?title=Hakarps_kyrka&amp;diff=prev&amp;oldid=14712739">revision</a> ?</p>

<p><a href="https://nips.cc/virtual/2024/poster/96326">RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks</a>,
<a href="https://openreview.net/forum?id=974ojuN0jU">OpenReview</a>,
<a href="https://github.com/Parallel-in-Time-Differential-Equations/RandNet-Parareal">code</a></p>

<p><a href="https://nips.cc/virtual/2024/poster/93351">SEL-BALD: Deep Bayesian Active Learning for Selective Labeling with Instance Rejection</a>,
<a href="https://openreview.net/forum?id=tDMTwto6jv">OpenReview</a></p>

<p><a href="https://nips.cc/virtual/2024/poster/96743">Theoretical Foundations of Deep Selective State-Space Models</a>,
<a href="https://openreview.net/forum?id=3SzrqwupUx">OpenReview</a></p>

<p><a href="https://nips.cc/virtual/2024/poster/96596">Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning</a>,
<a href="https://openreview.net/forum?id=5H4l37IsZ8">OpenReview</a>
<!-- not open source: https://github.com/grypesc/AdaGauss --></p>

<p>Related:
<a href="https://ieeexplore.ieee.org/abstract/document/9915459">Class-Incremental Learning: Survey and Performance Evaluation on Image Classification</a>,
<a href="https://github.com/mmasana/FACIL">code</a></p>

<p><a href="https://www.youtube.com/watch?v=4IfbPQgec2M">What if English actually SOUNDED like this??</a></p>

<p><a href="https://github.com/ros-visualization/rviz">rviz</a> — ROS 3D Robot Visualizer</p>

<p><a href="https://huggingface.co/parler-tts">parler-tts</a>,
<a href="https://github.com/huggingface/parler-tts">code</a>,
<a href="https://huggingface.co/parler-tts/parler_tts_mini_v0.1">parler_tts_mini_v0.1</a>,</p>

<p><a href="https://github.com/HCI-LAB-UGSPEECHDATA/speech_data_ghana_ug">HCI-LAB-UGSPEECHDATA/speech_data_ghana_ug</a> — The dataset comprises of 5000 hours speech corpus in Akan, Ewe, Dagbani, Daagare, and Ikposo. Each language includes 1000 hours of audio speech from indigenous speakers of the language and 100 hours of transcription.</p>

<p><a href="https://www.youtube.com/watch?app=desktop&amp;v=G3jdyzSmNNA&amp;list=PLlUFqz3WqFzr56S7ejSuUNfSyS08d2KSJ&amp;index=1">001 - Hungarian short narrative A0</a></p>

<p><a href="https://github.com/microsoft/GW-BASIC">microsoft/GW-BASIC</a> — The original source code of Microsoft GW-BASIC from 1983</p>

<p><a href="https://github.com/microsoft/MS-DOS">microsoft/MS-DOS</a> — The original sources of MS-DOS 1.25, 2.0, and 4.0 for reference purposes</p>

<p><a href="https://github.com/Standard-Intelligence/hertz-dev">Standard-Intelligence/hertz-dev</a> — first base model for full-duplex conversational audio</p>

<p><a href="https://huggingface.co/datasets/wav2gloss/fieldwork">wav2gloss/fieldwork</a> — Mostly open, but includes closed data</p>

<p><a href="https://github.com/juice500ml/finetune_owsm">juice500ml/finetune_owsm</a></p>

<p><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a> — A high-throughput and memory-efficient inference and serving engine for LLMs</p>

<p><a href="https://github.com/espnet/espnet/pull/5966">espnet - Phoneme Recognition with IPAPack</a></p>

<p><a href="https://arxiv.org/abs/2405.00307">Active Learning with Task Adaptation Pre-training for Speech Emotion Recognition</a></p>

<p><a href="https://arxiv.org/abs/2405.00233">SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound</a>,
<a href="https://github.com/haoheliu/SemantiCodec-inference">inference code</a></p>

<p><a href="https://johanwind.github.io/2023/03/23/rwkv_details.html">How the RWKV language model works</a>,
<a href="https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py">RWKV_in_150_lines.py</a></p>

<p><a href="https://github.com/clee704/audiodiff">clee704/audiodiff</a> — A commandline tool that compares two audio files and prints the difference</p>

<p><a href="https://pygyat.vercel.app/docs">PyGyat</a>,
<a href="https://github.com/shamith09/pygyat">code</a></p>

<p><a href="https://docs.google.com/document/u/0/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/mobilebasic">torch.compile, the missing manual</a></p>

<p><a href="http://blog.ezyang.com/2024/11/ways-to-use-torch-compile/">Ways to use torch.compile</a></p>

<p><a href="https://arxiv.org/abs/2112.05329">FaceFormer: Speech-Driven 3D Facial Animation with Transformers</a>,
<a href="https://github.com/EvelynFan/FaceFormer">code</a> — Depends on Max Planck stuff, so probably not useable.</p>

<p><a href="https://arxiv.org/abs/2410.00086">ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer</a>,
<a href="https://huggingface.co/scepter-studio/ACE-0.6B-512px">model</a>,
<a href="https://github.com/ali-vilab/ACE">code</a></p>

<p><a href="https://github.com/modelscope/scepter">modelscope/scepter</a> — SCEPTER is an open-source framework used for training, fine-tuning, and inference with generative models.</p>

<p><a href="https://arxiv.org/abs/2501.06320">TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer</a></p>

<p><a href="https://github.com/rusq/slackdump">rusq/slackdump</a></p>

<p><a href="https://github.com/black-forest-labs/flux">black-forest-labs/flux</a> — Official inference repo for FLUX.1 models.
<a href="https://huggingface.co/black-forest-labs/FLUX.1-schnell">open model</a></p>

<p><a href="https://arxiv.org/abs/2501.11378">Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio</a>,
<a href="https://github.com/DSP-AGH/ICASSP2025_Whisper_Hallucination">data</a></p>

<p><a href="https://huggingface.co/spaces/webml-community/deepseek-r1-webgpu">deepseek-r1-webgpu</a></p>

<p><a href="https://github.com/allenai/OLMo">allenai/OLMo</a> — Modeling, training, eval, and inference code for OLMo</p>

<p><a href="https://huggingface.co/datasets/m-a-p/Code-Feedback">m-a-p/Code-Feedback</a> — OpenCodeInterpreter is a family of open-source code generation systems designed to bridge the gap between large language models and advanced proprietary systems like the GPT-4 Code Interpreter. It significantly advances code generation capabilities by integrating execution and iterative refinement functionalities.</p>

<p><a href="https://www.kaggle.com/datasets/magnoliasis/persian-tts-dataset-male">persian-tts-dataset-male</a>,
<a href="https://www.kaggle.com/datasets/magnoliasis/persian-tts-dataset-famale">persian-tts-dataset-famale</a></p>

<p><a href="https://arxiv.org/abs/2408.00370">DiM-Gesture: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2 framework</a></p>

<p><a href="https://github.com/kamperh/lecture_dtw_notebook">Dynamic Time Warping Notebook</a></p>

<p><a href="https://github.com/kamperh/speech_dtw">kamperh/speech_dtw</a></p>

<p><a href="https://arxiv.org/abs/2405.05254">You Only Cache Once: Decoder-Decoder Architectures for Language Models</a>,
<a href="https://github.com/microsoft/unilm/tree/master/YOCO">code</a></p>

<p><a href="https://github.com/hitz-zentroa/latxa">hitz-zentroa/latxa</a> — Latxa: An Open Language Model and Evaluation Suite for Basque</p>

<p><a href="https://github.com/kamperh/VectorQuantizedCPC">kamperh/VectorQuantizedCPC</a></p>

<p><a href="https://arxiv.org/abs/2405.15757">Looking Backward: Streaming Video-to-Video Translation with Feature Banks</a></p>

<p><a href="https://arxiv.org/abs/2407.13142">A light-weight and efficient punctuation and word casing prediction model for on-device streaming ASR</a>,
<a href="https://github.com/frankyoujian/Edge-Punct-Casing">code</a></p>

<p><a href="https://mek.oszk.hu/06000/06058/">Büszkeség és balítélet</a></p>

<p><a href="https://huggingface.co/EvaByte/EvaByte">EvaByte/EvaByte</a> — EvaByte is a 6.5B byte-level language model built upon an improved architecture with multibyte prediction and EVA – an efficient attention mechanism designed for scalability and performance.</p>

<p><a href="https://github.com/HKUNLP/efficient-attention">HKUNLP/efficient-attention</a> — [EVA ICLR’23; LARA ICML’22] Efficient attention mechanisms via control variates, random features, and importance sampling</p>

<p><a href="https://arxiv.org/abs/2404.08636">Probing the 3D Awareness of Visual Foundation Models</a>,
<a href="https://github.com/mbanani/probe3d">mbanani/probe3d</a></p>

<p><a href="https://arxiv.org/abs/2211.15654">OpenScene: 3D Scene Understanding with Open Vocabularies</a>,
<a href="https://github.com/pengsongyou/openscene">code</a></p>

<p><a href="https://nips.cc/virtual/2024/poster/96742">Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</a>,
<a href="https://openreview.net/forum?id=3TxyhBZHT2">OpenReview</a>,
<a href="https://github.com/YunzeMan/Lexicon3D">code</a></p>

<p><a href="https://nips.cc/virtual/2024/poster/96803">Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images</a>,
<a href="https://openreview.net/forum?id=2dfBpyqh0A">OpenReview</a></p>

<p><a href="https://arxiv.org/abs/2103.14635">PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds</a>,
<a href="https://github.com/CVMI-Lab/PAConv">code</a></p>

<p><a href="https://arxiv.org/abs/2202.03052">OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework</a>,
<a href="https://github.com/OFA-Sys/OFA">code</a></p>

<p><a href="https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/">Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for Speech, Music, and Sound</a>,
<a href="https://github.com/facebookresearch/audiobox-aesthetics">code</a> (CC-BY)</p>

<p><a href="https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306">CTC Networks and Language Models: Prefix Beam Search Explained</a></p>

<p><a href="https://arxiv.org/abs/2008.01532">A Study on Effects of Implicit and Explicit Language Model Information for DBLSTM-CTC Based Handwriting Recognition</a></p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/1458347">Pronunciation modeling for speech technology</a></p>

<p><a href="https://arxiv.org/abs/2008.01532">A Study on Effects of Implicit and Explicit Language Model Information for DBLSTM-CTC Based Handwriting Recognition</a></p>

<p><a href="https://www.ubiquitypress.com/site/chapters/e/10.5334/bbi.15/">D-LUCEA: Curation of the UCU Accent Project Data</a></p>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inbook</span><span class="p">{</span><span class="nl">orr2017dlucea</span><span class="p">,</span>
<span class="na">author</span> <span class="p">=</span> <span class="s">{Orr, Rosemary and Quené, Hugo}</span><span class="p">,</span>
<span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="na">month</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
<span class="na">pages</span> <span class="p">=</span> <span class="s">{181-193}</span><span class="p">,</span>
<span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CLARIN in the Low Countries}</span><span class="p">,</span>
<span class="na">editor</span>    <span class="p">=</span> <span class="s">{Odijk, Jan and van~Hessen, Arjan}</span><span class="p">,</span>
<span class="na">publisher</span> <span class="p">=</span> <span class="s">{Ubiquity Press}</span><span class="p">,</span>
<span class="na">address</span>   <span class="p">=</span> <span class="s">{London}</span><span class="p">,</span>
<span class="na">title</span> <span class="p">=</span> <span class="s">{D-LUCEA: Curation of the UCU Accent Project data}</span><span class="p">,</span>
<span class="na">doi</span> <span class="p">=</span> <span class="s">{10.5334/bbi.15}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/rhasspy/sv_kaldi-rhasspy">rhasspy/sv_kaldi-rhasspy</a></p>

<p><a href="https://arxiv.org/abs/2502.06490">Recent Advances in Discrete Speech Tokens: A Review</a></p>

<p><a href="https://github.com/AbrahamSanders/codec-bpe">AbrahamSanders/codec-bpe</a> — Implementation of Acoustic BPE (Shen et al., 2024), extended for RVQ-based Neural Audio Codecs</p>

<p><a href="https://www.quantamagazine.org/undergraduate-upends-a-40-year-old-data-science-conjecture-20250210/">Undergraduate Upends a 40-Year-Old Data Science Conjecture</a></p>

<p><a href="Pages%20that%20link%20to" title="Q56216056">https://www.wikidata.org/w/index.php?title=Special:WhatLinksHere/Q56216056&amp;limit=50&amp;offset=120%7C110214728&amp;dir=next</a> — Wikidata property to identify lexicographical entities (Q56216056)</p>

<p><a href="https://github.com/espnet/espnet/commit/95dcc722b7c23dad2a7c980b9c3f8e7a453994bf">implemente batch decode for owsm-ctc</a></p>

<p><a href="https://www.jstor.org/stable/30200539?seq=1">Some of My Best Friends Are Linguists</a></p>

<p><a href="https://link.springer.com/article/10.1007/s42438-023-00440-6">Generative AI and the Automating of Academia</a></p>

<p><a href="https://aclanthology.org/2022.lrec-1.566/">VoxCommunis: A Corpus for Cross-linguistic Phonetic Analysis</a></p>

<p><a href="https://arxiv.org/abs/2502.10248">Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</a>,
<a href="https://huggingface.co/stepfun-ai/stepvideo-t2v">model</a></p>

<p><a href="https://digitalhumanities.hkust.edu.hk/tutorials/turn-your-jupyter-notebook-into-interactive-presentation-slides-using-anaconda/">Turn your Jupyter Notebook into interactive Presentation Slides using Anaconda</a></p>

<p><a href="https://www.youtube.com/watch?v=InVbYsX7Qjk">Introduction to Linear Prediction</a></p>

<p><a href="https://arxiv.org/abs/2311.08323">The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language</a>,
<a href="https://github.com/lingjzhu/clap-ipa">code</a></p>

<p><a href="https://www.keptar.oszk.hu/indexeng.phtml">Digital Archive of Pictures</a></p>

<p><a href="https://arxiv.org/abs/2502.07642">FoQA: A Faroese Question-Answering Dataset</a>,
<a href="https://github.com/alexandrainst/foqa">code</a></p>

<p><a href="https://arxiv.org/abs/2007.13465">Self-Supervised Contrastive Learning for Unsupervised Phoneme Segmentation</a>,
<a href="https://github.com/felixkreuk/UnsupSeg">code</a></p>

<p><a href="https://arxiv.org/abs/2502.20583">LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation</a>,
<a href="https://github.com/efeslab/LiteASR">code</a></p>

<p><a href="https://arxiv.org/abs/2401.14159">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</a>,
<a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">code</a></p>

<p><a href="https://arxiv.org/abs/2305.03902">Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting</a></p>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0924271624003563#b9">SAMPolyBuild: Adapting the Segment Anything Model for polygonal building extraction</a></p>

<p><a href="https://zenodo.org/records/7024894">The COUGHVID crowdsourcing dataset: A corpus for the study of large-scale cough analysis algorithms</a></p>

<p><a href="https://diffusion.csail.mit.edu/">Introduction to Flow Matching and Diffusion Models</a></p>

<p><a href="https://arxiv.org/abs/2412.08905">Phi-4 Technical Report</a></p>

<p><a href="https://github.com/microsoft/PhiCookBook">microsoft/PhiCookBook</a></p>

<p><a href="https://github.com/microsoft/dstoolkit-phi2-finetune">microsoft/dstoolkit-phi2-finetune</a> — This repository contains step by step instructions on how to finetune Microsoft’s Phi-2 model with your own data.</p>

<p><a href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct">microsoft/Phi-4-multimodal-instruct</a></p>

<p><a href="https://arxiv.org/abs/2502.05244">Probabilistic Artificial Intelligence</a></p>

<p><a href="https://github.com/sprakbankental/braxen">sprakbankental/braxen</a></p>

<p><a href="https://arxiv.org/abs/2503.01710">Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</a>,
<a href="https://github.com/SparkAudio/Spark-TTS">code</a></p>

<p><a href="https://existentialcomics.com/comic/521">March of the Penguins</a></p>


  </div><a class="u-url" href="/notes/links/2025/01/22/misc-links.html" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list">
  <li>
    <a href="https://jimregan.github.io/notes/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
