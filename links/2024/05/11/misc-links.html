<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 11/05/2024 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 11/05/2024" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2024/05/11/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2024/05/11/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-11T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2024/05/11/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 11/05/2024","dateModified":"2024-05-11T00:00:00-05:00","datePublished":"2024-05-11T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2024/05/11/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 11/05/2024</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2024-05-11T00:00:00-05:00" itemprop="datePublished">
        May 11, 2024
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      13 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://github.com/pytorch/translate/blob/master/pytorch_translate/word_prediction/word_prediction_criterion.py">word_prediction_criterion.py</a></p>

<p><a href="https://github.com/xbpeng/DeepMimic">xbpeng/DeepMimic</a> — Motion imitation with deep reinforcement learning.</p>

<p><a href="https://github.com/facebookresearch/fairmotion">facebookresearch/fairmotion</a></p>

<p><a href="https://github.com/facebookresearch/t2motion">facebookresearch/t2motion</a> — open source, but needs SMPLH.</p>

<p><a href="https://github.com/ricsinaruto/gutenberg-dialog">ricsinaruto/gutenberg-dialog</a> — Build a dialog dataset from online books in many languages</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">However you define "woke," anti-woke means being a cunt who wants to indulge bigots.</p>— regular steve albini (@electricalWSOP) <a href="https://twitter.com/electricalWSOP/status/1665265230673321984?ref_src=twsrc%5Etfw">June 4, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/futo-org/whisper-acft">Finetuning Whisper for dynamic audio context robustness</a></p>

<p><a href="https://github.com/andrewgcodes/xlstm">andrewgcodes/xlstm</a> — my attempts at implementing various bits of Sepp Hochreiter’s new xLSTM architecture</p>

<p><a href="http://mocap.cs.cmu.edu/">CMU Graphics Lab Motion Capture Database</a></p>

<p><a href="https://github.com/bulletphysics/bullet3">bulletphysics/bullet3</a> — Bullet Physics SDK: real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc.</p>

<p><a href="https://arxiv.org/abs/2210.13352">ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition</a></p>

<p><a href="https://github.com/huggingface/transformers/issues/26242">WhisperForCTC #26242</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Mathematicians throw shade like no others <a href="https://t.co/5s6Ctmamkk">pic.twitter.com/5s6Ctmamkk</a></p>— Anthony Bonato (@Anthony_Bonato) <a href="https://twitter.com/Anthony_Bonato/status/1787499373712077233?ref_src=twsrc%5Etfw">May 6, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://huggingface.co/BlinkDL/rwkv-4-world">BlinkDL/rwkv-4-world</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Many in edtech are inspired by the AI education in The Diamond Age.<br><br>The fictional edtech is a nanotech pseudointelligent book, The Young Lady’s Illustrated Primer. It bonds to a child at ~4 and educates them until ~16.<br><br>Features of interest of the Primer, then general thoughts… <a href="https://t.co/z81QSntpJf">pic.twitter.com/z81QSntpJf</a></p>— Matt Bateman (@mbateman) <a href="https://twitter.com/mbateman/status/1787160714827997471?ref_src=twsrc%5Etfw">May 5, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://allenai.org/olmo">Open Language Model: OLMo</a></p>

<p><a href="https://github.com/haosulab/ManiSkill">haosulab/ManiSkill</a> — SAPIEN Manipulation Skill Framework, a GPU parallelized robotics simulator and benchmark
(Code is open, assets are not)</p>

<p><a href="https://arxiv.org/abs/2404.19174">XFeat: Accelerated Features for Lightweight Image Matching</a>,
<a href="https://github.com/verlab/accelerated_features">code</a></p>

<p><a href="https://arxiv.org/abs/2405.01242">TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms</a></p>

<p><a href="https://github.com/sato-team/Stable-Text-to-motion-Framework">sato-team/Stable-Text-to-motion-Framework</a></p>

<p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf">Generating Diverse and Natural 3D Human Motions from Text</a>,
<a href="https://github.com/EricGuo5513/text-to-motion">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Simple rules to decide when to stop fine-tuning.<br><br>Here we have 2 plots:<br>- chart 1: validation loss<br>- chart 2: training + validation loss <a href="https://t.co/BRSxlHFqHK">pic.twitter.com/BRSxlHFqHK</a></p>— Boris Dayma 🖍️ (@borisdayma) <a href="https://twitter.com/borisdayma/status/1786158626496758056?ref_src=twsrc%5Etfw">May 2, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://www.youtube.com/watch?v=16ayjz7NoJQ">Kerry King- Trophies of The Tyrant/ Chemical Warfare</a></p>

<p><a href="https://www.theguardian.com/music/2023/aug/15/the-evolution-of-steve-albini-if-the-dumbest-person-is-on-your-side-youre-on-the-wrong-side">The evolution of Steve Albini: ‘If the dumbest person is on your side, you’re on the wrong side’</a></p>

<p><a href="https://github.com/muditbhargava66/PyxLSTM">muditbhargava66/PyxLSTM</a> — PyxLSTM is a Python library that provides an efficient and extensible implementation of the Extended Long Short-Term Memory (xLSTM) architecture. xLSTM enhances the traditional LSTM by introducing exponential gating, memory mixing, and a matrix memory structure, enabling improved performance and scalability for sequence modeling tasks.</p>

<p><a href="https://sesquiotic.com/2011/08/16/cepstrum-quefrency-rahmonic/">cepstrum, quefrency, rahmonic</a></p>

<p><a href="https://arxiv.org/abs/2405.08317">SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models</a></p>

<p><a href="https://arxiv.org/abs/2405.08417">Simple and Efficient Quantization Techniques for Neural Speech Coding</a></p>

<p><a href="https://arxiv.org/abs/2405.08295">SpeechVerse: A Large-scale Generalizable Audio Language Model</a></p>

<p><a href="https://arxiv.org/abs/2405.08237">A predictive learning model can simulate temporal dynamics and context effects found in neural representations of continuous speech</a></p>

<p><a href="https://onlinelibrary.wiley.com/doi/10.1111/cogs.13448">Learning the Meanings of Function Words From Grounded Language Using a Visual Question Answering Model</a></p>

<p><a href="https://arxiv.org/abs/2405.08054">Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning</a></p>

<p><a href="https://arxiv.org/abs/2405.08344">No Time to Waste: Squeeze Time into Channel for Mobile Video Understanding</a>,
<a href="https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SqueezeTime">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Robots with the help of neuroimplants helped a paralyzed man! 🦾<br><br>A 76-year-old paralyzed man has made history by using his thoughts to write 8 Chinese characters! <br><br>This incredible feat marks the first successful use of Zhejiang University's brain implants to enable writing… <a href="https://t.co/OvlBfl5lVy">pic.twitter.com/OvlBfl5lVy</a></p>— Lukas Ziegler (@lukas_m_ziegler) <a href="https://twitter.com/lukas_m_ziegler/status/1790648380080722323?ref_src=twsrc%5Etfw">May 15, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2405.09062">Naturalistic Music Decoding from EEG Data via Latent Diffusion Models</a></p>

<p><a href="https://arxiv.org/abs/2305.03568">A vector quantized masked autoencoder for audiovisual speech emotion recognition</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">(1/5) <a href="https://twitter.com/CKT_Conner?ref_src=twsrc%5Etfw">@CKT_Conner</a>, <a href="https://twitter.com/dill_pkl?ref_src=twsrc%5Etfw">@dill_pkl</a>, <a href="https://twitter.com/emilyzsh?ref_src=twsrc%5Etfw">@emilyzsh</a>, and I are excited to introduce Shard - a proof-of-concept for an infinitely scalable distributed system composed of consumer hardware for training and running ML models!<br><br>Features:<br>- Data + Pipeline Parallel for handling arbitrarily large… <a href="https://t.co/LkVwrvU3it">pic.twitter.com/LkVwrvU3it</a></p>— Aksh Garg (@AkshGarg03) <a href="https://twitter.com/AkshGarg03/status/1790824537904554351?ref_src=twsrc%5Etfw">May 15, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin"><p>There was a 'Not Found' error fetching URL: 'https://x.com/getnormality/status/1790942454688145484'</p></div>

<p><a href="https://www.youtube.com/watch?v=ozdDv1z1q24">Jan Kasprowicz - Krzak dzikiej róży w Ciemnych Smreczynach</a></p>

<p><a href="https://polska-poezja.pl/">polska-poezja.pl</a></p>

<p><a href="https://arxiv.org/abs/2403.13187">Evolutionary Optimization of Model Merging Recipes</a>,
<a href="https://github.com/SakanaAI/evolutionary-model-merge">code</a></p>

<p><a href="https://spraakbanken.gu.se/en/resources/kelly">Swedish Kelly list</a></p>

<p><a href="https://arxiv.org/abs/2404.09385">A Large-Scale Evaluation of Speech Foundation Models</a></p>

<p><a href="https://arxiv.org/abs/2402.13250">Video ReCap: Recursive Captioning of Hour-Long Videos</a>,
<a href="https://github.com/md-mohaiminul/VideoRecap">code</a></p>

<p><a href="https://arxiv.org/abs/2402.13199">Target Speech Extraction with Pre-trained Self-supervised Learning Models</a></p>

<p><a href="https://arxiv.org/abs/2309.10926">Semi-Autoregressive Streaming ASR With Label Context</a></p>

<p><a href="https://arxiv.org/abs/2402.13236">Towards audio language modeling – an overview</a></p>

<p><a href="https://arxiv.org/abs/2402.13200">Probing Self-supervised Learning Models with Target Speech Extraction</a></p>

<p><a href="https://arxiv.org/abs/2305.03582">A multimodal dynamical variational autoencoder for audiovisual speech representation learning</a></p>

<p><a href="https://arxiv.org/abs/2402.12423">On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models</a></p>

<p><a href="https://arxiv.org/abs/2402.07485">MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning</a></p>

<p><a href="https://www.kth.se/navet/calendar/best-practices-for-robot-death-1.1317985?date=2024-02-23&amp;orgdate=2024-02-19&amp;length=1&amp;orglength=317">Best Practices for Robot Death</a></p>

<p><a href="https://arxiv.org/abs/2402.12654">OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification</a></p>

<p><a href="https://arxiv.org/abs/2402.07485">MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning</a></p>

<p><a href="https://lightning.ai/lightning-ai/studios/understanding-using-and-finetuning-gemma">Understanding, Using, and Finetuning Gemma</a></p>

<p><a href="https://arxiv.org/abs/2310.11781">Blind estimation of audio effects using an auto-encoder approach and differentiable digital signal processing</a></p>

<p><a href="https://arxiv.org/abs/2303.15343">Sigmoid Loss for Language Image Pre-Training</a>,
<a href="https://github.com/google-research/big_vision">code</a></p>

<p><a href="https://www.theguardian.com/technology/article/2024/may/20/chatgpt-scarlett-johansson-voice">ChatGPT suspends Scarlett Johansson-like voice as actor speaks out against OpenAI</a></p>

<p><a href="https://arxiv.org/abs/2312.07661">CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor</a>,
<a href="https://github.com/google-research/google-research/tree/master/clip_as_rnn">code</a></p>

<p><a href="https://arxiv.org/abs/2105.12196">From Motor Control to Team Play in Simulated Humanoid Football</a></p>

<p><a href="https://arxiv.org/abs/2309.15826">Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing</a></p>

<p><a href="https://aclanthology.org/2022.naacl-main.376/">Cross-modal Contrastive Learning for Speech Translation</a>,
<a href="https://github.com/ReneeYe/ConST">code</a></p>

<p><a href="https://github.com/nateraw/hf-hub-lightning">nateraw/hf-hub-lightning</a> — A PyTorch Lightning Callback for pushing models to the Hugging Face Hub</p>

<p><a href="https://arxiv.org/abs/2306.01128">Learning Transformer Programs</a></p>

<p><a href="https://arxiv.org/abs/2405.12221">Images that Sound: Composing Images and Sounds on a Single Canvas</a>,
<a href="https://github.com/IFICL/images-that-sound">code</a>,
(model is not open).</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">The Doge Meme dog, Kabosu has died. <br><br>She was 18 years old. <a href="https://t.co/ScMhYn2kuF">pic.twitter.com/ScMhYn2kuF</a></p>— Dexerto (@Dexerto) <a href="https://twitter.com/Dexerto/status/1793899911852798319?ref_src=twsrc%5Etfw">May 24, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">we've officially reached AGI <a href="https://t.co/fKcex4ZFLH">pic.twitter.com/fKcex4ZFLH</a></p>— gaut (@0xgaut) <a href="https://twitter.com/0xgaut/status/1794019623324590475?ref_src=twsrc%5Etfw">May 24, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2405.18047">2BP: 2-Stage Backpropagation</a></p>

<p><a href="https://arxiv.org/abs/2405.17247">An Introduction to Vision-Language Modeling</a></p>

<p><a href="https://arxiv.org/abs/1912.03263">Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One</a>,
<a href="https://github.com/wgrathwohl/JEM">code</a></p>

<p><a href="https://github.com/MyoHub/myoconverter">MyoHub/myoconverter</a> — A tool to convert opensim 4.0+ MSK models into MuJoCo format with optimized muscle kinematics and kinetics</p>

<p><a href="https://arxiv.org/abs/2208.07363">MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control</a>,
<a href="https://github.com/microsoft/MoCapAct">code</a></p>

<p><a href="https://www.science.org/doi/10.1126/scirobotics.abo0235">From motor control to team play in simulated humanoid football</a></p>

<p><a href="https://github.com/langchain-ai/langchain">langchain-ai/langchain</a></p>

<p><a href="https://arxiv.org/abs/1803.10299">Multi-Modal Data Augmentation for End-to-End ASR</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Big announcement: <a href="https://twitter.com/pleiasfr?ref_src=twsrc%5Etfw">@pleiasfr</a> releases a massive open corpus of 2 million Youtube videos in Creative Commons (CC-By) on <a href="https://twitter.com/huggingface?ref_src=twsrc%5Etfw">@huggingface</a>. Youtube-Commons features 30 billion words of audio transcriptions in multiple languages, and soon other modalities <a href="https://t.co/BevSENB7KZ">https://t.co/BevSENB7KZ</a> <a href="https://t.co/31Ya7utO7D">pic.twitter.com/31Ya7utO7D</a></p>— Alexander Doria (@Dorialexander) <a href="https://twitter.com/Dorialexander/status/1780959636306481476?ref_src=twsrc%5Etfw">April 18, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://huggingface.co/datasets/PleIAs/YouTube-Commons">PleIAs/YouTube-Commons</a></p>

<p>(Dataset is noisy, no attempt made to determine if transcript in original
language in any way matches speech — or if there even is speech — and
often original transcript is omitted in favour of a translation).</p>

<p><a href="https://arxiv.org/abs/2401.10020">Self-Rewarding Language Models</a></p>

<p><a href="https://arxiv.org/abs/2401.08096">Learning Disentangled Speech Representations with Contrastive Learning and Time-Invariant Retrieval</a></p>

<p><a href="https://arxiv.org/abs/2401.08833">Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective</a></p>

<p><a href="https://www.nature.com/articles/s41598-022-13865-z">Phonemes based detection of parkinson’s disease for telehealth applications</a></p>

<p><a href="https://arxiv.org/abs/2312.16144">JaColBERT and Hard Negatives, Towards Better Japanese-First Embeddings for Retrieval: Early Technical Report</a>,
<a href="https://huggingface.co/bclavie/JaColBERT">model</a></p>

<p><a href="https://huggingface.co/colbert-ir/colbertv2.0">colbert-ir/colbertv2.0</a></p>

<blockquote>
  <p>ColBERT relies on fine-grained contextual late interaction: it encodes each passage into a matrix of token-level embeddings (shown above in blue). Then at search time, it embeds every query into another matrix (shown in green) and efficiently finds passages that contextually match the query using scalable vector-similarity (MaxSim) operators.</p>
</blockquote>

<p><a href="https://arxiv.org/abs/2401.08415">From Coarse to Fine: Efficient Training for Audio Spectrogram Transformers</a></p>

<p><a href="https://psycnet.apa.org/record/1999-10334-002">The seven sins of memory: Insights from psychology and cognitive neuroscience</a></p>

<p><a href="https://github.com/mcdermottLab/pycochleagram">mcdermottLab/pycochleagram</a> — Generate cochleagrams natively in Python. Ported from Josh McDermott’s MATLAB code.</p>

<p><a href="https://ceur-ws.org/Vol-3133/paper12.pdf">Codifying the Debates of the Riksdag:
Towards a Framework for Semi-automatic Annotation of
Swedish Parliamentary Discourse</a></p>

<p><a href="https://reffusion.github.io/">RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting</a>,
<a href="https://arxiv.org/abs/2404.10765">paper</a></p>

<p><a href="https://github.com/jaywalnut310/vits">jaywalnut310/vits</a></p>

<p><a href="https://flickr.com/photos/jimregan/52219137308/">Dutch people</a></p>

<p><a href="https://arxiv.org/abs/2307.07421">SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding</a></p>

<p><a href="https://arxiv.org/abs/2202.07765">General-purpose, long-context autoregressive modeling with Perceiver AR</a>,
<a href="https://github.com/google-research/perceiver-ar">code</a></p>

<p><a href="https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/">Uneasy on the Ear: An Interview with Lola De La Mata</a>,
<a href="https://www.youtube.com/watch?v=4SaHIYxpqc0">Left Ear</a>,
<a href="https://www.youtube.com/watch?v=c59-6B37jho">Right Ear</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Using 𝚝𝚘𝚛𝚌𝚑.𝚌𝚘𝚖𝚙𝚒𝚕𝚎 makes KANs as fast as MLPs!<br><br>I never thought I would be a fan, but they are starting to look pretty appetizing. <a href="https://t.co/ti031u18YF">pic.twitter.com/ti031u18YF</a></p>— Thomas Ahle (@thomasahle) <a href="https://twitter.com/thomasahle/status/1798408687981297844?ref_src=twsrc%5Etfw">June 5, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2406.02950">4D ASR: Joint Beam Search Integrating CTC, Attention, Transducer, and Mask Predict Decoders</a></p>

<p><a href="https://arxiv.org/abs/2406.03344">Audio Mamba: Bidirectional State Space Model for Audio Representation Learning</a>,
<a href="https://github.com/mhamzaerol/Audio-Mamba-AuM">code</a></p>

<p><a href="https://github.com/dgreenheck/tree-js">dgreenheck/tree-js</a> — Procedural tree generator written with JavaScript and Three.js</p>

<p><a href="https://github.com/xenova/transformers.js">xenova/transformers.js</a></p>

<p><a href="https://arxiv.org/abs/2401.09416">TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion</a></p>

<p><a href="https://arxiv.org/abs/2401.08025">Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination</a></p>

<p><a href="https://github.com/MarcusLoppe/meshgpt-pytorch">MarcusLoppe/meshgpt-pytorch</a>,
<a href="https://huggingface.co/MarcusLoren/MeshGPT-preview">model</a> — based on <a href="https://github.com/lucidrains/meshgpt-pytorch">lucidrains/meshgpt-pytorch</a></p>

<p><a href="https://arxiv.org/abs/2311.16241">SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance</a>,
<a href="https://github.com/google-research/semivl">code</a></p>

<p><a href="https://arxiv.org/abs/2306.05420">Scaling Spherical CNNs</a>,
<a href="https://github.com/google-research/spherical-cnn">code</a></p>

<p><a href="https://github.com/google-research/language-table">Language Table</a> — Suite of human-collected datasets and a multi-task continuous control benchmark for open vocabulary visuolinguomotor learning.</p>

<p><a href="https://arxiv.org/abs/2312.09147">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</a>,
<a href="https://github.com/VAST-AI-Research/TriplaneGaussian">code</a>
(depends on <code class="language-plaintext highlighter-rouge">diff-gaussian-rasterization</code> which is not open source)</p>

<p><a href="https://www.frontiersin.org/journals/neurology/articles/10.3389/fneur.2022.831428/full">Voice in Parkinson’s Disease: A Machine Learning Study</a></p>

<p><a href="https://pubmed.ncbi.nlm.nih.gov/34596531/">Parkinson’s Disease Detection Based on Running Speech Data From Phone Calls</a></p>

<p><a href="https://github.com/lucidrains/BS-RoFormer">lucidrains/BS-RoFormer</a> — Implementation of Band Split Roformer, SOTA Attention network for music source separation out of ByteDance AI Labs</p>

<p><a href="https://arxiv.org/abs/2406.07522">Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</a>,
<a href="https://github.com/microsoft/Samba">code</a></p>

<p><a href="https://arxiv.org/abs/2406.07969">LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning</a>,
<a href="https://github.com/line/LibriTTS-P">data</a></p>

<p><a href="https://arxiv.org/abs/2401.08342">ECAPA2: A Hybrid Neural Network Architecture and Training Strategy for Robust Speaker Embeddings</a></p>

<p><a href="https://arxiv.org/abs/2312.09147">Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</a>,
<a href="https://github.com/VAST-AI-Research/TriplaneGaussian">code</a></p>

<p><a href="https://arxiv.org/abs/2406.03184">Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion</a>,
<!-- [code](https://github.com/Costwen/Ouroboros3D) --></p>

<p><a href="https://arxiv.org/abs/2406.02075">ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition, Dot Multiplication, and ReLU</a></p>

<p><a href="https://github.com/hokema/Pop2Talk">hokema/Pop2Talk</a> — Pop2Talk foreign language prounnciation learning game. Code for the unity client app.</p>

<p><a href="https://github.com/kyegomez/VisionMamba">kyegomez/VisionMamba</a> — Implementation of Vision Mamba from the paper: “Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model” It’s 2.8x faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on high-res images</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">We embedded 250,000 works of art 🎨 from The Met using <a href="https://twitter.com/nomic_ai?ref_src=twsrc%5Etfw">@nomic_ai</a>'s new SOTA <a href="https://twitter.com/hashtag/multimodal?src=hash&amp;ref_src=twsrc%5Etfw">#multimodal</a> embeddings model!<br><br>It's the *first ever* semantic search tool of its kind 👩‍🎨 🔎<br>Search with smart queries like "oil painting with flowers &amp; dogs".<br><br>How we did it &amp; how to use it👇 <a href="https://t.co/sWjW78zUtI">pic.twitter.com/sWjW78zUtI</a></p>— andrew gao (@itsandrewgao) <a href="https://twitter.com/itsandrewgao/status/1798389860711166130?ref_src=twsrc%5Etfw">June 5, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">We recently open-sourced a relatively minimal implementation example of Transformer language model training in JAX, called NanoDO.<br><br>If you stick to vanilla JAX components, the code is relatively straightforward to read -- the model file is &lt;150 lines. We found it useful as a…</p>— Peter J. Liu (@peterjliu) <a href="https://twitter.com/peterjliu/status/1798430625315295290?ref_src=twsrc%5Etfw">June 5, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://pravse.medium.com/the-maze-is-in-the-mouse-980c57cfd61a">The maze is in the mouse</a></p>

<p><a href="https://github.com/lucidrains/soundstorm-pytorch">lucidrains/soundstorm-pytorch</a> — Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch</p>

<p><a href="https://github.com/lucidrains/mogrifier">lucidrains/mogrifier</a> — Usable implementation of Mogrifier, a circuit for enhancing LSTMs and potentially other networks, from Deepmind</p>

<p><a href="https://github.com/ina-foss/inaSpeechSegmenter">ina-foss/inaSpeechSegmenter</a> — CNN-based audio segmentation toolkit. Allows to detect speech, music, noise and speaker gender. Has been designed for large scale gender equality studies based on speech time per gender.</p>

<p><a href="https://arxiv.org/abs/2406.06371">mHuBERT-147: A Compact Multilingual HuBERT Model</a>,
<a href="https://github.com/utter-project/fairseq">fairseq fork</a>,
<a href="https://github.com/utter-project/mHuBERT-147-scripts/">pre-processing scripts</a></p>

<p><a href="https://www.nature.com/articles/s41586-024-07633-4">A virtual rodent predicts the structure of neural activity across behaviors</a></p>

<p><a href="https://github.com/openvla/openvla">openvla/openvla</a> — OpenVLA: An Open-Source Vision-Language-Action Model
(based on Llama, so model is not open)</p>

<p><a href="https://github.com/TRI-ML/prismatic-vlms">TRI-ML/prismatic-vlms</a> — A flexible and efficient codebase for training visually-conditioned language models (VLMs)</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">New paper just dropped, showing how to greatly increase math scores on LLMs by combining monte-carlo tree search (MCTS) with a language model.<br><br>Nice! But... what if instead, we simply tell the LLM to read the paper, and *pretend* it followed those steps? <a href="https://t.co/CizH4UnRwi">pic.twitter.com/CizH4UnRwi</a></p>— Jeremy Howard (@jeremyphoward) <a href="https://twitter.com/jeremyphoward/status/1801037736968913128?ref_src=twsrc%5Etfw">June 12, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2406.08641">ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets</a></p>

<p><a href="https://arxiv.org/abs/2406.08761">VISinger2+: End-to-End Singing Voice Synthesis Augmented by Self-Supervised Learning Representation</a></p>

<p><a href="https://arxiv.org/abs/2406.02528">Scalable MatMul-free Language Modeling</a>,
<a href="https://github.com/ridgerchu/matmulfreellm">code</a></p>

<p><a href="https://www.nature.com/articles/s41467-024-47221-8">Contextual and combinatorial structure in sperm whale vocalisations</a></p>

<p><a href="https://arxiv.org/abs/2403.06387">Towards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR</a></p>

<p><a href="https://arxiv.org/abs/2403.09753">SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages</a></p>

<p><a href="https://github.com/juletx/BertaQA">juletx/BertaQA</a> — BertaQA: How Much Do Language Models Know About Local Culture?</p>

<p><a href="https://edition.cnn.com/2018/05/17/health/snail-memory-rna-science-study-trnd/index.html">Scientists have transplanted memory from one snail to another. So, what does it mean for humans?</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Very nice paper - bGPT - Byte-Level Transformer. a model that processes data at the byte level and learns to simulate the digital world through next byte prediction. <br><br>Unlike traditional deep learning models that focus on human-interpretable data like text, audio and images, bGPT… <a href="https://t.co/xzL6AruFhz">pic.twitter.com/xzL6AruFhz</a></p>— Rohan Paul (@rohanpaul_ai) <a href="https://twitter.com/rohanpaul_ai/status/1800138420351562107?ref_src=twsrc%5Etfw">June 10, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2402.19155">Beyond Language Models: Byte Models are Digital World Simulators</a>,
<a href="https://github.com/sanderwood/bgpt">code</a></p>

<p><a href="https://arxiv.org/abs/2406.10735">How Should We Extract Discrete Audio Tokens from Self-Supervised Models?</a></p>

<p><a href="https://arxiv.org/abs/2406.11768">GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities</a></p>

<p><a href="https://arxiv.org/abs/2406.05359">Towards Lightweight Speaker Verification via Adaptive Neural Network Quantization</a></p>

<p><a href="https://arxiv.org/abs/2406.00022">Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning</a></p>

<p><a href="https://arxiv.org/abs/2406.00021">CrossVoice: Crosslingual Prosody Preserving Cascade-S2ST using Transfer Learning</a></p>

<p><a href="https://arxiv.org/abs/2406.06086">RawBMamba: End-to-End Bidirectional State Space Model for Audio Deepfake Detection</a></p>

<p><a href="https://arxiv.org/abs/2406.12387">Performant ASR Models for Medical Entities in Accented Speech</a></p>

<p><a href="https://arxiv.org/abs/2406.12141">A dual task learning approach to fine-tune a multilingual semantic speech encoder for Spoken Language Understanding</a></p>

<p><a href="https://arxiv.org/abs/2406.12611">Rapid Language Adaptation for Multilingual E2E Speech Recognition Using Encoder Prompting</a></p>

<p><a href="https://arxiv.org/abs/2406.12164">A Mel Spectrogram Enhancement Paradigm Based on CWT in Speech Synthesis</a></p>

<p><a href="https://arxiv.org/abs/2406.12209">Interface Design for Self-Supervised Speech Models</a></p>

<p><a href="https://arxiv.org/abs/2406.12434">Towards Audio Codec-based Speech Separation</a>
<!-- [code](https://github.com/Yip-Jia-Qi/codecformer) Nothing yet --></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Logistic regression defines fuzzy classification boundaries using the softmax operator. At the heart of many supervised learning classification approaches. Introduced by David Cox in 1956. <a href="https://t.co/8G451RDsVo">https://t.co/8G451RDsVo</a> <a href="https://t.co/z3CItiyeFA">https://t.co/z3CItiyeFA</a> <a href="https://t.co/6eqmoEIOGC">pic.twitter.com/6eqmoEIOGC</a></p>— Gabriel Peyré (@gabrielpeyre) <a href="https://twitter.com/gabrielpeyre/status/1802929231128596902?ref_src=twsrc%5Etfw">June 18, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
<p><a href="https://nbviewer.org/github/gpeyre/numerical-tours/blob/master/python/ml_3_classification.ipynb">Notebook</a></p>

<p><a href="https://www.tandfonline.com/doi/full/10.1080/13600826.2022.2052025">I Felt a Little Homosexual Today, So I Called in Sick: The Formation of “Reverse Discourse” by Swedish Gay Activists in the 1970s</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Susan Caplin, the voice behind Alexa confuses Alexa <a href="https://t.co/KjAD2rseKV">pic.twitter.com/KjAD2rseKV</a></p>— Historic Vids (@historyinmemes) <a href="https://twitter.com/historyinmemes/status/1803130558169620967?ref_src=twsrc%5Etfw">June 18, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">OK, time for some tweets about distances between Markov chains! Actually this is about a preprint we've just posted on arxiv with Sergio Calo, Anders Jonsson, Ludovic Schwartz &amp; Javier Segovia-Aguas. FFO optimal transport &amp; bisimulation. Let's dig in!<a href="https://t.co/bwtcBqCcHg">https://t.co/bwtcBqCcHg</a><br>1/n <a href="https://t.co/jYbXSrITYs">pic.twitter.com/jYbXSrITYs</a></p>— Gergely Neu (@neu_rips) <a href="https://twitter.com/neu_rips/status/1802863213760905540?ref_src=twsrc%5Etfw">June 18, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2406.04056">Bisimulation Metrics are Optimal Transport Distances, and Can be Computed Efficiently</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Check out the mind-blowing experiments of <a href="https://twitter.com/dante_leoncini?ref_src=twsrc%5Etfw">@dante_leoncini</a>, a 3D Artist and Programmer, who managed to run Blender on an 18-year-old Nokia phone.<br><br>Now I've seen everything: <a href="https://t.co/h7E1cXKCzT">https://t.co/h7E1cXKCzT</a><a href="https://twitter.com/hashtag/blender?src=hash&amp;ref_src=twsrc%5Etfw">#blender</a> <a href="https://twitter.com/hashtag/blender3d?src=hash&amp;ref_src=twsrc%5Etfw">#blender3d</a> <a href="https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw">#b3d</a> <a href="https://twitter.com/hashtag/blendercommunity?src=hash&amp;ref_src=twsrc%5Etfw">#blendercommunity</a> <a href="https://twitter.com/hashtag/nokia?src=hash&amp;ref_src=twsrc%5Etfw">#nokia</a> <a href="https://twitter.com/hashtag/mobilephone?src=hash&amp;ref_src=twsrc%5Etfw">#mobilephone</a> <a href="https://twitter.com/hashtag/3dsoftware?src=hash&amp;ref_src=twsrc%5Etfw">#3dsoftware</a> <a href="https://t.co/fvW4ckCgvF">pic.twitter.com/fvW4ckCgvF</a></p>— 80 LEVEL (@80Level) <a href="https://twitter.com/80Level/status/1803264785804324888?ref_src=twsrc%5Etfw">June 19, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://80.lv/articles/this-madlad-programmer-managed-to-run-blender-on-a-nokia-phone/">This Madlad Programmer Managed to Run Blender on a Nokia Phone</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Flow-matching implementation:<a href="https://t.co/sP5DXLr4jI">https://t.co/sP5DXLr4jI</a><br><br>Flow-matching is very similar to diffusion, but simplifies things. Noised images are linear interpolations between (data, noise) pairs, and the network predicts *velocity* of this trajectory. <a href="https://t.co/KHsxhPJvV6">pic.twitter.com/KHsxhPJvV6</a></p>— Kevin Frans (@kvfrans) <a href="https://twitter.com/kvfrans/status/1802758750748774637?ref_src=twsrc%5Etfw">June 17, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>
<p><a href="https://github.com/kvfrans/jax-flow">kvfrans/jax-flow</a> — Flow-matching algorithms in JAX</p>

<p><a href="https://www.youtube.com/playlist?list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0">Audio Signal Processing for Machine Learning</a>,
<a href="https://github.com/musikalkemist/AudioSignalProcessingForML">slides</a></p>

<p><a href="https://aclanthology.org/2022.sigmorphon-1.9/">Domain-Informed Probing of wav2vec 2.0 Embeddings for Phonetic Features</a></p>

<p><a href="https://huggingface.co/datasets/neongeckocom/cv-tts-clean">neongeckocom/cv-tts-clean</a> — TTS dataset from Common Voice</p>

<p><a href="https://huggingface.co/neongeckocom">neongeckocom</a> — multilingual ViTS models</p>

<p><a href="https://www.nature.com/articles/d41586-024-02012-5">Not all ‘open source’ AI models are actually open: here’s a ranking</a></p>

<p><a href="https://proceedings.mlr.press/v162/peng22a.html">Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="und" dir="ltr">A letter from Rosa (Ní Dhochartaigh) Uí Néill, wife of Eoghan Ruadh Ó Néill, 1642, Louvain, Belgium. Published in Gilbert's ‘Affairs of Ireland’ vol 1, part 2 (1879). Transcription and translation work done by <a href="https://twitter.com/silmeth?ref_src=twsrc%5Etfw">@silmeth</a>. <a href="https://t.co/jxSya2vdph">pic.twitter.com/jxSya2vdph</a></p>— Corbmacc (@erisceres) <a href="https://twitter.com/erisceres/status/1804919125543727348?ref_src=twsrc%5Etfw">June 23, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p>Wikipedia redirect for language code: <code class="language-plaintext highlighter-rouge">https://en.wikipedia.org/wiki/ISO_639:$CODE</code>
e.g.: <a href="https://en.wikipedia.org/wiki/ISO_639:gle">gle - Irish</a></p>

<p><a href="https://arxiv.org/abs/2405.12979">OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</a>,
<a href="https://github.com/google-research/omniglue">code</a></p>

<p><a href="https://arxiv.org/abs/2406.16808">Exploring the Capability of Mamba in Speech Applications</a></p>

<p><a href="https://arstechnica.com/science/2024/06/researchers-craft-smiling-robot-face-from-living-human-skin-cells/">Researchers craft smiling robot face from living human skin cells</a></p>


  </div><a class="u-url" href="/notes/links/2024/05/11/misc-links.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://jimregan.github.io/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
