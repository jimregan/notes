<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 04/01/2024 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 04/01/2024" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2024/01/04/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2024/01/04/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-01-04T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2024/01/04/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 04/01/2024","dateModified":"2024-01-04T00:00:00-06:00","datePublished":"2024-01-04T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2024/01/04/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/notes/">notes</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/notes/about/">About Me</a>
  <a class="nav-item" href="/notes/search/">Search</a>
  <a class="nav-item" href="/notes/categories/">Tags</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 04/01/2024</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2024-01-04T00:00:00-06:00" itemprop="datePublished">
        Jan 4, 2024
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://github.com/Dao-AILab/flash-attention">Dao-AILab/flash-attention</a> — Fast and memory-efficient exact attention</p>

<p><a href="https://github.com/facebookincubator/velox">facebookincubator/velox</a> — A C++ vectorized database acceleration library aimed to optimizing query engines and data processing systems.</p>

<p><a href="https://huggingface.co/blog/accelerate-large-models">How 🤗 Accelerate runs very large models thanks to PyTorch</a></p>

<p><a href="https://www.sciencedirect.com/science/article/pii/S1568494618305799">A comparative analysis of speech signal processing algorithms for Parkinson’s disease classification and the use of the tunable Q-factor wavelet transform</a></p>

<p><a href="https://github.com/karkirowle/relative_phoneme_analysis">karkirowle/relative_phoneme_analysis</a> — Repository for phoneme analysis on word-level Kaldi/ESPNet ASR transcripts</p>

<p><a href="https://readcoop.eu/model/irish-gaelic-seanchlo-print/">Irish Gaelic/seanchló print</a></p>

<p><a href="https://github.com/prajdabre/yanmtt">prajdabre/yanmtt</a> — Yet Another Neural Machine Translation Toolkit</p>

<p><a href="https://github.com/google-research-datasets/TextNormalizationCoveringGrammars">google-research-datasets/TextNormalizationCoveringGrammars</a> — Covering grammars for English and Russian text normalization</p>

<p><a href="https://arxiv.org/abs/2311.13647">Language Model Inversion</a></p>

<p><a href="https://arxiv.org/abs/2307.14335">WavJourney: Compositional Audio Creation with Large Language Models</a></p>

<p><a href="https://arxiv.org/abs/2106.02302">Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">It wasn't on my bingo card for 2024-W1, but MSFT dropped a decoder-only embedding model based on Mistral7B-instruct, trained on synthetic retrieval data (+ a bunch of train splits from datasets in BEIR &amp; co...), claiming SotA on MTEB.<br><br>Here are a few things that caught my eye: <a href="https://t.co/ObRUkmDgwg">https://t.co/ObRUkmDgwg</a></p>— dinos (@din0s_) <a href="https://twitter.com/din0s_/status/1742235150530851120?ref_src=twsrc%5Etfw">January 2, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2305.16107">VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation</a></p>

<p><a href="https://github.com/thuhcsi/VAENAR-TTS">thuhcsi/VAENAR-TTS</a> — The official implementation of VAENAR-TTS, a VAE based non-autoregressive TTS model.</p>

<p><a href="https://link.springer.com/chapter/10.1007/978-3-031-34020-8_30">Automatic Generation of Subtitles for Videos of the Government of La Rioja</a></p>

<p><a href="https://aniruddhadeb.com/articles/2023/properly-illustrated-transformer.html">The Properly Illustrated Transformer</a></p>

<p><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>

<p><a href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a></p>

<p><a href="https://arxiv.org/abs/2304.06795">Efficient Sequence Transduction by Jointly Predicting Tokens and Durations</a></p>

<p><a href="https://arxiv.org/abs/2311.08403">Instant3D: Instant Text-to-3D Generation</a></p>

<p><a href="https://arxiv.org/abs/2311.04400">LRM: Large Reconstruction Model for Single Image to 3D</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Can we model syntax from speech?<br><br>Most models of syntax are text-based.<br><br>Here we propose that basic syntax can be modeled from raw speech.<br><br>GANs trained on individual words start to concatenate them into multiple-word outputs.<br><br>Sometimes the model even concatenates three words: <a href="https://t.co/rZXAhEulmN">pic.twitter.com/rZXAhEulmN</a></p>— Gašper Beguš (@begusgasper) <a href="https://twitter.com/begusgasper/status/1655981693516517378?ref_src=twsrc%5Etfw">May 9, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2305.01626">Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks</a></p>

<p><a href="https://irishmetalarchive.com/videos/corr-mhona-dair-live-urban-assault-2018/">‘Dair’ (Live @ Urban Assault 2018)</a></p>

<p><a href="https://www.kaggle.com/code/fadyelkbeer/train-t5-model-from-scratch">Train T5 Model From Scratch</a></p>

<p><a href="https://github.com/lingjzhu/CharsiuG2P">lingjzhu/CharsiuG2P</a> — Multilingual G2P in 100 languages</p>

<p><a href="https://arxiv.org/abs/2312.13560">kNN-CTC: Enhancing ASR via Retrieval of CTC Pseudo Labels</a>,
<a href="https://github.com/NKU-HLT/KNN-CTC">“code”</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">A🧵on beating the hardware lottery for retrieval: the internals of the late interaction stack.<br><br>ColBERT introduced a quirky multi-vector retrieval architecture. It does wonders for quality.<br><br>But how can it search 100M docs in 0.1 sec on CPU? Or store 1 billion embeddings in 20GB? <a href="https://t.co/Nc3MDFxrj6">pic.twitter.com/Nc3MDFxrj6</a></p>— Omar Khattab (@lateinteraction) <a href="https://twitter.com/lateinteraction/status/1737578879454425202?ref_src=twsrc%5Etfw">December 20, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://huggingface.co/blog/whisper-speculative-decoding">Speculative Decoding for 2x Faster Whisper Inference</a></p>

<p><a href="https://github.com/SHI-Labs/VCoder">SHI-Labs/VCoder</a> — VCoder: Versatile Vision Encoders for Multimodal Large Language Models, arXiv 2023</p>

<p><a href="https://arxiv.org/abs/2310.16764">ConvNets Match Vision Transformers at Scale</a></p>

<p><a href="https://arxiv.org/abs/2310.10803">SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT</a></p>

<p><a href="https://arxiv.org/abs/2304.10557">An Introduction to Transformers</a></p>

<p><a href="https://arxiv.org/abs/2306.09384">MobileASR: A resource-aware on-device learning framework for user voice personalization applications on mobile phones</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Writing a good paper intro is difficult. I mostly recommend a 4-paragraph intro:<br><br>1) Motivation: Task description / why is it important?<br>2) Challenge: Why is problem so difficult?<br>3) Trends: How does SotA approach it? What's missing?<br>4) Method: How do you solve it? Contributions!</p>— Matthias Niessner (@MattNiessner) <a href="https://twitter.com/MattNiessner/status/1724795600456310844?ref_src=twsrc%5Etfw">November 15, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2311.08966">Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer</a></p>

<p><a href="https://github.com/huggingface/distil-whisper/tree/main/training?s=09#training-distil-whisper">Training Distil-Whisper</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Want high-quality Audio embeddings? CLAP! 👏<br><br>We support the latest general, music and speech CLAP models in Transformers! Use it for Text-to-Speech/ Text-to-Music training and more.<br><br>What is CLAP?<br><br>CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on… <a href="https://t.co/iQNF6Um9yJ">pic.twitter.com/iQNF6Um9yJ</a></p>— Vaibhav (VB) Srivastav (@reach_vb) <a href="https://twitter.com/reach_vb/status/1726699176698732929?ref_src=twsrc%5Etfw">November 20, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Open Whisper-style Speech Model (OWSM) 🔉<br><br>OWSM reproduces Whisper training using an open-source toolkit (ESPNet) and publicly available datasets. OWSM is much more efficient in training and is robust at multi-directional translations.<br><br>Open source training, inference scripts and… <a href="https://t.co/v9exxwevnO">pic.twitter.com/v9exxwevnO</a></p>— Vaibhav (VB) Srivastav (@reach_vb) <a href="https://twitter.com/reach_vb/status/1727065880918409674?ref_src=twsrc%5Etfw">November 21, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">What is Mixture-of-Experts (MoE)?<br><br>MoE is a neural network architecture design that integrates layers of experts/models within the Transformer block. As data flows through the MoE layers, each input token is dynamically routed to a subset of the experts for computation. This… <a href="https://t.co/AnYeITgHVi">pic.twitter.com/AnYeITgHVi</a></p>— Sophia Yang, Ph.D. (@sophiamyang) <a href="https://twitter.com/sophiamyang/status/1733505991600148892?ref_src=twsrc%5Etfw">December 9, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/wellecks/ntptutorial">wellecks/ntptutorial</a> — Tutorial on neural theorem proving</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Since Mixture of Expert (MoE) LLMs are all the rage as of this weekend, thanks to the Mixtral-8x-7B release, here's a quick explainer. The figure below shows the architecture behind the Switch Transformer (<a href="https://t.co/g3Awj99h24">https://t.co/g3Awj99h24</a>), a great intro to MoEs. <br><br>The model depicted in… <a href="https://t.co/2Wg5zjeFXU">pic.twitter.com/2Wg5zjeFXU</a></p>— Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1734234160154185730?ref_src=twsrc%5Etfw">December 11, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://fleuret.org/francois/lbdl.html">THE LITTLE BOOK OF DEEP LEARNING</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Solar rises ☀️ <a href="https://twitter.com/upstageai?ref_src=twsrc%5Etfw">@upstageai</a> just released Solar a 10B an open LLM outperforming other LLMs up to 30B parameters, including Mistral 7B. 🤯 Solar achieves an MMLU score of 65.48, which is only 4 points lower than Meta Llama 2 while being 7x smaller.<br><br>TL;DR;<br><br>🦙 Llama 2 architecture… <a href="https://t.co/tqgVExY8Yx">pic.twitter.com/tqgVExY8Yx</a></p>— Philipp Schmid (@_philschmid) <a href="https://twitter.com/_philschmid/status/1734992933764411788?ref_src=twsrc%5Etfw">December 13, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">fun idea I tested out this morning: Language model fine-tuning in embedding space<br><br>here's the idea: learn a model of *embeddings* of a certain text distribution; then, to generate text, sample embedding and map back to text with vec2text<br><br>this lets us generate language without… <a href="https://t.co/9PPI9q5KiM">pic.twitter.com/9PPI9q5KiM</a></p>— jack morris (is at iclr) (@jxmnop) <a href="https://twitter.com/jxmnop/status/1734961947227897983?ref_src=twsrc%5Etfw">December 13, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/open-mmlab/Amphion">open-mmlab/Amphion</a> — Amphion (/æmˈfaɪən/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.</p>

<p><a href="https://github.com/yangdongchao/AcademiCodec">yangdongchao/AcademiCodec</a></p>

<p><a href="https://arxiv.org/abs/2311.17425">SpeechAct: Towards Generating Whole-body Motion from Speech</a></p>

<p><a href="https://blog.ml6.eu/fine-tuning-whisper-for-dutch-language-the-crucial-role-of-size-dd5a7012d45f">Fine-tuning Whisper for Dutch Language: The Crucial Role of Size</a></p>

<p><a href="https://speechprocessingbook.aalto.fi/index.html">Introduction to Speech Processing</a></p>

<p><a href="https://zenodo.org/records/2619474">LibriSpeech Alignments</a></p>

<p><a href="https://github.com/OML-Team/open-metric-learning">OML-Team/open-metric-learning</a> — Library for metric learning pipelines and models.</p>

<p><a href="https://github.com/haotian-liu/LLaVA">haotian-liu/LLaVA</a> — [NeurIPS’23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.</p>

<p><a href="https://www.youtube.com/watch?v=7R6SITSeiRo">Flip - Glass Animals</a></p>

<p><a href="https://arxiv.org/abs/2311.01906">Simplifying Transformer Blocks</a></p>

<p><a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6">Advanced RAG Techniques: an Illustrated Overview</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Nvidia presents Incremental FastPitch<br><br>Chunk-based High Quality Text to Speech<br><br>paper page: <a href="https://t.co/v1FxDzo7uM">https://t.co/v1FxDzo7uM</a><br><br>Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process… <a href="https://t.co/pM4fnSdMAo">pic.twitter.com/pM4fnSdMAo</a></p>— AK (@_akhaliq) <a href="https://twitter.com/_akhaliq/status/1742757369895960950?ref_src=twsrc%5Etfw">January 4, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">I created my YouTube series on Reinforcement Learning because I saw it applied profitably at Lyft. It was a counterexample to the stigma: "RL is only good for scenarios where a perfect simulator can be accessed endlessly. It's general-but-slow trial-and-error."<br><br>There's truth… <a href="https://t.co/wowDxJaUWy">pic.twitter.com/wowDxJaUWy</a></p>— DJ (@DuaneJRich) <a href="https://twitter.com/DuaneJRich/status/1742777245821989224?ref_src=twsrc%5Etfw">January 4, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Parakeet RNNT &amp; CTC models top the Open ASR Leaderboard! 👑<br><br>Brought to you by <a href="https://twitter.com/NVIDIAAI?ref_src=twsrc%5Etfw">@NVIDIAAI</a> and <a href="https://twitter.com/suno_ai_?ref_src=twsrc%5Etfw">@suno_ai_</a>, parakeet beats Whisper and regains its first place. <br><br>The models are released under a commercially permissive license!  🥳<br><br>The models inherit the same FastConformer… <a href="https://t.co/jF96yecZ1t">pic.twitter.com/jF96yecZ1t</a></p>— Vaibhav (VB) Srivastav (@reach_vb) <a href="https://twitter.com/reach_vb/status/1742261240141918684?ref_src=twsrc%5Etfw">January 2, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://huggingface.co/nvidia/parakeet-rnnt-1.1b">nvidia/parakeet-rnnt-1.1b</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">The RAG wave is here to stay, but in practice, it's hard to retrieve the right docs w/ embdings, &amp; better IR models are hard to use!<br><br>Let's fix that: Introducing 🪤RAGatouille, a lib to train&amp;use SotA retrieval model, ColBERT, in just a few lines of code!<a href="https://t.co/VRHiGQl0Xv">https://t.co/VRHiGQl0Xv</a> <a href="https://t.co/0EpOfV6UWn">pic.twitter.com/0EpOfV6UWn</a></p>— Benjamin Clavié is at ICLR (@bclavie) <a href="https://twitter.com/bclavie/status/1742950315278672040?ref_src=twsrc%5Etfw">January 4, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/bclavie/RAGatouille">bclavie/RAGatouille</a></p>

<p><a href="https://huggingface.co/colbert-ir/colbertv2.0">colbert-ir/colbertv2.0</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Progress on dense retrievers is saturating.<br><br>The best retrievers in 2024 will apply new forms of late interaction, i.e. scalable attention-like scoring for multi-vector embeddings.<br><br>A🧵on late interaction, how it works efficiently, and why/where it's been shown to improve quality <a href="https://t.co/2XG33TtM9R">pic.twitter.com/2XG33TtM9R</a></p>— Omar Khattab (@lateinteraction) <a href="https://twitter.com/lateinteraction/status/1736804963760976092?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2401.01572">Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models</a></p>

<p><a href="https://arxiv.org/abs/2401.02412">LLM Augmented LLMs: Expanding Capabilities through Composition</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Everyone building RAG uses dense embedding retrieval, but simply doing cosine distance doesn’t always capture fine-grained similarity.<br><br>That’s why SOTA retrieval like ColBERT models are so important; these new architectures are fast but more powerful than pure dense retrieval.… <a href="https://t.co/W2RPBBxml4">pic.twitter.com/W2RPBBxml4</a></p>— Jerry Liu (@jerryjliu0) <a href="https://twitter.com/jerryjliu0/status/1743077679258320925?ref_src=twsrc%5Etfw">January 5, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2310.20360">Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Is it possible to teach LLMs a different language? 🤔 Can we transfer the capabilities of LLMs, like Llama, from English to non-English language?<br><br>A group of researchers from Fudan University tried to answer those questions by running vast experiments on extending vocabulary… <a href="https://t.co/fJLYFyQOqP">pic.twitter.com/fJLYFyQOqP</a></p>— Philipp Schmid (@_philschmid) <a href="https://twitter.com/_philschmid/status/1742888388401811795?ref_src=twsrc%5Etfw">January 4, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://www.marktechpost.com/2024/01/03/this-ai-paper-from-mete-introduces-hyper-voltran-a-novel-neural-network-for-transformative-3d-reconstruction-and-rendering/">This AI Paper from Meta Introduces Hyper-VolTran: A Novel Neural Network for Transformative 3D Reconstruction and Rendering</a>,
<a href="https://arxiv.org/abs/2312.16218">paper</a></p>

<p><a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Phi-2: The surprising power of small language models</a></p>

<p><a href="https://arxiv.org/abs/2401.01885">From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</a></p>

<p><a href="https://arxiv.org/abs/2312.12634">MotionScript: Natural Language Descriptions for Expressive 3D Human Motions</a></p>

<p><a href="https://github.com/pjyazdian/Gesture2Vec">pjyazdian/Gesture2Vec</a> — This is an official PyTorch implementation of “Gesture2Vec: Clustering Gestures using Representation Learning Methods for Co-speech Gesture Generation” (IROS 2022).</p>

<p><a href="https://github.com/neuromorphs/NIR">neuromorphs/NIR</a> — Neuromorphic Intermediate Representation reference implementation</p>

<p><a href="https://betterexplained.com/">Better Explained</a></p>

<p><a href="https://arxiv.org/abs/2401.02122">PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques</a></p>

<p><a href="https://arxiv.org/abs/2401.02411">What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Love how RAGatouille makes it so easy to train new ColBERTs.<br><br>ColBERT's real power is you can train it with as little as a few hundred queries. Other dense retrievers need tens of thousands!<br><br>Maybe the test for <a href="https://twitter.com/bclavie?ref_src=twsrc%5Etfw">@bclavie</a>'s library is whether we see an uptick in ColBERT downloads😆 <a href="https://t.co/TnTPT0smff">https://t.co/TnTPT0smff</a> <a href="https://t.co/n4hnHQODqB">pic.twitter.com/n4hnHQODqB</a></p>— Omar Khattab (@lateinteraction) <a href="https://twitter.com/lateinteraction/status/1743009556521975893?ref_src=twsrc%5Etfw">January 4, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://www.theguardian.com/lifeandstyle/2024/jan/01/100-tiny-changes-to-transform-your-life-from-the-one-minute-rule-to-pyjama-yoga">100 tiny changes to transform your life: from the one-minute rule to pyjama yoga</a></p>

<p><a href="https://www.marktechpost.com/2024/01/02/this-paper-from-mit-and-microsoft-introduces-laser-a-novel-machine-learning-approach-that-can-simultaneously-enhance-an-llms-task-performance-and-reduce-its-size-with-no-additional-training/">This Paper from MIT and Microsoft Introduces ‘LASER’: A Novel Machine Learning Approach that can Simultaneously Enhance an LLM’s Task Performance and Reduce its Size with no Additional Training</a></p>

<p><a href="https://arxiv.org/abs/2312.13558">The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction</a></p>

<p><a href="https://arxiv.org/abs/2309.08876">Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation</a></p>

<p><a href="https://huggingface.co/ahxt/LiteLlama-460M-1T">LiteLlama: Reduced-Scale Llama</a> — We present an open-source reproduction of Meta AI’s LLaMa 2. However, with significantly reduced model sizes, LiteLlama-460M-1T has 460M parameters trained with 1T tokens.</p>

<p><a href="https://www.turingpost.com/p/rag">Token 1.3: What is Retrieval-Augmented Generation (RAG)?</a></p>

<p><a href="https://github.com/VikParuchuri/surya">VikParuchuri/surya</a> — Accurate line-level text detection and recognition (OCR) in any language</p>

<p><a href="https://github.com/gchrupala/neurospoken">gchrupala/neurospoken</a> — Neural models of spoken language - LOT Winter school 2024</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">I just found a great introduction to embedding. <br><br>The book is comprehensive yet short. Historical encoding tools, neural nets, and production - all covered.<br><br>Fantastic job by <a href="https://twitter.com/vboykis?ref_src=twsrc%5Etfw">@vboykis</a>. Thanks for making it free to read!<br><br>Looking forward to diving in.<a href="https://t.co/uFwaSjaysn">https://t.co/uFwaSjaysn</a> <a href="https://t.co/SKl2ExOJaw">pic.twitter.com/SKl2ExOJaw</a></p>— Christoph Molnar 🦋 christophmolnar.bsky.social (@ChristophMolnar) <a href="https://twitter.com/ChristophMolnar/status/1745731602682982675?ref_src=twsrc%5Etfw">January 12, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://www.alexirpan.com/2024/01/10/ai-timelines-2024.html">My AI Timelines Have Sped Up (Again)</a></p>

<p><a href="https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/ASR_with_Transducers.ipynb">NeMo - ASR with Transducers</a></p>

<p><a href="https://the-decoder.com/mixtral-8x7b-is-currently-the-best-open-source-llm-surpassing-gpt-3-5/">Mixtral 8x7B is currently the best open-source LLM, surpassing GPT-3.5</a></p>

<p><a href="https://arxiv.org/abs/2401.09350">Foundations of Vector Retrieval</a></p>

<p><a href="https://arxiv.org/abs/2401.09419">GARField: Group Anything with Radiance Fields</a></p>

<p><a href="https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/">AlphaGeometry: An Olympiad-level AI system for geometry</a>,
<a href="https://github.com/google-deepmind/alphageometry">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Less horror. Probably full of typo.<br><br>Source tex there:<a href="https://t.co/M1CPZs1kPl">https://t.co/M1CPZs1kPl</a> <a href="https://t.co/Spiy0JvC3f">https://t.co/Spiy0JvC3f</a> <a href="https://t.co/9e4FdQol3b">pic.twitter.com/9e4FdQol3b</a></p>— François Fleuret (@francoisfleuret) <a href="https://twitter.com/francoisfleuret/status/1748011011590799462?ref_src=twsrc%5Etfw">January 18, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2401.09340">SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding</a></p>

<p><a href="https://arxiv.org/abs/2401.08565">Tuning Language Models by Proxy</a></p>

  </div><a class="u-url" href="/notes/links/2024/01/04/misc-links.html" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list">
  <li>
    <a href="https://jimregan.github.io/notes/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
