<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 01/02/2024 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 01/02/2024" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2024/02/01/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2024/02/01/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2024/02/01/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 01/02/2024","dateModified":"2024-02-01T00:00:00-06:00","datePublished":"2024-02-01T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2024/02/01/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 01/02/2024</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2024-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2024
      </time>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">VHS-Decode has to be one of the most interesting software projects I've stumbled upon recently.  <br><br>It replaces the decoding process of a VHS tape with a software stack, bypassing most of the original hardware.  <br><br>Using an FPGA device for RF capture like the MiSTer, it creates a‚Ä¶ <a href="https://t.co/9c2R2Xhkyf">pic.twitter.com/9c2R2Xhkyf</a></p>‚Äî LaurieWired (@lauriewired) <a href="https://twitter.com/lauriewired/status/1752759821550682508?ref_src=twsrc%5Etfw">January 31, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/oyvindln/vhs-decode">oyvindln/vhs-decode</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">How can we get LLM-based agents to understand the *visual structure* of a webpage? Announcing Llama2Dü¶ôüëÄ!<br><br>We fine-tuned Llama on OCR'd webpage screenshots but with 2D positional embeddings, enabling it to see the structure of a webpage rather than just a sequence of tokens. üßµ <a href="https://t.co/Rz2JocZyOq">pic.twitter.com/Rz2JocZyOq</a></p>‚Äî Rohan Pandey (e/acc) (@khoomeik) <a href="https://twitter.com/khoomeik/status/1753511199877333254?ref_src=twsrc%5Etfw">February 2, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">BlackMamba Mixture of Experts<br><br>BlackMamba is an novel architecture which combines state-space models (SSMs) with mixture of experts (MoE). It uses Mamba as its SSM block and switch transformer as its MoE block base. BlackMamba is extremely low latency for generation and‚Ä¶ <a href="https://t.co/ojhmAKfsUK">pic.twitter.com/ojhmAKfsUK</a></p>‚Äî Carlos E. Perez (@IntuitMachine) <a href="https://twitter.com/IntuitMachine/status/1753791747669315840?ref_src=twsrc%5Etfw">February 3, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2310.07707">MatFormer: Nested Transformer for Elastic Inference</a></p>

<p><a href="https://arxiv.org/abs/2307.00162">What Do Self-Supervised Speech Models Know About Words?</a></p>

<p><a href="https://arxiv.org/abs/2401.17632">What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Progress on dense retrievers is saturating.<br><br>The best retrievers in 2024 will apply new forms of late interaction, i.e. scalable attention-like scoring for multi-vector embeddings.<br><br>Aüßµon late interaction, how it works efficiently, and why/where it's been shown to improve quality <a href="https://t.co/2XG33TtM9R">pic.twitter.com/2XG33TtM9R</a></p>‚Äî Omar Khattab (@lateinteraction) <a href="https://twitter.com/lateinteraction/status/1736804963760976092?ref_src=twsrc%5Etfw">December 18, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Is it possible to teach LLMs a different language? ü§î Can we transfer the capabilities of LLMs, like Llama, from English to non-English language?<br><br>A group of researchers from Fudan University tried to answer those questions by running vast experiments on extending vocabulary‚Ä¶ <a href="https://t.co/fJLYFyQOqP">pic.twitter.com/fJLYFyQOqP</a></p>‚Äî Philipp Schmid (@_philschmid) <a href="https://twitter.com/_philschmid/status/1742888388401811795?ref_src=twsrc%5Etfw">January 4, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/RAIVNLab/MatFormer-OLMo">RAIVNLab/MatFormer-OLMo</a> ‚Äî Code repository for the public reproduction of the language modelling experiments on ‚ÄúMatFormer: Nested Transformer for Elastic Inference‚Äù</p>

<p><a href="https://github.com/arcee-ai/mergekit">arcee-ai/mergekit</a> ‚Äî Tools for merging pretrained large language models.</p>

<p><a href="https://arxiv.org/abs/2401.16658">OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer</a></p>

<p><a href="https://arxiv.org/abs/2210.00077">E-Branchformer: Branchformer with Enhanced merging for speech recognition</a></p>

<p><a href="https://arxiv.org/abs/2402.00282">PAM: Prompting Audio-Language Models for Audio Quality Assessment</a>,
<a href="https://github.com/soham97/PAM">no code yet</a></p>

<p><a href="https://arxiv.org/abs/2401.10225">ChatQA: Building GPT-4 Level Conversational QA Models</a></p>

<p><a href="https://towardsdatascience.com/3-advanced-document-retrieval-techniques-to-improve-rag-systems-0703a2375e1c">3 Advanced Document Retrieval Techniques To Improve RAG Systems</a></p>

<p><a href="https://arxiv.org/abs/2111.00396">Efficiently Modeling Long Sequences with Structured State Spaces</a>,
<a href="https://github.com/state-spaces/s4">code</a></p>

<p><a href="https://github.com/espnet/espnet/pull/4845/files#diff-d42e616b6aa4a81f1c284e32223506e87feb05f8fc16ef162a705c121aaf0cf0">Add S4 decoder in ESPnet2</a></p>

<p><a href="https://ieeexplore.ieee.org/document/9053040">Alignment-Length Synchronous Decoding for RNN Transducer</a></p>

<p><a href="https://github.com/espnet/espnet/commit/a735790f5138c8a898067c71a2f7344e6e6052a4">init owsm v3.1 recipe</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-65/">Lingit uttaleleksikon for nynorsk</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-52/">NLB uttaleleksikon for bokm√•l</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-44/">Tuva Taledatabase</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-23/">NST uttaleleksikon for bokm√•l</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-22/">NST uttaleleksikon for svensk</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-nb-no-sbr-11/">N-gram ‚Äì svensk</a></p>

<p><a href="https://www.nb.no/sprakbanken/ressurskatalog/oai-tekstlab-uio-no-lia-sapmi/">LIA s√°pmi ‚Äì LIA-korpuset for samiske dialekter</a></p>

<p><a href="https://github.com/collabora/WhisperSpeech">collabora/WhisperSpeech</a> ‚Äî An Open Source text-to-speech system built by inverting Whisper.
<a href="https://huggingface.co/spaces/collabora/WhisperSpeech">Space</a></p>

<p><a href="https://aclanthology.org/N18-1119/">Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation</a></p>

<p><a href="https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f">SentenceTransformer: A Model For Computing Sentence Embedding</a></p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/10095889">CLAP Learning Audio Concepts from Natural Language Supervision</a>,
<a href="https://github.com/microsoft/CLAP">code</a></p>

<p><a href="https://arxiv.org/abs/2401.13660">MambaByte: Token-free Selective State Space Model</a></p>

<p><a href="https://github.com/kyegomez/MambaByte">kyegomez/MambaByte</a> ‚Äî Implementation of MambaByte in ‚ÄúMambaByte: Token-free Selective State Space Model‚Äù in Pytorch and Zeta</p>

<p><a href="https://arxiv.org/abs/2401.09417">Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</a>,
<a href="https://github.com/hustvl/Vim">code</a></p>

<p><a href="https://arxiv.org/abs/2205.13147">Matryoshka Representation Learning</a>,
<a href="https://github.com/RAIVNLab/MRL">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">It‚Äôs year 2024, and n-gram LMs are making a comeback!!<br><br>We develop infini-gram, an engine that efficiently processes n-gram queries with unbounded n and trillion-token corpora. It takes merely 20 milliseconds to count the frequency of an arbitrarily long n-gram in RedPajama (1.4T‚Ä¶ <a href="https://t.co/07O1o5pahv">pic.twitter.com/07O1o5pahv</a></p>‚Äî Jiacheng Liu @ COLM (@liujc1998) <a href="https://twitter.com/liujc1998/status/1753080219735629992?ref_src=twsrc%5Etfw">February 1, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2402.01771">BlackMamba: Mixture of Experts for State-Space Models</a>,
<a href="https://github.com/Zyphra/BlackMamba">code</a></p>

<p><a href="https://arxiv.org/abs/2402.03310">V-IRL: Grounding Virtual Intelligence in Real Life</a></p>

<p><a href="https://arxiv.org/abs/2402.02617">Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study on Speech Emotion Recognition</a></p>

<p><a href="https://arxiv.org/abs/2401.13601">MM-LLMs: Recent Advances in MultiModal Large Language Models</a></p>

<p><a href="https://arxiv.org/abs/2402.03286">Training-Free Consistent Text-to-Image Generation</a></p>

<p><a href="https://arxiv.org/abs/2401.18059">RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</a>,
<a href="https://github.com/parthsarthi03/raptor">no code yet</a></p>

<p><a href="https://allenai.org/olmo">Open Language Model: OLMo</a></p>

<p><a href="https://wordwall.net/hu-hu/community/nyelvtan">Magyar nyelvtan</a></p>

<p><a href="https://arxiv.org/abs/2211.09949">Compressing Transformer-based self-supervised models for speech processing</a></p>

<p><a href="https://arxiv.org/abs/2305.19435">AdANNS: A Framework for Adaptive Semantic Search</a>,
<a href="https://github.com/RAIVNLab/AdANNS">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="und" dir="ltr">SGI's 3D File System Navigator (1993) was real <a href="https://t.co/UWbx3PS3Kk">pic.twitter.com/UWbx3PS3Kk</a></p>‚Äî Retro Tech Dreams (@RetroTechDreams) <a href="https://twitter.com/RetroTechDreams/status/1754868642968216037?ref_src=twsrc%5Etfw">February 6, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2402.03407">Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations</a></p>

<p><a href="https://arxiv.org/abs/2402.03710">Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience</a></p>

<p><a href="https://arxiv.org/abs/2402.03620">Self-Discover: Large Language Models Self-Compose Reasoning Structures</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">RAG From Scratch: Video series focused on understanding the RAG landscape<br><br>RAG is central for LLM application development, connecting LLMs to external data sources.<br><br>But, the pace of innovation and new approaches makes it challenging to keep up.<br><br>We're launching a new video‚Ä¶ <a href="https://t.co/963lOnVLcP">pic.twitter.com/963lOnVLcP</a></p>‚Äî LangChain (@LangChainAI) <a href="https://twitter.com/LangChainAI/status/1754915914796216654?ref_src=twsrc%5Etfw">February 6, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">We are releasing the Gen-2 weights. <br><br>This is a limited edition. Collect all 6,834 books to acquire the complete model. <a href="https://t.co/VVVdLPWYSO">pic.twitter.com/VVVdLPWYSO</a></p>‚Äî Crist√≥bal Valenzuela (@c_valenzuelab) <a href="https://twitter.com/c_valenzuelab/status/1754970403309605371?ref_src=twsrc%5Etfw">February 6, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2402.03988">REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR</a></p>

<p><a href="https://huggingface.co/spaces/Xenova/remove-background-web">Background Removal w/ ü§ó Transformers.js</a></p>

<p><a href="https://arxiv.org/abs/2402.04177">Scaling Laws for Downstream Task Performance of Large Language Models</a></p>

<p><a href="https://arxiv.org/abs/2402.04248">Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks</a></p>

<p><a href="https://ieeexplore.ieee.org/document/10095969">Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Led by <a href="https://twitter.com/GoogleDeepMind?ref_src=twsrc%5Etfw">@GoogleDeepMind</a>, we present ALOHA 2 ü§ô: An Enhanced Low-Cost Hardware for Bimanual Teleoperation.<br><br>ALOHA 2 ü§ô significantly improves the durability of the original ALOHA üèñÔ∏è, enabling fleet-scale data collection on more complex tasks.<br><br>As usual, everything is open-sourced! <a href="https://t.co/5OEpO8EFrG">pic.twitter.com/5OEpO8EFrG</a></p>‚Äî Tony Z. Zhao (@tonyzzhao) <a href="https://twitter.com/tonyzzhao/status/1755380475118719407?ref_src=twsrc%5Etfw">February 7, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/tonyzhaozh/aloha">tonyzhaozh/aloha</a></p>

<p><a href="https://arxiv.org/abs/2402.04825">Fast Timing-Conditioned Latent Audio Diffusion</a>,
<a href="https://github.com/Stability-AI/stable-audio-tools">code</a></p>

<p><a href="https://arxiv.org/abs/2401.11053">StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion</a></p>

<p><a href="https://arxiv.org/abs/2402.05054">LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</a>,
<a href="https://github.com/3DTopia/LGM">code</a>,
<a href="https://huggingface.co/ashawkey/LGM">weights</a>,
<a href="https://huggingface.co/spaces/ashawkey/LGM">space</a></p>

<p><a href="https://github.com/segmind/segmoe">segmind/segmoe</a> ‚Äî Segmind Mixture of Diffusion Experts,
<a href="https://huggingface.co/blog/segmoe">blog</a></p>

<p><a href="https://arxiv.org/abs/2402.05706">Unified Speech-Text Pretraining for Spoken Dialog Modeling</a></p>

<p><a href="https://arxiv.org/abs/2402.05861">Memory Consolidation Enables Long-Context Video Understanding</a></p>

<p><a href="https://arxiv.org/abs/2309.08030">AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">&gt; two genius tech hippies just want to make their music software run faster<br>&gt; develop an algorithm, publish a paper<br>&gt; algorithm not patented, but used in every commercial sampling synthesizer immediately after<br>&gt; tech hippies run out of grant money for their lab üòÖ<br><br>Gossett: We‚Ä¶ <a href="https://t.co/LSz4INRKVy">pic.twitter.com/LSz4INRKVy</a></p>‚Äî üë©‚Äçüíª Paige Bailey (@DynamicWebPaige) <a href="https://twitter.com/DynamicWebPaige/status/1755836630840279351?ref_src=twsrc%5Etfw">February 9, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://github.com/CNChTu/FCPE">CNChTu/FCPE</a> ‚Äî fast pitch
estimator using Transformer</p>

<p><a href="https://arxiv.org/abs/2402.05755">SpiRit-LM: Interleaved Spoken and Written Language Model</a></p>

<p><a href="https://arxiv.org/abs/2402.05672">Multilingual E5 Text Embeddings: A Technical Report</a>,
<a href="https://github.com/microsoft/unilm/tree/master/e5">code</a></p>

<p><a href="https://arxiv.org/abs/2402.05859">Learning to Route Among Specialized Experts for Zero-Shot Generalization</a>,
<a href="https://github.com/r-three/phatgoose">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Can "small" finetuned LLMs with less than 2B parameters outperform larger openly available LLMs (Mixtral, Llama 2 Chat) and proprietary LLMs (ChatGPT)? Here's a closer look at the Tiny Titans paper (<a href="https://t.co/WBFDJ9Q7th">https://t.co/WBFDJ9Q7th</a>), where researchers tried to find the answer to this‚Ä¶ <a href="https://t.co/z6rDkBrLEj">pic.twitter.com/z6rDkBrLEj</a></p>‚Äî Sebastian Raschka (@rasbt) <a href="https://twitter.com/rasbt/status/1756316089393270853?ref_src=twsrc%5Etfw">February 10, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2302.00856">idT5: Indonesian Version of Multilingual T5 Transformer</a></p>

<p><a href="https://aclanthology.org/2021.emnlp-main.125/">mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs</a></p>

<p><a href="https://arxiv.org/abs/2402.03620">Self-Discover: Large Language Models Self-Compose Reasoning Structures</a></p>

<p><a href="https://arxiv.org/abs/2402.00396">Efficient Exploration for LLMs</a></p>

<p><a href="https://arxiv.org/abs/2402.00858">Can Large Language Models Understand Context?</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Long is more for alignment<br><br>TL;DR: LIMA's paper [1] claimed that if you just train on 1000 high quality samples you will get a great model.<br><br>Well.. turns out it is even easier.<br><br>Just use the 1000 longest responses in the dataset.<br><br>You will get a surprisingly powerful model.<br><br>---‚Ä¶ <a href="https://t.co/0kaTByZ2ho">pic.twitter.com/0kaTByZ2ho</a></p>‚Äî Yam Peleg (@Yampeleg) <a href="https://twitter.com/Yampeleg/status/1756306614116597934?ref_src=twsrc%5Etfw">February 10, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2312.06837">Spectral State Space Models</a></p>

<p><a href="https://arxiv.org/abs/2002.01808">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a>,
<a href="https://github.com/microsoft/K-Adapter">code</a></p>

<p><a href="https://arxiv.org/abs/2402.04858">CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay</a></p>

<p><a href="https://magyarbagoly.blogspot.com/2022/08/geography.html">Hungarian - Geography</a></p>

<p><a href="https://www.easyhungarian.com/texts.html">Texts - Easy Hungarian</a></p>

<p><a href="https://wordwall.net/hu-hu/community/nyelvtan">Wordwall - Hungarian grammar</a></p>

<p><a href="https://www.reddit.com/r/Miskolc/comments/ypol9g/resource_list_for_learning_hungarian/">Resource List for Learning Hungarian</a>,
<a href="https://docs.google.com/document/d/1ZiIPCM62YbzXd01xkBPYhXtfw9ddQvwIUX_GwHjxjD4/edit?usp=sharing">doc</a></p>

<p><a href="https://lightning.ai/lightning-ai/studios/code-lora-from-scratch?view=public">Code LoRA from Scratch</a></p>

<p><a href="https://ieeexplore.ieee.org/document/9250505">Accelerating RNN Transducer Inference via Adaptive Expansion Search</a></p>

<p><a href="https://github.com/espnet/espnet/pull/3087">CTC Segmentation for ESPnet 2</a></p>

<p><a href="https://github.com/espnet/espnet/pull/5449">Implement wav2gloss</a></p>

<p><a href="https://github.com/gemelo-ai/vocos">gemelo-ai/vocos</a> ‚Äî Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis</p>

<p><a href="https://github.com/CPJKU/onset_detection">CPJKU/onset_detection</a> ‚Äî Python implementation of the most common spectral based onset detection algorithms.</p>

<p><a href="https://www.reddit.com/r/panelshow/wiki/taskmaster/">Taskmaster wiki</a></p>

<p><a href="https://www.youtube.com/watch?v=jkrNMKz9pWU">A Hackers‚Äô Guide to Language Models</a></p>

<p><a href="https://github.com/veeresht/CommPy">veeresht/CommPy</a> ‚Äî Digital Communication with Python</p>

<p><a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI">RVC-Project/Retrieval-based-Voice-Conversion-WebUI</a></p>

<p><a href="https://arxiv.org/abs/2311.04693">Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation</a>,
<a href="https://diff-hiervc.github.io/audio_demo/">demo</a></p>

<p><a href="https://aclanthology.org/2023.findings-emnlp.789/">Affective and Dynamic Beam Search for Story Generation</a></p>

<p><a href="https://www.nature.com/articles/s41598-020-58103-6">Automatic vocal tract landmark localization from midsagittal MRI data</a>,
<a href="https://github.com/christianpayer/MedicalDataAugmentationTool">code</a></p>

<p><a href="https://github.com/lucidrains/phenaki-pytorch">lucidrains/phenaki-pytorch</a> ‚Äî Implementation of Phenaki Video, which uses Mask GIT to produce text guided videos of up to 2 minutes in length, in Pytorch</p>

<p><a href="https://arxiv.org/abs/2312.02974">Describing Differences in Image Sets with Natural Language</a>,
<a href="https://github.com/Understanding-Visual-Datasets/VisDiff">code</a></p>

<p><a href="https://github.com/google-research/multinerf">google-research/multinerf</a> ‚Äî A Code Release for Mip-NeRF 360, Ref-NeRF, and RawNeRF</p>

<p><a href="https://github.com/vasistalodagala/whisper-finetune">vasistalodagala/whisper-finetune</a> ‚Äî Fine-tune and evaluate Whisper models for Automatic Speech Recognition (ASR) on custom datasets or datasets from huggingface.</p>

<p><a href="https://github.com/lucidrains/CALM-pytorch">lucidrains/CALM-pytorch</a> ‚Äî Implementation of CALM from the paper ‚ÄúLLM Augmented LLMs: Expanding Capabilities through Composition‚Äù, out of Google Deepmind</p>

<p><a href="https://github.com/lucidrains/llama-qrlhf">lucidrains/llama-qrlhf</a> ‚Äî Implementation of the Llama architecture with RLHF + Q-learning</p>

<p><a href="https://arxiv.org/abs/2212.04356">Robust Speech Recognition via Large-Scale Weak Supervision</a></p>

<p><a href="https://github.com/ActiveVisionLab/Awesome-LLM-3D">ActiveVisionLab/Awesome-LLM-3D</a> ‚Äî Awesome-LLM-3D: a curated list of Multi-modal Large Language Model in 3D world Resources</p>

<p><a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a></p>

<p><a href="https://github.com/facebookresearch/Pearl">facebookresearch/Pearl</a> ‚Äî A Production-ready Reinforcement Learning AI Agent Library brought by the Applied Reinforcement Learning team at Meta.</p>

<p><a href="https://github.com/metavoiceio/metavoice-src">metavoiceio/metavoice-src</a> ‚Äî Foundational model for human-like, expressive TTS</p>

<p><a href="https://github.com/lucidrains/retro-pytorch">lucidrains/retro-pytorch</a> ‚Äî Implementation of RETRO, Deepmind‚Äôs Retrieval based Attention net, in Pytorch</p>

<p><a href="https://jalammar.github.io/illustrated-retrieval-transformer/">The Illustrated Retrieval Transformer</a></p>

<p><a href="https://github.com/lifeiteng/vall-e">lifeiteng/vall-e</a> ‚Äî PyTorch implementation of VALL-E(Zero-Shot Text-To-Speech), Reproduced Demo</p>

<p><a href="https://arxiv.org/abs/2402.06619">Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning</a></p>

<p><a href="https://github.com/cisnlp/simalign">cisnlp/simalign</a> ‚Äî Obtain Word Alignments using Pretrained Language Models (e.g., mBERT)</p>

<p><a href="https://www.scitepress.org/PublishedPapers/2023/116827/">Speech Recognition for Minority Languages Using
HuBERT and Model Adaptation</a></p>

<p><a href="https://arxiv.org/abs/2305.13009">Textually Pretrained Speech Language Models</a></p>

<p><a href="https://arxiv.org/abs/2402.08846">An Embarrassingly Simple Approach for LLM with Strong ASR Capacity</a></p>

<p><a href="https://biomedical-engineering-online.biomedcentral.com/articles/10.1186/1475-925X-6-23">Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection</a></p>

<p><a href="https://arxiv.org/abs/2211.06687">Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</a>,
<a href="https://github.com/LAION-AI/CLAP">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Can a 2B LLM outperform Mistral 7B or Llama 13B? <br>Creators of the popular Ultrafeedback dataset released MiniCPM, a 2.4B parameter model claiming performance close to Mistral 7B, Llama 2 13B, or Falcon 40B. ü§Øü§î<br><br>As part of the release, the researchers released a detailed‚Ä¶</p>‚Äî Philipp Schmid (@_philschmid) <a href="https://twitter.com/_philschmid/status/1755268463457657263?ref_src=twsrc%5Etfw">February 7, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://shengdinghu.notion.site/MiniCPM-Unveiling-the-Potential-of-End-side-Large-Language-Models-d4d3a8c426424654a4e80e42a711cb20">MiniCPM: Unveiling the Potential of End-side Large Language Models</a>,
<a href="https://github.com/OpenBMB/MiniCPM">code</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">There's a strain of anti-anti-monopolist that insists that they're not *pro*-monopoly - they're just *realists* who understand that global gigacorporations are too big to fail, too big to jail, and that governments can't hope to rein them in. <br><br>1/ <a href="https://t.co/nx0lM4lKWu">pic.twitter.com/nx0lM4lKWu</a></p>‚Äî Cory Doctorow NONCONSENSUAL BLUE TICK (@doctorow) <a href="https://twitter.com/doctorow/status/1754999901073903739?ref_src=twsrc%5Etfw">February 6, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://sh-tsang.medium.com/review-flamingo-a-visual-language-model-for-few-shot-learning-ec477d47e7bf">Review ‚Äî Flamingo: A Visual Language Model for Few-Shot Learning</a></p>

<p><a href="https://towardsai.net/p/machine-learning/multimodal-language-models-explained-visual-instruction-tuning">Multimodal Language Models Explained: Visual Instruction Tuning</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">üì¢Mixtures of Experts unlock parameter scaling for deep RL!<br><br>Adding MoEs, and in particular Soft MoEs, to value-based deep RL agents results in more parameter-scalable models.<br><br>Performance keeps increasing as we increase number of experts (green line below)!<br>1/9 <a href="https://t.co/SMFUrpdNXN">https://t.co/SMFUrpdNXN</a> <a href="https://t.co/kb9mqfyg3m">pic.twitter.com/kb9mqfyg3m</a></p>‚Äî Pablo Samuel Castro (@pcastr) <a href="https://twitter.com/pcastr/status/1757728769383526844?ref_src=twsrc%5Etfw">February 14, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://arxiv.org/abs/2402.08609">Mixtures of Experts Unlock Parameter Scaling for Deep RL</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">üî•Half a year after its initial release we are upgrading self-expanding neural networksüî•<br><br>* SENN based on full-connectivity + now with convolutions<br>* layer &amp; width addition + now pruning any time during training<br>* jax code: <a href="https://t.co/ndnnGjBu4F">https://t.co/ndnnGjBu4F</a><a href="https://t.co/QrYBkWyAV8">https://t.co/QrYBkWyAV8</a><br><br>üßµ ‚¨áÔ∏è <a href="https://t.co/XLWcj8xoC1">https://t.co/XLWcj8xoC1</a> <a href="https://t.co/MnhObyE81B">pic.twitter.com/MnhObyE81B</a></p>‚Äî Martin Mundt (@mundt_martin) <a href="https://twitter.com/mundt_martin/status/1757128614829502521?ref_src=twsrc%5Etfw">February 12, 2024</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><a href="https://venturebeat.com/ai/cohere-for-ai-launches-open-source-llm-for-101-languages/">Cohere for AI launches open source LLM for 101 languages</a></p>

<p><a href="https://www.techradar.com/pro/100x-less-compute-with-gpt-level-llm-performance-how-a-little-known-open-source-project-could-help-solve-the-gpu-power-conundrum-rwkv-looks-promising-but-challenges-remain">100x less compute with GPT-level LLM performance: How a little known open source project could help solve the GPU power conundrum ‚Äî RWKV looks promising but challenges remain</a></p>

<p><a href="https://github.com/BAAI-DCAI/Bunny">BAAI-DCAI/Bunny</a> ‚Äî A family of lightweight multimodal models.</p>

<p>theodorblackbird/lina-speech seems interesting, but it‚Äôs not open source, so I don‚Äôt care.</p>

<p><a href="https://towardsdatascience.com/large-language-models-gpt-2-language-models-are-unsupervised-multitask-learners-33440081f808">Large Language Models, GPT-2 ‚Äî Language Models Are Unsupervised Multitask Learners</a></p>

<p><a href="https://github.com/vosen/ZLUDA">vosen/ZLUDA</a> ‚Äî CUDA on AMD GPUs</p>

  </div><a class="u-url" href="/notes/links/2024/02/01/misc-links.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://jimregan.github.io/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
