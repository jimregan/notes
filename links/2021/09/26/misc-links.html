<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 26/9/2021</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-26T00:00:00-05:00" itemprop="datePublished">
        Sep 26, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://pythonawesome.com/a-framework-for-any-to-any-voice-conversion-with-self-supervised-pretrained-representations/">A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</a></p>

<p><a href="https://github.com/howard1337/S2VC">howard1337/S2VC</a></p>

<p><a href="https://github.com/yistLin/universal-vocoder">yistLin/universal-vocoder</a>; paper: <a href="https://arxiv.org/abs/1811.06292">Towards achieving robust universal neural vocoding</a></p>

<p><a href="https://github.com/cywang97/unispeech">cywang97/unispeech</a>;
paper: <a href="https://arxiv.org/abs/2101.07597">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</a></p>

<p><a href="https://github.com/microsoft/unilm">microsoft/unilm</a> â€” UniLM AI - Large-scale Self-supervised Pre-training across Tasks, Languages, and Modalities</p>

<p><a href="https://arxiv.org/abs/2107.13530">Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition</a></p>

<p><a href="https://huggingface.co/spaces/nielsr/LayoutLMv2-FUNSD">Interactive demo: LayoutLMv2</a></p>

<p><a href="https://aclanthology.org/2021.acl-long.265/">Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment</a>;
<a href="https://github.com/CZWin32768/XLM-Align">CZWin32768/XLM-Align</a></p>

<p><a href="https://github.com/waydroid/waydroid">waydroid/waydroid</a></p>

<p><a href="https://github.com/huseinzol05/malaya-speech">huseinzol05/malaya-speech</a></p>

<p><a href="https://www.kaggle.com/kingabzpro/fine-tuning-xlsr-wav2vec2-for-wolof-asr-with">Fine-tuning XLSR-Wav2Vec2 for WOLOF ASR with ðŸ¤—</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Wav2Vec2ForCTC</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s">"facebook/wav2vec2-large-xlsr-53"</span><span class="p">,</span> 
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">hidden_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">feat_proj_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">mask_time_prob</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">layerdrop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">ctc_loss_reduction</span><span class="o">=</span><span class="s">"mean"</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="n">processor</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">processor</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
  <span class="n">output_dir</span><span class="o">=</span><span class="s">"./wav2vec2-large-xlsr-WOLOF"</span><span class="p">,</span>
  <span class="n">group_by_length</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
  <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
  <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s">"steps"</span><span class="p">,</span>
  <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
  <span class="n">fp16</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
  <span class="n">save_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
  <span class="n">eval_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
  <span class="n">logging_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">,</span>
  <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
  <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p><a href="https://github.com/Appen/UHV-OTS-Speech/blob/main/source_separation/run_spleeter.py">run_spleeter.py</a></p>

<p><a href="https://aclanthology.org/2021.naacl-main.59.pdf">Few-shot Intent Classification and Slot Filling with Retrieved Examples</a></p>

<p><a href="https://arxiv.org/abs/2104.02558">Comparing CTC and LFMMI for out-of-domain adaptation of wav2vec 2.0 acoustic model</a></p>

<p><a href="https://ckbjimmy.github.io/docs/chung2018unsupervised_p.pdf">Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces</a></p>


  </div><a class="u-url" href="/notes/links/2021/09/26/misc-links.html" hidden></a>
</article>