<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 07/03/2023</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-03-07T00:00:00-06:00" itemprop="datePublished">
        Mar 7, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://arxiv.org/abs/2303.01037">Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages</a>, <a href="https://sites.research.google/usm/">website</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2023usm</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2303.01037}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yu and Han, Wei and Qin, James and Wang, Yongqiang and Bapna, Ankur and Chen, Zhehuai and Chen, Nanxin and Li, Bo and Axelrod, Vera and Wang, Gary and Meng, Zhong and Hu, Ke and Rosenberg, Andrew and Prabhavalkar, Rohit and Park, Daniel S. and Haghani, Parisa and Riesa, Jason and Perng, Ginger and Soltau, Hagen and Strohman, Trevor and Ramabhadran, Bhuvana and Sainath, Tara and Moreno, Pedro and Chiu, Chung-Cheng and Schalkwyk, Johan and Beaufays, Françoise and Wu, Yonghui}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">alayrac2022flamingo</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2204.14198}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flamingo: a Visual Language Model for Few-Shot Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2303.00510">A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">huh2023augmentation</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2303.00510}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huh, Mina and Ray, Ruchira and Karnei, Corey}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/revdotcom/fstalign">revdotcom/fstalign</a> — An efficient OpenFST-based tool for calculating WER and aligning two transcript sequences.</p>


  </div><a class="u-url" href="/notes/links/2023/03/07/misc-links.html" hidden></a>
</article>