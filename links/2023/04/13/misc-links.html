<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 13/04/2023 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 13/04/2023" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2023/04/13/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2023/04/13/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-13T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2023/04/13/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 13/04/2023","dateModified":"2023-04-13T00:00:00-05:00","datePublished":"2023-04-13T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2023/04/13/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 13/04/2023</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-04-13T00:00:00-05:00" itemprop="datePublished">
        Apr 13, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="http://www.hungarianreference.com/Verbs/Verbs-Conjunctive-Imperative-Subjunctive.aspx">Conjunctive/Imperative/Subjunctive mood in Hungarian</a></p>

<p><a href="http://www.hungarianreference.com/Verbs/splitting-of-coverbs-verbal-prefixes-meg-el-ki-le-be-fel.aspx">Splitting coverbs from verb root</a></p>

<p><a href="https://www.personal.psu.edu/adr10/hungarian.html">A Hungarian Language Course</a></p>

<hr>

<p><a href="https://github.com/linto-ai/whisper-timestamped">linto-ai/whisper-timestamped</a> — Multilingual Automatic Speech Recognition with word-level timestamps and confidence</p>

<p><a href="https://github.com/lucidrains/medical-chatgpt">lucidrains/medical-chatgpt</a> — Implementation of ChatGPT, but tailored towards primary care medicine, with the reward being able to collect patient histories in a thorough and efficient manner and come up with a reasonable differential diagnosis</p>

<p><a href="https://github.com/DIVA-DIA/Text-Line-Segmentation-Method-for-Medieval-Manuscripts">DIVA-DIA/Text-Line-Segmentation-Method-for-Medieval-Manuscripts</a></p>

<p><a href="https://github.com/personwhofloat/Line-Segmentation-Model">personwhofloat/Line-Segmentation-Model</a> — LSM is short for Line Segmentation Model. It is a model for text line segmentation in document images. The model is robust to color, brightness, page warping.</p>

<p><a href="https://www.youtube.com/watch?v=kGXr1SF3WmA&amp;list=PLJHszsWbB6hpk5h8lSfBkVrpjsqvUGTCx">Tensor Calculus 0: Introduction</a></p>

<p><a href="https://zhauniarovich.com/post/2022/2022-09-matplotlib-graphs-in-research-papers/">Matplotlib Graphs in Research Papers</a></p>

<p><a href="https://tex.stackexchange.com/questions/387047/the-duck-pond-showcase-of-tikz-drawn-animals-ducks">“The duck pond”: showcase of TikZ-drawn animals/ducks</a></p>

<p><a href="https://github.com/neonbjb/ocotillo">neonbjb/ocotillo</a> — Performant and accurate speech recognition built on Pytorch</p>

<p><a href="https://towardsdatascience.com/customer-case-study-building-an-end-to-end-speech-recognition-model-in-pytorch-with-assemblyai-473030e47c7c">Customer Case Study: Building an end-to-end Speech Recognition model in PyTorch with AssemblyAI</a></p>

<p><a href="https://github.com/alibaba-damo-academy/FunASR">alibaba-damo-academy/FunASR</a></p>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/gao22b_interspeech.html">Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao22b_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{2063--2067}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-9996}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://segment-anything.com/">Segment Anything</a>, <a href="https://github.com/facebookresearch/segment-anything">code</a></p>

<p><a href="https://github.com/facebookresearch/habitat-lab">facebookresearch/habitat-lab</a></p>

<p><a href="https://github.com/facebookresearch/myosuite">facebookresearch/myosuite</a></p>

<p><a href="https://github.com/chroma-core/chroma">chroma-core/chroma</a> — the AI-native open-source embedding database</p>

<p><a href="https://github.com/Winfredy/SadTalker">Winfredy/SadTalker</a> — Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation, <a href="https://huggingface.co/spaces/vinthony/SadTalker">demo</a></p>

<p><a href="https://github.com/sdatkinson/neural-amp-modeler">sdatkinson/neural-amp-modeler</a> — Neural network emulator for guitar amplifiers.</p>

<p><a href="https://github.com/facebookresearch/Aria_data_tools">facebookresearch/Aria_data_tools</a> — Aria data tools provide the open-source toolkit in C++ and Python to interact with data from Project Aria</p>

<p><a href="https://github.com/bootphon/articulatory_inversion">bootphon/articulatory_inversion</a> — Inversion-articulatoire is a Python library for training/testing neural network models for the acoustic to articulatory reconstruction.</p>

<p><a href="https://ieeexplore.ieee.org/document/9640504">Acoustic-to-Articulatory Mapping With Joint Optimization of Deep Speech Enhancement and Articulatory Inversion Models</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@ARTICLE</span><span class="p">{</span><span class="nl">9640504</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Shahrebabaki, Abdolreza Sabzi and Salvi, Giampiero and Svendsen, Torbjørn and Siniscalchi, Sabato Marco}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEEE/ACM Transactions on Audio, Speech, and Language Processing}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Acoustic-to-Articulatory Mapping With Joint Optimization of Deep Speech Enhancement and Articulatory Inversion Models}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{30}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{135-147}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/TASLP.2021.3133218}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://huggingface.co/databricks/dolly-v2-12b">databricks/dolly-v2-12b</a></p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">AI is becoming powerful in 2023.<br><br>But most people feel left out with million things happening around AI.<br><br>Here's a MEGA THREAD🧵 (with resources) to keep you up-to-date:</p>— Barsee 🐶 (@heyBarsee) <a href="https://twitter.com/heyBarsee/status/1629497442868551681?ref_src=twsrc%5Etfw">February 25, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p><a href="https://arxiv.org/abs/2212.10465">SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization</a>, <a href="https://github.com/skywalker023/sodaverse">code</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">kim2022soda</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Hyunwoo Kim and Jack Hessel and Liwei Jiang and Ximing Lu and Youngjae Yu and Pei Zhou and Ronan Le Bras and Malihe Alikhani and Gunhee Kim and Maarten Sap and Yejin Choi}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2212.10465}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">This is a baby GPT with two tokens 0/1 and context length of 3, viewing it as a finite state markov chain. It was trained on the sequence "111101111011110" for 50 iterations. The parameters and the architecture of the Transformer modifies the probabilities on the arrows.<br><br>E.g. we… <a href="https://t.co/vj10nZEXlH">pic.twitter.com/vj10nZEXlH</a></p>— Andrej Karpathy (@karpathy) <a href="https://twitter.com/karpathy/status/1645115622517542913?ref_src=twsrc%5Etfw">April 9, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p><a href="https://google-research.github.io/self-organising-systems/2022/diff-fsm/">Differentiable Finite State Machines</a></p>

<p><a href="https://github.com/jerryjliu/llama_index">jerryjliu/llama_index</a> — a project that provides a central interface to connect your LLM’s with external data</p>

<p><a href="https://github.com/auspicious3000/SpeechSplit">auspicious3000/SpeechSplit</a> — Unsupervised Speech Decomposition Via Triple Information Bottleneck</p>

<p><a href="https://huggingface.co/bjelkenhed/whisper-large-sv">bjelkenhed/whisper-large-sv</a>, <a href="https://huggingface.co/bjelkenhed/whisper-train">train</a></p>

<p><a href="https://huggingface.co/datasets/KBLab/rixvox">KBLab/rixvox</a>
<a href="https://kb-labb.github.io/posts/2023-02-15-finding-speeches-in-the-riksdags-debates/">Finding Speeches in the Riksdag’s Debates</a>
<a href="https://kb-labb.github.io/posts/2023-03-09-rixvox-a-swedish-speech-corpus/">RixVox: A Swedish Speech Corpus with 5500 Hours of Speech from Parliamentary Debates</a>,
<a href="https://github.com/kb-labb/riksdagen_anforanden/">code</a></p>

<p><a href="https://huggingface.co/blog/controlnet">Ultra fast ControlNet with Diffusers</a></p>

<p><a href="https://huggingface.co/Jzuluaga/wav2vec2-xls-r-300m-en-atc-uwb-atcc-and-atcosim">Jzuluaga/wav2vec2-xls-r-300m-en-atc-uwb-atcc-and-atcosim</a></p>

<p><a href="https://sail.usc.edu/span/usc-timit/">USC-TIMIT: a database of multimodal speech production data</a></p>

<p><a href="https://yale.app.box.com/s/cfn8hj2puveo65fq54rp1ml2mk7moj3h/folder/30415804819">Haskins_IEEE_Rate_Comparison_DB</a></p>

<p><a href="https://www.mngu0.org/">mgnu0</a></p>

<p><a href="https://data.cstr.ed.ac.uk/mocha/">mocha</a></p>

<p><a href="https://sail.usc.edu/iemocap/">IEMOCAP</a></p>

<p><a href="https://github.com/lucidrains/lion-pytorch">lucidrains/lion-pytorch</a> — Lion, new optimizer discovered by Google Brain using genetic algorithms that is purportedly better than Adam(w), in Pytorch</p>

<p><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">lucidrains/denoising-diffusion-pytorch</a> — Implementation of Denoising Diffusion Probabilistic Model in Pytorch</p>

<p><a href="https://github.com/lucidrains/audiolm-pytorch">lucidrains/audiolm-pytorch</a> — Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch</p>

<p><a href="https://github.com/lucidrains/robotic-transformer-pytorch">lucidrains/robotic-transformer-pytorch</a> — Implementation of RT1 (Robotic Transformer) in Pytorch</p>

<p><a href="https://github.com/lucidrains/recurrent-interface-network-pytorch">lucidrains/recurrent-interface-network-pytorch</a> — Implementation of Recurrent Interface Network (RIN), for highly efficient generation of images and video without cascading networks, in Pytorch</p>

<p><a href="https://github.com/lucidrains/memory-efficient-attention-pytorch">lucidrains/memory-efficient-attention-pytorch</a> — Implementation of a memory efficient multi-head attention as proposed in the paper, “Self-attention Does Not Need O(n²) Memory”</p>

<p><a href="https://github.com/lucidrains/PaLM-rlhf-pytorch">lucidrains/PaLM-rlhf-pytorch</a> — Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM</p>

<p><a href="https://github.com/hazyResearch/flash-attention">hazyResearch/flash-attention</a> — Fast and memory-efficient exact attention</p>

<p><a href="https://github.com/lucidrains/make-a-video-pytorch">lucidrains/make-a-video-pytorch</a> — Implementation of Make-A-Video, new SOTA text to video generator from Meta AI, in Pytorch</p>

<p><a href="https://github.com/lucidrains/rvq-vae-gpt">lucidrains/rvq-vae-gpt</a> — My attempts at applying Soundstream design on learned tokenization of text and then applying hierarchical attention to text generation</p>

<p><a href="https://github.com/lucidrains/imagen-pytorch">lucidrains/imagen-pytorch</a> — Implementation of Imagen, Google’s Text-to-Image Neural Network, in Pytorch</p>

<p><a href="https://github.com/crowsonkb/v-diffusion-jax">crowsonkb/v-diffusion-jax</a> — v objective diffusion inference code for JAX.</p>

<p><a href="https://arxiv.org/abs/2201.07311">Datasheet for the Pile</a></p>

<p><a href="https://arxiv.org/abs/2303.11131">Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">fazelzarandi2023cocktail</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Maryam Fazel-Zarandi and Wei-Ning Hsu}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2303.11131}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/BlinkDL/RWKV-LM">BlinkDL/RWKV-LM</a> — RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it’s combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, “infinite” ctx_len, and free sentence embedding.</p>

<p><a href="https://huggingface.co/cohogain/whisper-large-v2-ga-IE">cohogain/whisper-large-v2-ga-IE</a>,
<a href="https://huggingface.co/cohogain/whisper-medium-ga-IE-cv11-fleurs-livaud">cohogain/whisper-medium-ga-IE-cv11-fleurs-livaud</a></p>

<p><a href="https://arxiv.org/abs/2303.17580">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace</a>, <a href="https://github.com/microsoft/JARVIS">code</a></p>

<p><a href="https://arxiv.org/abs/2304.02916">Efficient Audio Captioning Transformer with Patchout and Text Guidance</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">kouzelis2023efficient</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Efficient Audio Captioning Transformer with Patchout and Text Guidance}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Thodoris Kouzelis and Grigoris Bastas and Athanasios Katsamanis and Alexandros Potamianos}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2304.02916}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.SD}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2008.05773">Continuous Speech Separation with Conformer</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2020continuous</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Continuous Speech Separation with Conformer}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Sanyuan Chen and Yu Wu and Zhuo Chen and Jian Wu and Jinyu Li and Takuya Yoshioka and Chengyi Wang and Shujie Liu and Ming Zhou}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2020}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2008.05773}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{eess.AS}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/handekabil22_interspeech.html">From Undercomplete to Sparse Overcomplete Autoencoders to Improve LF-MMI based Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">handekabil22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Selen {Hande Kabil} and Herve Bourlard}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{From Undercomplete to Sparse Overcomplete Autoencoders to Improve LF-MMI based Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1061--1065}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-11390}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/sustek22_interspeech.html">Dealing with Unknowns in Continual Learning for End-to-end Automatic Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sustek22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Martin Sustek and Samik Sadhu and Hynek Hermansky}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Dealing with Unknowns in Continual Learning for End-to-end Automatic Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1046--1050}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-11139}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/vandermerwe22_interspeech.html">A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vandermerwe22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Werner {van der Merwe} and Herman Kamper and Johan {Adam du Preez}}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1426--1430}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-11369}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/xie22b_interspeech.html">DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xie22b_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Jiamin Xie and John H.L. Hansen}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1392--1396}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-11172}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/szalay22_interspeech.html">Knowledge of accent differences can be used to predict speech recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">szalay22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Tuende Szalay and Mostafa Shahin and Beena Ahmed and Kirrie Ballard}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Knowledge of accent differences can be used to predict speech recognition}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1372--1376}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-10162}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/rumberg22b_interspeech.html">Improving Phonetic Transcriptions of Children’s Speech by Pronunciation Modelling with Constrained CTC-Decoding</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rumberg22b_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Lars Rumberg and Christopher Gebauer and Hanna Ehlert and Ulrike Lüdtke and Jörn Ostermann}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Improving Phonetic Transcriptions of Children’s Speech by Pronunciation Modelling with Constrained CTC-Decoding}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1357--1361}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-332}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/kim22k_interspeech.html">Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kim22k_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Eesung Kim and Jae-Jin Jeon and Hyeji Seo and Hoon Kim}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1411--1415}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-10245}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/deseyssel22_interspeech.html">Probing phoneme, language and speaker information in unsupervised speech representations</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">deseyssel22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Maureen {de Seyssel} and Marvin Lavechin and Yossi Adi and Emmanuel Dupoux and Guillaume Wisniewski}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Probing phoneme, language and speaker information in unsupervised speech representations}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1402--1406}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-373}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/shi22b_interspeech.html">VQ-T: RNN Transducers using Vector-Quantized Prediction Network States</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shi22b_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Jiatong Shi and George Saon and David Haws and Shinji Watanabe and Brian Kingsbury}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{VQ-T: RNN Transducers using Vector-Quantized Prediction Network States}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1656--1660}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-414}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2210.15533">Source-Filter HiFi-GAN: Fast and Pitch Controllable High-Fidelity Neural Vocoder</a>,
<a href="https://github.com/chomeyama/SiFiGAN">chomeyama/SiFiGAN</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">yoneyama2023sourcefilter</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Source-Filter HiFi-GAN: Fast and Pitch Controllable High-Fidelity Neural Vocoder}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Reo Yoneyama and Yi-Chiao Wu and Tomoki Toda}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2210.15533}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.SD}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2303.10539">Textless Speech-to-Music Retrieval Using Emotion Similarity</a></p>

<p><a href="https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html">UL2 20B: An Open Source Unified Language Learner</a>, <a href="https://github.com/google-research/google-research/tree/master/ul2">checkpoints</a></p>

<p><a href="https://arxiv.org/abs/2006.03654">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a>, <a href="https://github.com/microsoft/DeBERTa">code</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">he2021deberta</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{DeBERTa: Decoding-enhanced BERT with Disentangled Attention}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2006.03654}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2106.04399">Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bengio2021flow</span><span class="p">,</span>
      <span class="na">title</span><span class="p">=</span><span class="s">{Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation}</span><span class="p">,</span> 
      <span class="na">author</span><span class="p">=</span><span class="s">{Emmanuel Bengio and Moksh Jain and Maksym Korablyov and Doina Precup and Yoshua Bengio}</span><span class="p">,</span>
      <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
      <span class="na">eprint</span><span class="p">=</span><span class="s">{2106.04399}</span><span class="p">,</span>
      <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
      <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.LG}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/thammegowda/016-many-eng-v2/">thammegowda/016-many-eng-v2</a> — Many-English v2</p>

<p><a href="https://distill.pub/2017/ctc/">Sequence Modeling
With CTC</a>,
<a href="https://gist.github.com/awni/56369a90d03953e370f3964c826ed4b0">Example CTC Decoder in Python</a></p>

<p><a href="https://www.together.xyz/blog/openchatkit">Announcing OpenChatKit</a>,
<a href="https://github.com/togethercomputer/OpenChatKit">togethercomputer/OpenChatKit</a>,
<a href="https://huggingface.co/spaces/togethercomputer/OpenChatKit">spaces</a></p>

<hr>

<p><a href="https://vifax.maynoothuniversity.ie/">VIFAX</a></p>

<p><a href="https://www.youtube.com/@presidentofireland">@presidentofireland - Youtube</a></p>

<p><a href="https://data.oireachtas.ie/ie/oireachtas/caighdeanOifigiul/2017/2017-08-03_an-caighdean-oifigiuil-2017_en.pdf">An Caighdeán Oifigiúil</a></p>

<p><a href="http://www.nualeargais.ie/gnag/gram.htm">Gramadach na Gaeilge</a></p>

<p><a href="https://www.bible.com/bible/883/JAS.3.BEDELL">Bedell, James 3</a></p>

<p><a href="https://irishpalatals.sites.ucsc.edu/">An Ultrasound Investigation of Irish Palatalization</a></p>

<p><a href="https://github.com/microsoft/computervision-recipes">microsoft/computervision-recipes</a></p>

<p><a href="https://github.com/asuni/wavelet_prosody_toolkit">asuni/wavelet_prosody_toolkit</a></p>

<hr>

<p><a href="https://fr.wiktionary.org/wiki/lohku">lohku</a>,
<a href="https://fr.wiktionary.org/w/index.php?title=Mod%C3%A8le:se-d%C3%A9cl-pari&amp;action=edit">Modèle:se-décl-pari</a>,
<a href="https://fr.wiktionary.org/wiki/Mod%C3%A8le:se-d%C3%A9cl-contract">Modèle:se-décl-contract</a>,
<a href="https://se.wikipedia.org/wiki/Buddhisma">Buddhisma</a>,
<a href="https://se.wikipedia.org/wiki/1700-lohku_%5C(jahke%C4%8Duohti%5C)">1700-lohku</a>,
<a href="https://fr.wiktionary.org/wiki/okr">okr</a>,
<a href="https://fr.wiktionary.org/wiki/Mod%C3%A8le:se-d%C3%A9cl-impari-sans-alt">Modèle:se-décl-impari-sans-alt</a>,
<a href="https://fr.wiktionary.org/wiki/ceahkki">ceahkki</a>,
<a href="https://fr.wiktionary.org/wiki/lihtter">lihtter</a>,
<a href="https://fr.wiktionary.org/wiki/h%C3%A1vvi">hávvi</a>,
<a href="https://www.minskole.no/DynamicContent/%5CDocuments%5C272-1ada5e26-dbff-423e-8065-f3f95ff0d735.pdf">Fága 6a ja 6á</a>,
<a href="https://fr.wiktionary.org/wiki/Conjugaison:same_du_Nord/h%C3%A1ddjet">Conjugaison:same_du_Nord/háddjet</a>,</p>


  </div><a class="u-url" href="/notes/links/2023/04/13/misc-links.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://jimregan.github.io/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
