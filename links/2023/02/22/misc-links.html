<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Interesting links, 22/02/2023 | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Interesting links, 22/02/2023" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Misc. interesting things." />
<meta property="og:description" content="Misc. interesting things." />
<link rel="canonical" href="https://jimregan.github.io/notes/links/2023/02/22/misc-links.html" />
<meta property="og:url" content="https://jimregan.github.io/notes/links/2023/02/22/misc-links.html" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-22T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jimregan.github.io/notes/links/2023/02/22/misc-links.html","@type":"BlogPosting","headline":"Interesting links, 22/02/2023","dateModified":"2023-02-22T00:00:00-06:00","datePublished":"2023-02-22T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jimregan.github.io/notes/links/2023/02/22/misc-links.html"},"description":"Misc. interesting things.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jimregan.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/notes/">notes</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/notes/about/">About Me</a>
  <a class="nav-item" href="/notes/search/">Search</a>
  <a class="nav-item" href="/notes/categories/">Tags</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Interesting links, 22/02/2023</h1><p class="page-description">Misc. interesting things.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-02-22T00:00:00-06:00" itemprop="datePublished">
        Feb 22, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#links">links</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p><a href="https://towardsdatascience.com/20-open-source-single-speaker-speech-datasets-2768561f44aa">20 Open-Source Single Speaker Speech Datasets</a></p>

<p><a href="http://www.hungarianreference.com/Adjectives/absolute-adjectives.aspx">Absolute adjectives</a></p>

<p><a href="https://arxiv.org/abs/2212.14518">ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2022resgrad</span>
  <span class="err">doi</span> <span class="err">=</span> <span class="err">{10.48550/ARXIV.2212.14518</span><span class="p">}</span><span class="c">,</span>
  <span class="c">author = {Chen, Zehua and Wu, Yihan and Leng, Yichong and Chen, Jiawei and Liu, Haohe and Tan, Xu and Cui, Yang and Wang, Ke and He, Lei and Zhao, Sheng and Bian, Jiang and Mandic, Danilo},</span>
  <span class="c">title = {ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech},</span>
  <span class="c">year = {2022},</span>
<span class="c">}</span>
</code></pre></div></div>

<p><a href="https://colab.research.google.com/drive/1u_16ZzHjKYFn1HNVuA4Qf_i2MMFB9olY?usp=sharing">Coqui TTS on CPU Real-Time Spanish Speech Synthesis</a></p>

<p><a href="https://github.com/google-research-datasets/cvss">CVSS: A Massively Multilingual Speech-to-Speech Translation Corpus</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jia2022cvss</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{ {CVSS} Corpus and Massively Multilingual Speech-to-Speech Translation}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Jia, Ye and Tadmor Ramanovich, Michelle and Wang, Quan and Zen, Heiga}</span><span class="p">,</span>
    <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of Language Resources and Evaluation Conference (LREC)}</span><span class="p">,</span>
    <span class="na">pages</span><span class="p">=</span><span class="s">{6691--6703}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://research.google/pubs/pub46930/">FonBund: A Library for Combining Cross-lingual Phonological Segment Data</a></p>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">46930</span><span class="p">,</span>
<span class="na">title</span>	<span class="p">=</span> <span class="s">{FonBund: A Library for Combining Cross-lingual Phonological Segment Data}</span><span class="p">,</span>
<span class="na">author</span>	<span class="p">=</span> <span class="s">{Alexander Gutkin and Martin Jansche and Tatiana Merkulova}</span><span class="p">,</span>
<span class="na">year</span>	<span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="na">URL</span>	<span class="p">=</span> <span class="s">{http://www.lrec-conf.org/proceedings/lrec2018/pdf/8889.pdf}</span><span class="p">,</span>
<span class="na">booktitle</span>	<span class="p">=</span> <span class="s">{Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)}</span><span class="p">,</span>
<span class="na">pages</span>	<span class="p">=</span> <span class="s">{2236--2240}</span><span class="p">,</span>
<span class="na">address</span>	<span class="p">=</span> <span class="s">{7-12 May 2018, Miyazaki, Japan}</span>
<span class="p">}</span>

</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2201.10881">The Norwegian Parliamentary Speech Corpus</a></p>

<p><a href="https://link.springer.com/article/10.1007/s10579-018-9411-5">The Talk of Norway: a richly annotated corpus of the Norwegian parliament, 1998–2016</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">lapponi_talk_2018</span><span class="p">,</span>
        <span class="na">title</span> <span class="p">=</span> <span class="s">{The {Talk} of {Norway}: a richly annotated corpus of the {Norwe
gian} parliament, 1998–2016}</span><span class="p">,</span>
        <span class="na">volume</span> <span class="p">=</span> <span class="s">{52}</span><span class="p">,</span>
        <span class="na">issn</span> <span class="p">=</span> <span class="s">{1574-0218}</span><span class="p">,</span>
        <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/s10579-018-9411-5}</span><span class="p">,</span>
        <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10579-018-9411-5}</span><span class="p">,</span>
        <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
        <span class="na">journal</span> <span class="p">=</span> <span class="s">{Language Resources and Evaluation}</span><span class="p">,</span>
        <span class="na">author</span> <span class="p">=</span> <span class="s">{Lapponi, Emanuele and Søyland, Martin G. and Velldal, Erik and Oepen, Stephan}</span><span class="p">,</span>
        <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
        <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
        <span class="na">pages</span> <span class="p">=</span> <span class="s">{873--893}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.languagesandnumbers.com/how-to-count-in-northern-sami/en/sme/">Counting in Northern Sami</a> – <a href="https://fr.wiktionary.org/wiki/guokte">French wiktionary</a> seems to have good inflection information.</p>

<p><a href="https://librivox.org/egri-csillagok-by-geza-gardonyi/">Egri Csillagok</a></p>

<p><a href="https://www.wellformedness.com/courses/fstp/later-years.pdf">Weighted finite-state transducers: the later years</a></p>

<p><a href="https://aclanthology.org/Q16-1036/">Minimally Supervised Number Normalization</a></p>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">gorman-sproat-2016-minimally</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"Minimally Supervised Number Normalization"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Gorman, Kyle  and
      Sproat, Richard"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">"Transactions of the Association for Computational Linguistics"</span><span class="p">,</span>
    <span class="na">volume</span> <span class="p">=</span> <span class="s">"4"</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2016"</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">"Cambridge, MA"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"MIT Press"</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">"https://aclanthology.org/Q16-1036"</span><span class="p">,</span>
    <span class="na">doi</span> <span class="p">=</span> <span class="s">"10.1162/tacl_a_00114"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"507--519"</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://aclanthology.org/2021.findings-emnlp.85/">Structured abbreviation expansion in context</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gorman-etal-2021-structured-abbreviation</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"Structured abbreviation expansion in context"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Gorman, Kyle  and
      Kirov, Christo  and
      Roark, Brian  and
      Sproat, Richard"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Findings of the Association for Computational Linguistics: EMNLP 2021"</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2021"</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">"Punta Cana, Dominican Republic"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"Association for Computational Linguistics"</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">"https://aclanthology.org/2021.findings-emnlp.85"</span><span class="p">,</span>
    <span class="na">doi</span> <span class="p">=</span> <span class="s">"10.18653/v1/2021.findings-emnlp.85"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"995--1005"</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2005.12368">FT Speech: Danish Parliament Speech Corpus</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kirkedal_2020</span><span class="p">,</span>
	<span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/interspeech.2020-3164}</span><span class="p">,</span>
	<span class="na">year</span> <span class="p">=</span> <span class="m">2020</span><span class="p">,</span>
	<span class="na">month</span> <span class="p">=</span> <span class="s">{oct}</span><span class="p">,</span>
	<span class="na">publisher</span> <span class="p">=</span> <span class="s">{ISCA}</span><span class="p">,</span>
	<span class="na">author</span> <span class="p">=</span> <span class="s">{Andreas Kirkedal and Marija Stepanovi{\'{c}} and Barbara Plank}</span><span class="p">,</span>
	<span class="na">title</span> <span class="p">=</span> <span class="s">{ {FT} Speech: Danish Parliament Speech Corpus}</span><span class="p">,</span>
	<span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2020}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_2015/_article">Committee-Based Active Learning for Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">2011</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Committee-Based Active Learning for Speech Recognition}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Yuzo HAMANAKA and Koichi SHINODA and Takuya TSUTAOKA and Sadaoki FURUI and Tadashi EMORI and Takafumi KOSHINAKA}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEICE Transactions on Information and Systems}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{E94.D}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{2015-2023}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2011}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1587/transinf.E94.D.2015}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2301.08810">Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2023phoneme_bert</span>
  <span class="err">doi</span> <span class="err">=</span> <span class="err">{10.48550/ARXIV.2301.08810</span><span class="p">}</span><span class="c">,</span>
  <span class="c">author = {Li, Yinghao Aaron and Han, Cong and Jiang, Xilin and Mesgarani, Nima},</span>
  <span class="c">title = {Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions},</span>
  <span class="c">publisher = {arXiv},</span>
  <span class="c">year = {2023},</span>
<span class="c">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2211.03541">Multi-blank Transducers for Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xu2022multiblank</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2211.03541}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Hainan and Jia, Fei and Majumdar, Somshubra and Watanabe, Shinji and Ginsburg, Boris}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-blank Transducers for Speech Recognition}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html">Alpa: Automated Model-Parallel Deep Learning</a></p>

<p><a href="https://arxiv.org/abs/2205.03026">Hearing voices at the National Library – a speech corpus and acoustic model for the Swedish language</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">malmsten2022kblabb_w2v</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2205.03026}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Malmsten, Martin and Haffenden, Chris and Börjeson, Love}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hearing voices at the National Library -- a speech corpus and acoustic model for the Swedish language}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://direct.mit.edu/coli/article/40/4/733/1496/Applications-of-Lexicographic-Semirings-to">Applications of Lexicographic Semirings to Problems in Speech and Language Processing</a>, <a href="https://aclanthology.org/J14-4002.pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1162/COLI_a_00198</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Sproat, Richard and Yarmohammadi, Mahsa and Shafran, Izhak and Roark, Brian}</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"{Applications of Lexicographic Semirings to Problems in Speech and Language Processing}"</span><span class="p">,</span>
    <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computational Linguistics}</span><span class="p">,</span>
    <span class="na">volume</span> <span class="p">=</span> <span class="s">{40}</span><span class="p">,</span>
    <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">{733-761}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
    <span class="na">issn</span> <span class="p">=</span> <span class="s">{0891-2017}</span><span class="p">,</span>
    <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/COLI_a_00198}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1162/COLI\_a\_00198}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/bakhturina22_interspeech.html">Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bakhturina22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Evelina Bakhturina and Yang Zhang and Boris Ginsburg}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{491--495}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-11074}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://hunlang.files.wordpress.com/2009/09/tmthtgfinal.pdf">There is more to Hungarian than goulash!</a></p>

<p><a href="https://google-research.github.io/noise2music/">Noise2Music</a></p>

<p><a href="https://aclanthology.org/L18-1001/">Augmenting Librispeech with French Translations: A Multimodal Corpus for Direct Speech Translation Evaluation</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kocabiyikoglu-etal-2018-augmenting</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"Augmenting Librispeech with {F}rench Translations: A Multimodal Corpus for Direct Speech Translation Evaluation"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Kocabiyikoglu, Ali Can  and
      Besacier, Laurent  and
      Kraif, Olivier"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)"</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2018"</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">"Miyazaki, Japan"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"European Language Resources Association (ELRA)"</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">"https://aclanthology.org/L18-1001"</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/1802.04200">End-to-End Automatic Speech Translation of Audiobooks</a>, <a href="https://github.com/alex-berard/seq2seq">code</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">berard2018speechtranslation</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1802.04200}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/1802.04200}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bérard, Alexandre and Besacier, Laurent and Kocabiyikoglu, Ali Can and Pietquin, Olivier}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{End-to-End Automatic Speech Translation of Audiobooks}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span>

</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2302.05543">Adding Conditional Control to Text-to-Image Diffusion Models</a>, <a href="https://github.com/lllyasviel/ControlNet">code</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2023controlnet</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2302.05543}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Lvmin and Agrawala, Maneesh}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adding Conditional Control to Text-to-Image Diffusion Models}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.dailymotion.com/video/x5727ds">New Year Concert 2017 Wiener Philarmoniker Part 1</a></p>

<p><a href="https://github.com/LiroyvH/signal-export">LiroyvH/signal-export</a> – PDF friendly; <a href="https://github.com/carderne/signal-export">carderne/signal-export</a> – HTML</p>

<p><a href="https://github.com/erwincoumans/motion_imitation">erwincoumans/motion_imitation</a></p>

<p><a href="https://github.com/MILVLG/openvqa">MILVLG/openvqa</a></p>

<p><a href="https://github.com/patilli/vqa_benchmarking">patilli/vqa_benchmarking</a></p>

<p><a href="https://www.philschmid.de/fine-tune-flan-t5">Fine-tune FLAN-T5 for chat &amp; dialogue summarization</a></p>

<p><a href="https://arxiv.org/abs/1906.08158">BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning</a>, <a href="https://github.com/BlackHC/batchbald_redux">BlackHC/batchbald_redux</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">kirsch2019batchbald</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1906.08158}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/yue22_interspeech.html">Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yue22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zhengjun Yue and Erfan Loweimi and Heidi Christensen and Jon Barker and Zoran Cvetkovic}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{31--35}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-163}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/lee22b_interspeech.html">Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lee22b_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Munhak Lee and Joon-Hyuk Chang and Sang-Eon Lee and Ju-Seok Seong and Chanhee Park and Haeyoung Kwon}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{56--60}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-362}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/song22b_interspeech.html">Use of prosodic and lexical cues for disambiguating wh-words in Korean</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">song22b_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Jieun Song and Hae-Sung Jeon and Jieun Kiaer}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Use of prosodic and lexical cues for disambiguating wh-words in Korean}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{81--85}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-561}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/r22_interspeech.html">Generalized Keyword Spotting using ASR embeddings</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">r22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Kirandevraj R and Vinod Kumar Kurmi and Vinay Namboodiri and C V Jawahar}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Generalized Keyword Spotting using ASR embeddings}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{126--130}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-10450}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/feinberg22_interspeech.html">VoiceLab: Software for Fully Reproducible Automated Voice Analysis</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">feinberg22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{David Feinberg}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {VoiceLab: Software for Fully Reproducible Automated Voice Analysis}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{351--355}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-113}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.hungarianpod101.com/blog/2020/08/07/hungarian-word-order/">Hungarian word order</a></p>

<p><a href="http://epa.oszk.hu/04100/04182/00062/pdf/EPA04182_nyelvtudomanyi_kozlemenyek_1890_21_6_401-434.pdf">Hangsúly a magyar nyelvben</a></p>

<p><a href="https://mandadb.hu/dokumentum/494756/ozdvk004_000033_OCR.pdf">A magyar nyelv könyve</a></p>

<p><a href="https://github.com/neonbjb/tortoise-tts">neonbjb/tortoise-tts</a> – A multi-voice TTS system trained with an emphasis on quality</p>

<p><a href="https://docs.google.com/document/d/13O_eyY65i6AkNrN_LdPhpUjGhyTNKYHvDrIvHnHe1GA/edit">TorToiSe - Spending Compute for High Quality TTS</a></p>

<p><a href="https://aclanthology.org/2020.parlaclarin-1.9/">Parsing Icelandic Alþingi Transcripts: Parliamentary Speeches as a Genre</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">runarsson-sigurdsson-2020-parsing</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"Parsing {I}celandic Al{\th}ingi Transcripts: Parliamentary Speeches as a Genre"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"R{\'u}narsson, Kristj{\'a}n  and
      Sigur{\dh}sson, Einar Freyr"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Proceedings of the Second ParlaCLARIN Workshop"</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2020"</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">"Marseille, France"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"European Language Resources Association"</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">"https://aclanthology.org/2020.parlaclarin-1.9"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"44--50"</span><span class="p">,</span>
    <span class="na">language</span> <span class="p">=</span> <span class="s">"English"</span><span class="p">,</span>
    <span class="na">ISBN</span> <span class="p">=</span> <span class="s">"979-10-95546-47-4"</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2204.05409">Unified Speech-Text Pre-training for Speech Translation and Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">tang2022unified</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2204.05409}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tang, Yun and Gong, Hongyu and Dong, Ning and Wang, Changhan and Hsu, Wei-Ning and Gu, Jiatao and Baevski, Alexei and Li, Xian and Mohamed, Abdelrahman and Auli, Michael and Pino, Juan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unified Speech-Text Pre-training for Speech Translation and Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/microsoft/unilm/tree/master/s2s-ft">s2s-ft: Sequence-to-Sequence Fine-Tuning</a></p>

<p><a href="https://arxiv.org/abs/2209.15329">SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2022speechlm</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2209.15329}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Ziqiang and Chen, Sanyuan and Zhou, Long and Wu, Yu and Ren, Shuo and Liu, Shujie and Yao, Zhuoyuan and Gong, Xun and Dai, Lirong and Li, Jinyu and Wei, Furu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://alphacephei.com/nsh/2021/07/13/active-learning.html">Active learning in speech recognition</a></p>

<p><a href="https://github.com/open-mmlab/mmhuman3d">open-mmlab/mmhuman3d</a> – OpenMMLab 3D Human Parametric Model Toolbox and Benchmark</p>

<p><a href="https://sjp.pwn.pl/poradnia/haslo/zapis-liczb-wielocyfrowych;9842.html">zapis liczb wielocyfrowych</a> – spacjami</p>

<p><a href="https://github.com/r9y9/pyreaper">r9y9/pyreaper</a> – A python wrapper for REAPER</p>

<p><a href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/flan-t5-samsum-summarization.ipynb">Fine-tune FLAN-T5 for chat &amp; dialogue summarization</a></p>

<p><a href="https://www.deepspeed.ai/tutorials/inference-tutorial/">Getting Started with DeepSpeed for Inferencing Transformer based Models</a></p>

<p><a href="https://github.com/facebookresearch/metaseq">facebookresearch/metaseq</a> – Repo for external large-scale work</p>

<p><a href="https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/asr/Intro_to_Transducers.ipynb">NeMo Intro to transducers</a></p>

<p><a href="https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/FastPitch_MixerTTS_Training.ipynb">NeMo FastPitch</a></p>

<p><a href="https://arxiv.org/abs/2204.00679">Learning Audio-Video Modalities from Image Captions</a>, <a href="https://github.com/google-research-datasets/videoCC-data">github</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nagrani2022learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Audio Video Modalities from Image Captions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nagrani, Arsha and Hongsuck Seo, Paul and Seybold, Bryan, and Hauth Anja, and Santiago, Manen, and Chen, Sun and Schmid, Cordelia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://alpa.ai/tutorials/opt_serving.html">Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa</a></p>

<p><a href="https://github.com/k2-fsa/sherpa">k2-fsa/sherpa</a> – Speech-to-text server framework with next-gen Kaldi</p>

<p><a href="https://github.com/k2-fsa/kaldifst">k2-fsa/kaldifst</a> – Python wrapper for OpenFST and its extensions from Kaldi. Also support reading/writing ark/scp files</p>

<p><a href="https://github.com/NVIDIA/NeMo/blob/stable/tutorials/nlp/Joint_Intent_and_Slot_Classification.ipynb">NeMo Joint Intent and Slot Classification</a></p>

<p><a href="https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-hungarian">jonatasgrosman/wav2vec2-large-xlsr-53-hungarian</a></p>

<p><a href="https://github.com/huspacy/huspacy">huspacy/huspacy</a></p>

<p><a href="https://divvun.no/en/tale/Manual_Sami.pdf">Sami Manual</a></p>

<p><a href="https://github.com/giellalt/lang-sme">giellalt/lang-sme</a></p>

<p><a href="https://library.oapen.org/handle/20.500.12657/30995">Kvensk grammatikk</a></p>

<p><a href="https://github.com/JaidedAI/EasyOCR">JaidedAI/EasyOCR</a></p>

<p><a href="https://sametinget.kommunetv.no/">Sami parliament TV</a></p>

<p><a href="http://www.hungarianreference.com/telling-the-time.aspx">Telling the time in Hungarian</a></p>

<p><a href="https://huggingface.co/spaces/NbAiLab/whisper-sami-demo">NbAiLab/whisper-sami-demo</a>, <a href="https://huggingface.co/NbAiLab/whisper-large-sme">model</a></p>

<p><a href="https://arxiv.org/abs/2210.03255">Damage Control During Domain Adaptation for Transducer Based Automatic Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">majumdar2022damagecontrol</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2210.03255}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Majumdar, Somshubra and Acharya, Shantanu and Lavrukhin, Vitaly and Ginsburg, Boris}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Damage Control During Domain Adaptation for Transducer Based Automatic Speech Recognition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-hungarian">jonatasgrosman/wav2vec2-large-xlsr-53-hungarian</a></p>

<p><a href="https://huggingface.co/vitouphy/wav2vec2-xls-r-300m-phoneme">vitouphy/wav2vec2-xls-r-300m-phoneme</a>, <a href="https://www.kaggle.com/code/vitouphy/phoneme-recognition-with-wav2vec2">training</a></p>

<p><a href="https://github.com/lucidrains/audiolm-pytorch">lucidrains/audiolm-pytorch</a> – Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch</p>

<p><a href="https://github.com/lucidrains/PaLM-rlhf-pytorch">lucidrains/PaLM-rlhf-pytorch</a> – Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM</p>

<p><a href="https://github.com/google-research/tuning_playbook">google-research/tuning_playbook</a> – A playbook for systematically maximizing the performance of deep learning models.</p>

<p><a href="https://data.oireachtas.ie/ie/oireachtas/caighdeanOifigiul/2017/2017-08-03_an-caighdean-oifigiuil-2017_en.pdf">AN CAIGHDEÁN OIFIGIÚIL</a></p>

<p><a href="https://github.com/unicode-org/cldr/blob/main/common/rbnf/ga.xml">CLDR Irish</a></p>

<p><a href="https://github.com/stts-se/wikispeech-annotator">stts-se/wikispeech-annotator</a></p>

<p><a href="https://visithungary.com/category/castles-and-palaces-of-greater-budapest">Castles and palaces of Greater Budapest</a></p>

<p><a href="https://ramsrigoutham.medium.com/openais-whisper-7-must-know-libraries-and-add-ons-built-on-top-of-it-10825bd08f76">OpenAI’s Whisper: 7 must-know libraries and add-ons built on top of it</a></p>

<p><a href="https://huggingface.co/blog/fine-tune-whisper">Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers</a></p>

<p><a href="https://github.com/jumon/zac">jumon/zac</a> – Zero-shot Audio Classification using Whisper</p>

<p><a href="https://github.com/linto-ai/whisper-timestamped">linto-ai/whisper-timestamped</a> – Multilingual Automatic Speech Recognition with word-level timestamps and confidence</p>

<p><a href="https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html">Guiding Frozen Language Models with Learned Soft Prompts</a></p>

<p><a href="https://github.com/google-research/prompt-tuning">google-research/prompt-tuning</a> – Original Implementation of Prompt Tuning from Lester, et al, 2021</p>

<p><a href="https://ai.googleblog.com/2023/02/the-flan-collection-advancing-open.html">The Flan Collection: Advancing open source methods for instruction tuning</a></p>

<p><a href="https://arxiv.org/abs/1903.02852">Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">drugman2019active</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1903.02852}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Drugman, Thomas and Pylkkonen, Janne and Kneser, Reinhard}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/1505.07818">Domain-Adversarial Training of Neural Networks</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">ganin2015domainadversarial</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1505.07818}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Domain-Adversarial Training of Neural Networks}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2211.02423">CLSE: Corpus of Linguistically Significant Entities</a>, <a href="https://github.com/google-research-datasets/clse">corpus</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">chuklin2022clse</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2211.02423}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chuklin, Aleksandr and Zhao, Justin and Kale, Mihir}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CLSE: Corpus of Linguistically Significant Entities}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://office.clarin.eu/v/CE-2022-2118-CLARIN2022_ConferenceProceedings.pdf#page=75">Linguistic Framing of Political Terror: Distant and Close Readings of the Discourse on Terrorism in the Swedish Parliament 1993–2018</a></p>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inProceedings</span><span class="p">{</span><span class="nl">angsal2022framing</span><span class="p">,</span>
	<span class="na">title</span>        <span class="p">=</span> <span class="s">{Linguistic Framing of Political Terror: Distant and Close Readings of the Discourse on Terrorism in the Swedish Parliament 1993–2018}</span><span class="p">,</span>
	<span class="na">booktitle</span>    <span class="p">=</span> <span class="s">{CLARIN Annual Conference Proceedings, 10–12 October 2022, Prague, Czechia. Eds. Tomaž Erjavec &amp; Maria Eskevich}</span><span class="p">,</span>
	<span class="na">author</span>       <span class="p">=</span> <span class="s">{Ängsal, Magnus Pettersson and Brodén, Daniel and Fridlund, Mats and Olsson, Leif-Jöran and Öhberg, Patrik}</span><span class="p">,</span>
	<span class="na">year</span>         <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
	<span class="na">address</span>      <span class="p">=</span> <span class="s">{Prag}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://aaltodoc.aalto.fi/handle/123456789/113719">Finland Swedish Automatic Speech Recognition</a>, <a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/113719/master_Raitolahti_Otto-Ville_2022.pdf?sequence=1">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">raitolahti2022</span><span class="p">,</span>
<span class="na">title</span><span class="p">=</span><span class="s">{ {Finland Swedish Automatic Speech Recognition}}</span><span class="p">,</span>
<span class="na">author</span><span class="p">=</span><span class="s">{Raitolahti, Otto-Ville}</span><span class="p">,</span>
<span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="na">language</span><span class="p">=</span><span class="s">{English}</span><span class="p">,</span>
<span class="na">pages</span><span class="p">=</span><span class="s">{53}</span><span class="p">,</span>
<span class="na">school</span><span class="p">=</span><span class="s">{Aalto University. School of Science}</span><span class="p">,</span>
<span class="na">type</span><span class="p">=</span><span class="s">{Master's thesis}</span><span class="p">,</span>
<span class="na">url</span><span class="p">=</span><span class="s">{http://urn.fi/URN:NBN:fi:aalto-202203272601}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive_v0/Interspeech_2017/abstracts/0903.html">Building an ASR Corpus Using Althingi’s Parliamentary Speeches</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">helgadottir2017</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Inga Rún Helgadóttir and Róbert Kjaran and Anna Björk Nikulásdóttir and Jón Guðnason}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Building an ASR Corpus Using Althingi’s Parliamentary Speeches}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2017</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2017}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{2163--2167}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2017-903}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{http://dx.doi.org/10.21437/Interspeech.2017-903}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/udupa22_interspeech.html">Streaming model for Acoustic to Articulatory Inversion with transformer
networks</a>, <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/udupa22_interspeech.pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">udupa22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Sathvik Udupa and Aravind Illa and Prasanta Ghosh}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Streaming model for Acoustic to Articulatory Inversion with transformer networks}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{625--629}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-10159}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/nagamine22_interspeech.html">Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers</a>, <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/nagamine22_interspeech.pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nagamine22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Takayuki Nagamine}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{644--648}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-11020}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/ni22_interspeech.html">Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition</a>, <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/ni22_interspeech.pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ni22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Junrui Ni and Liming Wang and Heting Gao and Kaizhi Qian and Yang Zhang and Shiyu Chang and Mark Hasegawa-Johnson}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{461--465}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-816}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/zhang22i_interspeech.html">Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech</a>, <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/zhang22i_interspeech.pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang22i_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Guangyan Zhang and Kaitao Song and Xu Tan and Daxin Tan and Yuzi Yan and Yanqing Liu and Gang Wang and Wei Zhou and Tao Qin and Tan Lee and Sheng Zhao}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{456--460}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-621}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://sigul-2023.ilc.cnr.it/">SIGUL 2023</a></p>

<p><a href="https://alphacephei.com/nsh/2021/07/13/active-learning.html">Active learning in speech recognition</a></p>

<p><a href="https://arxiv.org/abs/1610.02424">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vijayakumar2016diversebeamsearch</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.1610.02424}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vijayakumar, Ashwin K and Cogswell, Michael and Selvaraju, Ramprasath R. and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://aclanthology.org/P19-1455/">Towards Multimodal Sarcasm Detection (An <em>Obviously</em> Perfect Paper)</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">castro-etal-2019-towards</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"Towards Multimodal Sarcasm Detection (An {\_}{O}bviously{\_} Perfect Paper)"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Castro, Santiago  and
      Hazarika, Devamanyu  and
      P{\'e}rez-Rosas, Ver{\'o}nica  and
      Zimmermann, Roger  and
      Mihalcea, Rada  and
      Poria, Soujanya"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"</span><span class="p">,</span>
    <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2019"</span><span class="p">,</span>
    <span class="na">address</span> <span class="p">=</span> <span class="s">"Florence, Italy"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"Association for Computational Linguistics"</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">"https://aclanthology.org/P19-1455"</span><span class="p">,</span>
    <span class="na">doi</span> <span class="p">=</span> <span class="s">"10.18653/v1/P19-1455"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"4619--4629"</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://huggingface.co/jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli">jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli</a></p>

<p><a href="https://huggingface.co/facebook/wav2vec2-xlsr-53-phon-cv-babel-ft">facebook/wav2vec2-xlsr-53-phon-cv-babel-ft</a></p>

<p><a href="https://huggingface.co/Aditya3107/wav2vec2-Irish-common-voice-Fleurs-living-audio-300m">Aditya3107/wav2vec2-Irish-common-voice-Fleurs-living-audio-300m</a></p>

<p><a href="https://huggingface.co/microsoft/trocr-large-handwritten">microsoft/trocr-large-handwritten</a></p>

<p><a href="https://github.com/microsoft/unilm/blob/master/beit2/PRETRAINING.md">BEiT v2 Pretraining</a></p>

<p><a href="https://arxiv.org/abs/2109.10282">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2021trocr</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2109.10282}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.sciencedirect.com/science/article/pii/S0095447010000628">Pauses, gaps and overlaps in conversations</a></p>

<p><a href="https://github.com/dmort27/morphotactics">dmort27/morphotactics</a> – Library for implementing morphotactic FSTs using Pynini and OpenFST</p>

<p><a href="https://www.swedishnomad.com/hungarian-words/">25 Hungarian Words that every foreigner should learn</a></p>

<p><a href="https://ieeexplore.ieee.org/document/9363163">LMC-SMCA: A New Active Learning Method in ASR</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@ARTICLE</span><span class="p">{</span><span class="nl">9363163</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Sun, Xiusong and Wang, Bo and Liu, Shaohan and Lu, Tingxiang and Shan, Xin and Yang, Qun}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEEE Access}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{LMC-SMCA: A New Active Learning Method in ASR}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{9}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{37011-37021}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/ACCESS.2021.3062157}</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.researchgate.net/publication/2935348_Active_Learning_For_Automatic_Speech_Recognition">Active Learning For Automatic Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hakkanitur2002active</span><span class="p">,</span>
<span class="na">author</span> <span class="p">=</span> <span class="s">{Hakkani-Tur, Dilek and Gorin, Allen}</span><span class="p">,</span>
<span class="na">year</span> <span class="p">=</span> <span class="s">{2002}</span><span class="p">,</span>
<span class="na">month</span> <span class="p">=</span> <span class="s">{09}</span><span class="p">,</span>
<span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
<span class="na">title</span> <span class="p">=</span> <span class="s">{Active Learning For Automatic Speech Recognition}</span><span class="p">,</span>
<span class="na">journal</span> <span class="p">=</span> <span class="s">{Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on}</span><span class="p">,</span>
<span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICASSP.2002.5745510}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://ieeexplore.ieee.org/document/4960685">Maximizing global entropy reduction for active learning in speech recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@INPROCEEDINGS</span><span class="p">{</span><span class="nl">4960685</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Varadarajan, Balakrishnan and Yu, Dong and Li Deng and Acero, Alex}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{2009 IEEE International Conference on Acoustics, Speech and Signal Processing}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Maximizing global entropy reduction for active learning in speech recognition}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2009}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{4721-4724}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/ICASSP.2009.4960685}</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://ieeexplore.ieee.org/document/6424250">Active learning for accent adaptation in Automatic Speech Recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@INPROCEEDINGS</span><span class="p">{</span><span class="nl">6424250</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Nallasamy, Udhyakumar and Metze, Florian and Schultz, Tanja}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{2012 IEEE Spoken Language Technology Workshop (SLT)}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Active learning for accent adaptation in Automatic Speech Recognition}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2012}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{360-365}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/SLT.2012.6424250}</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://ieeexplore.ieee.org/document/1453593">Active learning: theory and applications to automatic speech recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@ARTICLE</span><span class="p">{</span><span class="nl">1453593</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Riccardi, G. and Hakkani-Tur, D.}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEEE Transactions on Speech and Audio Processing}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Active learning: theory and applications to automatic speech recognition}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2005}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{13}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{504-511}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/TSA.2005.848882}</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://ieeexplore.ieee.org/document/4906813">A confusion network based confidence measure for active learning in speech recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@INPROCEEDINGS</span><span class="p">{</span><span class="nl">4906813</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Chen, Wei and Liu, Gang and Guo, Jun}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{2008 International Conference on Natural Language Processing and Knowledge Engineering}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{A confusion network based confidence measure for active learning in speech recognition}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2008}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1-6}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/NLPKE.2008.4906813}</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://ieeexplore.ieee.org/document/5745510">Active learning for automatic speech recognition</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@INPROCEEDINGS</span><span class="p">{</span><span class="nl">5745510</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Hakkani-Tür, Dilek and Riccardi, Giuseppe and Gorin, Allen}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{2002 IEEE International Conference on Acoustics, Speech, and Signal Processing}</span><span class="p">,</span> 
  <span class="na">title</span><span class="p">=</span><span class="s">{Active learning for automatic speech recognition}</span><span class="p">,</span> 
  <span class="na">year</span><span class="p">=</span><span class="s">{2002}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{IV-3904-IV-3907}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1109/ICASSP.2002.5745510}</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://github.com/karpathy/nn-zero-to-hero">karpathy/nn-zero-to-hero</a></p>

<p><a href="https://github.com/ej0cl6/deep-active-learning">ej0cl6/deep-active-learning</a></p>

<p><a href="https://jacobgil.github.io/deeplearning/activelearning">Overview of Active Learning for Deep Learning</a></p>

<p><a href="https://www.isca-speech.org/archive/interspeech_2022/borsos22_interspeech.html">SpeechPainter: Text-conditioned Speech Inpainting</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">borsos22_interspeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zalan Borsos and Matthew Sharifi and Marco Tagliasacchi}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {SpeechPainter: Text-conditioned Speech Inpainting}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2022</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. Interspeech 2022}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{431--435}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Interspeech.2022-194}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.isca-speech.org/archive/eurospeech_2003/riccardi03_eurospeech.html">Active and unsupervised learning for automatic speech recognition</a>, <a href="https://www.isca-speech.org/archive/pdfs/eurospeech_2003/riccardi03_eurospeech.pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">riccardi03_eurospeech</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Giuseppe Riccardi and Dilek Z. Hakkani-Tur}</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{ {Active and unsupervised learning for automatic speech recognition}}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="m">2003</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proc. 8th European Conference on Speech Communication and Technology (Eurospeech 2003)}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{1825--1828}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.21437/Eurospeech.2003-552}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_2015/_article">Committee-Based Active Learning for Speech Recognition</a>, <a href="https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_2015/_pdf">pdf</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hamanaka2011committee</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Committee-Based Active Learning for Speech Recognition}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Yuzo HAMANAKA and Koichi SHINODA and Takuya TSUTAOKA and Sadaoki FURUI and Tadashi EMORI and Takafumi KOSHINAKA}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEICE Transactions on Information and Systems}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{E94.D}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{10}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{2015-2023}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2011}</span><span class="p">,</span>
  <span class="na">doi</span><span class="p">=</span><span class="s">{10.1587/transinf.E94.D.2015}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://arxiv.org/abs/2302.08579">Adaptable End-to-End ASR Models using Replaceable Internal LMs and Residual Softmax</a></p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">deng2023adaptableasr</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/ARXIV.2302.08579}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Deng, Keqi and Woodland, Philip C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptable End-to-End ASR Models using Replaceable Internal LMs and Residual Softmax}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>


  </div><a class="u-url" href="/notes/links/2023/02/22/misc-links.html" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>Things I know I&#39;ll forget</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list">
  <li>
    <a href="https://jimregan.github.io/notes/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
