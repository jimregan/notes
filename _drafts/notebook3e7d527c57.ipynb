{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 471627,
     "sourceType": "datasetVersion",
     "datasetId": 212391
    }
   ],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "def get_tuples(filename):\n    words = []\n    with open(filename) as wf:\n        for line in wf.readlines():\n            line = line.strip()\n            parts = line.split(\" \")\n            if len(parts) != 3:\n                continue\n            words.append((int(parts[0]), int(parts[1]), parts[2]))\n    return words\n\ndef filter_junk(phones):\n    filtered = []\n    for phone in phones:\n        if phone[2].endswith(\"cl\"):\n            continue\n        if phone[2] in [\"epi\", \"pau\", \"h#\"]:\n            continue\n        if phone[2] == \"ax-h\":\n            filtered.append((phone[0], phone[1], \"ax\"))\n        else:\n            filtered.append(phone)\n    return filtered\n\ndef get_phonetic_words(filename):\n    if filename.endswith(\".WRD\"):\n        wordfile = filename\n        phonfile = wordfile.replace(\".WRD\", \".PHN\")\n    elif filename.endswith(\".PHN\"):\n        phonfile = filename\n        wordfile = phonfile.replace(\".PHN\", \".WRD\")\n    else:\n        return None\n    \n    words = get_tuples(wordfile)\n    phones = get_tuples(phonfile)\n    phones = filter_junk(phones)\n\n    def in_word(phone, word):\n        return (phone[0] >= word[0]) and (phone[1] <= word[1])\n    \n    merged = []\n    \n    i = j = 0\n    while i < len(words):\n        word = words[i]\n        current = {\n            \"start\": word[0],\n            \"end\": word[1],\n            \"word\": word[2],\n            \"phones\": []\n        }\n        while j < len(phones):\n            phone = phones[j]\n            if in_word(phone, word):\n                current[\"phones\"].append(phone[2])\n                j += 1\n            elif phone[0] >= word[1]:\n                # Phone starts at or after word end - move to next word\n                break\n            else:\n                # Phone starts before word but doesn't fit - skip it\n                j += 1\n        merged.append(current)\n        i += 1\n\n    return merged",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-03T21:09:51.513888Z",
     "iopub.status.idle": "2024-11-03T21:09:51.514309Z",
     "shell.execute_reply.started": "2024-11-03T21:09:51.514108Z",
     "shell.execute_reply": "2024-11-03T21:09:51.514129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Test the fixed function\nresult = get_phonetic_words(\"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data/TRAIN/DR1/FCJF0/SA1.WRD\")\nfor item in result:\n    print(f\"{item['word']}: {' '.join(item['phones'])}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-03T21:09:51.826457Z",
     "iopub.execute_input": "2024-11-03T21:09:51.826880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import glob\nfrom collections import defaultdict\n\n# Collect all pronunciations from the corpus\nBASE_PATH = \"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data\"\nword_pronunciations = defaultdict(list)\n\nfor wrd_file in glob.glob(f\"{BASE_PATH}/**/*.WRD\", recursive=True):\n    phonetic_words = get_phonetic_words(wrd_file)\n    if phonetic_words:\n        for item in phonetic_words:\n            word = item[\"word\"].lower()\n            phones = tuple(item[\"phones\"])\n            word_pronunciations[word].append(phones)\n\nprint(f\"Collected pronunciations for {len(word_pronunciations)} unique words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load TIMIT dictionary\ndef load_timit_dict(filename):\n    \"\"\"Parse TIMITDIC.TXT format: word /pronunciation/\"\"\"\n    timit_dict = {}\n    with open(filename) as f:\n        for line in f:\n            line = line.strip()\n            if not line or line.startswith(\";\"):\n                continue\n            # Format: word /p r o n u n c i a t i o n/\n            if \"/\" not in line:\n                continue\n            word_part, pron_part = line.split(\"/\", 1)\n            word = word_part.strip().lower()\n            pron = tuple(pron_part.rstrip(\"/\").strip().split())\n            timit_dict[word] = pron\n    return timit_dict\n\n# Try common locations for the dictionary\ndict_paths = [\n    f\"{BASE_PATH}/TIMITDIC.TXT\",\n    f\"{BASE_PATH}/../TIMITDIC.TXT\",\n    \"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/TIMITDIC.TXT\",\n]\n\ntimit_dict = None\nfor path in dict_paths:\n    try:\n        timit_dict = load_timit_dict(path)\n        print(f\"Loaded dictionary from {path}: {len(timit_dict)} entries\")\n        break\n    except FileNotFoundError:\n        continue\n\nif timit_dict is None:\n    print(\"TIMIT dictionary not found - listing available files...\")\n    import os\n    for item in os.listdir(\"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/\"):\n        print(f\"  {item}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def align_sequences(ref, hyp):\n    \"\"\"\n    Align two phone sequences using dynamic programming.\n    Returns list of operations: ('match', r, h), ('sub', r, h), ('del', r, None), ('ins', None, h)\n    \"\"\"\n    m, n = len(ref), len(hyp)\n    \n    # DP table\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    # Initialize\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    \n    # Fill table\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if ref[i-1] == hyp[j-1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i-1][j],      # deletion\n                    dp[i][j-1],      # insertion\n                    dp[i-1][j-1]     # substitution\n                )\n    \n    # Backtrace\n    ops = []\n    i, j = m, n\n    while i > 0 or j > 0:\n        if i > 0 and j > 0 and ref[i-1] == hyp[j-1]:\n            ops.append(('match', ref[i-1], hyp[j-1]))\n            i -= 1\n            j -= 1\n        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:\n            ops.append(('sub', ref[i-1], hyp[j-1]))\n            i -= 1\n            j -= 1\n        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:\n            ops.append(('del', ref[i-1], None))\n            i -= 1\n        else:\n            ops.append(('ins', None, hyp[j-1]))\n            j -= 1\n    \n    return list(reversed(ops))\n\n# Test alignment\nref = ('dh', 'ax', 's', 't', 'ao', 'r')\nhyp = ('dh', 'ix', 's', 'ao', 'r')\nprint(\"Reference:\", ref)\nprint(\"Hypothesis:\", hyp)\nprint(\"Alignment:\")\nfor op in align_sequences(ref, hyp):\n    print(f\"  {op}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract transformations across the corpus\nfrom collections import Counter\n\nsubstitutions = Counter()  # (ref_phone, actual_phone) -> count\ndeletions = Counter()      # ref_phone -> count\ninsertions = Counter()     # actual_phone -> count\nmatches = Counter()        # phone -> count (for computing rates)\n\nwords_analyzed = 0\nwords_not_in_dict = 0\n\nif timit_dict:\n    for word, pronunciations in word_pronunciations.items():\n        if word not in timit_dict:\n            words_not_in_dict += 1\n            continue\n        \n        dict_pron = timit_dict[word]\n        \n        for actual_pron in pronunciations:\n            words_analyzed += 1\n            alignment = align_sequences(dict_pron, actual_pron)\n            \n            for op, ref_phone, actual_phone in alignment:\n                if op == 'match':\n                    matches[ref_phone] += 1\n                elif op == 'sub':\n                    substitutions[(ref_phone, actual_phone)] += 1\n                elif op == 'del':\n                    deletions[ref_phone] += 1\n                elif op == 'ins':\n                    insertions[actual_phone] += 1\n\n    print(f\"Words analyzed: {words_analyzed}\")\n    print(f\"Words not in dictionary: {words_not_in_dict}\")\n    print(f\"\\nUnique substitution types: {len(substitutions)}\")\n    print(f\"Unique deletions: {len(deletions)}\")\n    print(f\"Unique insertions: {len(insertions)}\")\nelse:\n    print(\"Cannot analyze - dictionary not loaded\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display most common transformations\nprint(\"=== Top 20 Substitutions ===\")\nfor (ref, actual), count in substitutions.most_common(20):\n    # Compute rate: how often this phone gets this substitution vs staying the same\n    total_occurrences = matches[ref] + sum(c for (r, _), c in substitutions.items() if r == ref)\n    rate = count / total_occurrences * 100 if total_occurrences > 0 else 0\n    print(f\"  {ref} -> {actual}: {count} ({rate:.1f}%)\")\n\nprint(\"\\n=== Top 20 Deletions ===\")\nfor phone, count in deletions.most_common(20):\n    total_occurrences = matches[phone] + deletions[phone] + sum(c for (r, _), c in substitutions.items() if r == phone)\n    rate = count / total_occurrences * 100 if total_occurrences > 0 else 0\n    print(f\"  {phone} deleted: {count} ({rate:.1f}%)\")\n\nprint(\"\\n=== Top 20 Insertions ===\")\nfor phone, count in insertions.most_common(20):\n    print(f\"  {phone} inserted: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Build transformation rules with probabilities\n# Format suitable for applying to CMUdict\n\ndef compute_transformation_rules(matches, substitutions, deletions, min_count=5, min_rate=1.0):\n    \"\"\"\n    Compute transformation rules from the collected statistics.\n    Returns dict: phone -> list of (target, probability) where target can be a phone or None (deletion)\n    \"\"\"\n    rules = {}\n    \n    # Get all phones that appear in the reference\n    all_ref_phones = set(matches.keys())\n    all_ref_phones.update(r for r, _ in substitutions.keys())\n    all_ref_phones.update(deletions.keys())\n    \n    for phone in all_ref_phones:\n        # Total occurrences of this phone in reference\n        total = matches[phone]\n        total += deletions.get(phone, 0)\n        total += sum(c for (r, _), c in substitutions.items() if r == phone)\n        \n        if total == 0:\n            continue\n        \n        transformations = []\n        \n        # Add substitutions\n        for (ref, actual), count in substitutions.items():\n            if ref == phone and count >= min_count:\n                rate = count / total * 100\n                if rate >= min_rate:\n                    transformations.append((actual, count, rate))\n        \n        # Add deletions\n        del_count = deletions.get(phone, 0)\n        if del_count >= min_count:\n            rate = del_count / total * 100\n            if rate >= min_rate:\n                transformations.append((None, del_count, rate))\n        \n        if transformations:\n            # Sort by count descending\n            transformations.sort(key=lambda x: -x[1])\n            rules[phone] = transformations\n    \n    return rules\n\nrules = compute_transformation_rules(matches, substitutions, deletions)\n\nprint(\"=== Transformation Rules (min 5 occurrences, min 1% rate) ===\")\nfor phone, transforms in sorted(rules.items()):\n    print(f\"\\n{phone}:\")\n    for target, count, rate in transforms:\n        if target is None:\n            print(f\"  -> ∅ (delete): {count} ({rate:.1f}%)\")\n        else:\n            print(f\"  -> {target}: {count} ({rate:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# TIMIT to ARPABET (CMUdict) phone mapping\n# TIMIT uses a slightly different phoneset than CMUdict\nTIMIT_TO_ARPABET = {\n    # Vowels - TIMIT often has more distinctions\n    'ax': 'AH',      # schwa\n    'ix': 'IH',      # reduced high front (often schwa-like)\n    'ux': 'UW',      # reduced high back\n    'axr': 'ER',     # schwa + r\n    'ax-h': 'AH',    # breathy schwa\n    'em': 'M',       # syllabic m (CMU doesn't have this)\n    'en': 'N',       # syllabic n (CMU doesn't have this)  \n    'eng': 'NG',     # syllabic ng\n    'el': 'L',       # syllabic l (CMU doesn't have this)\n    'nx': 'N',       # flap (alveolar nasal)\n    'dx': 'D',       # flap (often realized as D or T)\n    'q': '',         # glottal stop (not in CMU)\n    'hv': 'HH',      # voiced h\n    # Direct mappings (lowercase to uppercase)\n    'aa': 'AA', 'ae': 'AE', 'ah': 'AH', 'ao': 'AO', 'aw': 'AW',\n    'ay': 'AY', 'eh': 'EH', 'er': 'ER', 'ey': 'EY', 'ih': 'IH',\n    'iy': 'IY', 'ow': 'OW', 'oy': 'OY', 'uh': 'UH', 'uw': 'UW',\n    'b': 'B', 'ch': 'CH', 'd': 'D', 'dh': 'DH', 'f': 'F', 'g': 'G',\n    'hh': 'HH', 'jh': 'JH', 'k': 'K', 'l': 'L', 'm': 'M', 'n': 'N',\n    'ng': 'NG', 'p': 'P', 'r': 'R', 's': 'S', 'sh': 'SH', 't': 'T',\n    'th': 'TH', 'v': 'V', 'w': 'W', 'y': 'Y', 'z': 'Z', 'zh': 'ZH',\n}\n\ndef timit_to_arpabet(timit_phones):\n    \"\"\"Convert TIMIT phone sequence to ARPABET (CMUdict format)\"\"\"\n    result = []\n    for phone in timit_phones:\n        mapped = TIMIT_TO_ARPABET.get(phone, phone.upper())\n        if mapped:  # Skip empty mappings (like glottal stop)\n            result.append(mapped)\n    return tuple(result)\n\n# Convert rules to ARPABET\narpabet_rules = {}\nfor phone, transforms in rules.items():\n    src = TIMIT_TO_ARPABET.get(phone, phone.upper())\n    if not src:\n        continue\n    if src not in arpabet_rules:\n        arpabet_rules[src] = []\n    for target, count, rate in transforms:\n        if target is None:\n            arpabet_rules[src].append((None, count, rate))\n        else:\n            tgt = TIMIT_TO_ARPABET.get(target, target.upper())\n            if tgt and tgt != src:  # Don't add identity mappings\n                arpabet_rules[src].append((tgt, count, rate))\n\nprint(\"=== Rules in ARPABET format ===\")\nfor phone, transforms in sorted(arpabet_rules.items()):\n    if transforms:\n        print(f\"\\n{phone}:\")\n        for target, count, rate in transforms:\n            if target is None:\n                print(f\"  -> ∅ (delete): {count} ({rate:.1f}%)\")\n            else:\n                print(f\"  -> {target}: {count} ({rate:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export rules to JSON for later use\nimport json\n\nexport_rules = {}\nfor phone, transforms in arpabet_rules.items():\n    if transforms:\n        export_rules[phone] = [\n            {\"target\": t, \"count\": c, \"rate\": round(r, 2)} \n            for t, c, r in transforms\n        ]\n\nwith open(\"timit_transformation_rules.json\", \"w\") as f:\n    json.dump(export_rules, f, indent=2)\n    \nprint(f\"Exported {len(export_rules)} phone rules to timit_transformation_rules.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example: Generate pronunciation variants for a CMUdict entry\ndef generate_variants(pronunciation, rules, max_variants=10):\n    \"\"\"\n    Generate pronunciation variants by applying transformation rules.\n    Uses a simple approach: apply one rule at a time to generate variants.\n    \"\"\"\n    variants = set()\n    variants.add(tuple(pronunciation))\n    \n    for i, phone in enumerate(pronunciation):\n        # Strip stress markers for lookup\n        phone_base = ''.join(c for c in phone if not c.isdigit())\n        \n        if phone_base in rules:\n            for rule in rules[phone_base]:\n                target = rule[\"target\"]\n                # Create variant\n                new_pron = list(pronunciation)\n                if target is None:\n                    # Deletion\n                    new_pron = new_pron[:i] + new_pron[i+1:]\n                else:\n                    # Preserve stress marker if present\n                    stress = ''.join(c for c in phone if c.isdigit())\n                    new_pron[i] = target + stress\n                variants.add(tuple(new_pron))\n                \n                if len(variants) >= max_variants:\n                    break\n        \n        if len(variants) >= max_variants:\n            break\n    \n    return list(variants)\n\n# Example with a word\nexample_pron = ['W', 'AO1', 'T', 'ER0']  # \"water\" in CMUdict format\nprint(f\"Base pronunciation: {' '.join(example_pron)}\")\nprint(\"Variants:\")\nfor var in generate_variants(example_pron, export_rules):\n    print(f\"  {' '.join(var)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}