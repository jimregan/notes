{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (0.1.96)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Collecting pyarrow>=5.0.0\n",
      "  Downloading pyarrow-7.0.0-cp38-cp38-macosx_10_13_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (1.21.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-macosx_10_9_x86_64.whl (34 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pandas in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (1.4.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (2.27.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 18.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/joregan/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, responses, pyarrow, multiprocess, datasets\n",
      "Successfully installed datasets-2.0.0 dill-0.3.4 multiprocess-0.70.12.2 pyarrow-7.0.0 responses-0.18.0 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj_ngraphs = \"ch sch sh si sj sk skj ssi ssj stj ti\".split(\" \")\n",
    "j_digraphs = \"dj gj hj lj\".split(\" \")\n",
    "tj_digraphs = \"kj tj\".split(\" \")\n",
    "retroflex_digraphs = \"rd rl rn rs rt\".split(\" \")\n",
    "assimilations = \"fv dt\".split(\" \")\n",
    "diphthongs = \"au eu\".split(\" \")\n",
    "other = \"ng gn ck\".split(\" \")\n",
    "doublings = [f\"{a}{a}\" for a in \"bcdfglmnpqrstvz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngraphs = sj_ngraphs + j_digraphs + tj_digraphs + retroflex_digraphs + assimilations + diphthongs + doublings + other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset common_voice (/Users/joregan/.cache/huggingface/datasets/common_voice/sv-SE/6.1.0/d3d5467c15716a2699f2ea3710fdc8bed7c20ae8ed66c248185735a0695dcc3b)\n",
      "100%|██████████| 6/6 [00:00<00:00, 127.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"common_voice\", \"sv-SE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = set()\n",
    "_ALPHA = \"abcdefghijklmnopqrstuvwxyzåäöéABCDEFGHIJKLMNOPQRSTUVWXYZÅÄÖ\"\n",
    "skipped = set()\n",
    "for sentence in dataset[\"train\"][\"sentence\"]:\n",
    "    chars = []\n",
    "    i = 0\n",
    "    end = len(sentence)\n",
    "    while i < end:\n",
    "        if sentence[i] in \".,!?\\\"\":\n",
    "            skipped.add(sentence[i])\n",
    "        elif sentence[i] == \":\" or sentence[i] == \"-\":\n",
    "            if (i + 1) < end and sentence[i+1] in _ALPHA:\n",
    "                chars.append(sentence[i])\n",
    "            else:\n",
    "                skipped.add(sentence[i])\n",
    "        else:\n",
    "            chars.append(sentence[i])\n",
    "        i += 1\n",
    "    jnd = \"\".join(chars)\n",
    "    sentences.add(jnd.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cv-text.txt\", \"w\") as f:\n",
    "    for sent in sentences:\n",
    "        f.write(sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: cv-text.txt\n",
      "  input_format: \n",
      "  model_prefix: swedish1k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: cv-text.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2329 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=73874\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9729% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999729\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2329 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 7180 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2329\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 3157\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 3157 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2974 obj=9.61491 num_tokens=6167 num_tokens/piece=2.07364\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=2615 obj=8.4955 num_tokens=6250 num_tokens/piece=2.39006\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1959 obj=8.61981 num_tokens=6861 num_tokens/piece=3.5023\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1954 obj=8.52871 num_tokens=6887 num_tokens/piece=3.52456\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1465 obj=8.9205 num_tokens=7851 num_tokens/piece=5.35904\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1465 obj=8.82118 num_tokens=7863 num_tokens/piece=5.36724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1100 obj=9.3069 num_tokens=8927 num_tokens/piece=8.11545\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1100 obj=9.21039 num_tokens=8931 num_tokens/piece=8.11909\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: swedish1k.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: swedish1k.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=\"cv-text.txt\", model_prefix=\"swedish1k\", vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: cv-text.txt\n",
      "  input_format: \n",
      "  model_prefix: swedish1k_aug\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: ch\n",
      "  user_defined_symbols: sch\n",
      "  user_defined_symbols: sh\n",
      "  user_defined_symbols: si\n",
      "  user_defined_symbols: sj\n",
      "  user_defined_symbols: sk\n",
      "  user_defined_symbols: skj\n",
      "  user_defined_symbols: ssi\n",
      "  user_defined_symbols: ssj\n",
      "  user_defined_symbols: stj\n",
      "  user_defined_symbols: ti\n",
      "  user_defined_symbols: dj\n",
      "  user_defined_symbols: gj\n",
      "  user_defined_symbols: hj\n",
      "  user_defined_symbols: lj\n",
      "  user_defined_symbols: kj\n",
      "  user_defined_symbols: tj\n",
      "  user_defined_symbols: rd\n",
      "  user_defined_symbols: rl\n",
      "  user_defined_symbols: rn\n",
      "  user_defined_symbols: rs\n",
      "  user_defined_symbols: rt\n",
      "  user_defined_symbols: fv\n",
      "  user_defined_symbols: dt\n",
      "  user_defined_symbols: au\n",
      "  user_defined_symbols: eu\n",
      "  user_defined_symbols: bb\n",
      "  user_defined_symbols: cc\n",
      "  user_defined_symbols: dd\n",
      "  user_defined_symbols: ff\n",
      "  user_defined_symbols: gg\n",
      "  user_defined_symbols: ll\n",
      "  user_defined_symbols: mm\n",
      "  user_defined_symbols: nn\n",
      "  user_defined_symbols: pp\n",
      "  user_defined_symbols: qq\n",
      "  user_defined_symbols: rr\n",
      "  user_defined_symbols: ss\n",
      "  user_defined_symbols: tt\n",
      "  user_defined_symbols: vv\n",
      "  user_defined_symbols: zz\n",
      "  user_defined_symbols: ng\n",
      "  user_defined_symbols: gn\n",
      "  user_defined_symbols: ck\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: cv-text.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2329 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ch\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: sch\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: sh\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: si\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: sj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: sk\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: skj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ssi\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ssj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: stj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ti\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: dj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: gj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: hj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: lj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: kj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: tj\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: rd\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: rl\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: rn\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: rs\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: rt\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: fv\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: dt\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: au\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: eu\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: bb\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: cc\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: dd\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ff\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: gg\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ll\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: mm\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: nn\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: pp\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: qq\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: rr\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ss\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: tt\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: vv\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: zz\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ng\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: gn\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: ck\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=69112\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9711% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999711\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2329 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 4063 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2329\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 3062\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 3062 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=2102 obj=16.2578 num_tokens=8417 num_tokens/piece=4.00428\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1831 obj=15.8322 num_tokens=8566 num_tokens/piece=4.67832\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1372 obj=15.7379 num_tokens=9056 num_tokens/piece=6.60058\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1369 obj=15.7253 num_tokens=9127 num_tokens/piece=6.66691\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=1099 obj=15.8864 num_tokens=9623 num_tokens/piece=8.75614\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=1098 obj=15.8711 num_tokens=9699 num_tokens/piece=8.83333\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: swedish1k_aug.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: swedish1k_aug.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input=\"cv-text.txt\", model_prefix=\"swedish1k_aug\", vocab_size=1000, user_defined_symbols=ngraphs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a4cadf7f6c4a04eb0c3890146b6c9b435f49945caf51e54d888e6c2304c7653"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('pyannote': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
