import jiwer
from nemo_text_processing.text_normalization.normalize import Normalizer
normalizer = Normalizer(input_case='cased', lang='hu')
normalizer.normalize("23,00003")
normalizer.normalize("mar. 29-ben")
normalizer.normalize("mar. 29-cen")
normalizer.normalize("mar. 29-én")
normalizer.normalize("marc. 29-én")
normalizer.normalize("marciusz 29-én")
from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch
model = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", device_map="auto", load_in_8bit=True)                                                                 
from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch
model = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", device_map="auto", load_in_8bit=True)                                                                 
from transformers import T5ForConditionalGeneration, AutoTokenizer
import torch
model = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", device_map="auto", load_in_8bit=True)                                                                 
tokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")
input_string = "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?"                                               
inputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")
outputs = model.generate(inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
input_string = "Translate from English to Dutch: Hello, how are you?"
inputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")
outputs = model.generate(inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
model = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", device_map="auto")
input_string = "Translate from English to Dutch: Hello, how are you?"
inputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")
outputs = model.generate(inputs, max_length=200)
print(outputs)
print(tokenizer.decode(outputs[0]))
input_string = "Translate from English to Dutch: Hello, how are you?"
input_string = "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?"                                               
inputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")
outputs = model.generate(inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
input_string = "Translate from English to Dutch: what do you think about booking the hotel in Budapest?"
inputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")
outputs = model.generate(inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
from transformers import MODEL_MAPPING
import transformer
 from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
 from datasets import load_dataset
from datasets import load_dataset
import torch
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-lv-60-espeak-cv-ft")
model = "jimregan/wav2vec2-swedish-phonetic-waxholm"
from huggingface_hub import hf_hub_download
import torch
checkpoint_path = hf_hub_download("openflamingo/OpenFlamingo-3B-vitl-mpt1b", "checkpoint.pt")
model.load_state_dict(torch.load(checkpoint_path), strict=False)
from open_flamingo import create_model_and_transforms
model, image_processor, tokenizer = create_model_and_transforms(
	clip_vision_encoder_path="ViT-L-14",
	clip_vision_encoder_pretrained="openai",
	lang_encoder_path="anas-awadalla/mpt-1b-redpajama-200b",
	tokenizer_path="anas-awadalla/mpt-1b-redpajama-200b",
	cross_attn_every_n_layers=1
)
model.load_state_dict(torch.load(checkpoint_path), strict=False)
from PIL import Image
import requests
import torch
demo_image_one = Image.open(
    requests.get(
        "http://images.cocodataset.org/val2017/000000039769.jpg", stream=True
    ).raw
)
demo_image_two = Image.open(
    requests.get(
        "http://images.cocodataset.org/test-stuff2017/000000028137.jpg",
        stream=True
    ).raw
)
query_image = Image.open(
    requests.get(
        "http://images.cocodataset.org/test-stuff2017/000000028352.jpg", 
        stream=True
    ).raw
)
vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)
tokenizer.padding_side = "left" # For generation padding tokens should be on the left
lang_x = tokenizer(
    ["<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of"],
    return_tensors="pt",
)
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x["input_ids"],
    attention_mask=lang_x["attention_mask"],
    max_new_tokens=20,
    num_beams=3,
)
generated_text
print("Generated text: ", tokenizer.decode(generated_text[0]))
checkpoint_path = hf_hub_download("openflamingo/OpenFlamingo-9B-vitl-mpt7b", "checkpoint.pt")
model.load_state_dict(torch.load(checkpoint_path), strict=False)
model, image_processor, tokenizer = create_model_and_transforms(
    clip_vision_encoder_path="ViT-L-14",
    clip_vision_encoder_pretrained="openai",
    lang_encoder_path="anas-awadalla/mpt-7b",
    tokenizer_path="anas-awadalla/mpt-7b",
    cross_attn_every_n_layers=4
)
model.load_state_dict(torch.load(checkpoint_path), strict=False)
vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)
tokenizer.padding_side = "left" # For generation padding tokens should be on the left
lang_x = tokenizer(
    ["<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of"],
    return_tensors="pt",
)
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x["input_ids"],
    attention_mask=lang_x["attention_mask"],
    max_new_tokens=20,
    num_beams=3,
)
print("Generated text: ", tokenizer.decode(generated_text[0]))
import wespeaker
mdl = "voxceleb_CAM++_LM"
model = wespeaker.load_model(mdl)
import torchaudio
from speechbrain.pretrained import EncoderClassifier
classifier = EncoderClassifier.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb")
signal, fs =torchaudio.load('output.wav')
signal, fs =torchaudio.load('/home/joregan/output.wav')
signal, fs =torchaudio.load('file:/home/joregan/output.wav')
signal, fs =torchaudio.load(Path('/home/joregan/output.wav'))
from pathlib import Path
signal, fs =torchaudio.load(Path('/home/joregan/output.wav'))
signal, fs =torchaudio.load(Path('output.wav'))
wp = Path('output.wav')
wp
wp = Path('/home/joregan/output.wav')
wp.exists()
signal, fs =torchaudio.load(wp, "WAV")
with open(wp) as wpf:
	signal, fs =torchaudio.load(wp)
with open(wp) as wpf:
	signal, fs =torchaudio.load(wpf)
with open(wp) as wpf:
	signal, fs =torchaudio.load(wpf, "wav")
	signal, fs =torchaudio.load(wpf, format="wav")
with open(wp) as wpf:
	signal, fs =torchaudio.load(wpf, format="wav")
with open(wp) as wpf:
	signal, fs =torchaudio.load(wpf, format="PCM_S")
signal, fs =torchaudio.load('“Exploring the Connection Between CTEs and ALS…”.mp3', format="mp3")
signal, fs =torchaudio.load('/home/joregan/“Exploring the Connection Between CTEs and ALS…”.mp3', format="mp3")
signal, fs =torchaudio.load(wpf, format="PCM_S")
signal, fs =torchaudio.load(wpf, format="PCM")
signal, fs =torchaudio.load(wpf, format="PCM_S16")
signal, fs = torchaudio.load(wpf, format="PCM_S16")
torchaudio.utils.sox_utils.list_read_formats()
from speechbrain.pretrained import SpeakerRecognition
p = Path('speaker_01')
from pathlib import Path
Path('speaker_01')
a = Path('speaker_01')
a.absolute()
[str(a.absolute()) for a in Path('/home/joregan/speaker_01').glob('*.mp3')]
file = [str(a.absolute()) for a in Path('/home/joregan/speaker_01').glob('*.mp3')]
files = [str(a.absolute()) for a in Path('/home/joregan/speaker_01').glob('*.mp3')]
verification = SpeakerRecognition.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb", savedir="pretrained_models/spkrec-ecapa-voxceleb")
score, prediction = verification.verify_files(files)
score, prediction = verification.verify_files(files[0], files[1:])
score, prediction = verification.verify_files(files[0], files[1])
score
prediction
#for i in range(1, len(files):
preds = []
for i in range(1, len(files):
for i in range(1, len(files)):
	preds.append(verification.verify_files(files[0], files[i]))
preds
MODEL_ENDPOINT = "jimregan/w2v-bert-2.0-north-sami"
DATASET_ENDPOINT = "dt2112-vt24-sami-project/north-sami-data"
PUSH_TO_HUB = True # Push changes to HuggingFace
from datasets import load_dataset
training_set = load_dataset(DATASET_ENDPOINT, split="train")
test_set = load_dataset(DATASET_ENDPOINT, split="test")
# Dataset and model end points in HuggingFace
MODEL_ENDPOINT = "jimregan/w2v-bert-2.0-north-sami"
DATASET_ENDPOINT = "dt2112-vt24-sami-project/north-sami-data"
# Settings
PUSH_TO_HUB = True # Push changes to HuggingFace
from datasets import load_dataset
training_set = load_dataset(DATASET_ENDPOINT, split="train")
test_set = load_dataset(DATASET_ENDPOINT, split="test")
import re
def clean_label(item):
    chars_to_remove_regex = '[' + re.escape(',?.!-;:"“%‘”\'’–…&()»«´') + ']'
    s = item["sentence"]
    s = s.replace('\r', ' ').replace('\n', ' ') # Remove line breaks
    s = s.strip() # Trim leading and trailing whitespace
    s = re.sub(r'\s+', ' ', s) # Replace multiple spaces with a single space
    s = re.sub(chars_to_remove_regex, '', s).lower() # Remove special characters
    item["sentence"] = s
    return item
training_set = training_set.map(clean_label)
test_set = test_set.map(clean_label)
import numpy as np
def ray_hp_space(trial):
    return {
        "learning_rate": tune.loguniform(1e-6, 1e-4),
        "per_device_train_batch_size": tune.choice([16, 32, 64, 128]),
    }
def ray_hp_space(trial):
        "learning_rate": tune.loguniform(1e-6, 1e-4),
def ray_hp_space(trial):
    return {
        "learning_rate": tune.loguniform(1e-6, 1e-4),
        "per_device_train_batch_size": tune.choice([16, 32, 64, 128]),
    }
import pickle5 as pickle
from ray.tune.schedulers import PopulationBasedTraining
from ray.tune import uniform
from random import randint
# Dataset and model end points in HuggingFace
MODEL_ENDPOINT = "jimregan/w2v-bert-2.0-north-sami"
DATASET_ENDPOINT = "dt2112-vt24-sami-project/north-sami-data"
# Settings
PUSH_TO_HUB = True # Push changes to HuggingFace
from datasets import load_dataset
training_set = load_dataset(DATASET_ENDPOINT, split="train")
from pathlib import Path
fpath = Path("/home/joregan/orange/files")
jpath = Path("/home/joregan/phjson")
jpath.mkdir((
jpath.mkdir()
MODEL = "jimregan/wav2vec2-xls-r-300m-phoneme-timit"
from transformers import pipeline
MODEL = "jimregan/wav2vec2-xls-r-300m-phoneme-timit"
jpath = Path("/home/joregan/phjson")
from pathlib import Path
jpath = Path("/home/joregan/phjson")
fpath = Path("/home/joregan/orange/files")
for file in fpath.glob("*.wav"):
	output = pipe(file, chunk_length_s=10, return_timestamps="word")
	import json
	with open(jpath / f"{file.stem}.json", "w") as outf:
		json.dump(output, outf)
pipe = pipeline(model=MODEL)
import json
for file in fpath.glob("*.wav"):
	output = pipe(file, chunk_length_s=10, return_timestamps="word")
	with open(jpath / f"{file.stem}.json", "w") as outf:
		json.dump(output, outf)
fpath
fpath.glob("*.wav")
[x for x in fpath.glob("*.wav")]
fpath = Path("/home/joregan/orange/files/hsi")
for file in fpath.glob("*.wav"):
	output = pipe(file, chunk_length_s=10, return_timestamps="word")
	with open(jpath / f"{file.stem}.json", "w") as outf:
		json.dump(output, outf)
pipe = pipeline(model=MODEL)
for file in fpath.glob("*.wav"):
	output = pipe(str(file), chunk_length_s=10, return_timestamps="word")
	with open(jpath / f"{file.stem}.json", "w") as outf:
		json.dump(output, outf)
from datasets import load_dataset
import torchaudio
dataset = load_dataset("KTH/waxholm")
def compute_duration(batch):
    audio = batch["audio"]
    duration = len(audio["array"]) / audio["sampling_rate"]
    return {"duration": duration}
dataset = dataset.map(compute_duration)
print(f"Total Duration (seconds): {total_duration}")
total_duration = sum(dataset["duration"])
dataset
sum(dataset["duration"])
dataset['duration']
dataset['train']['duration']
sum(dataset['train']['duration'])
sec = sum(dataset['train']['duration'])
total_duration = sum(dataset['train']['duration'])
print(f"Total Duration (seconds): {total_duration}")
print(f"Total Duration (hours): {total_duration / 3600:.2f}")
test_duration = sum(dataset['test']['duration'])
print(f"Total Duration (hours): {test_duration / 3600:.2f}")
dataset
dataset['test'][0]
print(f"Total Duration (hours): {test_duration / 60:.2f}")
print(f"Total Duration (hours): {total_duration / 60:.2f}")
from datasets import load_dataset
ds = load_dataset("KBLab/rixvox-v2")
from datasets import load_dataset
ds = load_dataset("KBLab/rixvox-v2")
from datasets import load_dataset
ds = load_dataset("KBLab/rixvox-v2", cache_dir="/sbtal/rixvox2")
from datasets import load_dataset
ds = load_dataset("KBLab/rixvox-v2", cache_dir="/sbtal/rixvox2")
from datasets import load_dataset
dataset = load_dataset(
    'KBLab/rixvox-v2',
    cache_dir='./cache',
    local_files_only=True
)
from datasets import load_dataset
dataset = load_dataset(
    "parquet",
	data_files="./cache/datasets--KBLab--rixvox-v2/snapshots/1f5f37f5ec8740eae318eeae7bf190074454d0d1/data/",
	split="train"
	}
dataset = load_dataset(
    "parquet",
	data_files="./cache/datasets--KBLab--rixvox-v2/snapshots/1f5f37f5ec8740eae318eeae7bf190074454d0d1/data/",
	split="train"
)
dataset = load_dataset(
    "parquet",
	data_files="./cache/datasets--KBLab--rixvox-v2/snapshots/1f5f37f5ec8740eae318eeae7bf190074454d0d1/data/*.parquet",
	split="train"
)
dataset.features
import os
output_audio_dir = "audio_files"
import json
import soundfile as sf
output_jsonl_path = "metadata.jsonl"
os.makedirs(output_audio_dir, exist_ok=True)
with open(output_jsonl_path, "w", encoding="utf-8") as jsonl_file:
    for i, sample in enumerate(dataset):
        # Save audio to .wav
        audio = sample["audio"]
        audio_path = os.path.join(output_audio_dir, f"{i}.wav")
        sf.write(audio_path, audio["array"], audio["sampling_rate"])
with open(output_jsonl_path, "w", encoding="utf-8") as jsonl_file:
    for i, sample in enumerate(dataset):
        audio = sample["audio"]
        audio_path = os.path.join(output_audio_dir, f"{i}.wav")
        sf.write(audio_path, audio["array"], audio["sampling_rate"])
        sample_copy = dict(sample)
        sample_copy.pop("audio", None)
        sample_copy["audio_filepath"] = audio_path
        jsonl_file.write(json.dumps(sample_copy, ensure_ascii=False) + "\n")
def make_json_serializable(record):
    """Convert non-serializable fields like timestamps to strings."""
    def convert(value):
        if isinstance(value, (datetime, pd.Timestamp, np.datetime64)):
            return str(value)
        elif isinstance(value, list):
            return [convert(v) for v in value]
        elif isinstance(value, dict):
            return {k: convert(v) for k, v in value.items()}
        else:
            return value
        return {k: convert(v) for k, v in record.items()}
import pandas as pd
from datetime import datetime
import numpy as np
with open(output_jsonl_path, "w", encoding="utf-8") as jsonl_file:
    for i, sample in enumerate(dataset):
        audio = sample["audio"]
        audio_path = os.path.join(output_audio_dir, f"{i}.wav")
        sf.write(audio_path, audio["array"], audio["sampling_rate"])
        sample_copy = dict(sample)
        sample_copy.pop("audio", None)
        sample_copy["audio_filepath"] = audio_path
        serializable_sample = make_json_serializable(sample_copy)
        jsonl_file.write(json.dumps(serializable_sample, ensure_ascii=False) + "\n")
sample_copy
json.dumps(sample_copy)
make_json_serializable(sample_copy)
sample_copy
json.dumps(sample_copy)
def convert_for_json(obj):
    """Recursively convert objects to JSON serializable formats."""
    if isinstance(obj, (pd.Timestamp, datetime, np.datetime64)):
        return str(obj)
    elif isinstance(obj, (np.integer)):
        return int(obj)
    elif isinstance(obj, (np.floating)):
        return float(obj)
    elif isinstance(obj, (np.ndarray, list)):
        return [convert_for_json(v) for v in obj]
    elif isinstance(obj, dict):
        return {k: convert_for_json(v) for k, v in obj.items()}
    else:
        return obj
convert_for_json(sample_copy)
json.dumps(convert_for_json(sample_copy))
with open(output_jsonl_path, "w", encoding="utf-8") as jsonl_file:
    for i, sample in enumerate(dataset):
        audio = sample["audio"]
        audio_path = os.path.join(output_audio_dir, f"{i}.wav")
        sf.write(audio_path, audio["array"], audio["sampling_rate"])
        sample_copy = dict(sample)
        sample_copy.pop("audio", None)
        sample_copy["audio_filepath"] = audio_path
        serializable_sample = convert_for_json(sample_copy)
        jsonl_file.write(json.dumps(serializable_sample, ensure_ascii=False) + "\n")
import accelerate
from transformers import MimiModel
dataset
accelerator = Accelerator()
from accelerate import Accelerator
accelerator = Accelerator()
accelerator.device
accelerator.device.index
accelerator.device.type
dataset[0]["audio"]
phonmodel = "jimregan/wav2vec2-swedish-phonetic-waxholm"
dataset
from transformers import pipeline
device = accelerator.device
total_samples = len(dataset)
world_size = accelerator.num_processes
rank = accelerator.process_index
per_proc = math.ceil(total_samples / world_size)
import math
per_proc = math.ceil(total_samples / world_size)
start_index = rank * per_proc
end_index = min(total_samples, (rank + 1) * per_proc)
indices = list(range(start_index, end_index))
asr_pipeline = pipeline(
    task="automatic-speech-recognition",
    model="jimregan/wav2vec2-swedish-phonetic-waxholm",
    chunk_length_s=10,
    return_timestamps="word",
    device=device,                # ensure pipeline model loads on this GPU
    torch_dtype="auto"            # use automatic mixed precision (fp16) if supported
)
from transformers import pipeline, Wav2Vec2ForCTC, Wav2Vec2Processor
model_id = "jimregan/wav2vec2-swedish-phonetic-waxholm"
processor = Wav2Vec2Processor.from_pretrained(model_id)
model = Wav2Vec2ForCTC.from_pretrained(model_id)
asr_pipeline = pipeline(
    task="automatic-speech-recognition",
    model="jimregan/wav2vec2-swedish-phonetic-waxholm",
    chunk_length_s=10,
    return_timestamps="word",
    device=device,                # ensure pipeline model loads on this GPU
    torch_dtype="auto"            # use automatic mixed precision (fp16) if supported
)
transcriptions = []
batch_size = 16  # adjust based on GPU memory and audio length
for i in range(0, len(indices), batch_size):
    batch_indices = indices[i : i + batch_size]
    # Fetch a batch of audio samples from the dataset
    batch = dataset[batch_indices]              # this returns a dict of columns with lists
    audio_list = []  
    for audio in batch["audio"]:
        # Each `audio` could be a dict like {"array": ..., "sampling_rate": ...}
        # Extract the waveform array (or use the object directly if it's already an array)
        waveform = audio["array"] if isinstance(audio, dict) else audio
        audio_list.append(waveform)
    # Run the ASR pipeline on the batch of waveforms
    outputs = asr_pipeline(audio_list, batch_size=batch_size)
    # `outputs` is a list of results (one per input audio)
    for result in outputs:
        # Each result is a dict; the transcribed text is under the 'text' key (phonetic transcription)
        transcriptions.append(result.get("text", ""))
dataset[0]["audio"]
#dsview = dataset.
dataset
dsview = dataset.select_columns(['audio', 'start', 'end', 'audio_file'])
pipeline(dsview[0])
pipeline(dsview[0]['audio'])
dsview[0]['audio']
pipeline(dsview[0]['audio']['array'])
info(pipeline)
help(pipeline)
asr_pipeline(dsview[0]['audio'])
asr_pipeline(dsview[0]['audio']['array'])
asr_pipeline(dsview[0])
sample = dataset[0]["audio"]
result = asr_pipeline(sample, chunk_length_s=10, return_timestamps="word")
print(result["text"])
sample = dataset[0]["audio"]
result = asr_pipeline(sample, chunk_length_s=10, return_timestamps="word")
sample
dataset
dataset[0]
dataset[0]['audio']
result = asr_pipeline(dataset[0]['audio'], chunk_length_s=10, return_timestamps="word")
dataset[0]['audio']
result = asr_pipeline(dataset[0]['audio']['array'], chunk_length_s=10, return_timestamps="word")
dataset[0]['audio']
foo = {'raw': dataset[0]['audio']['array'], 'sampling_rate': dataset[0]['audio']['sampling_rate']
}
result = asr_pipeline(foo, chunk_length_s=10, return_timestamps="word")
foo = {'array': dataset[0]['audio']['array'], 'sampling_rate': dataset[0]['audio']['sampling_rate']}
result = asr_pipeline(foo, chunk_length_s=10, return_timestamps="word")
dataset[0]
pipe = pipeline(
    "automatic-speech-recognition",
    model="jimregan/wav2vec2-swedish-phonetic-waxholm",
    chunk_length_s=10,
    return_timestamps="word"
)
sample = dataset[0]["audio"]
input_obj = {"array": sample["array"], "sampling_rate": sample["sampling_rate"]}
result = pipe(input_obj)
result
from pathlib import Path
homedir = Path("/home/joregan")
homedir.is_dir()
with open(homedir / "phonrec-rv.json", "w") as outf:
import tqdm
from tqdm import tqdm
import json
pipe = pipeline(
    "automatic-speech-recognition",
    model="jimregan/wav2vec2-swedish-phonetic-waxholm",
    return_timestamps="word"
)
output_path = homedir / "phonrec-rv.json"
with open(output_path, "w", encoding="utf-8") as f_out:
	for example in tqdm(dataset, desc="Transcribing"):
		audio_input = {
			"array": example["audio"]["array"],
			"sampling_rate": example["audio"]["sampling_rate"]
		}
		try:
			result = pipe(audio_input)
		except Exception as e:
			result = {"text": "", "error": str(e)}
		output = {
			'audio_file': example.get('audio_file')
			'start': example['start']
with open(output_path, "w", encoding="utf-8") as f_out:
	for example in tqdm(dataset, desc="Transcribing"):
		audio_input = {
			"array": example["audio"]["array"],
			"sampling_rate": example["audio"]["sampling_rate"]
		}
		try:
			result = pipe(audio_input)
		except Exception as e:
			result = {"text": "", "error": str(e)}
		output = {
			'audio_file': example[‘audio_file’],
result
result.keys
result.keys()
result['chunks']
with open(output_path, "w", encoding="utf-8") as f_out:
    for example in tqdm(dataset, desc="Transcribing"):
            audio_input = {
                    "array": example["audio"]["array"],
                    "sampling_rate": example["audio"]["sampling_rate"]
            }
            try:
                    result = pipe(audio_input)
            except Exception as e:
                    result = {"text": "", "error": str(e)}
            output = {
                    'audio_file': example['audio_file'],
                    'start': example['start'],
                    'end': example['end'],
                    'speech_id': example['speech_id'],
                    'phonetic': result['chunks'],
            }
            f_out.write(json.dumps(output, ensure_ascii=False) + "\n")
