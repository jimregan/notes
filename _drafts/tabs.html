<html><head></head><body><a href="https://readcomicsonline.ru/?">Read Comics Online | Home</a><br/><a href="https://readcomicsonline.ru/comic/star-wars-the-high-republic-adventures-2021">Star Wars: The High Republic Adventures (2021-) by - Info Page</a><br/><a href="https://readcomicsonline.ru/comic/star-wars-the-high-republic-2021/12/18">Star Wars: The High Republic (2021-) Chapter 12 - Page 18</a><br/><a href="https://github.com/KnugiHK/WhatsApp-Chat-Exporter?tab=readme-ov-file">KnugiHK/WhatsApp-Chat-Exporter: A customizable Android and iOS/iPadOS WhatsApp database parser that will give you the history of your WhatsApp conversations in HTML and JSON. Android Backup Crypt12, Crypt14, Crypt15, and new schema supported.</a><br/><a href="https://github.com/EliteAndroidApps/WhatsApp-Key-DB-Extractor">EliteAndroidApps/WhatsApp-Key-DB-Extractor: Allows WhatsApp users to extract their cipher key and databases on non-rooted Android devices.</a><br/><a href="https://huggingface.co/facebook/w2v-bert-2.0">facebook/w2v-bert-2.0 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/blog/fine-tune-w2v2-bert">Fine-Tune W2V2-Bert for low-resource ASR with ü§ó Transformers</a><br/><a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py">transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py at main ¬∑ huggingface/transformers</a><br/><a href="file:///Users/joregan/Downloads/The-Art-of-Linear-Algebra.pdf">The-Art-of-Linear-Algebra.pdf</a><br/><a href="https://polyai-ldn.github.io/pheme/">PHEME: Efficient and Conversational Speech Generation. | pheme</a><br/><a href="https://github.com/lucidrains?tab=repositories">lucidrains (lucidrains) / Repositories</a><br/><a href="https://github.com/lucidrains/classifier-free-guidance-pytorch">lucidrains/classifier-free-guidance-pytorch: Implementation of Classifier Free Guidance in Pytorch, with emphasis on text conditioning, and flexibility to include multiple text embedding models</a><br/><a href="https://github.com/lucidrains/gigagan-pytorch/blob/main/gigagan_pytorch/open_clip.py">gigagan-pytorch/gigagan_pytorch/open_clip.py at main ¬∑ lucidrains/gigagan-pytorch</a><br/><a href="https://github.com/lucidrains/rotary-embedding-torch/blob/main/rotary_embedding_torch/rotary_embedding_torch.py">rotary-embedding-torch/rotary_embedding_torch/rotary_embedding_torch.py at main ¬∑ lucidrains/rotary-embedding-torch</a><br/><a href="https://github.com/lucidrains/byol-pytorch">lucidrains/byol-pytorch: Usable Implementation of "Bootstrap Your Own Latent" self-supervised learning, from Deepmind, in Pytorch</a><br/><a href="https://github.com/MahmoudAshraf97/whisper-diarization">MahmoudAshraf97/whisper-diarization: Automatic Speech Recognition with Speaker Diarization based on OpenAI Whisper</a><br/><a href="https://github.com/mu4farooqi/whisper/blob/28769fcfe50755a817ab922a7bc83483159600a9/notebooks/Multilingual_ASR.ipynb">whisper/notebooks/Multilingual_ASR.ipynb at 28769fcfe50755a817ab922a7bc83483159600a9 ¬∑ mu4farooqi/whisper</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/tools/nemo_forced_aligner.html">NeMo Forced Aligner (NFA) - NVIDIA Docs</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/results.html#asr-checkpoint-list-by-language">Checkpoints - NVIDIA Docs</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/datasets.html">Datasets - NVIDIA Docs</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/speaker_diarization/results.html">Checkpoints ‚Äî NVIDIA NeMo Framework User Guide latest documentation</a><br/><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/vad_multilingual_marblenet">VAD multilingual Marblenet | NVIDIA NGC</a><br/><a href="https://github.com/SYSTRAN/similarity">SYSTRAN/similarity: Bilingual sentence similarity classifier using Tensorflow</a><br/><a href="https://github.com/SYSTRAN/fuzzy-match">SYSTRAN/fuzzy-match: Library and command line utility to do approximate string matching of a source against a bitext index and get matched source and target.</a><br/><a href="https://huggingface.co/pyannote/segmentation-3.0">pyannote/segmentation-3.0 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/pyannote/speaker-diarization-3.1">pyannote/speaker-diarization-3.1 ¬∑ Hugging Face</a><br/><a href="https://github.com/clab/fast_align">clab/fast_align: Simple, fast unsupervised word aligner</a><br/><a href="https://github.com/bclavie/RAGatouille?tab=readme-ov-file">bclavie/RAGatouille</a><br/><a href="https://betterexplained.com/articles/quadratic-formula/">Intuition for the Quadratic Formula ‚Äì BetterExplained</a><br/><a href="https://towardsdatascience.com/3-advanced-document-retrieval-techniques-to-improve-rag-systems-0703a2375e1c">3 Advanced Document Retrieval Techniques To Improve RAG Systems | by Ahmed Besbes | Towards Data Science</a><br/><a href="https://x.com/1littlecoder/status/1747976274537013606">(2) 1LittleCoderüíª on X: "ü§ØInverted Whisper == Whisper Speech! ‚úÖ working only with properly licensed speech ‚úÖ all the code is OPEN Source ‚úÖ SAFE to use for commercial applications. I didn't even know this was possible! https://t.co/SCFGj96vsG" / X</a><br/><a href="https://github.com/collabora/WhisperSpeech">collabora/WhisperSpeech: An Open Source text-to-speech system built by inverting Whisper.</a><br/><a href="https://vickiboykis.com/what_are_embeddings/">What are embeddings?</a><br/><a href="https://github.com/veekaybee/what_are_embeddings/blob/main/notebooks/fig_4_bert.ipynb">what_are_embeddings/notebooks/fig_4_bert.ipynb at main ¬∑ veekaybee/what_are_embeddings</a><br/><a href="file:///Users/joregan/Downloads/embeddings.pdf">embeddings.pdf</a><br/><a href="https://github.com/SamsungLabs/SummaryMixing/blob/main/speechbrain/nnet/summary_mixing.py">SummaryMixing/speechbrain/nnet/summary_mixing.py at main ¬∑ SamsungLabs/SummaryMixing</a><br/><a href="https://huggingface.co/togethercomputer/m2-bert-80M-32k-retrieval">togethercomputer/m2-bert-80M-32k-retrieval ¬∑ Hugging Face</a><br/><a href="https://drive.google.com/file/d/1ADD8js8eMA88yMLMKwbNBr5OahnxFYZg/view">bast.i.test.s08e01.swedish.720p.web.h264-ollonborre.mkv - Google Drive</a><br/><a href="https://www.reddit.com/r/panelshow/comments/19b0tuy/b%C3%A4st_i_test_taskmaster_sweden_s08e02_w_eng_subs/">B√§st i Test (Taskmaster Sweden) S08E02 [w/ Eng subs] : r/panelshow</a><br/><a href="https://github.com/google-research/perceiver-ar/blob/main/README.md">perceiver-ar/README.md at main ¬∑ google-research/perceiver-ar</a><br/><a href="https://github.com/google-research/perceiver-ar/blob/main/perceiver_ar/perceiver_ar_model.py">perceiver-ar/perceiver_ar/perceiver_ar_model.py at main ¬∑ google-research/perceiver-ar</a><br/><a href="https://x.com/iScienceLuvr/status/1798177427493101899">(1) Tanishq Mathew Abraham, Ph.D. on X: "Guiding a Diffusion Model with a Bad Version of Itself abs: https://t.co/m8iLx8rvb1 New Karras et al. paper! Another banger! This paper studies classifier-free guidance and why it works. The authors demonstrate that score matching leads to outliers (over-emphasizing low https://t.co/S6jme1e46M" / X</a><br/><a href="https://www.niu.edu/citl/resources/toolkits/crisis/fear.shtml#02">Fear, Anxiety, and Guilt After A Traumatic Event | Center for Innovative Teaching and Learning | Northern Illinois University</a><br/><a href="https://keep.google.com/u/0/#home">Google Keep</a><br/><a href="https://my.clevelandclinic.org/health/diseases/24099-rejection-sensitive-dysphoria-rsd">Rejection Sensitive Dysphoria (RSD): Symptoms & Treatment</a><br/><a href="https://deploymentpsych.org/content/cpt-session-notes">CPT Session Notes | Center for Deployment Psychology</a><br/><a href="https://www.apa.org/ptsd-guideline/treatments/cognitive-processing-therapist.pdf">Cognitive processing therapy Veteran/military version: Therapist‚Äôs manual</a><br/><a href="https://deploymentpsych.org/search/node/cpt%20session%20notes">Search | Center for Deployment Psychology</a><br/><a href="https://deploymentpsych.org/content/cpt-session-notes-session-7-challenging-beliefs-worksheets-and-introduction-modules">CPT Session Notes Session 7: Challenging Beliefs Worksheets and Introduction to Modules | Center for Deployment Psychology</a><br/><a href="https://deploymentpsych.org/content/cpt-session-notes-phase-5-trauma-themes">CPT Session Notes Phase 5: Trauma Themes | Center for Deployment Psychology</a><br/><a href="https://deploymentpsych.org/content/cpt-session-notes-phase-6-session-12-final-impact-statment">CPT Session Notes Phase 6: Session 12 The Final Impact Statment | Center for Deployment Psychology</a><br/><a href="https://www.div12.org/wp-content/uploads/2015/07/CPT-Materials-Manual.pdf">Section II</a><br/><a href="https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/wav2vec_featurize.py">fairseq/examples/wav2vec/wav2vec_featurize.py at main ¬∑ facebookresearch/fairseq</a><br/><a href="https://stackoverflow.com/questions/69266293/getting-embeddings-from-wav2vec2-models-in-huggingface">python - Getting embeddings from wav2vec2 models in HuggingFace - Stack Overflow</a><br/><a href="https://datascience.stackexchange.com/questions/118124/how-to-extract-embeddings-from-an-audio-file-using-wav2vec-along-with-context">nlp - How to extract embeddings from an audio file using wav2vec along with context - Data Science Stack Exchange</a><br/><a href="https://www.google.com/search?q=wav2vec2+embeddings+from+specific+layer&rlz=1C5GCEM_enSE990SE991&oq=wav2vec2+embeddings+from+specific+layer&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCDkzNzBqMGo0qAIAsAIB&sourceid=chrome&ie=UTF-8">wav2vec2 embeddings from specific layer - Google Search</a><br/><a href="https://github.com/hubertsiuzdak/snac?tab=readme-ov-file">hubertsiuzdak/snac: Multi-Scale Neural Audio Codec (SNAC) compresses audio into discrete codes at a low bitrate</a><br/><a href="https://hubertsiuzdak.github.io/snac/">Audio samples from SNAC: Multi-Scale Neural Audio Codec | Multi-Scale Neural Audio Codec (SNAC) compresses audio into discrete codes at a low bitrate</a><br/><a href="https://huggingface.co/hubertsiuzdak/snac_24khz">hubertsiuzdak/snac_24khz ¬∑ Hugging Face</a><br/><a href="https://github.com/k2-fsa/text_search/blob/master/textsearch/csrc/levenshtein.h">text_search/textsearch/csrc/levenshtein.h at master ¬∑ k2-fsa/text_search</a><br/><a href="https://huggingface.co/docs/transformers/en/model_doc/siglip">SigLIP</a><br/><a href="https://github.com/google-research/big_vision/blob/main/big_vision/models/proj/clippo/one_tower.py">big_vision/big_vision/models/proj/clippo/one_tower.py at main ¬∑ google-research/big_vision</a><br/><a href="https://github.com/facebookresearch/faiss/blob/main/faiss/utils/distances.h#L200">faiss/faiss/utils/distances.h at main ¬∑ facebookresearch/faiss</a><br/><a href="https://gist.github.com/mdouze/a8c914eb8c5c8306194ea1da48a577d2">demo_numpy_knn.ipynb</a><br/><a href="https://x.com/ChannelInteres/status/1751546948358684920">(1) Interesting Channel on X: "It's simple sleight-of-hand! üòâüëçüëç https://t.co/hl3tIC4b06" / X</a><br/><a href="https://towardsdatascience.com/understanding-faiss-619bb6db2d1a">Understanding FAISS. ‚Ä¶.And the world of Similarity Searching | by Vedashree Patil | Towards Data Science</a><br/><a href="https://medium.com/@kvrware/embedding-similarity-search-25c6911240af">Embedding similarity search. Searching for something similar is a‚Ä¶ | by Roman Kyslyi, PhD | Medium</a><br/><a href="https://x.com/Ethan_smith_20/status/1801466041660273033">(1) Ethan (in SYDüá¶üá∫) on X: "UnCLIP showed us the power of decoding from representation spaces, and really the power of hiearchical modeling across different levels of abstraction. But really this approach (imho wrongfully) kinda fell out of style https://t.co/wiatkVMe60" / X</a><br/><a href="https://eclipse-t2i.github.io/Lambda-ECLIPSE/">Œª-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space</a><br/><a href="https://github.com/huggingface/diffusers/tree/main/src/diffusers/models/unets">diffusers/src/diffusers/models/unets at main ¬∑ huggingface/diffusers</a><br/><a href="https://github.com/search?q=repo%3Ahuggingface%2Fdiffusers+fourier&type=code">Code search results</a><br/><a href="https://github.com/huggingface/diffusers/pulls?q=is%3Apr+is%3Aclosed">Pull requests ¬∑ huggingface/diffusers</a><br/><a href="https://github.com/huggingface/diffusers/blob/7833ed957bcb4bd8c9ed0a9a8a172513bf896e51/src/diffusers/pipelines/deprecated/audio_diffusion/mel.py#L121">diffusers/src/diffusers/pipelines/deprecated/audio_diffusion/mel.py at 7833ed957bcb4bd8c9ed0a9a8a172513bf896e51 ¬∑ huggingface/diffusers</a><br/><a href="https://github.com/huggingface/diffusers/blob/7833ed957bcb4bd8c9ed0a9a8a172513bf896e51/docs/source/en/api/pipelines/audioldm.md?plain=1#L13">diffusers/docs/source/en/api/pipelines/audioldm.md at 7833ed957bcb4bd8c9ed0a9a8a172513bf896e51 ¬∑ huggingface/diffusers</a><br/><a href="https://github.com/huggingface/diffusers/blob/7833ed957bcb4bd8c9ed0a9a8a172513bf896e51/docs/source/en/quicktour.md?plain=1#L17">diffusers/docs/source/en/quicktour.md at 7833ed957bcb4bd8c9ed0a9a8a172513bf896e51 ¬∑ huggingface/diffusers</a><br/><a href="https://github.com/huggingface/diffusers/blob/7833ed957bcb4bd8c9ed0a9a8a172513bf896e51/tests/pipelines/dance_diffusion/test_dance_diffusion.py#L56">diffusers/tests/pipelines/dance_diffusion/test_dance_diffusion.py at 7833ed957bcb4bd8c9ed0a9a8a172513bf896e51 ¬∑ huggingface/diffusers</a><br/><a href="https://huggingface.co/papers/2204.06125">Paper page - Hierarchical Text-Conditional Image Generation with CLIP Latents</a><br/><a href="https://huggingface.co/docs/diffusers/en/api/pipelines/unidiffuser">UniDiffuser</a><br/><a href="https://github.com/lucidrains/imagen-pytorch">lucidrains/imagen-pytorch: Implementation of Imagen, Google's Text-to-Image Neural Network, in Pytorch</a><br/><a href="https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py">v-diffusion-jax/diffusion/utils.py at master ¬∑ crowsonkb/v-diffusion-jax</a><br/><a href="https://arxiv.org/pdf/2406.08929">Step-by-Step Diffusion: An Elementary Tutorial</a><br/><a href="https://x.com/HamelHusain/status/1802106197782438019">(1) Hamel Husain on X: "This talk by @bclavie is the highest value per second talk I have ever watched on RAG Chapter summaries and additional links in next tweet https://t.co/5uzmSbU6pa" / X</a><br/><a href="https://parlance-labs.com/education/rag/ben.html">Beyond the Basics of RAG ‚Äì Parlance</a><br/><a href="https://www.youtube.com/watch?v=0nA5QG3087g&t=684s">Beyond the Basics of Retrieval for Augmenting Generation (w/ Ben Clavi√©) - YouTube</a><br/><a href="https://www.google.com/search?q=audio+fingerprinting+pytorch&sca_esv=bb6fb22019ea88f6&sca_upv=1&rlz=1C5GCEM_enSE990SE991&sxsrf=ADLYWIL3Wl8ldjIcZp9ewcR2LEwzl9_BVw%3A1719930245530&ei=hQ2EZv76H6qHwPAPltCV-Ag&oq=neural+audio+search&gs_lp=Egxnd3Mtd2l6LXNlcnAiE25ldXJhbCBhdWRpbyBzZWFyY2gqAggAMgoQABiwAxjWBBhHMgoQABiwAxjWBBhHSIkfUABYAHACeAGQAQCYAQCgAQCqAQC4AQHIAQCYAgKgAgeYAwCIBgGQBgKSBwEyoAcA&sclient=gws-wiz-serp">audio fingerprinting pytorch - Google Search</a><br/><a href="https://towhee.io/audio-embedding/nnfp">audio-embedding/nnfp - nnfp - Towhee</a><br/><a href="https://upcommons.upc.edu/bitstream/handle/2117/386744/Master_Thesis_Macia_Amoros_Cortiella.pdf?sequence=5&isAllowed=y">Master_Thesis_Macia_Amoros_Cortiella.pdf</a><br/><a href="https://github.com/mimbres/neural-audio-fp/blob/main/model/generate.py">neural-audio-fp/model/generate.py at main ¬∑ mimbres/neural-audio-fp</a><br/><a href="https://arxiv.org/pdf/2406.13139">Audio Fingerprinting with Holographic Reduced Representations</a><br/><a href="https://github.com/stdio2016/pfann">stdio2016/pfann: Neural Network Audio FingerPrint</a><br/><a href="https://arxiv.org/abs/2010.11910">[2010.11910] Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning</a><br/><a href="https://github.com/ChrisNick92/deep-audio-fingerprinting">ChrisNick92/deep-audio-fingerprinting: A repository for my MSc thesis in Data Science & Machine Learning @ NTUA. A deep learning approach to audio fingerprinting for recognizing songs on real time through the microphone.</a><br/><a href="https://www.kaggle.com/datasets/mimbres/neural-audio-fingerprint">Neural Audio Fingerprint Dataset</a><br/><a href="https://ieee-dataport.org/open-access/neural-audio-fingerprint-dataset">Neural Audio Fingerprint Dataset | IEEE DataPort</a><br/><a href="https://mimbres.github.io/neural-audio-fp/">Neural Audio Fingerprint</a><br/><a href="https://github.com/bclavie/ragatouille">bclavie/RAGatouille: Easily use and train state of the art late-interaction retrieval methods (ColBERT) in any RAG pipeline. Designed for modularity and ease-of-use, backed by research.</a><br/><a href="https://www.youtube.com/watch?v=0nA5QG3087g&t=1574s">Beyond the Basics of Retrieval for Augmenting Generation (w/ Ben Clavi√©) - YouTube</a><br/><a href="https://arxiv.org/pdf/2406.07887#page=1&zoom=100,0,0">2406.07887</a><br/><a href="https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file">NVIDIA/Megatron-LM: Ongoing research training transformer models at scale</a><br/><a href="https://github.com/NVIDIA">NVIDIA Corporation</a><br/><a href="https://github.com/NVIDIA/open-gpu-kernel-modules">NVIDIA/open-gpu-kernel-modules: NVIDIA Linux open GPU kernel module source</a><br/><a href="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</a><br/><a href="https://github.com/NVIDIA/NeMo?tab=readme-ov-file">NVIDIA/NeMo: A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)</a><br/><a href="https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/">Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models | NVIDIA Technical Blog</a><br/><a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">Open ASR Leaderboard - a Hugging Face Space by hf-audio</a><br/><a href="https://huggingface.co/nvidia/parakeet-tdt-1.1b">nvidia/parakeet-tdt-1.1b ¬∑ Hugging Face</a><br/><a href="https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/">New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model | NVIDIA Technical Blog</a><br/><a href="https://aclanthology.org/2023.calcs-1.7.pdf">2023.calcs-1.7.pdf</a><br/><a href="https://huggingface.co/nvidia/parakeet-tdt-1.1b">nvidia/parakeet-tdt-1.1b ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/nvidia/parakeet-tdt-1.1b">nvidia/parakeet-tdt-1.1b ¬∑ Hugging Face</a><br/><a href="https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py">NeMo/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py at main ¬∑ NVIDIA/NeMo</a><br/><a href="https://github.com/NVIDIA/NeMo/tree/main?tab=readme-ov-file">NVIDIA/NeMo: A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)</a><br/><a href="https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/">Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT | NVIDIA Technical Blog</a><br/><a href="https://arxiv.org/pdf/2304.06795">Efficient Sequence Transduction by Jointly Predicting Tokens and Durations</a><br/><a href="https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml">NeMo/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml at main ¬∑ NVIDIA/NeMo</a><br/><a href="https://arxiv.org/pdf/2205.03026">2205.03026</a><br/><a href="https://www.google.com/search?q=p4+uppland&rlz=1C5GCEM_enSE990SE991&oq=p4+&gs_lcrp=EgZjaHJvbWUqBwgDEAAYgAQyBggAEEUYOTIHCAEQABiABDIHCAIQABiABDIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIGCAcQRRhB0gEINTM3N2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8">p4 uppland - Google Search</a><br/><a href="https://www.faithcomesbyhearing.com/audio-bible-resources/bible-is">Bible.is - Faith Comes By Hearing</a><br/><a href="https://www.faithcomesbyhearing.com/audio-bible-resources/recordings-database">Recordings Database - Faith Comes By Hearing</a><br/><a href="https://live.bible.is/bible/KNWXUN">1 | Bible.is</a><br/><a href="https://live.bible.is/bible/AGXIBT/LUK/1">–õ—É–∫–∞–π–∏–Ω –ö–∏—Ç–∞–± –ú–µ—Å–∏–≥—å ”Ä–∏—Å–∞–π–∏—Ö—ä–∞—Å –ò–¥–∂–µ –•–∞–±–∞—Ä 1 | Bible.is</a><br/><a href="https://live.bible.is/bible/AKVIBT">1 | Bible.is</a><br/><a href="https://www.google.com/search?q=AGHMILOST&rlz=1C5GCEM_enSE990SE991&oq=AGHMILOST&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBNIBBzM2M2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8">AGHMILOST - Google Search</a><br/><a href="https://huggingface.co/facebook/w2v-bert-2.0">facebook/w2v-bert-2.0 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/collections/facebook/seamless-communication-6568d486ef451c6ba62c7724">Seamless Communication - a facebook Collection</a><br/><a href="https://huggingface.co/facebook">facebook (AI at Meta)</a><br/><a href="https://huggingface.co/collections/facebook/hubert-651fca95d57549832161e6b6">HuBERT - a facebook Collection</a><br/><a href="https://huggingface.co/facebook/hubert-large-ll60k">facebook/hubert-large-ll60k ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/facebook/hubert-large-ls960-ft">facebook/hubert-large-ls960-ft ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/models?other=hubert">Models - Hugging Face</a><br/><a href="https://huggingface.co/rinna/japanese-hubert-base/tree/main">rinna/japanese-hubert-base at main</a><br/><a href="https://huggingface.co/utter-project/mHuBERT-147">utter-project/mHuBERT-147 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/superb">superb (superb)</a><br/><a href="https://huggingface.co/datasets/marsyas/gtzan">marsyas/gtzan ¬∑ Datasets at Hugging Face</a><br/><a href="https://huggingface.co/anton-l/distilhubert-ft-common-language">anton-l/distilhubert-ft-common-language ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/collections/facebook/fairseq-s2-tts-65243f6dfe08c8c7e3add9ba">Fairseq S^2 TTS - a facebook Collection</a><br/><a href="https://huggingface.co/facebook/seamless-m4t-v2-large">facebook/seamless-m4t-v2-large ¬∑ Hugging Face</a><br/><a href="https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/finetune">seamless_communication/src/seamless_communication/cli/m4t/finetune at main ¬∑ facebookresearch/seamless_communication</a><br/><a href="https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file#w2v-bert-20-speech-encoder">facebookresearch/seamless_communication: Foundational Models for State-of-the-Art Speech and Text Translation</a><br/><a href="https://github.com/facebookresearch/SONAR?tab=readme-ov-file">facebookresearch/SONAR: SONAR, a new multilingual and multimodal fixed-size sentence embedding space, with a full suite of speech and text encoders and decoders.</a><br/><a href="https://github.com/facebookresearch/SONAR/blob/main/LICENSE.md">SONAR/LICENSE.md at main ¬∑ facebookresearch/SONAR</a><br/><a href="https://ai.meta.com/research/publications/sonar-sentence-level-multimodal-and-language-agnostic-representations/">SONAR: Sentence-Level Multimodal and Language-Agnostic Representations | Research - AI at Meta</a><br/><a href="https://scontent-arn2-1.xx.fbcdn.net/v/t39.2365-6/385083099_996183704796913_6712193664436461927_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Fyfo3THv7UQQ7kNvgFfT8e9&_nc_ht=scontent-arn2-1.xx&oh=00_AYC8otapZIO48oM_aguyoLrjyTBvQNXacnIpUMS6NI1sgA&oe=66775FFC">385083099_996183704796913_6712193664436461927_n.pdf</a><br/><a href="https://github.com/facebookresearch/stopes">facebookresearch/stopes: A library for preparing data for machine translation research (monolingual preprocessing, bitext mining, etc.) built by the FAIR NLLB team.</a><br/><a href="https://huggingface.co/facebook/w2v-bert-2.0">facebook/w2v-bert-2.0 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/models?other=wav2vec2-bert&p=1&sort=trending">Models - Hugging Face</a><br/><a href="https://huggingface.co/Aspik101/w2v-bert-2.0-polish-CV16.0/tree/main">Aspik101/w2v-bert-2.0-polish-CV16.0 at main</a><br/><a href="https://huggingface.co/spygaurad/wav2vec2-bert">spygaurad/wav2vec2-bert ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/hf-audio/wav2vec2-bert-CV16-en">hf-audio/wav2vec2-bert-CV16-en ¬∑ Hugging Face</a><br/><a href="https://www.google.com/search?q=euro+2024+today&rlz=1C5GCEM_enSE990SE991&oq=euro&gs_lcrp=EgZjaHJvbWUqDggAEEUYJxg7GIAEGIoFMg4IABBFGCcYOxiABBiKBTIGCAEQRRg5MgYIAhBFGEAyBggDEEUYPDIGCAQQRRg8MgYIBRBFGDwyBggGEEUYQTIGCAcQRRhB0gEIMTE3N2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8#sie=lg;/m/0ynsg3h;2;/m/01l10v;mt;fp;1;;;">euro 2024 today - Google Search</a><br/><a href="https://arxiv.org/pdf/2306.12907">2306.12907</a><br/><a href="https://github.com/facebookresearch/LASER?tab=readme-ov-file">facebookresearch/LASER: Language-Agnostic SEntence Representations</a><br/><a href="https://github.com/facebookresearch/LASER/tree/main/tasks/pxsim">LASER/tasks/pxsim at main ¬∑ facebookresearch/LASER</a><br/><a href="https://arxiv.org/abs/2306.12907">[2306.12907] xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages</a><br/><a href="https://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1">LASER/data/tatoeba/v1 at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/LASER/blob/main/data/tatoeba/v1/tatoeba.ang-eng.ang">LASER/data/tatoeba/v1/tatoeba.ang-eng.ang at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/LASER/blob/main/data/tatoeba/v1/tatoeba.ang-eng.eng">LASER/data/tatoeba/v1/tatoeba.ang-eng.eng at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/LASER/blob/main/data/tatoeba/v1/tatoeba.swe-eng.eng">LASER/data/tatoeba/v1/tatoeba.swe-eng.eng at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/LASER/blob/main/data/tatoeba/v1/tatoeba.swe-eng.swe">LASER/data/tatoeba/v1/tatoeba.swe-eng.swe at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/LASER/blob/main/data/tatoeba/v1/tatoeba.hun-eng.eng">LASER/data/tatoeba/v1/tatoeba.hun-eng.eng at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/LASER/blob/main/data/tatoeba/v1/tatoeba.hun-eng.hun">LASER/data/tatoeba/v1/tatoeba.hun-eng.hun at main ¬∑ facebookresearch/LASER</a><br/><a href="https://github.com/facebookresearch/flores/tree/main/ocr">flores/ocr at main ¬∑ facebookresearch/flores</a><br/><a href="https://oldi.org/guidelines">Contribution Guidelines ‚Äì Open Language Data Initiative</a><br/><a href="https://oldi.org/guidelines#monolingual-guidelines">Contribution Guidelines ‚Äì Open Language Data Initiative</a><br/><a href="https://github.com/facebookresearch/flores/blob/main/toxicity/README.md">flores/toxicity/README.md at main ¬∑ facebookresearch/flores</a><br/><a href="https://www.google.com/search?q=salary+package&rlz=1C5GCEM_enSE990SE991&oq=salary+packe&gs_lcrp=EgZjaHJvbWUqCQgBEAAYChiABDIGCAAQRRg5MgkIARAAGAoYgAQyCAgCEAAYFhgeMggIAxAAGBYYHjIICAQQABgWGB4yCAgFEAAYFhgeMggIBhAAGBYYHjIICAcQABgWGB4yCAgIEAAYFhgeMgoICRAAGA8YFhge0gEINDU1NWowajeoAgCwAgA&sourceid=chrome&ie=UTF-8#ip=1">salary package - Google Search</a><br/><a href="https://x.com/home">(2) Home / X</a><br/><a href="https://x.com/historyinmemes/status/1802929483282080005">(2) Historic Vids on X: ""Scientists said that humans would only be able to run the marathon under 2 hours in 2075, but I proved them wrong," said Eliud Kipchoge. https://t.co/AYdXm8rFqc" / X</a><br/><a href="https://x.com/Gerashchenko_en/status/1803072719187062811">(2) Anton Gerashchenko on X: ""F*ck, why do they act like that? F*ckers. Why the f*ck are they allowed to do anything? Have they lost their f*cking minds?" - Russian fencer Maya Guchmazova (competing under the Georgian flag) when the Ukrainian fencer Olena Kryvytska refused to salute her. The incident took‚Ä¶ https://t.co/5FEGw3bT8f" / X</a><br/><a href="https://x.com/anshpay/status/1802406305799283093">x.com/anshpay/status/1802406305799283093</a><br/><a href="https://x.com/lqiao/status/1803063570596254171">x.com/lqiao/status/1803063570596254171</a><br/><a href="https://x.com/eyishazyer/status/1802660385176142261">(2) Eyisha Zyer on X: "Do you think your boss is scary? Look at the brutal emails from the CEOs of Apple, Microsoft, Tesla and Facebook: https://t.co/hGTgCoDS2k" / X</a><br/><a href="https://x.com/AdeenaY8/status/1803006922674557108">(2) Adina Yakup on X: "Open-Sora 1.2 is outüî• Open-Sora is an initiative dedicated to efficiently producing high-quality video in open-source way , released by @HPCAITech üëè Model: https://t.co/9gJB6iLGLC Demo: https://t.co/npE3DeXbFo ‚ú® Video compression network ‚ú®Rectifie-flow training ‚ú®More data‚Ä¶" / X</a><br/><a href="https://huggingface.co/spaces/hpcai-tech/open-sora">Open Sora - a Hugging Face Space by hpcai-tech</a><br/><a href="https://huggingface.co/hpcai-tech/OpenSora-VAE-v1.2">hpcai-tech/OpenSora-VAE-v1.2 ¬∑ Hugging Face</a><br/><a href="https://hpcaitech.github.io/Open-Sora/">Open-Sora Gallery</a><br/><a href="https://github.com/intel/openvino-plugins-ai-audacity?tab=readme-ov-file">intel/openvino-plugins-ai-audacity: A set of AI-enabled effects, generators, and analyzers for Audacity¬Æ.</a><br/><a href="https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/llm-rag-langchain/llm-rag-langchain.ipynb">openvino_notebooks/notebooks/llm-rag-langchain/llm-rag-langchain.ipynb at latest ¬∑ openvinotoolkit/openvino_notebooks</a><br/><a href="https://github.com/facebookresearch/demucs?tab=readme-ov-file">facebookresearch/demucs: Code for the paper Hybrid Spectrogram and Waveform Source Separation</a><br/><a href="https://github.com/adefossez/demucs/blob/main/docs/training.md#model-zoo">demucs/docs/training.md at main ¬∑ adefossez/demucs</a><br/><a href="https://arxiv.org/pdf/2406.10735">How Should We Extract Discrete Audio Tokens from Self-Supervised Models?</a><br/><a href="https://arxiv.org/pdf/2406.11768">2406.11768</a><br/><a href="https://sreyan88.github.io/gamaaudio/">GAMA Audio</a><br/><a href="https://github.com/Sreyan88/GAMA">Sreyan88/GAMA</a><br/><a href="https://x.com/Gerashchenko_en/status/1802982112666038523">(2) Anton Gerashchenko on X: "The BRICS Games are now being held in Russia. They were supposed to be an "alternative" to the Olympics. Russian media already report that Russian athletes won hundreds of medals. The secret: in many competitions, there is just one athlete competing. https://t.co/6KJIZinmRu" / X</a><br/><a href="https://x.com/dr_chizhang/status/1802534849796034724">(2) Chi Zhang on X: "üî•üî•We are excited to introduce MeshAnything, an auto-regressive model for generating artist-created 3D meshes. MeshAnything seamlessly integrates with various existing models to produce high-quality, text/image/shape-conditioned mesh generation. Explore our project page! https://t.co/9cxJUHRT9b" / X</a><br/><a href="https://x.com/notgwendalupe/status/1802832945821950144">(2) popculture on X: "some fashion items in movies that i think about constantly: 1) sarah michelle gellar's gogo boots in 'scooby doo' https://t.co/sj5FeOS5vA" / X</a><br/><a href="https://x.com/reach_vb/status/1802640148665958674">(2) Vaibhav (VB) Srivastav on X: "Apple dropped 4M: Massively Multilingual Masked Modeling! üî• Is this what powers the on-device vision-text backbone? &gt; A framework for training any-to-any multimodal foundational models. Training/ Finetuning/ Inference. &gt; Release 4M-7 and 4M-21 model checkpoints (traied acros‚Ä¶ https://t.co/ZnPXiJ1Rja" / X</a><br/><a href="https://huggingface.co/collections/EPFL-VILAB/4m-models-660193abe3faf4b4d98a2742">4M Models - a EPFL-VILAB Collection</a><br/><a href="https://4m.epfl.ch/">4M: Massively Multimodal Masked Modeling</a><br/><a href="https://www.instagram.com/">Instagram</a><br/><a href="https://www.instagram.com/sophie_xdt_crushxo/">Sophie üíû (@sophie_xdt_crushxo) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://www.instagram.com/beautyfulgirlls/">Anna (@beautyfulgirlls) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://www.instagram.com/amalia.amour/">Mila üåπ (@amalia.amour) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://www.instagram.com/llama_maja/">Llama Maja ü¶ôüåµ (@llama_maja) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://www.instagram.com/p/C8UfaGzR_VM/?img_index=1">Dr. Adam McCluskey PT, DPT (@theptinitiative) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://app.teeveeing.com/live-tv">Teeveeing - Livetv</a><br/><a href="chrome://newtab/">New Tab</a><br/><a href="chrome://newtab/">New Tab</a><br/><a href="https://www.tvguide.co.uk/">TV Guide | TVGuide.co.uk</a><br/><a href="https://www.instagram.com/">Instagram</a><br/><a href="https://www.youtube.com/">YouTube</a><br/><a href="https://www.youtube.com/watch?v=6YW-xEYBm2U">Soundgarden - Tighter & Tighter Live at The Palladium Dallas, TX 5-26-13 - YouTube</a><br/><a href="https://www.youtube.com/watch?v=x3UBaO2tCEc">Diving into Okinawa's finest UECHI-RYU Karate with the legendary Karateka: KIYOHIDE SHINJO - YouTube</a><br/><a href="https://www.youtube.com/watch?v=yAEjS14H4tQ">Wing Chun for Beginners 101 Siu Nim Tao Form (Lesson 1 of 49) - YouTube</a><br/><a href="https://humogen.github.io/">HuMoGen Workshop@CVPR24</a><br/><a href="https://x.com/humogen11384">(2) HuMoGen - CVPR Workshop 2024 (@humogen11384) / X</a><br/><a href="https://www.instagram.com/">Instagram</a><br/><a href="https://arxiv.org/pdf/2406.08641">ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets</a><br/><a href="https://www.google.com/search?q=xls-r+lora&rlz=1C5GCEM_enSE990SE991&oq=xls-r+lora&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCDM2OTNqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8#ip=1">xls-r lora - Google Search</a><br/><a href="https://discuss.huggingface.co/t/how-to-train-wav2vec2-in-lora/57074/2">How to train Wav2Vec2 in LoRA? - Models - Hugging Face Forums</a><br/><a href="https://medium.com/@gitau_am/exploring-asr-model-development-fine-tuning-xls-r-wav2vec2-model-with-swahili-data-b95134d116b8">Exploring ASR Model Development: Fine-Tuning XLS-R Wav2Vec2 Model with Swahili Data | by Antony M. Gitau | Medium</a><br/><a href="https://www.kaggle.com/code/abhranta/lora-wav2vec-v1-bengali-training">'LoRA' wav2vec_v1_bengali [Training]</a><br/><a href="https://www.google.com/search?q=huggingface+xlsr+lora&sca_esv=378a656012f4430f&sca_upv=1&rlz=1C5GCEM_enSE990SE991&sxsrf=ADLYWIJYoYTuNR7s80-xQsC-guQ6fRjrDw%3A1718741259101&ei=C-lxZpbiBbvLwPAPgKCnsAU&ved=0ahUKEwjWk9aR-uWGAxW7JRAIHQDQCVYQ4dUDCBA&uact=5&oq=huggingface+xlsr+lora&gs_lp=Egxnd3Mtd2l6LXNlcnAiFWh1Z2dpbmdmYWNlIHhsc3IgbG9yYTIIEAAYgAQYogRI7xNQ7gVY-BFwA3gAkAEAmAGCAaABwAOqAQMzLjK4AQPIAQD4AQGYAgigAtEDwgIOEAAYgAQYsAMYhgMYigXCAgsQABiABBiwAxiiBJgDAIgGAZAGBJIHAzYuMqAHhQQ&sclient=gws-wiz-serp">huggingface xlsr lora - Google Search</a><br/><a href="https://huggingface.co/Joshua-Abok/finetuning-wav2vec-lora-large-swahili-asr-model_v20">Joshua-Abok/finetuning-wav2vec-lora-large-swahili-asr-model_v20 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/models?other=wav2vec2">Models - Hugging Face</a><br/><a href="https://huggingface.co/facebook/wav2vec2-large-xlsr-53">facebook/wav2vec2-large-xlsr-53 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/facebook/mms-1b-all#supported-languages">facebook/mms-1b-all ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/models?other=mms">Models - Hugging Face</a><br/><a href="https://huggingface.co/facebook/mms-1b">facebook/mms-1b ¬∑ Hugging Face</a><br/><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms#asr">fairseq/examples/mms at main ¬∑ facebookresearch/fairseq</a><br/><a href="https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html">MMS</a><br/><a href="https://huggingface.co/facebook/mms-lid-4017">facebook/mms-lid-4017 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/blog/mms_adapters">Fine-Tune MMS Adapter Models for low-resource ASR</a><br/><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms/data_prep">fairseq/examples/mms/data_prep at main ¬∑ facebookresearch/fairseq</a><br/><a href="https://github.com/NVIDIA/NeMo/tree/main/tools/speech_data_explorer">NeMo/tools/speech_data_explorer at main ¬∑ NVIDIA/NeMo</a><br/><a href="https://github.com/nateanl/audio/commit/950ce86fee45e0d161a6f13c98d8c617dfafca98#diff-eaf79b37ac7eddf3c37ebfa0e444c571a37ef84e63251a0de993349dc6ace9bc">Add forced_align function to torchaudio (#3348) ¬∑ nateanl/audio@950ce86</a><br/><a href="https://huggingface.co/facebook/mms-300m">facebook/mms-300m ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/models?other=mms">Models - Hugging Face</a><br/><a href="https://huggingface.co/facebook/mms-lid-4017">facebook/mms-lid-4017 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/facebook/mms-1b-all">facebook/mms-1b-all ¬∑ Hugging Face</a><br/><a href="https://nbviewer.org/github/gpeyre/numerical-tours/blob/master/python/ml_3_classification.ipynb">Jupyter Notebook Viewer</a><br/><a href="https://x.com/rowancheung/status/1803269485215797615">Rowan Cheung on X: "AI NEWS: OpenAI is expanding to AI cancer care. Plus, big developments from TikTok, Meta, Google DeepMind, Notion, Runway, Hedra, ElevenLabs, and Nvidia. Here's everything going on in AI right now:" / X</a><br/><a href="https://x.com/PicturesFoIder/status/1803060869632229688">non aesthetic things on X: "9 different mugshots doesn't matter https://t.co/GcU2CHegEM" / X</a><br/><a href="https://x.com/AIatMeta/status/1803107817345393136">AI at Meta on X: "Today is a good day for open science. As part of our continued commitment to the growth and development of an open ecosystem, today at Meta FAIR we‚Äôre announcing four new publicly available AI models and additional research artifacts to inspire innovation in the community and https://t.co/8PVczc0tNV" / X</a><br/><a href="https://x.com/home">Home / X</a><br/><a href="https://x.com/skalskip92/status/1803076369368412204">SkalskiP @CVPR2024 üá∫üá∏ on X: "ViP-LLaVA, a model that understands not only textual prompts but also visual prompts, such as pointing with an arrow, drawing an ellipse, or marking with a specific color cool model presented yesterday by @yong_jae_lee at #CVPR2024 "Prompting in Vision" workshop ‚Üì read more https://t.co/5KE15X20cE" / X</a><br/><a href="https://github.com/WisconsinAIVision/ViP-LLaVA?tab=readme-ov-file">WisconsinAIVision/ViP-LLaVA: [CVPR2024] ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</a><br/><a href="https://github.com/WisconsinAIVision/ViP-LLaVA/blob/main/docs/study_llm_backbone.md">ViP-LLaVA/docs/study_llm_backbone.md at main ¬∑ WisconsinAIVision/ViP-LLaVA</a><br/><a href="https://x.com/80Level/status/1803264785804324888">80 LEVEL on X: "Check out the mind-blowing experiments of @dante_leoncini, a 3D Artist and Programmer, who managed to run Blender on an 18-year-old Nokia phone. Now I've seen everything: https://t.co/h7E1cXKCzT #blender #blender3d #b3d #blendercommunity #nokia #mobilephone #3dsoftware https://t.co/fvW4ckCgvF" / X</a><br/><a href="https://x.com/home">Home / X</a><br/><a href="https://x.com/Manifest_Lord/status/1803361515820527973">Manifest_Lord on X: "Jason Statham is one of the finest action heroes of this generation, some says he‚Äôs our last action hero. but, before he stepped foot on his first movie set, He had a past life that would rival any of the characters he‚Äôs been on the screen. Let's dig into his life which helped https://t.co/7hCudHSpaY" / X</a><br/><a href="https://x.com/GoogleDeepMind/status/1802733643992850760">Google DeepMind on X: "We're sharing progress on our video-to-audio (V2A) generative technology. üé• It can add sound to silent clips that match the acoustics of the scene, accompany on-screen action, and more. Here are 4 examples - turn your sound on. üßµüîä https://t.co/VHpJ2cBr24 https://t.co/S5m159Ye62" / X</a><br/><a href="https://x.com/notgwendalupe/status/1803216405661360181">popculture on X: "a small thread of 5 movie fight scenes that live in my mind rent free: 1) gogo vs the bride in 'kill bill' https://t.co/auHOT5KibJ" / X</a><br/><a href="https://x.com/mehdirhasan/status/1803200625234100303">Mehdi Hasan on X: "So, we lost the Munk debate on "anti-Zionism is antisemitism" last night. Given the pro-Israeli audience jeered @gideonle when he mentioned Palestinian deaths &amp; then booed me when I mentioned the ICC &amp; Oxfam, I can't say I was shocked. Who boos... Oxfam? https://t.co/9HfMTCkpkl" / X</a><br/><a href="https://x.com/PicturesFoIder/status/1803337196054737353">non aesthetic things on X: "Bro rizzed up his teacher üò≠ https://t.co/2zQOlJw6ot" / X</a><br/><a href="https://x.com/xbresson/status/1803349729373536358">Xavier Bresson on X: "Notebooks for Lecture 5 on Recommendation on Graphs Lab1: Google PageRank https://t.co/WtbFZdzmou Lab2: Collaborative/low-rank recom https://t.co/QdRsseuSAd Lab3: Content/graph-Dirichlet recom https://t.co/BWgZpdnMeZ Lab4: Hybrid recom https://t.co/8M7tmYP0qv https://t.co/T3w8xCNEVc" / X</a><br/><a href="https://x.com/Yuchenj_UW/status/1802933446249271618">Yuchen Jin on X: "Finally finished watching @karpathy's 4-hour "GPT-2 The Documentary" üé•üçø If you've ever wondered how much faster llm.c is compared to nanoGPT (a PyTorch implementation with torch.compile enabled), the answer is approximately 20% faster on a single A100 GPU. The speedup of https://t.co/0iJOVJa2jP" / X</a><br/><a href="https://x.com/abierkhatib/status/1803128277650870699">Abier on X: "Next time some Zionist blathers that it's all about "Israel's right to self defense", link to this video. A chilling documentary that @zeteo_news produced will leave u aghast af‚Ä¶ This is just a 2 minute trailer‚Ä¶ https://t.co/I8yTkhfTR2" / X</a><br/><a href="https://x.com/KerryBurgess/status/1802812225436135444">Kerry Burgess on X: "Is this woman not aware that Israel is committing genocide and of the international repercussions for its citizens???? https://t.co/YSpix8ZY0F" / X</a><br/><a href="https://wikisource.org/wiki/Page:Eachtradh_Eibhl%C3%ADs_i_dT%C3%ADr_na_nlongantas_-_%C3%93_Cadhla.djvu/56">Page:Eachtradh Eibhl√≠s i dT√≠r na nlongantas - √ì Cadhla.djvu/56 - Wikisource</a><br/><a href="https://upload.wikimedia.org/wikipedia/sources/thumb/d/de/Eachtradh_Eibhl%C3%ADs_i_dT%C3%ADr_na_nlongantas_-_%C3%93_Cadhla.djvu/page56-2048px-Eachtradh_Eibhl%C3%ADs_i_dT%C3%ADr_na_nlongantas_-_%C3%93_Cadhla.djvu.jpg">page56-2048px-Eachtradh_Eibhl√≠s_i_dT√≠r_na_nlongantas_-_√ì_Cadhla.djvu.jpg (2048√ó3364)</a><br/><a href="https://medium.com/@nelsonizah/text-detection-in-images-with-easyocr-in-python-3e336c462c16">Text Detection in Images with EasyOCR in Python | by Nelson izah | Medium</a><br/><a href="https://github.com/JaidedAI/EasyOCR?tab=readme-ov-file">JaidedAI/EasyOCR: Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.</a><br/><a href="https://github.com/JaidedAI/EasyOCR/blob/master/trainer/craft/README.md">EasyOCR/trainer/craft/README.md at master ¬∑ JaidedAI/EasyOCR</a><br/><a href="https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md">EasyOCR/custom_model.md at master ¬∑ JaidedAI/EasyOCR</a><br/><a href="https://github.com/Belval/TextRecognitionDataGenerator/blob/master/trdg/generators/from_strings.py">TextRecognitionDataGenerator/trdg/generators/from_strings.py at master ¬∑ Belval/TextRecognitionDataGenerator</a><br/><a href="https://github.com/JaidedAI/EasyOCR/blob/master/trainer/trainer.ipynb">EasyOCR/trainer/trainer.ipynb at master ¬∑ JaidedAI/EasyOCR</a><br/><a href="https://jaided.ai/easyocr/modelhub/">Jaided AI: EasyOCR model hub</a><br/><a href="https://github.com/jimregan/sjoestedt-jonval-description/tree/sections/phonetique">sjoestedt-jonval-description/phonetique at sections ¬∑ jimregan/sjoestedt-jonval-description</a><br/><a href="https://github.com/jimregan/UD_Irish/tree/master">jimregan/UD_Irish: Irish data</a><br/><a href="https://github.com/jimregan/irish-g2p/blob/main/wordlists/quiggin.yaml">irish-g2p/wordlists/quiggin.yaml at main ¬∑ jimregan/irish-g2p</a><br/><a href="https://www.flickr.com/photos/jimregan/53803469745/in/photostream/">Screenshot_20240620-022404.png | Jim O'Regan | Flickr</a><br/><a href="https://www.instagram.com/iamaya.sz/reels/">Aya Summers‚ú® (@iamaya.sz) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://github.com/Belval/TextRecognitionDataGenerator?tab=readme-ov-file">Belval/TextRecognitionDataGenerator: A synthetic data generator for text recognition</a><br/><a href="https://www.google.com/search?q=fontforge+python&rlz=1C5GCEM_enSE990SE991&oq=fontforge+p&gs_lcrp=EgZjaHJvbWUqDQgBEAAYkQIYgAQYigUyBggAEEUYOTINCAEQABiRAhiABBiKBTIHCAIQABiABDIMCAMQABgUGIcCGIAEMgwIBBAAGBQYhwIYgAQyDQgFEAAYkQIYgAQYigUyBggGEEUYPDIGCAcQRRg80gEINTU1MWowajeoAgCwAgA&sourceid=chrome&ie=UTF-8#ip=1">fontforge python - Google Search</a><br/><a href="https://fontforge.org/docs/scripting/python.html">Python Scripting ‚Äî FontForge 20230101 documentation</a><br/><a href="https://github.com/jimregan/tesseract-gle-uncial/releases/tag/v0.1beta4_gen">Release v0.1beta4_gen: add generated data ¬∑ jimregan/tesseract-gle-uncial</a><br/><a href="https://www.kaggle.com/models/mistral-ai/mistral/code">Mistral AI | Mistral | Kaggle</a><br/><a href="https://www.youtube.com/watch?v=ZZ9u1vUtcIA&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=5">Types of Audio Features for Machine Learning - YouTube</a><br/><a href="https://x.com/GrantSlatton/status/1803233673518727614">Grant Slatton on X: "Back when AWS did a phone screen before onsite interviews, the first technical question I would ask, regardless of level, was ‚Äútell me what you know about hash tables‚Äù You would be blown away by the number of allegedly-senior devs who don‚Äôt know a thing about them ‚Äî around 50%" / X</a><br/><a href="https://x.com/anand_bhattad/status/1803665956889301299">Anand Bhattad on X: "Impressed by the dedication and effort! We narrowly avoided a poster mishap this morning and managed to get a reprint just in time. https://t.co/B40uYumCJa" / X</a><br/><a href="https://x.com/RemiCadene/status/1803711154218254754">Remi Cadene on X: "I am mind blown by this new technology! AI is now embodied. And we are open-sourcing it all. Listen to @HaixuanT casually discussing with its cute robot at the @linuxfoundation: üôÇ What's your name? &gt; I am Reachy, a robot from @pollenrobotics, I have two arms. üòÄ What do you‚Ä¶ https://t.co/VgzhReFOTA" / X</a><br/><a href="https://pytorch.org/audio/main/generated/torchaudio.pipelines.Wav2Vec2Bundle.html#torchaudio.pipelines.Wav2Vec2Bundle">Wav2Vec2Bundle ‚Äî Torchaudio 2.4.0.dev20240620 documentation</a><br/><a href="https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html">Forced Alignment with Wav2Vec2 ‚Äî Torchaudio 2.3.0 documentation</a><br/><a href="https://pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html">CTC forced alignment API tutorial ‚Äî Torchaudio 2.4.0.dev20240620 documentation</a><br/><a href="https://pytorch.org/audio/main/generated/torchaudio.functional.forced_align.html#torchaudio.functional.forced_align">torchaudio.functional.forced_align ‚Äî Torchaudio 2.4.0.dev20240620 documentation</a><br/><a href="https://pytorch.org/audio/main/transforms.html">torchaudio.transforms ‚Äî Torchaudio 2.4.0.dev20240620 documentation</a><br/><a href="https://pytorch.org/audio/0.10.0/_modules/torchaudio/pipelines/_wav2vec2.html#Wav2Vec2Bundle">torchaudio.pipelines._wav2vec2 ‚Äî Torchaudio 0.10.0 documentation</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/intro.html">Automatic Speech Recognition (ASR) - NVIDIA Docs</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/results.html#asr-checkpoint-list-by-language">Checkpoints - NVIDIA Docs</a><br/><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_pl_quartznet15x5">STT Pl Quartznet15x5 | NVIDIA NGC</a><br/><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_pl_fastconformer_hybrid_large_pc">STT Pl FastConformer Hybrid Transducer-CTC Large P&C | NVIDIA NGC</a><br/><a href="https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_hybrid_transducer_ctc/speech_to_text_hybrid_rnnt_ctc_bpe.py">NeMo/examples/asr/asr_hybrid_transducer_ctc/speech_to_text_hybrid_rnnt_ctc_bpe.py at main ¬∑ NVIDIA/NeMo</a><br/><a href="https://jqlang.github.io/jq/tutorial/">Tutorial</a><br/><a href="https://github.com/c-blake/hldiff">c-blake/hldiff: A port of Python difflib to compute diffs and (re)highlight diff output intraline</a><br/><a href="https://github.com/microsoft/vscode-jupyter/issues/13495">Unable to install Jupyter extension within vscode ¬∑ Issue #13495 ¬∑ microsoft/vscode-jupyter</a><br/><a href="https://x.com/sahouraxo/status/1803426158790287469">sarah on X: "BREAKING : The UN accuses Israel of committing EXTERMINATION in Gaza. EXTERMINATION. Repeat it. Spread it. Let the world know. ‚ÄúIsrael is responsible for extermination, murder, using starvation as a method of war..‚Äù ‚ÄîThe UN Human Rights Council https://t.co/Xv12y4LqTO" / X</a><br/><a href="https://x.com/jon_barron/status/1803821046690095466">Jon Barron on X: "Be sure to check out the DUSt3R poster tomorrow (Friday) morning. It's absolutely insane to me that DUSt3R isn't an highlight, oral, award candidate, or award winner. I'm willing to bet it'll win a Test of Time award a decade from now. https://t.co/jdic5qMiJW" / X</a><br/><a href="https://x.com/unilightwf/status/1803732499664924689">x.com/unilightwf/status/1803732499664924689</a><br/><a href="https://x.com/FilecoinTLDR/status/1802972711804895632">Filecoin TL;DR on X: "4/ Zero-Knowledge Machine Learning (ZKML) ‚Äì combining the power of ZKPs with ML Zero-Knowledge Machine Learning (ZKML) is like a game where one player (the prover) proves to another player (the verifier) that they know something, without actually telling them what it is. This‚Ä¶ https://t.co/3BbcgsYZpl" / X</a><br/><a href="https://x.com/alphacep/status/1803782619433206205">AlphaCephei on X: "@unilightwf I actually evaluated 30+ VC repos recently. Big mess. FACodec is more or less recent too. Something like https://t.co/jsSOX2nl23 is probably worth attention" / X</a><br/><a href="https://alphacephei.com/vosk/models">VOSK Models</a><br/><a href="https://github.com/Vaibhavs10/insanely-fast-whisper/issues/82">please correct and/or update the readme comparing other whisper implementations ¬∑ Issue #82 ¬∑ Vaibhavs10/insanely-fast-whisper</a><br/><a href="https://github.com/gweltou/vosk-br">gweltou/vosk-br: Anaouder mouezh e Brezhoneg gant Vosk</a><br/><a href="https://github.com/alphacep/vosk-api/tree/master/training">vosk-api/training at master ¬∑ alphacep/vosk-api</a><br/><a href="https://www.instagram.com/celinebethmann/">c√©line (@celinebethmann) ‚Ä¢ Instagram photos and videos</a><br/><a href="https://x.com/karpathy/status/1803963383018066272">Andrej Karpathy on X: "These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency. This is my earlier project Micrograd. It implements a scalar-valued auto-grad engine. You start with some numbers at the leafs (usually the input data and the https://t.co/2zVJP3cNJ0" / X</a><br/><a href="https://github.com/rhasspy/piper-samples/tree/master">rhasspy/piper-samples: Samples for Piper text to speech system</a><br/><a href="https://github.com/rhasspy/piper-samples/blob/master/sv-se/nst/medium/MODEL_CARD">piper-samples/sv-se/nst/medium/MODEL_CARD at master ¬∑ rhasspy/piper-samples</a><br/><a href="https://github.com/rhasspy/piper-samples/tree/master/pl/mls_6892/low">piper-samples/pl/mls_6892/low at master ¬∑ rhasspy/piper-samples</a><br/><a href="https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-17/">NST Swedish Dictation (22 kHz) - Spr√•kbanken</a><br/><a href="https://www.nb.no/sbfil/dok/nst_taledat_se.pdf">nst_taledat_se.pdf</a><br/><a href="https://x.com/pengyf21">Yifan Peng (@pengyf21) / X</a><br/><a href="https://x.com/GoogleDeepMind/status/1803433385102057585">Google DeepMind on X: "From producing environmental sounds to rich background scores, V2A can help create unlimited audio options for videos from scratch. üé∂ Here‚Äôs a look into its capabilities. üßµ https://t.co/7r53E0rq9b" / X</a><br/><a href="https://pyf98.github.io/">About me - Yifan Peng‚Äôs Homepage</a><br/><a href="https://proceedings.mlr.press/v162/peng22a.html">Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding</a><br/><a href="https://www.isca-archive.org/interspeech_2023/peng23c_interspeech.html">ISCA Archive</a><br/><a href="https://ieeexplore.ieee.org/abstract/document/10389676">Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data | IEEE Conference Publication | IEEE Xplore</a><br/><a href="https://arxiv.org/abs/2403.12402">[2403.12402] An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis</a><br/><a href="https://www.wavlab.org/activities/2024/owsm/">WAVLab | Open Whisper-style Speech Models (OWSM)</a><br/><a href="https://proceedings.mlr.press/v162/peng22a/peng22a.pdf">peng22a.pdf</a><br/><a href="https://x.com/pengyf21/status/1770835738411581844">Yifan Peng on X: "I have updated the OWSM-CTC model page to include examples of short-form ASR/ST, long-form ASR/ST, and CTC forced alignment (it takes a few seconds to align a 20min audio on a GPU): https://t.co/E6AOF0pZEW https://t.co/w8bbVO6aug" / X</a><br/><a href="https://huggingface.co/pyf98/owsm_ctc_v3.1_1B/tree/main">pyf98/owsm_ctc_v3.1_1B at main</a><br/><a href="https://colab.research.google.com/drive/156zzT4HigJtQBxd5IeK_P0fu51-5AktQ#scrollTo=WQfLYF7-qXxX">Untitled40.ipynb - Colab</a><br/><a href="https://github.com/pyf98/espnet/commit/8881a71d054a9415a539d4681ea1f3d0c7759b5a">add ctc segmentation using owsm-ctc ¬∑ pyf98/espnet@8881a71</a><br/><a href="https://github.com/pyf98/espnet/commit/fdf0c325757015a7dd9c97a38c33dec3b66e4e44#diff-b5ddcf1e2244d7b6c4b31ef72530413effbbea77d34ba63e898655107eb436b3">add files for owsm-ctc ¬∑ pyf98/espnet@fdf0c32</a><br/><a href="https://github.com/espnet/espnet?tab=readme-ov-file#ctc-segmentation-demo">espnet/espnet: End-to-End Speech Processing Toolkit</a><br/><a href="https://github.com/espnet/espnet">espnet/espnet: End-to-End Speech Processing Toolkit</a><br/><a href="https://www.youtube.com/watch?v=BP1C64XPPBE">My Epic Return to Opeth! Vocal Analysis of "Blackwater Park" and wow is this a long analysis... - YouTube</a><br/><a href="https://www.leighleat.com/pages/181">Jim√≠n Mh√°ire Thaidhg - Leathanach 1 | Leigh Leat</a><br/><a href="https://www.leighleat.com/sc%C3%A9alta">Sc√©alta | Leigh Leat</a><br/><a href="https://www.leighleat.com/pages/15">An Bhean Chaointe - Leathanach 0 | Leigh Leat</a><br/><a href="https://www.leighleat.com/poems/57">leighleat.com/poems/57</a><br/><a href="https://www.leighleat.com/sc%C3%A9alta">Sc√©alta | Leigh Leat</a><br/><a href="https://www.leighleat.com/pages/548">Leabhar na Mianta - Leathanach 2 | Leigh Leat</a><br/><a href="view-source:https://www.leighleat.com/poems/25">view-source:https://www.leighleat.com/poems/25</a><br/><a href="https://colab.research.google.com/drive/13R0vkfVfkUHvQvecBLqxcfRc7yf0OaoX#scrollTo=T4tqTE_zB2CY">Untitled39.ipynb - Colab</a><br/><a href="https://x.com/TheFigen_/status/1804529980480626885">Figen on X: "Cool üòÇ https://t.co/0pTpMKIzpa" / X</a><br/><a href="https://x.com/ChrisO_wiki/status/1804977593818779776">ChrisO_wiki on X: "1/ A year ago today, Yevgeny Prigozhin launched his failed rebellion against Vladimir Putin. Exactly two months later he died in a suspicious plane crash which has been the focus of a pseudo-investigation by the Russian authorities. What has it found? ‚¨áÔ∏è https://t.co/qzXoNPIue1" / X</a><br/><a href="https://x.com/ClinOncDoc/status/1804919010938511772">ClinOncDoc on X: "An anecdote of an interaction with a patient that has stayed with me for 15 years. When I was a fresh-faced 2nd year doctor in a GP practice, a young lady came for an appointment. She couldn‚Äôt have been more than 5 years older than me. The reason was ‚Äòwork stress‚Äô A long üßµ https://t.co/mAF41F2qRl" / X</a><br/><a href="https://www.youtube.com/watch?v=SDXyR3rz4xk">How to learn a language by yourself - YouTube</a><br/><a href="https://www.youtube.com/watch?v=6EQQSLARk2c">Why You Should Learn Hungarian - How I Learned Hungarian! üá≠üá∫ - YouTube</a><br/><a href="https://github.com/espnet/espnet/pulls">Pull requests ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/pull/5808">ESPnet Codec Implmentation by ftshijt ¬∑ Pull Request #5808 ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/pull/5780">ESPnet-SPK: add support for multi-task learning by Alexgichamba ¬∑ Pull Request #5780 ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/pull/5791/files#diff-2ff757b1283dec567d62fcca9ca042b2bef5bbcbd463e41ec4e0178833c55db6">version 1 speechtokenizer by massabaali7 ¬∑ Pull Request #5791 ¬∑ espnet/espnet</a><br/><a href="https://www.scientificamerican.com/article/is-it-true-that-hot-water/#:~:text=%22It%20all%20depends%20on%20how,cooler%20than%2060%20degrees%20C.">Is It True that Hot Water Freezes Faster than Cold Water or that Cold Water Boils Faster than Hot Water? | Scientific American</a><br/><a href="https://ai4bharat.iitm.ac.in/shrutilipi/">Shrutilipi ‚Äì AI4BHƒÄRAT</a><br/><a href="https://x.com/OliLondonTV/status/1804900861606232454">Oli London on X: "Tourist gets headbutted and thrown to the floor after standing next to a Kings Guard horsemen. Tourists are warned not to get close to the horses or guards with a nearby sign reading ‚ÄòBeware. Horse may kick or bite. Don‚Äôt touch the reigns. Thank you.‚Äô https://t.co/USdXsOGCHH" / X</a><br/><a href="https://hifialbedo.github.io/">High-Fidelity Facial Albedo Estimation via Texture Quantization</a><br/><a href="https://x.com/Yuchenj_UW/status/1805006766507426252">Yuchen Jin on X: "Training GPT-2 (124M) using @karpathy's llm.c with 32 H100 GPUs *GPUs go brrr on Sunday* üî• - Setup: 4 H100 nodes connected with 400Gb/s InfiniBand - Training speed: 15M tokens/s (MFU drops from 40.5% for a single node to 38.4%, not too bad) - mpirun works smoothly https://t.co/9gazICU1Wk" / X</a><br/><a href="https://x.com/Yuchenj_UW/status/1805006766507426252">Yuchen Jin on X: "Training GPT-2 (124M) using @karpathy's llm.c with 32 H100 GPUs *GPUs go brrr on Sunday* üî• - Setup: 4 H100 nodes connected with 400Gb/s InfiniBand - Training speed: 15M tokens/s (MFU drops from 40.5% for a single node to 38.4%, not too bad) - mpirun works smoothly https://t.co/9gazICU1Wk" / X</a><br/><a href="https://x.com/TheFigen_/status/1804855403693371623">Figen on X: "A refrigerator from the 1960s was much more functional and cooler than today's. https://t.co/V7DQ7u4CgU" / X</a><br/><a href="https://pbs.twimg.com/media/GQxcYO_W4AAy8P4?format=png&name=large">GQxcYO_W4AAy8P4 (1329√ó1894)</a><br/><a href="https://github.com/salesforce/LAVIS?tab=readme-ov-file">salesforce/LAVIS: LAVIS - A One-stop Library for Language-Vision Intelligence</a><br/><a href="https://arxiv.org/pdf/2209.09019">2209.09019</a><br/><a href="https://github.com/phonlab-tcd/expo-irish-synthesis/tree/main/src">expo-irish-synthesis/src at main ¬∑ phonlab-tcd/expo-irish-synthesis</a><br/><a href="https://huggingface.co/models?other=mms&sort=trending&search=mms-lid">Models - Hugging Face</a><br/><a href="https://huggingface.co/facebook/mms-1b-all">facebook/mms-1b-all ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/facebook/mms-tts-swe">facebook/mms-tts-swe ¬∑ Hugging Face</a><br/><a href="https://colab.research.google.com/drive/1Alul7CPkn2SWJoMLF8l9Ti_UJ-6t-nXP#scrollTo=GY160Cx16HeL">Untitled41.ipynb - Colab</a><br/><a href="https://huggingface.co/docs/transformers/en/tasks/audio_classification">Audio classification</a><br/><a href="https://sverigesradio.se/artikel/dal-boahta-gallok-duopmu">D√°l G√°llok duopmu boahta - Sameradion | Sveriges Radio</a><br/><a href="https://x.com/arpit20adlakha/status/1805084468870521100/photo/1">Arpit Adlakha on X: "One of the finest roadmaps I have seen for Senior Software Interviews, a guy posted on LeetCode for clearing Uber L5A, L5B or Google L5/L6 levels. https://t.co/YwvHpnDNBk" / X</a><br/><a href="https://x.com/emileifrem/status/1804972481675235362">Emil Eifrem on X: "Folks, seriously. The GraphRAG Ecosystem Tools are frickin' amazing. The KG Builder is a visual cloud service that easily (click click!) turns unstructured data into a knowledge graph! You can point it to some random PDFs, a wikipedia page or a youtube video, and a few seconds https://t.co/YnQLNgWHtt" / X</a><br/><a href="https://x.com/Teddarific/status/1805268979508015589">Teddy Ni on X: "Today, we‚Äôre releasing the beta of HTML to React to the public. It‚Äôs stupid powerful. ‚¨áÔ∏è Watch how fast I can go from seeing a design I like on an a website to getting a personalized React component. More examples of what this extension can do below https://t.co/tJlCeP0KmI" / X</a><br/><a href="https://github.com/apple/ml-mobileclip">apple/ml-mobileclip: This repository contains the official implementation of the research paper, "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training" CVPR 2024</a><br/><a href="https://github.com/google-research/omniglue">google-research/omniglue: Code release for CVPR'24 submission 'OmniGlue'</a><br/><a href="https://arxiv.org/pdf/2311.06242">2311.06242</a><br/><a href="https://huggingface.co/spaces/gokaygokay/Florence-2/blob/main/app.py">app.py ¬∑ gokaygokay/Florence-2 at main</a><br/><a href="https://arxiv.org/abs/2405.04408">[2405.04408] DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks</a><br/><a href="https://github.com/ZZZHANG-jx/DocRes/blob/master/inference.py">DocRes/inference.py at master ¬∑ ZZZHANG-jx/DocRes</a><br/><a href="https://arxiv.org/abs/2311.17049">[2311.17049] MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</a><br/><a href="https://github.com/apple/ml-mobileclip?tab=readme-ov-file">apple/ml-mobileclip: This repository contains the official implementation of the research paper, "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training" CVPR 2024</a><br/><a href="https://github.com/mlfoundations/open_clip/tree/main">mlfoundations/open_clip: An open source implementation of CLIP.</a><br/><a href="https://github.com/microsoft/CLAP">microsoft/CLAP: Learning audio concepts from natural language supervision</a><br/><a href="https://github.com/microsoft/CLAP/pulls?q=is%3Apr+is%3Aclosed">Pull requests ¬∑ microsoft/CLAP</a><br/><a href="https://github.com/microsoft/CLAP?tab=readme-ov-file">microsoft/CLAP: Learning audio concepts from natural language supervision</a><br/><a href="https://github.com/microsoft/CLAP/blob/main/examples/esc50_dataset.py">CLAP/examples/esc50_dataset.py at main ¬∑ microsoft/CLAP</a><br/><a href="https://github.com/microsoft/CLAP/tree/main?tab=readme-ov-file">microsoft/CLAP: Learning audio concepts from natural language supervision</a><br/><a href="https://huggingface.co/microsoft/msclap">microsoft/msclap ¬∑ Hugging Face</a><br/><a href="https://github.com/microsoft/CLAP/blob/main/msclap/models/audio.py">CLAP/msclap/models/audio.py at main ¬∑ microsoft/CLAP</a><br/><a href="https://ieeexplore.ieee.org/abstract/document/10095889">CLAP Learning Audio Concepts from Natural Language Supervision | IEEE Conference Publication | IEEE Xplore</a><br/><a href="https://arxiv.org/pdf/2309.05767">2309.05767</a><br/><a href="https://github.com/sooftware/RNN-Transducer/blob/main/rnnt/transducer.py">RNN-Transducer/rnnt/transducer.py at main ¬∑ sooftware/RNN-Transducer</a><br/><a href="https://arxiv.org/abs/1211.3711">[1211.3711] Sequence Transduction with Recurrent Neural Networks</a><br/><a href="https://arxiv.org/pdf/1211.3711">Sequence Transduction with Recurrent Neural Networks</a><br/><a href="https://github.com/ZhengkunTian/rnn-transducer/blob/master/rnnt/model.py">rnn-transducer/rnnt/model.py at master ¬∑ ZhengkunTian/rnn-transducer</a><br/><a href="https://en.wikipedia.org/wiki/How_to_Be_a_Human_Being">How to Be a Human Being - Wikipedia</a><br/><a href="https://www.google.com/search?q=euro+2024+tables&rlz=1C5GCEM_enSE990SE991&oq=euro+2024+tables&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTINCAEQABiRAhiABBiKBTIHCAIQABiABDINCAMQABiRAhiABBiKBTINCAQQABiRAhiABBiKBTIHCAUQABiABDIHCAYQABiABDIHCAcQABiABDIHCAgQABiABDIHCAkQABiABNIBCTI1Nzk4ajBqN6gCALACAA&sourceid=chrome&ie=UTF-8">euro 2024 tables - Google Search</a><br/><a href="https://www.isca-archive.org/interspeech_2023/praveen23b_interspeech.pdf">praveen23b_interspeech.pdf</a><br/><a href="https://www.youtube.com/">YouTube</a><br/><a href="https://www.youtube.com/watch?v=t3r0UD00eT4">3 Important Skills You Need To Work On Every Day - YouTube</a><br/><a href="https://www.inference.org.uk/itprnn/book.pdf">book.pdf</a><br/><a href="https://www.youtube.com/@metrolinamartialarts/videos">Metrolina Martial Arts - YouTube</a><br/><a href="https://www.youtube.com/watch?v=JsUI40uSOTU">Stirling's Incredible Approximation // Gamma Functions, Gaussians, and Laplace's Method - YouTube</a><br/><a href="https://www.youtube.com/watch?v=SI7mn6GU8QQ">Stirling's Approximation - YouTube</a><br/><a href="https://github.com/Zulko/moviepy">Zulko/moviepy: Video editing with Python</a><br/><a href="https://github.com/SRA2/SPELL">SRA2/SPELL: Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection (ECCV 2022)</a><br/><a href="https://github.com/fuankarion/active-speakers-context">fuankarion/active-speakers-context: Code for the Active Speakers in Context Paper (CVPR2020)</a><br/><a href="https://x.com/ymas0315/status/1805531822031257638">„Åæ„Å£„Åô„Éº on X: "Yet another Mamba paper in #INTERSPEECH2024! We explore Mamba's capability in ASR, TTS, SLU, and SUMM on top of ESPnet. Mamba outperforms S4 in the decoder and demonstrates better length generalization and computational efficiency compared to Transformer. https://t.co/yFd6SfopjO" / X</a><br/><a href="https://github.com/espnet/espnet/pulls">Pull requests ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/pull/5780">ESPnet-SPK: add support for multi-task learning by Alexgichamba ¬∑ Pull Request #5780 ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/pull/5696/files">add ASR evaluation for long speech by wyh2000 ¬∑ Pull Request #5696 ¬∑ espnet/espnet</a><br/><a href="https://github.com/qiuqiangkong/audioset_tagging_cnn">qiuqiangkong/audioset_tagging_cnn</a><br/><a href="https://x.com/dankuntz/status/1806058882265231578">Daniel Kuntz on X: "Ted Nelson‚Äôs Xanadu is the "longest-running vaporware project in the history of computing" Basically a bidirectional document linking system - Every character is addressable - Links are nestable - Glanceable high level structure of multiple docs simultaneously https://t.co/QYZZnJ3pAL" / X</a><br/><a href="https://gist.github.com/ldodds/a7f901c7f0118e83a645">How to make your own Xanadu demo</a><br/><a href="https://x.com/fleetwood___/status/1806060910806523946">Fleetwood on X: "Best tiled matmul animation I've found on the internet. Thanks @wentasah https://t.co/qONnxx6ZDl" / X</a><br/><a href="https://www.youtube.com/watch?v=FgrDjsb_kJU">Physics 32.5 Statistical Thermodynamics (7 of 39) Stirling's Approximation Explained - YouTube</a><br/><a href="https://www.youtube.com/watch?v=bzc27UJR_2I">How Professional Spies Learn Languages FAST - YouTube</a><br/><a href="https://x.com/TaraBull808/status/1806075299224060315">TaraBull on X: "Full Hawk Tuah interview for this who missed it https://t.co/MJqhakS6Df" / X</a><br/><a href="https://x.com/TrungTPhan/status/1806066940386983985">Trung Phan on X: "The American (Meme) Dream https://t.co/M4Ybe3oftP" / X</a><br/><a href="https://docs.google.com/spreadsheets/d/1ALHTtsxha2pZgszXHnsT1XcLlo2Mr303Ql_F5pT1Eqs/edit?gid=705467618#gid=705467618">TMH Gpu Access (Responses) - Google Sheets</a><br/><a href="https://kth.diva-portal.org/dream/info.jsf">DiVA</a><br/><a href="https://gits-15.sys.kth.se/tmh/gpu-admin/pull/17">use IP2 on relevant machines by joregan ¬∑ Pull Request #17 ¬∑ tmh/gpu-admin</a><br/><a href="https://huggingface.co/charsiu/g2p_multilingual_byT5_small_100/tree/main">charsiu/g2p_multilingual_byT5_small_100 at main</a><br/><a href="https://github.com/lingjzhu/CharsiuG2P?tab=readme-ov-file">lingjzhu/CharsiuG2P: Multilingual G2P in 100 languages</a><br/><a href="https://github.com/lingjzhu/CharsiuG2P/tree/main/notebooks">CharsiuG2P/notebooks at main ¬∑ lingjzhu/CharsiuG2P</a><br/><a href="https://github.com/lingjzhu/CharsiuG2P/blob/main/data/train/gle.tsv">CharsiuG2P/data/train/gle.tsv at main ¬∑ lingjzhu/CharsiuG2P</a><br/><a href="https://github.com/lingjzhu/CharsiuG2P/blob/main/notebooks/finetuning_low_resource.ipynb">CharsiuG2P/notebooks/finetuning_low_resource.ipynb at main ¬∑ lingjzhu/CharsiuG2P</a><br/><a href="https://github.com/lingjzhu/CharsiuG2P/blob/main/notebooks/train_individual.ipynb">CharsiuG2P/notebooks/train_individual.ipynb at main ¬∑ lingjzhu/CharsiuG2P</a><br/><a href="https://huggingface.co/bilguun/mn-g2p-t5-small/tree/main">bilguun/mn-g2p-t5-small at main</a><br/><a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/NeMoToolKit/tts/g2p.html">Page Not Found</a><br/><a href="https://github.com/JoseLlarena/Britfone/blob/master/britfone.main.3.0.1.csv">Britfone/britfone.main.3.0.1.csv at master ¬∑ JoseLlarena/Britfone</a><br/><a href="https://github.com/google-research/byt5">google-research/byt5</a><br/><a href="https://arxiv.org/pdf/1901.01342">1901.01342</a><br/><a href="https://github.com/kscanne/gbb/blob/main/proofing/diacritics/README.md">gbb/proofing/diacritics/README.md at main ¬∑ kscanne/gbb</a><br/><a href="https://github.com/kscanne/gbb/blob/main/datasets/tuairisc/README.md">gbb/datasets/tuairisc/README.md at main ¬∑ kscanne/gbb</a><br/><a href="https://github.com/kscanne/gbb/blob/main/datasets/charles/README.md">gbb/datasets/charles/README.md at main ¬∑ kscanne/gbb</a><br/><a href="https://github.com/kscanne/gbb/blob/main/classification/sentiment/README.md">gbb/classification/sentiment/README.md at main ¬∑ kscanne/gbb</a><br/><a href="https://colab.research.google.com/drive/1syXmhEQ5s7C59zU8RtHVru0wAvMXTSQ8">ByT5-Finetuning-Datasets.ipynb - Colab</a><br/><a href="https://www.google.com/search?q=huggingface+finetune+t5&rlz=1C5GCEM_enSE990SE991&oq=huggingface+finetune+t5&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABgKGBYYHjIKCAIQABiABBiiBDIKCAMQABiABBiiBNIBCDg5NDVqMGo0qAIAsAIB&sourceid=chrome&ie=UTF-8">huggingface finetune t5 - Google Search</a><br/><a href="https://huggingface.co/docs/transformers/en/model_doc/t5">T5</a><br/><a href="https://huggingface.co/docs/transformers/notebooks">ü§ó Transformers Notebooks</a><br/><a href="chrome://newtab/">New Tab</a><br/><a href="https://www.google.com/search?q=huggingface+fine+tune+t5+on+multiple+tasks&rlz=1C5GCEM_enSE990SE991&oq=huggingface+fine+tune+t5+on+multiple+tasks&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIKCAIQABiABBiiBDIKCAMQABiABBiiBNIBCTEyNTU2ajBqN6gCALACAA&sourceid=chrome&ie=UTF-8">huggingface fine tune t5 on multiple tasks - Google Search</a><br/><a href="https://discuss.huggingface.co/t/t5-finetuning-tips/684/5">T5 Finetuning Tips - Models - Hugging Face Forums</a><br/><a href="https://www.youtube.com/">YouTube</a><br/><a href="https://www.youtube.com/watch?v=wxtJKvp3wP8">‰∫îÊ≠•Êã≥ ¬∑ Wu Bu Quan (5 Stances Beginner Form / 2. Section) - YouTube</a><br/><a href="https://www.youtube.com/watch?v=W6V9yS7nTuM">Reduce Inches in Your Midsection Within 2 Weeks - Dr Alan Mandell, DC - YouTube</a><br/><a href="https://www.youtube.com/watch?v=3qH2OukBdTc">Nuno Bettencourt's Secret Shred Techniques Made Easy - YouTube</a><br/><a href="https://drive.google.com/drive/search?q=owner:me%20(type:application/vnd.google.colaboratory%20||%20type:application/vnd.google.colab)">Search results - Google Drive</a><br/><a href="https://www.google.com/search?q=colab+install+git+lfs&rlz=1C5GCEM_enSE990SE991&oq=colab+install+git+lfs&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRifBTIHCAMQIRifBTIHCAQQIRifBTIHCAUQIRifBTIHCAYQIRifBdIBCDcyNTRqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8">colab install git lfs - Google Search</a><br/><a href="https://www.youtube.com/watch?v=x3NPAxiMRPw">HOW TO GET YOUR FIRST PULL-UP | Most Common Weakpoints, Progression + Accessories - YouTube</a><br/><a href="https://github.com/pytorch/pytorch/issues/104259">ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory ¬∑ Issue #104259 ¬∑ pytorch/pytorch</a><br/><a href="https://stackoverflow.com/questions/55256671/how-to-install-latest-cudnn-to-conda">tensorflow - How to install latest cuDNN to conda? - Stack Overflow</a><br/><a href="https://askubuntu.com/questions/1338314/setting-up-tensorflow-gpu-conda-environment-with-cuda-11-2-and-cudnn-8-1-8-2-cu">nvidia - Setting up tensorflow-GPU Conda environment with CUDA 11.2 and cuDNN 8.1-8.2 (CUDA 460 Driver - Ask Ubuntu</a><br/><a href="https://www.google.com/search?q=fine+tune+byt5+multitask&rlz=1C5GCEM_enSE990SE991&oq=fine+tune+byt5+multitask&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCDgwNjBqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8">fine tune byt5 multitask - Google Search</a><br/><a href="https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/byt5">ByT5</a><br/><a href="https://github.com/google-research/byt5">google-research/byt5</a><br/><a href="https://discuss.huggingface.co/t/finetuning-t5-for-a-task/9558/11">Finetuning T5 for a task - Intermediate - Hugging Face Forums</a><br/><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb#scrollTo=voWiw8C7IrJV">Summarization - Colab</a><br/><a href="https://github.com/loretoparisi/docker/blob/master/fairseq/Dockerfile">docker/fairseq/Dockerfile at master ¬∑ loretoparisi/docker</a><br/><a href="https://donghwa-kim.github.io/pretrain_wav2.html">Fairseq ÏΩîÎìúÎ¶¨Î∑∞ Wav2vec 2.0 (Pretrain)</a><br/><a href="https://github.com/k2-fsa/sherpa-onnx/blob/master/python-api-examples/audio-tagging-from-a-file.py">sherpa-onnx/python-api-examples/audio-tagging-from-a-file.py at master ¬∑ k2-fsa/sherpa-onnx</a><br/><a href="https://github.com/pytorch/audio?tab=readme-ov-file">pytorch/audio: Data manipulation and transformation for audio signal processing, powered by PyTorch</a><br/><a href="https://github.com/pytorch/audio/pulls?page=3&q=is%3Apr+is%3Aclosed">Pull requests ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/pull/3670">Fix cuctc module by moto-meta ¬∑ Pull Request #3670 ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/tree/main/src">audio/src at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/tree/main/src/torchaudio">audio/src/torchaudio at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/tree/main/src/torchaudio/datasets">audio/src/torchaudio/datasets at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/blob/main/src/torchaudio/datasets/cmudict.py">audio/src/torchaudio/datasets/cmudict.py at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/tree/main/src/torchaudio/models">audio/src/torchaudio/models at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/blob/main/src/torchaudio/models/rnnt.py">audio/src/torchaudio/models/rnnt.py at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/blob/main/src/torchaudio/models/wav2vec2/wavlm_attention.py">audio/src/torchaudio/models/wav2vec2/wavlm_attention.py at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/blob/main/src/torchaudio/models/conformer.py">audio/src/torchaudio/models/conformer.py at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/impl.py">audio/src/torchaudio/pipelines/_wav2vec2/impl.py at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/tree/main/src/torchaudio/transforms">audio/src/torchaudio/transforms at main ¬∑ pytorch/audio</a><br/><a href="https://github.com/pytorch/audio/tree/main/src/torio">audio/src/torio at main ¬∑ pytorch/audio</a><br/><a href="https://pytorch.org/audio/main/generated/torchaudio.functional.forced_align.html">torchaudio.functional.forced_align ‚Äî Torchaudio 2.4.0.dev20240628 documentation</a><br/><a href="https://pytorch.org/audio/main/kaldi_io.html">torchaudio.kaldi_io ‚Äî Torchaudio 2.4.0.dev20240628 documentation</a><br/><a href="https://pytorch.org/audio/main/datasets.html">torchaudio.datasets ‚Äî Torchaudio 2.4.0.dev20240628 documentation</a><br/><a href="https://zenodo.org/records/4660670#.ZBtWPOxuerN">DAPS (Device and Produced Speech) Dataset</a><br/><a href="https://pytorch.org/audio/main/">Torchaudio Documentation ‚Äî Torchaudio 2.4.0.dev20240628 documentation</a><br/><a href="https://github.com/espnet/espnet/commits/master/">Commits ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/pulls">Pull requests ¬∑ espnet/espnet</a><br/><a href="https://github.com/espnet/espnet/blob/master/espnet2/asr/layers/cgmlp.py">espnet/espnet2/asr/layers/cgmlp.py at master ¬∑ espnet/espnet</a><br/><a href="https://arxiv.org/pdf/2309.14922">2309.14922</a><br/><a href="https://huggingface.co/blog/finetune-florence2">Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models</a><br/><a href="https://colab.research.google.com/drive/1hKDrJ5AH_o7I95PtZ9__VlCTNAo1Gjpf?usp=sharing#scrollTo=CpCRb7PfYZP-">Fine tune Florence-2.ipynb - Colab</a><br/><a href="https://github.com/huggingface">Hugging Face</a><br/><a href="https://github.com/huggingface/transformers/pull/31506">Add Florence2 support by D4ve-R ¬∑ Pull Request #31506 ¬∑ huggingface/transformers</a><br/><a href="https://huggingface.co/danelcsb/Florence-2-FT-cavity/tree/main">danelcsb/Florence-2-FT-cavity at main</a><br/><a href="https://github.com/SangbumChoi/florence2-finetuning/blob/main/train_object_detection.py">florence2-finetuning/train_object_detection.py at main ¬∑ SangbumChoi/florence2-finetuning</a><br/><a href="https://github.com/SangbumChoi/florence2-finetuning/blob/main/train_object_detection.py">florence2-finetuning/train_object_detection.py at main ¬∑ SangbumChoi/florence2-finetuning</a><br/><a href="https://huggingface.co/microsoft/Florence-2-large/blob/main/modeling_florence2.py">modeling_florence2.py ¬∑ microsoft/Florence-2-large at main</a><br/><a href="https://github.com/huggingface/transformers/tree/main/notebooks">transformers/notebooks at main ¬∑ huggingface/transformers</a><br/><a href="https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb">notebooks/examples/video_classification.ipynb at main ¬∑ huggingface/notebooks</a><br/><a href="https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb">notebooks/examples/semantic_segmentation.ipynb at main ¬∑ huggingface/notebooks</a><br/><a href="https://github.com/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb">notebooks/examples/speech_recognition.ipynb at main ¬∑ huggingface/notebooks</a><br/><a href="https://huggingface.co/spaces/annadeichler/CSMP-diffusion">CSMP Gesture - a Hugging Face Space by annadeichler</a><br/><a href="https://github.com/bytedance/SALMONN/tree/main">bytedance/SALMONN: SALMONN: Speech Audio Language Music Open Neural Network</a><br/><a href="https://huggingface.co/tsinghua-ee/SALMONN">tsinghua-ee/SALMONN ¬∑ Hugging Face</a><br/><a href="https://arxiv.org/abs/2212.09058">[2212.09058] BEATs: Audio Pre-Training with Acoustic Tokenizers</a><br/><a href="https://github.com/microsoft/unilm/tree/master/beats">unilm/beats at master ¬∑ microsoft/unilm</a><br/><a href="https://arxiv.org/abs/2308.11683">[2308.11683] Learning to generate and corr- uh I mean repair language in real-time</a><br/><a href="https://www.google.com/maps/dir/KTH+Speech,+Music+and+Hearing,+Lindstedtsv%C3%A4gen+24,+114+28+Stockholm,+Sweden/Studentbacken+25,+115+57+Stockholm,+Sweden/@59.3450789,18.073463,15z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x465f9d4013dc1941:0x7f16100282386d0c!2m2!1d18.0748591!2d59.348301!1m5!1m1!1s0x465f9d379eff696f:0x3d118d72ff7ebe42!2m2!1d18.0962342!2d59.3490682!3e3?entry=ttu">Division of Speech, Music and Hearing - KTH to Studentbacken 25 - Google Maps</a><br/><a href="https://www.theguardian.com/football/live/2024/jul/01/portugal-v-slovenia-euro-2024-last-16-live">(2) Portugal 0-0 Slovenia (pens: 3-0): Euro 2024, last 16 ‚Äì live reaction</a><br/><a href="https://www.instagram.com/">Instagram</a><br/><a href="https://www.ladbible.com/news/us-news/hawk-tuah-girl-viral-interview-podcast-946080-20240701">Hawk Tuah girl speaks out for the first time and confirms her future plans</a><br/><a href="chrome://newtab/">New Tab</a><br/><a href="http://192.168.0.100:1234/storage/emulated/0/Pictures/SneakyCamera/">SneakyCamera - moto g(7) power - WiFi File Transfer</a><br/><a href="https://topup.eir.ie/EirExternalWeb/quickTopupThreeDSVerified.do?PaymentCorrelationID=12U6ZSRW0N8I">Quick Top Up</a><br/><a href="https://x.com/ActualNames1">(1) Actual Names (@ActualNames1) / X</a><br/><a href="https://x.com/PiotrZelasko/status/1808115109841256728/photo/1">(1) Home / X</a><br/><a href="https://arxiv.org/pdf/2406.19674">Less is More: Accurate Speech Recognition & Translation without Web-Scale Data</a><br/><a href="https://arxiv.org/abs/2406.19674">[2406.19674] Less is More: Accurate Speech Recognition & Translation without Web-Scale Data</a><br/><a href="https://github.com/merveenoyan/example_notebooks/blob/main/RT_DETR_Notebook.ipynb">example_notebooks/RT_DETR_Notebook.ipynb at main ¬∑ merveenoyan/example_notebooks</a><br/><a href="https://github.com/NVIDIA/NeMo/pull/9474">[do-not-merge] SpeechLLM dev branch by pzelasko ¬∑ Pull Request #9474 ¬∑ NVIDIA/NeMo</a><br/><a href="https://github.com/NVIDIA/NeMo/pull/9409">Enable encoder adapters for Canary and MultiTaskAED models by titu1994 ¬∑ Pull Request #9409 ¬∑ NVIDIA/NeMo</a><br/><a href="https://github.com/NVIDIA/NeMo/blob/2bb5e4700dc8020534f62ec0959e384ba7ce5c9c/tutorials/asr/README.md">NeMo/tutorials/asr/README.md at 2bb5e4700dc8020534f62ec0959e384ba7ce5c9c ¬∑ NVIDIA/NeMo</a><br/><a href="https://github.com/NVIDIA/NeMo/pull/9584">Adding support for mcore T5 PEFT by huvunvidia ¬∑ Pull Request #9584 ¬∑ NVIDIA/NeMo</a><br/><a href="https://chatgpt.com/c/c04cee8a-56e6-4d1d-9e64-767a5bee0424">ChatGPT</a><br/><a href="https://huggingface.co/blog/finetune-florence2">Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models</a><br/><a href="https://huggingface.co/blog?p=4">Hugging Face ‚Äì Blog</a><br/><a href="https://huggingface.co/blog/asr-diarization">Powerful ASR + diarization + speculative decoding with Hugging Face Inference Endpoints</a><br/><a href="https://huggingface.co/blog/idefics2">Introducing Idefics2: A Powerful 8B Vision-Language Model for the community</a><br/><a href="https://huggingface.co/blog/vlms">Vision Language Models Explained</a><br/><a href="https://huggingface.co/01-ai/Yi-VL-34B">01-ai/Yi-VL-34B ¬∑ Hugging Face</a><br/><a href="https://github.com/microsoft/unilm/tree/master/kosmos-2#training">unilm/kosmos-2 at master ¬∑ microsoft/unilm</a><br/><a href="https://huggingface.co/llava-hf/llava-v1.6-34b-hf">llava-hf/llava-v1.6-34b-hf ¬∑ Hugging Face</a><br/><a href="https://llava-vl.github.io/">LLaVA</a><br/><a href="https://huggingface.co/NousResearch">NousResearch (NousResearch)</a><br/><a href="https://nousresearch.com/releases/">Releases - NOUS RESEARCH</a><br/><a href="https://huggingface.co/Systran/faster-whisper-large-v3">Systran/faster-whisper-large-v3 ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/blog/train-sentence-transformers">Training and Finetuning Embedding Models with Sentence Transformers v3</a><br/><a href="https://github.com/lhotse-speech/lhotse/pulls?q=is%3Apr+is%3Aclosed">Pull requests ¬∑ lhotse-speech/lhotse</a><br/><a href="https://github.com/lhotse-speech/lhotse/pull/1365">Add GigaSpeech 2 recipe by yfyeung ¬∑ Pull Request #1365 ¬∑ lhotse-speech/lhotse</a><br/><a href="https://github.com/lhotse-speech/lhotse/pull/1353/files">Add KsponSpeech recipe by whsqkaak ¬∑ Pull Request #1353 ¬∑ lhotse-speech/lhotse</a><br/><a href="https://github.com/lhotse-speech/lhotse/pull/1330">Add the ReazonSpeech recipe by Triplecq ¬∑ Pull Request #1330 ¬∑ lhotse-speech/lhotse</a><br/><a href="https://github.com/lhotse-speech/lhotse/blob/master/lhotse/recipes/this_american_life.py">lhotse/lhotse/recipes/this_american_life.py at master ¬∑ lhotse-speech/lhotse</a><br/><a href="https://github.com/pytorch/pytorch/blob/main/Dockerfile">pytorch/Dockerfile at main ¬∑ pytorch/pytorch</a><br/><a href="https://icml.cc/virtual/2024/papers.html?filter=titles">ICML 2024 Papers</a><br/><a href="https://openreview.net/group?id=ICML.cc/2024/Conference#tab-accept-oral">ICML 2024 Conference | OpenReview</a><br/><a href="https://openreview.net/attachment?id=kAfYYg6PX8&name=pdf">Listenable Maps for Audio Classifiers</a><br/><a href="https://icml.cc/virtual/2024/papers.html?filter=titles&search=speech">ICML 2024 Papers</a><br/><a href="https://icml.cc/virtual/2024/poster/33487">ICML Poster Speech Self-Supervised Learning Using Diffusion Model Synthetic Data</a><br/><a href="https://icml.cc/virtual/2024/poster/35635">ICML Poster Scaling Speech Technology to 1,000+ Languages</a><br/><a href="https://github.com/the-anonymous-bs/av-SALMONN/tree/main/video_llama/common">av-SALMONN/video_llama/common at main ¬∑ the-anonymous-bs/av-SALMONN</a><br/><a href="https://github.com/bytedance/SALMONN">bytedance/SALMONN: SALMONN: Speech Audio Language Music Open Neural Network</a><br/><a href="https://huggingface.co/tsinghua-ee/SALMONN-7B">tsinghua-ee/SALMONN-7B ¬∑ Hugging Face</a><br/><a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">LAVIS/projects/blip2 at main ¬∑ salesforce/LAVIS</a><br/><a href="https://github.com/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb">LAVIS/examples/blip2_instructed_generation.ipynb at main ¬∑ salesforce/LAVIS</a><br/><a href="https://github.com/ViTAE-Transformer/QFormer">ViTAE-Transformer/QFormer: The official repo for [TPAMI'23] "Vision Transformer with Quadrangle Attention"</a><br/><a href="https://deerajmanjaray.medium.com/querying-transformer-q-former-in-blip-2-improves-image-text-generation-in-e-commerce-applications-2d2402cb93e3">Querying Transformer (Q-Former) in BLIP-2 improves Image-Text Generation in E-Commerce Applications | by Deeraj Manjaray | Medium</a><br/><a href="https://huggingface.co/tsinghua-ee/SALMONN-7B">tsinghua-ee/SALMONN-7B ¬∑ Hugging Face</a><br/><a href="https://huggingface.co/spaces/tsinghua-ee/SALMONN-7B-gradio/blob/main/qformer/Qformer.py">qformer/Qformer.py ¬∑ tsinghua-ee/SALMONN-7B-gradio at main</a><br/><a href="https://arxiv.org/pdf/2311.11745">ELF: Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis</a><br/><a href="https://www.youtube.com/watch?v=8S99iQH2Rvg">Ryan Reynolds & Hugh Jackman Interview Each Other | PEOPLE - YouTube</a><br/><a href="https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip2_models/blip2_qformer.py">LAVIS/lavis/models/blip2_models/blip2_qformer.py at main ¬∑ salesforce/LAVIS</a><br/><a href="https://blog.salesforceairesearch.com/blip-2/">BLIP-2: Scalable Multimodal Pre-training Method</a><br/><a href="https://arxiv.org/pdf/2301.12597">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a><br/><a href="https://www.youtube.com/watch?v=HqwMLCh8RnE">Who Was The Best English Monarch? David Mitchell Rates The Royals! - YouTube</a><br/><a href="https://www.psychologytoday.com/intl/blog/unlocking-shame/202310/three-steps-to-healing-shame-and-trauma">Three Steps to Healing Shame (and Trauma) | Psychology Today</a><br/><a href="https://github.com/JonathanCrabbe/FourierDiffusion/blob/main/src/fdiff/utils/extraction.py">FourierDiffusion/src/fdiff/utils/extraction.py at main ¬∑ JonathanCrabbe/FourierDiffusion</a><br/><a href="https://arxiv.org/pdf/2402.05933">eri</a><br/><a href="https://github.com/JonathanCrabbe/FourierDiffusion/commit/db83cf632c85cf14444420f54cadb186ab94ba7f">Remove all obsolete DDPM dependencies ¬∑ JonathanCrabbe/FourierDiffusion@db83cf6</a><br/><a href="https://arxiv.org/abs/2303.13285">[2303.13285] Fourier Diffusion Models: A Method to Control MTF and NPS in Score-Based Stochastic Image Generation</a><br/><a href="https://arxiv.org/pdf/2303.13285">2303.13285</a><br/><a href="https://github.com/GuyTevet/motion-diffusion-model/tree/main">GuyTevet/motion-diffusion-model: The official PyTorch implementation of the paper "Human Motion Diffusion Model"</a><br/><a href="https://github.com/GuyTevet/MotionCLIP/tree/main?tab=readme-ov-file">GuyTevet/MotionCLIP: Official Pytorch implementation of the paper "MotionCLIP: Exposing Human Motion Generation to CLIP Space"</a><br/><a href="https://guytevet.github.io/motionclip-page/">MotionCLIP: Exposing Human Motion Generation to CLIP Space</a><br/><a href="https://github.com/ga642381/speech-trident?tab=readme-ov-file">ga642381/speech-trident: Awesome speech/audio LLMs, representation learning, and codec models</a><br/><a href="https://arxiv.org/abs/2310.02720">[2310.02720] Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction</a><br/><a href="https://arxiv.org/pdf/2310.16338">2310.16338</a><br/><a href="https://github.com/cwx-worst-one/EAT?tab=readme-ov-file">cwx-worst-one/EAT: [IJCAI 2024] EAT: Self-Supervised Pre-Training with Efficient Audio Transformer</a><br/><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mr_hubert">fairseq/examples/mr_hubert at main ¬∑ facebookresearch/fairseq</a><br/><a href="https://github.com/s3prl/s3prl/pull/517/files#diff-b9d553dce0a0be4ef949272f048c1141098b0b6de2e4fd810c87851d7b12d778">Add Multiresolution HuBERT as an additional upstream model by ftshijt ¬∑ Pull Request #517 ¬∑ s3prl/s3prl</a><br/><a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mr_hubert">fairseq/examples/mr_hubert at main ¬∑ facebookresearch/fairseq</a><br/><a href="https://arxiv.org/pdf/2407.01911">Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model</a><br/><a href="https://huggingface.co/datasets/YuKuanFu/Podcast_Dialogue">YuKuanFu/Podcast_Dialogue ¬∑ Datasets at Hugging Face</a><br/><a href="https://x.com/home">(1) Home / X</a><br/><a href="https://x.com/rowancheung/status/1810183283986067922">(1) Rowan Cheung on X: "üö® Chinese AI company SenseTime just revealed SenseNova 5.5, an AI model that claims to beat GPT-4o across key metrics Plus, big developments from Apple, YouTube, KLING, Neuralink, and Google DeepMind. Here's everything going on in AI right now:" / X</a><br/><a href="https://x.com/rowancheung/status/1810183456581693749">(1) Rowan Cheung on X: "Google DeepMind researchers published new research introducing JEST. It's a new method that accelerates AI model training while significantly reducing computing requirements. Faster training capabilities = the acceleration of advanced model releases is just getting started https://t.co/LVSDC2xQfi" / X</a><br/><a href="https://x.com/nealmohan/status/1808587459132825844">(1) Neal Mohan on X: "Good news creators: our updated Erase Song tool helps you easily remove copyright-claimed music from your video (while leaving the rest of your audio intact). Learn more‚Ä¶ https://t.co/KeWIw3RFeH" / X</a><br/><a href="https://arxiv.org/pdf/2407.01178">2407.01178</a><br/><a href="https://github.com/google-research/big_vision/pull/103/commits/d2c2aff360c7300e316843ec27f6193f28e86108">PaliGemma model release by akolesnikoff ¬∑ Pull Request #103 ¬∑ google-research/big_vision</a><br/><a href="https://github.com/google-research/big_vision/commits/main/">Commits ¬∑ google-research/big_vision</a><br/><a href="https://www.instagram.com/">Instagram</a><br/><a href="https://www.theguardian.com/lifeandstyle/article/2024/jul/07/the-moment-i-knew-we-could-barely-make-eye-contact-because-of-the-chemistry-radiating-between-us?utm_source=instagramstories&utm_campaign=themomentiknewinsta">The moment I knew: we could barely make eye contact because of the chemistry radiating between us | Relationships | The Guardian</a><br/><a href="undefined"></a><br/></body></html>