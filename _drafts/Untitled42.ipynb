{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_TUT3ZmAkUWD"
      },
      "outputs": [],
      "source": [
        "input_textgrid = \"/content/alcott-male-kobietki_001_rozdzial-i-pielgrzymki.textgrid\" # @param {\"type\":\"string\"}\n",
        "input_json = \"/content/alcott-male-kobietki_001_rozdzial-i-pielgrzymki.json\" # @param {\"type\":\"string\"}\n",
        "pronounce_as_tsv = \"/content/pronounce-as.tsv\" # @param {\"type\":\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install praatio"
      ],
      "metadata": {
        "id": "80w6mV8fkl0K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read huggingface json\n",
        "import json\n",
        "\n",
        "def read_huggingface_json(input_json):\n",
        "    with open(input_json, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    words = []\n",
        "    for chunk in data[\"chunks\"]:\n",
        "        words.append((chunk[\"timestamp\"][0], chunk[\"timestamp\"][1], chunk[\"text\"]))\n",
        "    return words"
      ],
      "metadata": {
        "id": "ue_gA9c2kqfV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read pronounce-as data\n",
        "def read_pronounce_as(pronounce_as_tsv):\n",
        "    pronounce_as = {}\n",
        "    with open(pronounce_as_tsv, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.strip().split('\\t')\n",
        "            word = line[0].lower()\n",
        "            if not word in pronounce_as:\n",
        "                pronounce_as[word] = set()\n",
        "            pronounce_as[word].add(line[1])\n",
        "    if len(pronounce_as) == 0:\n",
        "        return None\n",
        "    return pronounce_as"
      ],
      "metadata": {
        "id": "3_3IXZjjn8kz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read praat utterances\n",
        "from praatio import textgrid\n",
        "\n",
        "def read_praat_utterances(input_textgrid):\n",
        "    utterances = []\n",
        "    tg = textgrid.openTextgrid(input_textgrid, includeEmptyIntervals=False)\n",
        "\n",
        "    for tmp_tier in tg.tiers:\n",
        "        if tmp_tier.name == \"utterances\":\n",
        "            tier = tmp_tier\n",
        "            break\n",
        "\n",
        "    for interval in tier.entries:\n",
        "        if interval.label.strip() == \"\":\n",
        "            continue\n",
        "        utterances.append((interval.start, interval.end, interval.label))\n",
        "    return utterances"
      ],
      "metadata": {
        "id": "e6xXXl_eqL5c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utterances = read_praat_utterances(input_textgrid)\n",
        "words = read_huggingface_json(input_json)\n",
        "pronounce_as = read_pronounce_as(pronounce_as_tsv)"
      ],
      "metadata": {
        "id": "8WAcce3Arazh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_by_start = {x[0]: x for x in words}\n",
        "words_by_end = {x[1]: x for x in words}"
      ],
      "metadata": {
        "id": "CpT3vu1SsZ0i"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_words_by_utterances(words, utterances):\n",
        "\n",
        "    collected = []\n",
        "    new_utterances = []\n",
        "    for utterance in utterances:\n",
        "        start = utterance[0]\n",
        "        end = utterance[1]\n",
        "\n",
        "        ut_dict = {\n",
        "            \"start\": utterance[0],\n",
        "            \"end\": utterance[1],\n",
        "            \"text\": utterance[2],\n",
        "            \"words\": []\n",
        "        }\n",
        "        for word in words:\n",
        "            if word[0] < start:\n",
        "                if word[1] > start:\n",
        "                    ut_dict[\"maybe_start\"] = word\n",
        "                continue\n",
        "            elif word[1] > end:\n",
        "                if word[0] < end:\n",
        "                    ut_dict[\"maybe_end\"] = word\n",
        "                break\n",
        "            elif word[0] >= start and word[1] <= end:\n",
        "                ut_dict[\"words\"].append(word)\n",
        "                collected.append(word)\n",
        "            else:\n",
        "                print(word)\n",
        "        new_utterances.append(ut_dict)\n",
        "\n",
        "    not_collected = []\n",
        "    for word in words:\n",
        "        if word not in collected:\n",
        "            not_collected.append(word)\n",
        "\n",
        "    return new_utterances, not_collected"
      ],
      "metadata": {
        "id": "OeGjX1DEshAx"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped, uncollected = group_words_by_utterances(words, utterances)"
      ],
      "metadata": {
        "id": "SrO-JcaPuSnl"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalised_words(text):\n",
        "    text = text.lower()\n",
        "    words = [word.strip(\",;:!?—…„”\\\"“.«»*()[]‘/\\\\\") for word in text.split()]\n",
        "    words = [x for x in words if x != \"\"]\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "3bk7KP98xhal"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in grouped:\n",
        "    item[\"normalised_words\"] = get_normalised_words(item[\"text\"])"
      ],
      "metadata": {
        "id": "V_OeVNwfyscW"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leftover = ''\n",
        "for item in grouped:\n",
        "    w2v_words = [x[2] for x in item[\"words\"]]\n",
        "    if leftover != \"\":\n",
        "        w2v_words = [leftover] + w2v_words\n",
        "        item[\"maybe_start_word\"] = leftover\n",
        "        leftover = ''\n",
        "    elif \"maybe_start\" in item:\n",
        "        if item[\"maybe_start\"][2].endswith(item[\"normalised_words\"][0]):\n",
        "            w2v_words = [item[\"normalised_words\"][0]] + w2v_words\n",
        "            item[\"maybe_start_word\"] = item[\"normalised_words\"][0]\n",
        "    if \"maybe_end\" in item:\n",
        "        if item[\"maybe_end\"][2].startswith(item[\"normalised_words\"][-1]):\n",
        "            item[\"maybe_end_word\"] = item[\"normalised_words\"][-1]\n",
        "            w2v_words = w2v_words + [item[\"normalised_words\"][-1]]\n",
        "            leftover = item[\"maybe_end\"][2][len(item[\"maybe_end_word\"]):]\n",
        "    if w2v_words == item[\"normalised_words\"]:\n",
        "        item[\"ok\"] = True"
      ],
      "metadata": {
        "id": "4IjgHGfF0PWP"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped[-20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98TSs6c59jPg",
        "outputId": "e9871059-3705-4cf0-89a9-9cee38400086"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'start': 1337.8755,\n",
              " 'end': 1349.0268,\n",
              " 'text': 'Gdyby nie to, że jestem za duża na takie rzeczy, chętnie bym się jeszcze w to bawiła — rzekła Amelka, która zaczynała mówić o porzuceniu dziecinnych rozrywek w poważnym wieku lat dwunastu.',\n",
              " 'words': [(1337.96, 1338.18, 'gdyby'),\n",
              "  (1338.22, 1338.32, 'nie'),\n",
              "  (1338.38, 1338.46, 'to'),\n",
              "  (1338.56, 1338.62, 'że'),\n",
              "  (1338.66, 1338.9, 'jestem'),\n",
              "  (1338.96, 1339.04, 'za'),\n",
              "  (1339.08, 1339.3, 'duża'),\n",
              "  (1339.36, 1339.44, 'na'),\n",
              "  (1339.48, 1339.68, 'takie'),\n",
              "  (1339.72, 1340.0, 'rzeczy'),\n",
              "  (1340.28, 1340.6, 'chętnie'),\n",
              "  (1340.62, 1340.74, 'bym'),\n",
              "  (1340.8, 1340.88, 'się'),\n",
              "  (1340.92, 1341.1, 'jeszcze'),\n",
              "  (1341.14, 1341.26, 'wto'),\n",
              "  (1341.3, 1341.66, 'bawiła'),\n",
              "  (1342.5, 1342.78, 'rzekła'),\n",
              "  (1342.84, 1343.26, 'amelka'),\n",
              "  (1343.32, 1343.56, 'która'),\n",
              "  (1343.64, 1344.06, 'zaczynała'),\n",
              "  (1344.12, 1344.4, 'mówić'),\n",
              "  (1344.5, 1344.52, 'o'),\n",
              "  (1344.58, 1345.1, 'porzuceniu'),\n",
              "  (1345.18, 1345.68, 'dziecinnych'),\n",
              "  (1345.74, 1346.22, 'rozrywek'),\n",
              "  (1346.5, 1346.52, 'w'),\n",
              "  (1346.6, 1347.12, 'poważnym'),\n",
              "  (1347.22, 1347.48, 'wieku'),\n",
              "  (1347.54, 1347.74, 'lat')],\n",
              " 'maybe_end': (1348.02, 1349.78, 'dwunastunie'),\n",
              " 'normalised_words': ['gdyby',\n",
              "  'nie',\n",
              "  'to',\n",
              "  'że',\n",
              "  'jestem',\n",
              "  'za',\n",
              "  'duża',\n",
              "  'na',\n",
              "  'takie',\n",
              "  'rzeczy',\n",
              "  'chętnie',\n",
              "  'bym',\n",
              "  'się',\n",
              "  'jeszcze',\n",
              "  'w',\n",
              "  'to',\n",
              "  'bawiła',\n",
              "  'rzekła',\n",
              "  'amelka',\n",
              "  'która',\n",
              "  'zaczynała',\n",
              "  'mówić',\n",
              "  'o',\n",
              "  'porzuceniu',\n",
              "  'dziecinnych',\n",
              "  'rozrywek',\n",
              "  'w',\n",
              "  'poważnym',\n",
              "  'wieku',\n",
              "  'lat',\n",
              "  'dwunastu'],\n",
              " 'maybe_end_word': 'dwunastu'}"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "# run a sequence matcher, but filter for the common\n",
        "# case of a difference in spacing.\n",
        "# Also, extract the relevant pieces.\n",
        "def filter_smatcher(ref, hyp):\n",
        "    items = []\n",
        "\n",
        "    s = SequenceMatcher(None, ref, hyp)\n",
        "    for opcode in s.get_opcodes():\n",
        "        if opcode[0] == 'equal':\n",
        "            items.append((\"OK\", ref[opcode[1]:opcode[2]]))\n",
        "        elif opcode[0] == 'insert':\n",
        "            items.append((\"INSERT\", hyp[opcode[3]:opcode[4]]))\n",
        "        elif opcode[0] == 'delete':\n",
        "            items.append((\"DELETE\", ref[opcode[1]:opcode[2]]))\n",
        "        elif opcode[0] == 'replace':\n",
        "            left = ref[opcode[1]:opcode[2]]\n",
        "            right = hyp[opcode[3]:opcode[4]]\n",
        "            if \"\".join(left) == \"\".join(right):\n",
        "                items.append((\"OK\", left))\n",
        "            else:\n",
        "                items.append((\"REPLACE\", left, right))\n",
        "    return items"
      ],
      "metadata": {
        "id": "lWe8i1Vn9gpc"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_filter_smatcher(item):\n",
        "    a = item[\"normalised_words\"]\n",
        "    b = [x[2] for x in item[\"words\"]]\n",
        "    if \"maybe_start_word\" in item:\n",
        "        b = [item[\"maybe_start_word\"]] + b\n",
        "    if \"maybe_end_word\" in item:\n",
        "        b = b + [item[\"maybe_end_word\"]]\n",
        "    return filter_smatcher(a, b)"
      ],
      "metadata": {
        "id": "yrvEa8GUAnzF"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_filter_smatcher(grouped[12])\n",
        "#grouped[12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK5l34j-BIX5",
        "outputId": "06fcd98b-c5d8-4e0d-f040-c08f6cd7c592"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('OK', ['bardzo', 'jest']),\n",
              " ('OK', ['nieładnie']),\n",
              " ('OK',\n",
              "  ['że',\n",
              "   'niektóre',\n",
              "   'dziewczęta',\n",
              "   'mają',\n",
              "   'mnóstwo',\n",
              "   'pięknych',\n",
              "   'rzeczy',\n",
              "   'a']),\n",
              " ('REPLACE', ['inne'], ['inny']),\n",
              " ('OK', ['nie', 'mają', 'nic', 'dodała', 'amelka', 'z', 'gniewną', 'minką'])]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    }
  ]
}