{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtW5rX6n2WjU",
        "outputId": "25779c93-0346-4495-8709-b4e5d5fbb22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynini\n",
            "  Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading pynini-2.1.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (165.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.5/165.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynini\n",
            "Successfully installed pynini-2.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install pynini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cldr2fst.py\n",
        "# Requires: pynini (which depends on OpenFst)\n",
        "#   pip install pynini==2.1.5\n",
        "#   (or a version compatible with your platform)\n",
        "#\n",
        "# Usage example (see bottom of file):\n",
        "#   python cldr2fst.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import pynini\n",
        "from pynini import *\n",
        "from pynini.lib import pynutil\n",
        "from pynini.lib import utf8\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Utilities\n",
        "# -------------------------------\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes like \\\\u0259 and \\\\U0001F600.\"\"\"\n",
        "    def rpl4(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    def rpl8(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    # Keep other backslashes as literals (CLDR has many constructs; we stay conservative)\n",
        "    return s\n",
        "\n",
        "def _expand_char_class(cls: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Expand a simple bracket class like [abc] or [a-z].\n",
        "    This is intentionally minimal; it does not handle nested classes, properties, or set ops.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(cls):\n",
        "        if i + 2 < len(cls) and cls[i+1] == \"-\":\n",
        "            start = ord(cls[i])\n",
        "            end = ord(cls[i+2])\n",
        "            for cp in range(start, end + 1):\n",
        "                out.append(chr(cp))\n",
        "            i += 3\n",
        "        else:\n",
        "            out.append(cls[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "def _tokenize_pattern(pat: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a very small subset of CLDR pattern syntax:\n",
        "    - Literal characters\n",
        "    - Character classes [..]\n",
        "    Returns a list of alternatives (strings) if the pattern is a single char-class;\n",
        "    otherwise returns [pat] (treated as a literal string).\n",
        "    \"\"\"\n",
        "    pat = pat.strip()\n",
        "    if len(pat) >= 2 and pat[0] == \"[\" and pat[-1] == \"]\":\n",
        "        inner = _decode_escapes(pat[1:-1])\n",
        "        alts = _expand_char_class(inner)\n",
        "        return alts\n",
        "    # Otherwise we treat it as a literal sequence (already escape-decoded)\n",
        "    return [_decode_escapes(pat)]\n",
        "\n",
        "def _string_map(pairs: List[Tuple[str, str]]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a union of literal string transductions, determinize & minimize.\n",
        "    \"\"\"\n",
        "    # Pynini.string_map can take a dict or list of pairs\n",
        "    t = pynini.string_map(pairs)\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Parsing CLDR rules (subset)\n",
        "# -------------------------------\n",
        "\n",
        "_RULE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<lhs>.+?)\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?)\n",
        "    ;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    lhs: str\n",
        "    rhs: str\n",
        "    op: str  # \">\", \"<\", \"<>\"\n",
        "\n",
        "def parse_cldr_rules_simple(text: str) -> List[Rule]:\n",
        "    \"\"\"\n",
        "    Parse a subset of CLDR rule lines:\n",
        "      LHS > RHS ;\n",
        "      LHS < RHS ;\n",
        "      LHS <> RHS ;\n",
        "    Strips comments (# ...) and blank lines.\n",
        "    Ignores directives (:: ... ;)\n",
        "    \"\"\"\n",
        "    rules: List[Rule] = []\n",
        "    for raw in text.splitlines():\n",
        "        line = raw.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        if line.startswith(\"::\"):\n",
        "            # ignore directives in this simple converter\n",
        "            continue\n",
        "        m = _RULE_RE.match(line)\n",
        "        if not m:\n",
        "            # Not supported yet; skip quietly so you can still test quickly.\n",
        "            continue\n",
        "        lhs = m.group(\"lhs\").strip()\n",
        "        rhs = m.group(\"rhs\").strip()\n",
        "        op = m.group(\"op\")\n",
        "        rules.append(Rule(lhs=lhs, rhs=rhs, op=op))\n",
        "    return rules\n",
        "\n",
        "\n",
        "\n",
        "def build_transducer_from_rules(rules: List[Rule]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a transducer from simple (context-free) CLDR rules.\n",
        "    Uses cdrewrite with utf8.VALID_UTF8_STAR so non-matching chars pass through.\n",
        "    \"\"\"\n",
        "    # Collect simple pair mappings (after class expansion).\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "    for r in rules:\n",
        "        lhs_alts = _tokenize_pattern(r.lhs)\n",
        "        rhs_alts = _tokenize_pattern(r.rhs)\n",
        "\n",
        "        def add_pairs(A: List[str], B: List[str]):\n",
        "            if len(A) == len(B):\n",
        "                for x, y in zip(A, B):\n",
        "                    pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "            else:\n",
        "                for x in A:\n",
        "                    for y in B:\n",
        "                        pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "\n",
        "        if r.op == \">\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "        elif r.op == \"<\":\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "        elif r.op == \"<>\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "\n",
        "    # If nothing parsed, just return identity over UTF-8.\n",
        "    if not pairs:\n",
        "        return pynini.cdrewrite(pynini.cross(\"\", \"\"), \"\", \"\", utf8.VALID_UTF8_STAR)\n",
        "\n",
        "    # Build a cascade of context-free rewrites; this naturally preserves\n",
        "    # all other symbols (no need for a hand-built sigma or lenient trick).\n",
        "    t = None\n",
        "    for i, (src, tgt) in enumerate(pairs):\n",
        "        rule = pynini.cdrewrite(\n",
        "            pynini.cross(src, tgt),  # rewrite src -> tgt\n",
        "            \"\",                      # left context (none)\n",
        "            \"\",                      # right context (none)\n",
        "            utf8.VALID_UTF8_STAR     # sigma*\n",
        "        )\n",
        "        t = rule if t is None else (t @ rule)\n",
        "\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    \"\"\"\n",
        "    Create an acceptor for a language prefix token like '<eng>'.\n",
        "    We allow it as literal at the beginning and then delete it from output:\n",
        "      '<eng>' x ...  -> ... (so the prefix doesn't appear in the output)\n",
        "    \"\"\"\n",
        "    # Literal token\n",
        "    token_acceptor = pynini.accep(lang_token)\n",
        "    # We want to delete it from output: cross(lang_token, \"\").\n",
        "    return pynini.cross(lang_token, \"\")\n",
        "\n",
        "\n",
        "def build_multilingual_transducer(\n",
        "    lang_to_rules_text: Dict[str, str],\n",
        "    token_fmt: str = \"<{lang}>\",\n",
        ") -> Fst:\n",
        "    \"\"\"\n",
        "    For each language:\n",
        "      - parse rules\n",
        "      - build transducer\n",
        "      - prepend a required language token (deleted on output)\n",
        "    Then union all.\n",
        "    \"\"\"\n",
        "    unified = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        rules = parse_cldr_rules_simple(text)\n",
        "        t = build_transducer_from_rules(rules)\n",
        "        pref = prefix_language_token(token_fmt.format(lang=lang))\n",
        "        lang_t = pref + t  # concatenation; prefix must come first\n",
        "        lang_t.optimize()\n",
        "        unified = lang_t if unified is None else (unified | lang_t)\n",
        "\n",
        "    if unified is None:\n",
        "        # No rules: accept anything and echo it (after removing an imaginary token)\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "    unified.optimize()\n",
        "    # Determinize/minimize for speed\n",
        "    unified = pynini.determinize(unified).minimize()\n",
        "    return unified\n",
        "\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Quick demo\n",
        "# -------------------------------\n",
        "\n",
        "def demo():\n",
        "    # Two tiny “CLDR-like” rule sets aiming toward IPA-ish output just for demonstration.\n",
        "    # (These are *not* authoritative IPA mappings—replace with your real CLDR rules.)\n",
        "    cldr_en = r\"\"\"\n",
        "        # English-ish toy rules\n",
        "        th > θ ;\n",
        "        sh > ʃ ;\n",
        "        ch > t͡ʃ ;\n",
        "        ng > ŋ ;\n",
        "        [aeiou] > a ;   # silly vowel collapse to 'a'\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        # Spanish-ish toy rules\n",
        "        ll > ʎ ;\n",
        "        ñ > ɲ ;\n",
        "        qu > k ;\n",
        "        c > k ;\n",
        "        z > s ;\n",
        "        [aeiou] > a ;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\n",
        "        \"eng\": cldr_en,\n",
        "        \"spa\": cldr_es,\n",
        "    }\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    # Tiny run test: compose an input with the FST and output the best path\n",
        "    # Example inputs must be prefixed with the language token.\n",
        "    test_inputs = [\n",
        "        \"<eng>thing\",\n",
        "        \"<eng>mashing\",\n",
        "        \"<spa>llama\",\n",
        "        \"<spa>quiza\",\n",
        "    ]\n",
        "\n",
        "    def apply(input_str: str) -> str:\n",
        "        lattice = pynini.compose(input_str, fst)\n",
        "        if lattice.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "        try:\n",
        "            return pynini.shortestpath(lattice, 1).string()\n",
        "        except Exception:\n",
        "            return \"<no-output>\"\n",
        "\n",
        "    for s in test_inputs:\n",
        "        print(s, \"->\", apply(s))\n",
        "\n",
        "    print(\"Wrote combined FST to multilang.fst\")\n"
      ],
      "metadata": {
        "id": "__HGtWPx2X_7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "99J8j-Ct3Djl",
        "outputId": "de17655e-fa10-46bf-9675-3b019420ae8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pynini.lib.utf8' has no attribute 'VALID_UTF8_STAR'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1790250070.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3642090184.py\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     }\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mfst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multilingual_transducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_fmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<{lang}>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0msave_fst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilang.fst\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3642090184.py\u001b[0m in \u001b[0;36mbuild_multilingual_transducer\u001b[0;34m(lang_to_rules_text, token_fmt)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_to_rules_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mrules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_cldr_rules_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transducer_from_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix_language_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mlang_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpref\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# concatenation; prefix must come first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3642090184.py\u001b[0m in \u001b[0;36mbuild_transducer_from_rules\u001b[0;34m(rules)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# left context (none)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# right context (none)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mutf8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALID_UTF8_STAR\u001b[0m     \u001b[0;31m# sigma*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pynini.lib.utf8' has no attribute 'VALID_UTF8_STAR'"
          ]
        }
      ]
    }
  ]
}