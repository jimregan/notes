{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtW5rX6n2WjU",
        "outputId": "d5a06676-e003-499a-dc1f-e333f47eb41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynini in /usr/local/lib/python3.12/dist-packages (2.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pynini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cldr2fst.py\n",
        "# Requires: pynini (which depends on OpenFst)\n",
        "#   pip install pynini==2.1.5\n",
        "#   (or a version compatible with your platform)\n",
        "#\n",
        "# Usage example (see bottom of file):\n",
        "#   python cldr2fst.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import pynini\n",
        "from pynini import *\n",
        "from pynini.lib import pynutil\n",
        "from pynini.lib import utf8\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Utilities\n",
        "# -------------------------------\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes like \\\\u0259 and \\\\U0001F600.\"\"\"\n",
        "    def rpl4(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    def rpl8(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    # Keep other backslashes as literals (CLDR has many constructs; we stay conservative)\n",
        "    return s\n",
        "\n",
        "def _expand_char_class(cls: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Expand a simple bracket class like [abc] or [a-z].\n",
        "    This is intentionally minimal; it does not handle nested classes, properties, or set ops.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(cls):\n",
        "        if i + 2 < len(cls) and cls[i+1] == \"-\":\n",
        "            start = ord(cls[i])\n",
        "            end = ord(cls[i+2])\n",
        "            for cp in range(start, end + 1):\n",
        "                out.append(chr(cp))\n",
        "            i += 3\n",
        "        else:\n",
        "            out.append(cls[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "def _tokenize_pattern(pat: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a very small subset of CLDR pattern syntax:\n",
        "    - Literal characters\n",
        "    - Character classes [..]\n",
        "    Returns a list of alternatives (strings) if the pattern is a single char-class;\n",
        "    otherwise returns [pat] (treated as a literal string).\n",
        "    \"\"\"\n",
        "    pat = pat.strip()\n",
        "    if len(pat) >= 2 and pat[0] == \"[\" and pat[-1] == \"]\":\n",
        "        inner = _decode_escapes(pat[1:-1])\n",
        "        alts = _expand_char_class(inner)\n",
        "        return alts\n",
        "    # Otherwise we treat it as a literal sequence (already escape-decoded)\n",
        "    return [_decode_escapes(pat)]\n",
        "\n",
        "def _string_map(pairs: List[Tuple[str, str]]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a union of literal string transductions, determinize & minimize.\n",
        "    \"\"\"\n",
        "    # Pynini.string_map can take a dict or list of pairs\n",
        "    t = pynini.string_map(pairs)\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Parsing CLDR rules (subset)\n",
        "# -------------------------------\n",
        "\n",
        "_RULE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<lhs>.+?)\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?)\n",
        "    ;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    lhs: str\n",
        "    rhs: str\n",
        "    op: str  # \">\", \"<\", \"<>\"\n",
        "\n",
        "def parse_cldr_rules_simple(text: str) -> List[Rule]:\n",
        "    \"\"\"\n",
        "    Parse a subset of CLDR rule lines:\n",
        "      LHS > RHS ;\n",
        "      LHS < RHS ;\n",
        "      LHS <> RHS ;\n",
        "    Strips comments (# ...) and blank lines.\n",
        "    Ignores directives (:: ... ;)\n",
        "    \"\"\"\n",
        "    rules: List[Rule] = []\n",
        "    for raw in text.splitlines():\n",
        "        line = raw.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        if line.startswith(\"::\"):\n",
        "            # ignore directives in this simple converter\n",
        "            continue\n",
        "        m = _RULE_RE.match(line)\n",
        "        if not m:\n",
        "            # Not supported yet; skip quietly so you can still test quickly.\n",
        "            continue\n",
        "        lhs = m.group(\"lhs\").strip()\n",
        "        rhs = m.group(\"rhs\").strip()\n",
        "        op = m.group(\"op\")\n",
        "        rules.append(Rule(lhs=lhs, rhs=rhs, op=op))\n",
        "    return rules\n",
        "\n",
        "\n",
        "def build_transducer_from_rules(rules: List[Rule]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a transducer from simple (context-free) CLDR rules.\n",
        "    Uses a Unicode sigma* that passes through unmapped characters.\n",
        "    \"\"\"\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    for r in rules:\n",
        "        lhs_alts = _tokenize_pattern(r.lhs)\n",
        "        rhs_alts = _tokenize_pattern(r.rhs)\n",
        "\n",
        "        def add_pairs(A: List[str], B: List[str]):\n",
        "            if len(A) == len(B):\n",
        "                for x, y in zip(A, B):\n",
        "                    pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "            else:\n",
        "                for x in A:\n",
        "                    for y in B:\n",
        "                        pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "\n",
        "        if r.op == \">\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "        elif r.op == \"<\":\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "        elif r.op == \"<>\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "\n",
        "    # --- Robust sigma* definition ---\n",
        "    # Use acceptors for each character and union them\n",
        "    sigma = None\n",
        "    for i in range(1, 256):\n",
        "        char_accep = pynini.accep(chr(i))\n",
        "        sigma = char_accep if sigma is None else (sigma | char_accep)\n",
        "    sigma = sigma.optimize()\n",
        "    sigma_star = sigma.closure()\n",
        "\n",
        "\n",
        "    if not pairs:\n",
        "        return pynini.cdrewrite(pynini.cross(\"\", \"\"), \"\", \"\", sigma_star)\n",
        "\n",
        "    t = None\n",
        "    for src, tgt in pairs:\n",
        "        rule = pynini.cdrewrite(pynini.cross(src, tgt), \"\", \"\", sigma_star)\n",
        "        t = rule if t is None else (t @ rule)\n",
        "\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    \"\"\"\n",
        "    Create an acceptor for a language prefix token like '<eng>'.\n",
        "    We allow it as literal at the beginning and then delete it from output:\n",
        "      '<eng>' x ...  -> ... (so the prefix doesn't appear in the output)\n",
        "    \"\"\"\n",
        "    # Literal token\n",
        "    token_acceptor = pynini.accep(lang_token)\n",
        "    # We want to delete it from output: cross(lang_token, \"\").\n",
        "    return pynini.cross(lang_token, \"\")\n",
        "\n",
        "\n",
        "def build_multilingual_transducer(\n",
        "    lang_to_rules_text: Dict[str, str],\n",
        "    token_fmt: str = \"<{lang}>\",\n",
        ") -> Fst:\n",
        "    \"\"\"\n",
        "    For each language:\n",
        "      - parse rules\n",
        "      - build transducer\n",
        "      - prepend a required language token (deleted on output)\n",
        "    Then union all.\n",
        "    \"\"\"\n",
        "    unified = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        rules = parse_cldr_rules_simple(text)\n",
        "        t = build_transducer_from_rules(rules)\n",
        "        pref = prefix_language_token(token_fmt.format(lang=lang))\n",
        "        lang_t = pref + t  # concatenation; prefix must come first\n",
        "        lang_t.optimize()\n",
        "        unified = lang_t if unified is None else (unified | lang_t)\n",
        "\n",
        "    if unified is None:\n",
        "        # No rules: accept anything and echo it (after removing an imaginary token)\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "    unified.optimize()\n",
        "    # Determinize/minimize for speed\n",
        "    unified = pynini.determinize(unified).minimize()\n",
        "    return unified\n",
        "\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Quick demo\n",
        "# -------------------------------\n",
        "\n",
        "def demo():\n",
        "    # Two tiny “CLDR-like” rule sets aiming toward IPA-ish output just for demonstration.\n",
        "    # (These are *not* authoritative IPA mappings—replace with your real CLDR rules.)\n",
        "    cldr_en = r\"\"\"\n",
        "        # English-ish toy rules\n",
        "        th > θ ;\n",
        "        sh > ʃ ;\n",
        "        ch > t͡ʃ ;\n",
        "        ng > ŋ ;\n",
        "        [aeiou] > a ;   # silly vowel collapse to 'a'\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        # Spanish-ish toy rules\n",
        "        ll > ʎ ;\n",
        "        ñ > ɲ ;\n",
        "        qu > k ;\n",
        "        c > k ;\n",
        "        z > s ;\n",
        "        [aeiou] > a ;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\n",
        "        \"eng\": cldr_en,\n",
        "        \"spa\": cldr_es,\n",
        "    }\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    # Tiny run test: compose an input with the FST and output the best path\n",
        "    # Example inputs must be prefixed with the language token.\n",
        "    test_inputs = [\n",
        "        \"<eng>thing\",\n",
        "        \"<eng>mashing\",\n",
        "        \"<spa>llama\",\n",
        "        \"<spa>quiza\",\n",
        "    ]\n",
        "\n",
        "    def apply(input_str: str) -> str:\n",
        "        lattice = pynini.compose(input_str, fst)\n",
        "        if lattice.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "        try:\n",
        "            return pynini.shortestpath(lattice, 1).string()\n",
        "        except Exception:\n",
        "            return \"<no-output>\"\n",
        "\n",
        "    for s in test_inputs:\n",
        "        print(s, \"->\", apply(s))\n",
        "\n",
        "    print(\"Wrote combined FST to multilang.fst\")"
      ],
      "metadata": {
        "id": "__HGtWPx2X_7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "99J8j-Ct3Djl",
        "outputId": "fac41c9c-fe87-4ed7-a585-78eead1acf16"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FstStringCompilationError",
          "evalue": "String compilation failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFstStringCompilationError\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1790250070.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4093846662.py\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m     }\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mfst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multilingual_transducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_fmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<{lang}>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0msave_fst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilang.fst\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4093846662.py\u001b[0m in \u001b[0;36mbuild_multilingual_transducer\u001b[0;34m(lang_to_rules_text, token_fmt)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_to_rules_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mrules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_cldr_rules_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transducer_from_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix_language_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mlang_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpref\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# concatenation; prefix must come first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4093846662.py\u001b[0m in \u001b[0;36mbuild_transducer_from_rules\u001b[0;34m(rules)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mchar_accep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpynini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_accep\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mchar_accep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mextensions/_pynini.pyx\u001b[0m in \u001b[0;36m_pynini.accep\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mextensions/_pynini.pyx\u001b[0m in \u001b[0;36m_pynini.accep\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFstStringCompilationError\u001b[0m: String compilation failed"
          ]
        }
      ]
    }
  ]
}