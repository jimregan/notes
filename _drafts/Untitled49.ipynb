{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtW5rX6n2WjU",
        "outputId": "d5a06676-e003-499a-dc1f-e333f47eb41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynini in /usr/local/lib/python3.12/dist-packages (2.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pynini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cldr2fst.py\n",
        "# Requires: pynini (which depends on OpenFst)\n",
        "#   pip install pynini==2.1.5\n",
        "#   (or a version compatible with your platform)\n",
        "#\n",
        "# Usage example (see bottom of file):\n",
        "#   python cldr2fst.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import pynini\n",
        "from pynini import *\n",
        "from pynini.lib import pynutil\n",
        "from pynini.lib import utf8\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Utilities\n",
        "# -------------------------------\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes like \\\\u0259 and \\\\U0001F600.\"\"\"\n",
        "    def rpl4(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    def rpl8(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    # Keep other backslashes as literals (CLDR has many constructs; we stay conservative)\n",
        "    return s\n",
        "\n",
        "def _expand_char_class(cls: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Expand a simple bracket class like [abc] or [a-z].\n",
        "    This is intentionally minimal; it does not handle nested classes, properties, or set ops.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(cls):\n",
        "        if i + 2 < len(cls) and cls[i+1] == \"-\":\n",
        "            start = ord(cls[i])\n",
        "            end = ord(cls[i+2])\n",
        "            for cp in range(start, end + 1):\n",
        "                out.append(chr(cp))\n",
        "            i += 3\n",
        "        else:\n",
        "            out.append(cls[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "def _tokenize_pattern(pat: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a very small subset of CLDR pattern syntax:\n",
        "    - Literal characters\n",
        "    - Character classes [..]\n",
        "    Returns a list of alternatives (strings) if the pattern is a single char-class;\n",
        "    otherwise returns [pat] (treated as a literal string).\n",
        "    \"\"\"\n",
        "    pat = pat.strip()\n",
        "    if len(pat) >= 2 and pat[0] == \"[\" and pat[-1] == \"]\":\n",
        "        inner = _decode_escapes(pat[1:-1])\n",
        "        alts = _expand_char_class(inner)\n",
        "        return alts\n",
        "    # Otherwise we treat it as a literal sequence (already escape-decoded)\n",
        "    return [_decode_escapes(pat)]\n",
        "\n",
        "def _string_map(pairs: List[Tuple[str, str]]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a union of literal string transductions, determinize & minimize.\n",
        "    \"\"\"\n",
        "    # Pynini.string_map can take a dict or list of pairs\n",
        "    t = pynini.string_map(pairs)\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Parsing CLDR rules (subset)\n",
        "# -------------------------------\n",
        "\n",
        "_RULE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<lhs>.+?)\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?)\n",
        "    ;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    lhs: str\n",
        "    rhs: str\n",
        "    op: str  # \">\", \"<\", \"<>\"\n",
        "\n",
        "def parse_cldr_rules_simple(text: str) -> List[Rule]:\n",
        "    \"\"\"\n",
        "    Parse a subset of CLDR rule lines:\n",
        "      LHS > RHS ;\n",
        "      LHS < RHS ;\n",
        "      LHS <> RHS ;\n",
        "    Strips comments (# ...) and blank lines.\n",
        "    Ignores directives (:: ... ;)\n",
        "    \"\"\"\n",
        "    rules: List[Rule] = []\n",
        "    for raw in text.splitlines():\n",
        "        line = raw.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        if line.startswith(\"::\"):\n",
        "            # ignore directives in this simple converter\n",
        "            continue\n",
        "        m = _RULE_RE.match(line)\n",
        "        if not m:\n",
        "            # Not supported yet; skip quietly so you can still test quickly.\n",
        "            continue\n",
        "        lhs = m.group(\"lhs\").strip()\n",
        "        rhs = m.group(\"rhs\").strip()\n",
        "        op = m.group(\"op\")\n",
        "        rules.append(Rule(lhs=lhs, rhs=rhs, op=op))\n",
        "    return rules\n",
        "\n",
        "\n",
        "def build_transducer_from_rules(rules: List[Rule]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a transducer from simple (context-free) CLDR rules.\n",
        "    Uses cdrewrite with utf8.VALID_UTF8.closure() so non-matching chars pass through.\n",
        "    \"\"\"\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    for r in rules:\n",
        "        lhs_alts = _tokenize_pattern(r.lhs)\n",
        "        rhs_alts = _tokenize_pattern(r.rhs)\n",
        "\n",
        "        def add_pairs(A: List[str], B: List[str]):\n",
        "            if len(A) == len(B):\n",
        "                for x, y in zip(A, B):\n",
        "                    pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "            else:\n",
        "                for x in A:\n",
        "                    for y in B:\n",
        "                        pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "\n",
        "        if r.op == \">\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "        elif r.op == \"<\":\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "        elif r.op == \"<>\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "\n",
        "    sigma_star = utf8.VALID_UTF8_CHAR.closure()\n",
        "\n",
        "    if not pairs:\n",
        "        # Identity FST if no rules\n",
        "        return pynini.cdrewrite(pynini.cross(\"\", \"\"), \"\", \"\", sigma_star)\n",
        "\n",
        "    # Sequentially compose cdrewrite rules\n",
        "    t = None\n",
        "    for src, tgt in pairs:\n",
        "        rule = pynini.cdrewrite(\n",
        "            pynini.cross(src, tgt),\n",
        "            \"\", \"\", sigma_star\n",
        "        )\n",
        "        t = rule if t is None else (t @ rule)\n",
        "\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    \"\"\"\n",
        "    Create an acceptor for a language prefix token like '<eng>'.\n",
        "    We allow it as literal at the beginning and then delete it from output:\n",
        "      '<eng>' x ...  -> ... (so the prefix doesn't appear in the output)\n",
        "    \"\"\"\n",
        "    # Literal token\n",
        "    token_acceptor = pynini.accep(lang_token)\n",
        "    # We want to delete it from output: cross(lang_token, \"\").\n",
        "    return pynini.cross(lang_token, \"\")\n",
        "\n",
        "\n",
        "def build_multilingual_transducer(\n",
        "    lang_to_rules_text: Dict[str, str],\n",
        "    token_fmt: str = \"<{lang}>\",\n",
        ") -> Fst:\n",
        "    \"\"\"\n",
        "    For each language:\n",
        "      - parse rules\n",
        "      - build transducer\n",
        "      - prepend a required language token (deleted on output)\n",
        "    Then union all.\n",
        "    \"\"\"\n",
        "    unified = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        rules = parse_cldr_rules_simple(text)\n",
        "        t = build_transducer_from_rules(rules)\n",
        "        pref = prefix_language_token(token_fmt.format(lang=lang))\n",
        "        lang_t = pref + t  # concatenation; prefix must come first\n",
        "        lang_t.optimize()\n",
        "        unified = lang_t if unified is None else (unified | lang_t)\n",
        "\n",
        "    if unified is None:\n",
        "        # No rules: accept anything and echo it (after removing an imaginary token)\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "    unified.optimize()\n",
        "    # Determinize/minimize for speed\n",
        "    unified = pynini.determinize(unified).minimize()\n",
        "    return unified\n",
        "\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Quick demo\n",
        "# -------------------------------\n",
        "\n",
        "def demo():\n",
        "    # Two tiny ‚ÄúCLDR-like‚Äù rule sets aiming toward IPA-ish output just for demonstration.\n",
        "    # (These are *not* authoritative IPA mappings‚Äîreplace with your real CLDR rules.)\n",
        "    cldr_en = r\"\"\"\n",
        "        # English-ish toy rules\n",
        "        th > Œ∏ ;\n",
        "        sh >  É ;\n",
        "        ch > tÕ° É ;\n",
        "        ng > ≈ã ;\n",
        "        [aeiou] > a ;   # silly vowel collapse to 'a'\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        # Spanish-ish toy rules\n",
        "        ll >  é ;\n",
        "        √± > …≤ ;\n",
        "        qu > k ;\n",
        "        c > k ;\n",
        "        z > s ;\n",
        "        [aeiou] > a ;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\n",
        "        \"eng\": cldr_en,\n",
        "        \"spa\": cldr_es,\n",
        "    }\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    # Tiny run test: compose an input with the FST and output the best path\n",
        "    # Example inputs must be prefixed with the language token.\n",
        "    test_inputs = [\n",
        "        \"<eng>thing\",\n",
        "        \"<eng>mashing\",\n",
        "        \"<spa>llama\",\n",
        "        \"<spa>quiza\",\n",
        "    ]\n",
        "\n",
        "    def apply(input_str: str) -> str:\n",
        "        lattice = pynini.compose(input_str, fst)\n",
        "        if lattice.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "        try:\n",
        "            return pynini.shortestpath(lattice, 1).string()\n",
        "        except Exception:\n",
        "            return \"<no-output>\"\n",
        "\n",
        "    for s in test_inputs:\n",
        "        print(s, \"->\", apply(s))\n",
        "\n",
        "    print(\"Wrote combined FST to multilang.fst\")"
      ],
      "metadata": {
        "id": "__HGtWPx2X_7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99J8j-Ct3Djl",
        "outputId": "b0d2f7c2-5ac0-4564-b522-5d0e49cce29b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eng>thing -> Œ∏i≈ã\n",
            "<eng>mashing -> ma Éi≈ã\n",
            "<spa>llama ->  éama\n",
            "<spa>quiza -> kasa\n",
            "Wrote combined FST to multilang.fst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import sys\n",
        "import io\n",
        "\n",
        "import pynini\n",
        "from pynini import Fst\n",
        "from pynini.lib import utf8\n",
        "\n",
        "# -------------------------------\n",
        "# Core Utilities\n",
        "# -------------------------------\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "_ESCAPED = re.compile(r\"\\\\([][\\\\/\\-^(){}_.*+?|])\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes.\"\"\"\n",
        "    def rpl4(m): return chr(int(m.group(1), 16))\n",
        "    def rpl8(m): return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    s = _ESCAPED.sub(lambda m: m.group(1), s)\n",
        "    return s\n",
        "\n",
        "def _char_range(a: str, b: str) -> List[str]:\n",
        "    return [chr(cp) for cp in range(ord(a), ord(b) + 1)]\n",
        "\n",
        "def _parse_unicode_set(text: str) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"Simple parser for CLDR-style character sets like [a-z] or [^abc].\"\"\"\n",
        "    assert text.startswith(\"[\") and text.endswith(\"]\"), \"not a set\"\n",
        "    inner = text[1:-1]\n",
        "    neg = inner.startswith(\"^\")\n",
        "    if neg: inner = inner[1:]\n",
        "    items: List[str] = []\n",
        "    i = 0\n",
        "    def read_char(ix: int) -> Tuple[str, int]:\n",
        "        if ix < len(inner) and inner[ix] == \"\\\\\":\n",
        "            m4 = _U_HEX.match(inner, ix)\n",
        "            if m4: return (chr(int(m4.group(1), 16)), m4.end())\n",
        "            m8 = _U_HEX_LONG.match(inner, ix)\n",
        "            if m8: return (chr(int(m8.group(1), 16)), m8.end())\n",
        "            if ix + 1 < len(inner): return (inner[ix + 1], ix + 2)\n",
        "            return (\"\\\\\", ix + 1)\n",
        "        return (inner[ix], ix + 1)\n",
        "    while i < len(inner):\n",
        "        c1, j = read_char(i)\n",
        "        if j < len(inner) - 1 and inner[j] == \"-\" and j + 1 < len(inner):\n",
        "            c2, k = read_char(j + 1)\n",
        "            items.extend(_char_range(c1, c2))\n",
        "            i = k\n",
        "        else:\n",
        "            items.append(c1)\n",
        "            i = j\n",
        "    return (neg, items)\n",
        "\n",
        "# üîë FIX 1: Use pynini.string_accep for all literal strings, ensuring UTF-8 symbol table use.\n",
        "def _acceptor(s: str) -> Fst:\n",
        "    return pynini.accep(s, token_type=\"utf8\")\n",
        "\n",
        "# -------------------------------\n",
        "# CLDR Parsing\n",
        "# -------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    lhs: str\n",
        "    rhs: str\n",
        "    op: str\n",
        "    left: Optional[str] = None\n",
        "    right: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class Directive:\n",
        "    kind: str\n",
        "    payload: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class VarAssign:\n",
        "    name: str\n",
        "    expr: str\n",
        "\n",
        "Line = Tuple[str, object]\n",
        "\n",
        "_RULE_CTX_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<lhs>.+?)\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?)\n",
        "    (?:\n",
        "        /\n",
        "        \\s*\n",
        "        (?P<L>.*?)\n",
        "        \\s*\n",
        "        _\n",
        "        \\s*\n",
        "        (?P<R>.*?)\n",
        "    )?\n",
        "    \\s*;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "_VAR_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<name>\\$[A-Za-z0-9_]+)\n",
        "    \\s*=\\s*\n",
        "    (?P<expr>.+?)\n",
        "    \\s*;?\\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "def _parse_line(line: str) -> Optional[Line]:\n",
        "    s = line.strip()\n",
        "    if not s or s.startswith(\"#\"):\n",
        "        return None\n",
        "\n",
        "    mvar = _VAR_RE.match(s)\n",
        "    if mvar and mvar.group(\"expr\").strip():\n",
        "        name = mvar.group(\"name\")\n",
        "        expr = mvar.group(\"expr\").strip().rstrip(';')\n",
        "        return (\"var\", VarAssign(name=name, expr=expr))\n",
        "\n",
        "    if s.startswith(\"::\"):\n",
        "        body = s[2:].strip()\n",
        "        if body.lower() == \"null;\": return (\"dir\", Directive(kind=\"null\"))\n",
        "        if body.lower() == \"nfd;\": return (\"dir\", Directive(kind=\"nfd\"))\n",
        "        if body.lower() == \"nfc;\": return (\"dir\", Directive(kind=\"nfc\"))\n",
        "        if body.startswith(\"[\") and body.endswith(\";\"):\n",
        "            payload = body[:-1].strip()\n",
        "            return (\"dir\", Directive(kind=\"filter\", payload=payload))\n",
        "        return (\"dir\", Directive(kind=\"unknown\", payload=body))\n",
        "\n",
        "    m = _RULE_CTX_RE.match(s)\n",
        "    if m:\n",
        "        L_ctx = m.group(\"L\")\n",
        "        R_ctx = m.group(\"R\")\n",
        "        return (\"rule\", Rule(\n",
        "            lhs=m.group(\"lhs\").strip(),\n",
        "            rhs=m.group(\"rhs\").strip(),\n",
        "            op=m.group(\"op\"),\n",
        "            left=(L_ctx.strip() if L_ctx else None),\n",
        "            right=(R_ctx.strip() if R_ctx else None),\n",
        "        ))\n",
        "    return None\n",
        "\n",
        "def parse_cldr(text: str) -> List[Line]:\n",
        "    out: List[Line] = []\n",
        "    for raw in text.splitlines():\n",
        "        p = _parse_line(raw)\n",
        "        if p: out.append(p)\n",
        "    return out\n",
        "\n",
        "# -------------------------------\n",
        "# FST Compilation\n",
        "# -------------------------------\n",
        "\n",
        "class Env:\n",
        "    def __init__(self) -> None:\n",
        "        self.vars: Dict[str, Fst] = {}\n",
        "        # üîë FIX 2: Define sigma using a standard set that is deterministic.\n",
        "        self.sigma: Fst = utf8.VALID_UTF8_CHAR.optimize()\n",
        "        self.sigma_star: Fst = self.sigma.closure().optimize()\n",
        "\n",
        "    def get(self, name: str) -> Fst:\n",
        "        if name not in self.vars:\n",
        "            raise KeyError(f\"Undefined variable {name}\")\n",
        "        return self.vars[name]\n",
        "\n",
        "def _compile_atom(expr: str, env: Env) -> Fst:\n",
        "    expr = expr.strip()\n",
        "    if expr.startswith(\"$\"):\n",
        "        return env.get(expr)\n",
        "    if expr.startswith(\"[\") and expr.endswith(\"]\"):\n",
        "        neg, items = _parse_unicode_set(expr)\n",
        "        if not items: return pynini.Fst()\n",
        "        u = pynini.union(*(_acceptor(ch) for ch in items)).optimize()\n",
        "        if neg:\n",
        "            return (env.sigma_star - u).optimize()\n",
        "        return u\n",
        "\n",
        "    # üîë FIX 3: Robustly accept the literal string (e.g., 'th' or 'Œ∏')\n",
        "    return _acceptor(_decode_escapes(expr))\n",
        "\n",
        "def _compile_seq(expr: Optional[str], env: Env) -> Fst:\n",
        "    \"\"\"Compiles a sequence of atoms (literals, sets, variables) into an Fst acceptor.\"\"\"\n",
        "    if not expr: return _acceptor(\"\")\n",
        "    s = expr\n",
        "    parts: List[str] = []\n",
        "    i = 0\n",
        "    cur = []\n",
        "    depth = 0\n",
        "\n",
        "    # Simple tokenizer logic\n",
        "    while i < len(s):\n",
        "        ch = s[i]\n",
        "        is_escape_char = (ch == '\\\\') and (i + 1 < len(s) and s[i+1].isalpha()) # Heuristic for non-literal escape\n",
        "\n",
        "        if ch == \"[\" and not is_escape_char: depth += 1\n",
        "        elif ch == \"]\" and depth > 0 and not is_escape_char: depth -= 1\n",
        "        elif ch.isspace() and depth == 0:\n",
        "            if cur: parts.append(\"\".join(cur)); cur = []\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        cur.append(ch)\n",
        "        i += 1\n",
        "        # Handle escapes within the sequence\n",
        "        if is_escape_char and i < len(s):\n",
        "            # This logic is complex; we rely on _decode_escapes in _compile_atom to resolve \\u/\\\\/etc.\n",
        "            pass\n",
        "\n",
        "    if cur: parts.append(\"\".join(cur))\n",
        "    if not parts: return _acceptor(\"\")\n",
        "\n",
        "    # Composition of FST atoms\n",
        "    fst = _compile_atom(parts[0], env)\n",
        "    for p in parts[1:]:\n",
        "        fst = fst + _compile_atom(p, env)\n",
        "    return fst.optimize()\n",
        "\n",
        "def compile_lines(lines: List[Line]) -> Fst:\n",
        "    env = Env()\n",
        "\n",
        "    # --- PASS 1: Compile all Variables (Fixes KeyError) ---\n",
        "    vars_to_compile: List[VarAssign] = []\n",
        "    for kind, payload in lines:\n",
        "        if kind == \"var\":\n",
        "            vars_to_compile.append(payload) # type: ignore[arg-type]\n",
        "\n",
        "    for va in vars_to_compile:\n",
        "        env.vars[va.name] = _compile_seq(va.expr, env).optimize()\n",
        "\n",
        "    # --- PASS 2 & 3: Sequential CD Rewrite for All Rules (Fixes <no-path> and order issues) ---\n",
        "\n",
        "    cascade = env.sigma_star @ env.sigma_star\n",
        "\n",
        "    for kind, payload in lines:\n",
        "        if kind == \"rule\":\n",
        "            r: Rule = payload # type: ignore[assignment]\n",
        "\n",
        "            # Variables are now defined.\n",
        "            lhs = _compile_seq(r.lhs, env)\n",
        "            rhs = _compile_seq(r.rhs, env)\n",
        "\n",
        "            Lctx = _compile_seq(r.left, env) if r.left else pynini.accep(\"\")\n",
        "            Rctx = _compile_seq(r.right, env) if r.right else pynini.accep(\"\")\n",
        "\n",
        "            def add_rule(X: Fst, Y: Fst, L: Fst, R: Fst):\n",
        "                nonlocal cascade\n",
        "\n",
        "                # Check for empty FSTs which could cause composition failures\n",
        "                if X.num_states() == 0:\n",
        "                    print(f\"Warning: Empty LHS FST for rule {r.lhs} > {r.rhs}. Skipping.\")\n",
        "                    return\n",
        "\n",
        "                rewrite = pynini.cdrewrite(\n",
        "                    pynini.cross(X, Y),\n",
        "                    L, R,\n",
        "                    env.sigma_star\n",
        "                ).optimize()\n",
        "\n",
        "                cascade = cascade @ rewrite\n",
        "\n",
        "            if r.op == \">\":\n",
        "                add_rule(lhs, rhs, Lctx, Rctx)\n",
        "            elif r.op == \"<\":\n",
        "                add_rule(rhs, lhs, Rctx, Lctx)\n",
        "            elif r.op == \"<>\":\n",
        "                add_rule(lhs, rhs, Lctx, Rctx)\n",
        "                add_rule(rhs, lhs, Rctx, Lctx)\n",
        "\n",
        "    cascade.optimize()\n",
        "    return cascade\n",
        "\n",
        "# -------------------------------\n",
        "# Multilingual Wrapper & Demo\n",
        "# -------------------------------\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    return pynini.cross(_acceptor(lang_token), _acceptor(\"\")) # delete prefix\n",
        "\n",
        "def build_multilingual_transducer(lang_to_rules_text: Dict[str, str], token_fmt: str = \"<{lang}>\") -> Fst:\n",
        "    unified: Optional[Fst] = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        lines = parse_cldr(text)\n",
        "        t = compile_lines(lines)\n",
        "        lt = prefix_language_token(token_fmt.format(lang=lang)) + t\n",
        "        lt.optimize()\n",
        "        unified = lt if unified is None else (unified | lt)\n",
        "    if unified is None:\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "    return pynini.determinize(unified).minimize()\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "def demo():\n",
        "    # üîë FIX 4: Explicitly override the output stream encoding to handle IPA characters (Mojibake fix).\n",
        "    try:\n",
        "        sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8', line_buffering=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    cldr_en = r\"\"\"\n",
        "        $V = [aeiou] ;\n",
        "        th > Œ∏ ;\n",
        "        sh >  É ;\n",
        "        ng > ≈ã ;\n",
        "        ch > tÕ° É ;\n",
        "        c > s / _ [ei] ;\n",
        "        $V > a ;\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        $V = [aeiou] ;\n",
        "        :: [a-z√±] ;\n",
        "        ll >  é ;\n",
        "        √± > …≤ ;\n",
        "        qu > k ;\n",
        "        c > s / _ [ei] ;\n",
        "        c > k ;\n",
        "        z > s ;\n",
        "        $V > a ;\n",
        "        ::Null;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\"eng\": cldr_en, \"spa\": cldr_es}\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    tests = [\n",
        "        \"<eng>thing\",\n",
        "        \"<eng>mashing\",\n",
        "        \"<spa>llama\",\n",
        "        \"<spa>quiza\",\n",
        "        \"<spa>cita\",\n",
        "        \"<spa>cuna\"\n",
        "    ]\n",
        "\n",
        "    def apply(s: str) -> str:\n",
        "        lat = pynini.compose(_acceptor(s), fst)\n",
        "        if lat.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "\n",
        "        # Ensures correct string decoding regardless of environment confusion\n",
        "        raw_output = pynini.shortestpath(lat, 1).string(token_type=\"utf8\")\n",
        "        if isinstance(raw_output, bytes):\n",
        "            return raw_output.decode(\"utf-8\")\n",
        "        return raw_output\n",
        "\n",
        "    print(\"--- Test Results ---\")\n",
        "    for t in tests:\n",
        "        print(f\"{t} -> {apply(t)}\")\n",
        "\n",
        "    print(\"Wrote multilang.fst\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm09hR72On1z",
        "outputId": "93a8b6b2-b652-47c0-eebc-eecee155e613"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Results ---\n",
            "<eng>thing -> <no-path>\n",
            "<eng>mashing -> <no-path>\n",
            "<spa>llama -> <no-path>\n",
            "<spa>quiza -> kasa\n",
            "<spa>cita -> sata\n",
            "<spa>cuna -> kana\n",
            "Wrote multilang.fst\n"
          ]
        }
      ]
    }
  ]
}