{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtW5rX6n2WjU",
        "outputId": "d5a06676-e003-499a-dc1f-e333f47eb41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynini in /usr/local/lib/python3.12/dist-packages (2.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pynini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cldr2fst.py\n",
        "# Requires: pynini (which depends on OpenFst)\n",
        "#   pip install pynini==2.1.5\n",
        "#   (or a version compatible with your platform)\n",
        "#\n",
        "# Usage example (see bottom of file):\n",
        "#   python cldr2fst.py\n",
        "\n",
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import pynini\n",
        "from pynini import *\n",
        "from pynini.lib import pynutil\n",
        "from pynini.lib import utf8\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Utilities\n",
        "# -------------------------------\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes like \\\\u0259 and \\\\U0001F600.\"\"\"\n",
        "    def rpl4(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    def rpl8(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    # Keep other backslashes as literals (CLDR has many constructs; we stay conservative)\n",
        "    return s\n",
        "\n",
        "def _expand_char_class(cls: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Expand a simple bracket class like [abc] or [a-z].\n",
        "    This is intentionally minimal; it does not handle nested classes, properties, or set ops.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(cls):\n",
        "        if i + 2 < len(cls) and cls[i+1] == \"-\":\n",
        "            start = ord(cls[i])\n",
        "            end = ord(cls[i+2])\n",
        "            for cp in range(start, end + 1):\n",
        "                out.append(chr(cp))\n",
        "            i += 3\n",
        "        else:\n",
        "            out.append(cls[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "def _tokenize_pattern(pat: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a very small subset of CLDR pattern syntax:\n",
        "    - Literal characters\n",
        "    - Character classes [..]\n",
        "    Returns a list of alternatives (strings) if the pattern is a single char-class;\n",
        "    otherwise returns [pat] (treated as a literal string).\n",
        "    \"\"\"\n",
        "    pat = pat.strip()\n",
        "    if len(pat) >= 2 and pat[0] == \"[\" and pat[-1] == \"]\":\n",
        "        inner = _decode_escapes(pat[1:-1])\n",
        "        alts = _expand_char_class(inner)\n",
        "        return alts\n",
        "    # Otherwise we treat it as a literal sequence (already escape-decoded)\n",
        "    return [_decode_escapes(pat)]\n",
        "\n",
        "def _string_map(pairs: List[Tuple[str, str]]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a union of literal string transductions, determinize & minimize.\n",
        "    \"\"\"\n",
        "    # Pynini.string_map can take a dict or list of pairs\n",
        "    t = pynini.string_map(pairs)\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Parsing CLDR rules (subset)\n",
        "# -------------------------------\n",
        "\n",
        "_RULE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<lhs>.+?)\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?)\n",
        "    ;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    lhs: str\n",
        "    rhs: str\n",
        "    op: str  # \">\", \"<\", \"<>\"\n",
        "\n",
        "def parse_cldr_rules_simple(text: str) -> List[Rule]:\n",
        "    \"\"\"\n",
        "    Parse a subset of CLDR rule lines:\n",
        "      LHS > RHS ;\n",
        "      LHS < RHS ;\n",
        "      LHS <> RHS ;\n",
        "    Strips comments (# ...) and blank lines.\n",
        "    Ignores directives (:: ... ;)\n",
        "    \"\"\"\n",
        "    rules: List[Rule] = []\n",
        "    for raw in text.splitlines():\n",
        "        line = raw.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        if line.startswith(\"::\"):\n",
        "            # ignore directives in this simple converter\n",
        "            continue\n",
        "        m = _RULE_RE.match(line)\n",
        "        if not m:\n",
        "            # Not supported yet; skip quietly so you can still test quickly.\n",
        "            continue\n",
        "        lhs = m.group(\"lhs\").strip()\n",
        "        rhs = m.group(\"rhs\").strip()\n",
        "        op = m.group(\"op\")\n",
        "        rules.append(Rule(lhs=lhs, rhs=rhs, op=op))\n",
        "    return rules\n",
        "\n",
        "\n",
        "def build_transducer_from_rules(rules: List[Rule]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a transducer from simple (context-free) CLDR rules.\n",
        "    Uses cdrewrite with utf8.VALID_UTF8.closure() so non-matching chars pass through.\n",
        "    \"\"\"\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    for r in rules:\n",
        "        lhs_alts = _tokenize_pattern(r.lhs)\n",
        "        rhs_alts = _tokenize_pattern(r.rhs)\n",
        "\n",
        "        def add_pairs(A: List[str], B: List[str]):\n",
        "            if len(A) == len(B):\n",
        "                for x, y in zip(A, B):\n",
        "                    pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "            else:\n",
        "                for x in A:\n",
        "                    for y in B:\n",
        "                        pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "\n",
        "        if r.op == \">\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "        elif r.op == \"<\":\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "        elif r.op == \"<>\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "\n",
        "    sigma_star = utf8.VALID_UTF8_CHAR.closure()\n",
        "\n",
        "    if not pairs:\n",
        "        # Identity FST if no rules\n",
        "        return pynini.cdrewrite(pynini.cross(\"\", \"\"), \"\", \"\", sigma_star)\n",
        "\n",
        "    # Sequentially compose cdrewrite rules\n",
        "    t = None\n",
        "    for src, tgt in pairs:\n",
        "        rule = pynini.cdrewrite(\n",
        "            pynini.cross(src, tgt),\n",
        "            \"\", \"\", sigma_star\n",
        "        )\n",
        "        t = rule if t is None else (t @ rule)\n",
        "\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    \"\"\"\n",
        "    Create an acceptor for a language prefix token like '<eng>'.\n",
        "    We allow it as literal at the beginning and then delete it from output:\n",
        "      '<eng>' x ...  -> ... (so the prefix doesn't appear in the output)\n",
        "    \"\"\"\n",
        "    # Literal token\n",
        "    token_acceptor = pynini.accep(lang_token)\n",
        "    # We want to delete it from output: cross(lang_token, \"\").\n",
        "    return pynini.cross(lang_token, \"\")\n",
        "\n",
        "\n",
        "def build_multilingual_transducer(\n",
        "    lang_to_rules_text: Dict[str, str],\n",
        "    token_fmt: str = \"<{lang}>\",\n",
        ") -> Fst:\n",
        "    \"\"\"\n",
        "    For each language:\n",
        "      - parse rules\n",
        "      - build transducer\n",
        "      - prepend a required language token (deleted on output)\n",
        "    Then union all.\n",
        "    \"\"\"\n",
        "    unified = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        rules = parse_cldr_rules_simple(text)\n",
        "        t = build_transducer_from_rules(rules)\n",
        "        pref = prefix_language_token(token_fmt.format(lang=lang))\n",
        "        lang_t = pref + t  # concatenation; prefix must come first\n",
        "        lang_t.optimize()\n",
        "        unified = lang_t if unified is None else (unified | lang_t)\n",
        "\n",
        "    if unified is None:\n",
        "        # No rules: accept anything and echo it (after removing an imaginary token)\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "    unified.optimize()\n",
        "    # Determinize/minimize for speed\n",
        "    unified = pynini.determinize(unified).minimize()\n",
        "    return unified\n",
        "\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Quick demo\n",
        "# -------------------------------\n",
        "\n",
        "def demo():\n",
        "    # Two tiny “CLDR-like” rule sets aiming toward IPA-ish output just for demonstration.\n",
        "    # (These are *not* authoritative IPA mappings—replace with your real CLDR rules.)\n",
        "    cldr_en = r\"\"\"\n",
        "        # English-ish toy rules\n",
        "        th > θ ;\n",
        "        sh > ʃ ;\n",
        "        ch > t͡ʃ ;\n",
        "        ng > ŋ ;\n",
        "        [aeiou] > a ;   # silly vowel collapse to 'a'\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        # Spanish-ish toy rules\n",
        "        ll > ʎ ;\n",
        "        ñ > ɲ ;\n",
        "        qu > k ;\n",
        "        c > k ;\n",
        "        z > s ;\n",
        "        [aeiou] > a ;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\n",
        "        \"eng\": cldr_en,\n",
        "        \"spa\": cldr_es,\n",
        "    }\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    # Tiny run test: compose an input with the FST and output the best path\n",
        "    # Example inputs must be prefixed with the language token.\n",
        "    test_inputs = [\n",
        "        \"<eng>thing\",\n",
        "        \"<eng>mashing\",\n",
        "        \"<spa>llama\",\n",
        "        \"<spa>quiza\",\n",
        "    ]\n",
        "\n",
        "    def apply(input_str: str) -> str:\n",
        "        lattice = pynini.compose(input_str, fst)\n",
        "        if lattice.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "        try:\n",
        "            return pynini.shortestpath(lattice, 1).string()\n",
        "        except Exception:\n",
        "            return \"<no-output>\"\n",
        "\n",
        "    for s in test_inputs:\n",
        "        print(s, \"->\", apply(s))\n",
        "\n",
        "    print(\"Wrote combined FST to multilang.fst\")"
      ],
      "metadata": {
        "id": "__HGtWPx2X_7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99J8j-Ct3Djl",
        "outputId": "b0d2f7c2-5ac0-4564-b522-5d0e49cce29b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<eng>thing -> θiŋ\n",
            "<eng>mashing -> maʃiŋ\n",
            "<spa>llama -> ʎama\n",
            "<spa>quiza -> kasa\n",
            "Wrote combined FST to multilang.fst\n"
          ]
        }
      ]
    }
  ]
}