{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSIS_PATH = Path(\"/shared/mm_conv/analysis/analysis_results/\")\n",
    "ANALYSIS_PATH = Path(\"/Users/joregan/Playing/hsi/analysis_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_timings = {}\n",
    "for object_file in ANALYSIS_PATH.glob(\"hsi_*/*_object_analysis.json\"):\n",
    "    file_id = object_file.stem.replace(\"_object_analysis\", \"\")\n",
    "    if not file_id in object_timings:\n",
    "        object_timings[file_id] = {}\n",
    "    with open(object_file) as inf:\n",
    "        data = json.load(inf)\n",
    "    for item in data:\n",
    "        for one_item in data[item]:\n",
    "            if not one_item[\"utterance_id\"] in object_timings[file_id]:\n",
    "                object_timings[file_id][one_item[\"utterance_id\"]] = []\n",
    "            \n",
    "            object_timings[file_id][one_item[\"utterance_id\"]].append({\n",
    "                \"phrase\": one_item[\"phrase\"],\n",
    "                \"start\": one_item[\"timing\"][0],\n",
    "                \"end\": one_item[\"timing\"][1],\n",
    "                \"topic\": one_item[\"topic\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def slice_tsv_data(data, start, end):\n",
    "    ret = []\n",
    "    for datum in data:\n",
    "        if type(datum[\"start\"]) is str:\n",
    "            datum[\"start\"] = float(datum[\"start\"])\n",
    "        if type(datum[\"end\"]) is str:\n",
    "            datum[\"end\"] = float(datum[\"end\"])\n",
    "        if datum[\"start\"] >= start and datum[\"end\"] <= end:\n",
    "            ret.append(datum)\n",
    "        elif datum[\"end\"] > end:\n",
    "            return ret\n",
    "    return ret\n",
    "\n",
    "def load_tsv(filename):\n",
    "    data = []\n",
    "    with open(filename) as inf:\n",
    "        for line in inf.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            data.append({\n",
    "                \"start\": float(parts[0]),\n",
    "                \"end\": float(parts[1]),\n",
    "                \"word\": parts[2]\n",
    "            })\n",
    "    return data\n",
    "\n",
    "def norm_spaces(text):\n",
    "    return re.sub(\"  +\", \" \", text.strip())\n",
    "\n",
    "def clean_text(text):\n",
    "    text = norm_spaces(text)\n",
    "    return \" \".join([x.lower().strip(\".,;?!\") for x in text.split(\" \")])\n",
    "\n",
    "def get_indices(needle, haystack, checkpos=True):\n",
    "    ret = []\n",
    "    nwords = [x.lower().strip(\",?.;:()\") for x in needle.split(\" \")]\n",
    "    hwords = [x.lower().strip(\",?.;:\") for x in haystack.split(\" \")]\n",
    "    nwordspos = nwords[:-1] + [f\"{nwords[-1]}'s\"]\n",
    "    nlen = len(nwords)\n",
    "\n",
    "    for i in range(len(hwords)):\n",
    "        if hwords[i:i+nlen] == nwords:\n",
    "            ret.append((i, i+nlen))\n",
    "        elif checkpos and hwords[i:i+nlen] == nwordspos:\n",
    "            ret.append((i, i+nlen))\n",
    "    return ret\n",
    "\n",
    "def clean_text2(text):\n",
    "    nums = {\n",
    "        \"60\": \"sixty\",\n",
    "        \"1\": \"one\",\n",
    "        \"20th\": \"twentieth\",\n",
    "        \"9th\": \"ninth\",\n",
    "        \"5\": \"five\"\n",
    "    }\n",
    "    text = norm_spaces(text)\n",
    "    words = [x.lower().strip(\".,;?!\") for x in text.split(\" \")]\n",
    "    ret = []\n",
    "    for word in words:\n",
    "        if word.startswith(\"[\") and word.endswith(\"]\"):\n",
    "            continue\n",
    "        elif word.startswith(\"{\") and word.endswith(\"}\"):\n",
    "            continue\n",
    "        word = nums.get(word, word)\n",
    "        word = word.replace(\".\", \" \").replace(\",\", \" \")\n",
    "        ret.append(word)\n",
    "    return \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL = \"\"\"\n",
    "hsi_5_0718_210_001\t17\n",
    "hsi_5_0718_210_001\t18\n",
    "hsi_5_0718_210_001\t114\n",
    "hsi_4_0717_211_003\t36\n",
    "hsi_4_0717_211_003\t42\n",
    "hsi_3_0715_210_010\t89\n",
    "hsi_3_0715_209_008\t31\n",
    "hsi_3_0715_210_011\t48\n",
    "hsi_4_0717_211_002\t6\n",
    "hsi_5_0718_210_001\t49\n",
    "hsi_5_0718_209_003\t7\n",
    "hsi_6_0718_227_002\t63\n",
    "hsi_5_0718_209_001\t1\n",
    "hsi_6_0718_210_002\t102\n",
    "hsi_6_0718_210_002\t33\n",
    "hsi_6_0718_210_002\t18\n",
    "hsi_6_0718_209_001\t95\n",
    "hsi_3_0715_209_006\t18\n",
    "hsi_3_0715_227_001\t21\n",
    "hsi_4_0717_210_001\t47\n",
    "hsi_3_0715_210_010\t87\n",
    "hsi_3_0715_210_010\t15\n",
    "hsi_3_0715_209_006\t30\n",
    "hsi_3_0715_209_006\t43\n",
    "hsi_6_0718_211_002\t14\n",
    "\"\"\"\n",
    "\n",
    "manual_segments = {}\n",
    "for line in MANUAL.split(\"\\n\"):\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    parts = line.split(\"\\t\")\n",
    "    if not parts[0] in manual_segments:\n",
    "        manual_segments[parts[0]] = []\n",
    "    manual_segments[parts[0]].append(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsv_for_segment(segment, tsv_data, filename=None, segment_id=None):\n",
    "    start = segment[\"timing\"][\"phrase_start\"]\n",
    "    end = segment[\"timing\"][\"phrase_end\"]\n",
    "\n",
    "    tsv = slice_tsv_data(tsv_data, start, end)\n",
    "    tsv_words = \" \".join([x[\"word\"] for x in tsv])\n",
    "    print(\"TSV words\", tsv_words)\n",
    "\n",
    "    if filename and filename in manual_segments and segment_id and segment_id in manual_segments[filename]:\n",
    "        return tsv\n",
    "\n",
    "    if segment[\"utterance\"] != tsv_words:\n",
    "        cleaned_snippet = clean_text2(segment[\"utterance\"])\n",
    "        if \"[loud locking noise]\" in cleaned_snippet:\n",
    "            cleaned_snippet = cleaned_snippet.replace(\" [loud locking noise] \", \" \")\n",
    "        if \"(we'll)\" in cleaned_snippet:\n",
    "            cleaned_snippet = cleaned_snippet.replace(\"(we'll)\", \"we'll\")\n",
    "        cleaned_text = clean_text2(tsv_words)\n",
    "\n",
    "        if cleaned_snippet not in cleaned_text:\n",
    "            if filename is not None and segment_id is not None:\n",
    "                print(f\"{filename}\\t{segment_id}\\t{segment['utterance']}\\t{tsv_words}\")\n",
    "            else:\n",
    "                print(\"üôÄ mismatch:\", \"üñáÔ∏è\", segment[\"utterance\"], \"üéß\", tsv_words, cleaned_text.find(cleaned_snippet))\n",
    "            return []\n",
    "        else:\n",
    "            idxes = get_indices(cleaned_snippet, cleaned_text)\n",
    "            assert len(idxes) == 1\n",
    "            tsv = tsv[idxes[0][0]:idxes[0][1]]\n",
    "            tsv_words = \" \".join([x[\"word\"] for x in tsv])\n",
    "            cleaned_text = clean_text(tsv_words)\n",
    "            assert cleaned_snippet == cleaned_text, f\"üñáÔ∏è {cleaned_snippet} üéß {cleaned_text}\"\n",
    "    return tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"/shared/mm_conv/meta_final_set/meta_pronomial_single.json\"\n",
    "INPUT_FILE = \"/tmp/meta_pronomial_single.json\"\n",
    "with open(INPUT_FILE) as inf:\n",
    "    data = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'phrase': 'that', 'start': 6.112, 'end': 6.152, 'topic': 'room'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_timings[\"hsi_4_0717_222_003\"][\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV words I don't think it goes very well together with the old Farmers. I don't, I don't remember what that one's called.\n"
     ]
    }
   ],
   "source": [
    "TSVS = Path(\"/Users/joregan/Playing/hsi/word_annotations/\")\n",
    "base = \"hsi_4_0717_222_003\"\n",
    "tsv_data = load_tsv(str(TSVS / f\"{base}_main.tsv\"))\n",
    "a = get_tsv_for_segment(data[\"0\"], tsv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_collisions(timings, key):\n",
    "    collisions = []\n",
    "    clean_key = clean_text2(key)\n",
    "    if len(timings) == 1:\n",
    "        return []\n",
    "    for i in range(len(timings)):\n",
    "        phrase = clean_text2(timings[i][\"phrase\"])\n",
    "        if phrase == clean_key:\n",
    "            continue\n",
    "        if get_indices(clean_key, phrase) != []:\n",
    "            collisions.append(timings[i][\"phrase\"])\n",
    "    return collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(segment, mark_start = '<span style=\"background-color: yellow;\">', mark_end = '</span>'):\n",
    "    rec_id = segment[\"recording_id\"]\n",
    "    seg_id = segment[\"segment_id\"]\n",
    "    this_segment = object_timings[rec_id][seg_id]\n",
    "    phrase = clean_text2(segment[\"phrase\"])\n",
    "    utterance = segment[\"utterance\"]\n",
    "    if \"[loud locking noise]\" in utterance:\n",
    "        utterance = utterance.replace(\" [loud locking noise] \", \" \")\n",
    "\n",
    "    collisions = find_collisions(this_segment, phrase)\n",
    "\n",
    "    tsv_data = load_tsv(str(TSVS / f\"{rec_id}_main.tsv\"))\n",
    "    tsv = get_tsv_for_segment(segment, tsv_data)\n",
    "\n",
    "    filtered = []\n",
    "    for item in this_segment:\n",
    "        if clean_text2(item[\"phrase\"]) == phrase:\n",
    "            filtered.append(item)\n",
    "    \n",
    "    indices = get_indices(phrase, utterance)\n",
    "    if indices == []:\n",
    "        return utterance\n",
    "\n",
    "    utt_words = utterance.split(\" \")\n",
    "\n",
    "    collision_indices = []\n",
    "    if collisions != []:\n",
    "        for collision in collisions:\n",
    "            collision_indices += get_indices(collision, utterance)\n",
    "            for ii in indices:\n",
    "                i_start = tsv[ii[0]:ii[1]][0][\"start\"]\n",
    "                i_end = tsv[ii[0]:ii[1]][-1][\"end\"]\n",
    "                for ci in collision_indices:\n",
    "                    c_start = tsv[ci[0]:ci[1]][0][\"start\"]\n",
    "                    c_end = tsv[ci[0]:ci[1]][-1][\"end\"]\n",
    "                    if i_start >= c_start and i_end <= c_end:\n",
    "                        indices.remove(ii)\n",
    "\n",
    "    if len(indices) == 1:\n",
    "        pre = \" \".join(utt_words[0:indices[0][0]])\n",
    "        inner = \" \".join(utt_words[indices[0][0]:indices[0][1]])\n",
    "        end = \" \".join(utt_words[indices[0][1]:])\n",
    "\n",
    "        inner = mark_start + inner + mark_end\n",
    "        return \" \".join([pre, inner, end])\n",
    "    elif indices == []:\n",
    "        return utterance\n",
    "    else:\n",
    "        for filt in filtered:\n",
    "            f_start = filt[\"start\"]\n",
    "            i_starts = [tsv[i[0]:i[1]][0][\"start\"] for i in indices]\n",
    "            i_diffs = [abs(i - f_start) for i in i_starts]\n",
    "            closest = i_diffs.index(min(i_diffs))\n",
    "\n",
    "            pre = \" \".join(utt_words[0:indices[closest][0]])\n",
    "            inner = \" \".join(utt_words[indices[closest][0]:indices[closest][1]])\n",
    "            end = \" \".join(utt_words[indices[closest][1]:])\n",
    "\n",
    "            inner = mark_start + inner + mark_end\n",
    "            return \" \".join([pre, inner, end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data:\n",
    "    new = process_segment(data[key])\n",
    "    data[key][\"utterance\"] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/done.json\", \"w\") as outf:\n",
    "    json.dump(data, outf, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
