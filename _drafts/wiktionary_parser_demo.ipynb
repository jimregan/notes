{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8217dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = '''\n",
    "{{a|Ulster}} {{IPA|ga|/mˠaːsˠ/|/mˠaːʃ/|qual2=before {{m|ga|é}}, {{m|ga|ea}}, {{m|ga|í}}, {{m|ga|iad}} and their emphatic equivalents}}\n",
    "{{a|Galway}} {{IPA|ga|/lʲoːbˠ/}} {{a|corresponding to the spelling {{m|ga|leob}}}}<ref>{{R:ga:Finck|I|196}}</ref><ref>{{R:ga:GCFD|308}}</ref>\n",
    "{{IPA|ga|/n̪ˠõːsˠ/|ref={{R:ga:Quiggin|17}}}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1f5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lark\n",
      "  Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "693b1d12",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedToken",
     "evalue": "Unexpected token Token('__ANON_3', 'before') at line 1, column 20.\nExpected one of: \n\t* \"{{\"\n\t* INLINE_TEXT\nPrevious tokens: [Token('EQUAL', '=')]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedCharacters\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/lexer.py:665\u001b[0m, in \u001b[0;36mContextualLexer.lex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    664\u001b[0m         lexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexers[parser_state\u001b[38;5;241m.\u001b[39mposition]\n\u001b[0;32m--> 665\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mlexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/lexer.py:598\u001b[0m, in \u001b[0;36mBasicLexer.next_token\u001b[0;34m(self, lex_state, parser_state)\u001b[0m\n\u001b[1;32m    597\u001b[0m         allowed \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<END-OF-FILE>\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedCharacters(lex_state\u001b[38;5;241m.\u001b[39mtext, line_ctr\u001b[38;5;241m.\u001b[39mchar_pos, line_ctr\u001b[38;5;241m.\u001b[39mline, line_ctr\u001b[38;5;241m.\u001b[39mcolumn,\n\u001b[1;32m    599\u001b[0m                                allowed\u001b[38;5;241m=\u001b[39mallowed, token_history\u001b[38;5;241m=\u001b[39mlex_state\u001b[38;5;241m.\u001b[39mlast_token \u001b[38;5;129;01mand\u001b[39;00m [lex_state\u001b[38;5;241m.\u001b[39mlast_token],\n\u001b[1;32m    600\u001b[0m                                state\u001b[38;5;241m=\u001b[39mparser_state, terminals_by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminals_by_name)\n\u001b[1;32m    602\u001b[0m value, type_ \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mUnexpectedCharacters\u001b[0m: No terminal matches 'b' in the current parser context, at line 1 col 20\n\n{{IPA|ga|/x/|qual2=before {{m|ga|é}}, {{m|ga|ea}} and {{m|g\n                   ^\nExpected one of: \n\t* \"{{\"\n\t* INLINE_TEXT\n\nPrevious tokens: Token('EQUAL', '=')\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnexpectedToken\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WiktionaryTransformer()\u001b[38;5;241m.\u001b[39mtransform(tree)\n\u001b[1;32m     48\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mIPA|ga|/x/|qual2=before \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mm|ga|é}}, \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mm|ga|ea}} and \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mm|ga|í}}}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mparse_wiktionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(result, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mparse_wiktionary\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_wiktionary\u001b[39m(text):\n\u001b[1;32m     44\u001b[0m     parser \u001b[38;5;241m=\u001b[39m Lark(grammar, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m, parser\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlalr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WiktionaryTransformer()\u001b[38;5;241m.\u001b[39mtransform(tree)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/lark.py:655\u001b[0m, in \u001b[0;36mLark.parse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, start: Optional[\u001b[38;5;28mstr\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, on_error: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptional[Callable[[UnexpectedInput], bool]]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParseTree\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;124;03m\"\"\"Parse the given text, according to the options provided.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m \n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/parser_frontends.py:104\u001b[0m, in \u001b[0;36mParsingFrontend.parse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    102\u001b[0m kw \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_error\u001b[39m\u001b[38;5;124m'\u001b[39m: on_error}\n\u001b[1;32m    103\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_lexer_thread(text)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/parsers/lalr_parser.py:42\u001b[0m, in \u001b[0;36mLALR_Parser.parse\u001b[0;34m(self, lexer, start, on_error)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, lexer, start, on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/parsers/lalr_parser.py:88\u001b[0m, in \u001b[0;36m_Parser.parse\u001b[0;34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_interactive:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state\u001b[38;5;241m.\u001b[39mlexer)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/parsers/lalr_parser.py:111\u001b[0m, in \u001b[0;36m_Parser.parse_from_state\u001b[0;34m(self, state, last_token)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/parsers/lalr_parser.py:100\u001b[0m, in \u001b[0;36m_Parser.parse_from_state\u001b[0;34m(self, state, last_token)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     token \u001b[38;5;241m=\u001b[39m last_token\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mlexer\u001b[38;5;241m.\u001b[39mlex(state):\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         state\u001b[38;5;241m.\u001b[39mfeed_token(token)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages/lark/lexer.py:674\u001b[0m, in \u001b[0;36mContextualLexer.lex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    672\u001b[0m     last_token \u001b[38;5;241m=\u001b[39m lexer_state\u001b[38;5;241m.\u001b[39mlast_token  \u001b[38;5;66;03m# Save last_token. Calling root_lexer.next_token will change this to the wrong token\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_lexer\u001b[38;5;241m.\u001b[39mnext_token(lexer_state, parser_state)\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, e\u001b[38;5;241m.\u001b[39mallowed, state\u001b[38;5;241m=\u001b[39mparser_state, token_history\u001b[38;5;241m=\u001b[39m[last_token], terminals_by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_lexer\u001b[38;5;241m.\u001b[39mterminals_by_name)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedCharacters:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mUnexpectedToken\u001b[0m: Unexpected token Token('__ANON_3', 'before') at line 1, column 20.\nExpected one of: \n\t* \"{{\"\n\t* INLINE_TEXT\nPrevious tokens: [Token('EQUAL', '=')]\n"
     ]
    }
   ],
   "source": [
    "from lark import Lark, Transformer, v_args\n",
    "import json\n",
    "\n",
    "grammar = r\"\"\"\n",
    "start: template\n",
    "\n",
    "template: \"{{\" name ( \"|\" param )* \"}}\"\n",
    "\n",
    "param: key \"=\" value   -> named_param\n",
    "     | value           -> positional_param\n",
    "\n",
    "?value: (template | INLINE_TEXT)+\n",
    "\n",
    "INLINE_TEXT: /[^{}|=]+(?=(}}|\\|)|$)/\n",
    "\n",
    "key: /[a-zA-Z0-9_]+/\n",
    "name: /[a-zA-Z0-9_:]+/\n",
    "\n",
    "%import common.WS\n",
    "%ignore WS\n",
    "\"\"\"\n",
    "\n",
    "@v_args(inline=True)\n",
    "class WiktionaryTransformer(Transformer):\n",
    "    def template(self, name, *params):\n",
    "        return {\"template\": name, \"args\": list(params)}\n",
    "\n",
    "    def named_param(self, key, value):\n",
    "        return {key: value if isinstance(value, list) else [value]}\n",
    "\n",
    "    def positional_param(self, value):\n",
    "        return value if isinstance(value, list) else [value]\n",
    "\n",
    "    def INLINE_TEXT(self, token):\n",
    "        return token.value\n",
    "\n",
    "    def key(self, token):\n",
    "        return token.value\n",
    "\n",
    "    def name(self, token):\n",
    "        return token.value\n",
    "\n",
    "def parse_wiktionary(text):\n",
    "    parser = Lark(grammar, start=\"start\", parser=\"lalr\")\n",
    "    tree = parser.parse(text)\n",
    "    return WiktionaryTransformer().transform(tree)\n",
    "\n",
    "input_text = \"{{IPA|ga|/x/|qual2=before {{m|ga|é}}, {{m|ga|ea}} and {{m|ga|í}}}}\"\n",
    "result = parse_wiktionary(input_text)\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24ec139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from parsimonious.grammar import Grammar\n",
    "from parsimonious.nodes import NodeVisitor\n",
    "import json\n",
    "\n",
    "grammar = Grammar(\n",
    "    r\"\"\"\n",
    "    block           = (template / ref / text)+\n",
    "    template        = \"{{\" name ( \"|\" param )* \"}}\"\n",
    "    ref             = \"<ref>\" template \"</ref>\"\n",
    "    param           = named / positional\n",
    "    named           = key \"=\" value\n",
    "    positional      = value\n",
    "    value           = (template / text)+\n",
    "    name            = ~r\"[a-zA-Z0-9:_]+\"\n",
    "    key             = ~r\"[a-zA-Z0-9_]+\"\n",
    "    text            = ~r\"[^\\{\\}\\|\\=<]+\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "class ParseTreeVisitor(NodeVisitor):\n",
    "    def visit_block(self, node, children):\n",
    "        return children\n",
    "\n",
    "    def visit_template(self, node, children):\n",
    "        _, name, *param_parts, _ = children\n",
    "        params = [p for part in param_parts if part for p in (part if isinstance(part, list) else [part])]\n",
    "        return {\"template\": name.text, \"args\": params}\n",
    "\n",
    "    def visit_ref(self, node, children):\n",
    "        return {\"type\": \"ref\", \"content\": children[0]}\n",
    "\n",
    "    def visit_param(self, node, children):\n",
    "        return children[0]\n",
    "\n",
    "    def visit_named(self, node, children):\n",
    "        key, _, value = children\n",
    "        return {key.text: value}\n",
    "\n",
    "    def visit_positional(self, node, children):\n",
    "        return children[0]\n",
    "\n",
    "    def visit_value(self, node, children):\n",
    "        return children\n",
    "\n",
    "    def visit_text(self, node, _):\n",
    "        return node.text\n",
    "\n",
    "    def generic_visit(self, node, visited_children):\n",
    "        return visited_children or node\n",
    "\n",
    "def flatten(x):\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    elif isinstance(x, dict):\n",
    "        if x.get(\"template\") and x.get(\"args\"):\n",
    "            return {\n",
    "                \"template\": x[\"template\"],\n",
    "                \"args\": [flatten(a) for a in x[\"args\"]]\n",
    "            }\n",
    "        return x\n",
    "    elif isinstance(x, list):\n",
    "        flat = []\n",
    "        for item in x:\n",
    "            f = flatten(item)\n",
    "            if isinstance(f, list):\n",
    "                flat.extend(f)\n",
    "            else:\n",
    "                flat.append(f)\n",
    "        return flat\n",
    "    return x\n",
    "\n",
    "def render_qualifier(value):\n",
    "    parts = []\n",
    "    for v in value:\n",
    "        if isinstance(v, str):\n",
    "            parts.append(v)\n",
    "        elif isinstance(v, dict) and v.get(\"template\") == \"m\":\n",
    "            parts.append(flatten(v[\"args\"])[-1])\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "def normalize(parsed):\n",
    "    out = []\n",
    "    current_ipa = None\n",
    "\n",
    "    for item in parsed:\n",
    "        if isinstance(item, dict) and item.get(\"template\") == \"IPA\":\n",
    "            variants = []\n",
    "            qual_map = {}\n",
    "            ref_map = {}\n",
    "            for i, arg in enumerate(item[\"args\"]):\n",
    "                if isinstance(arg, dict) and len(arg) == 1:\n",
    "                    k, v = list(arg.items())[0]\n",
    "                    if k.startswith(\"qual\"):\n",
    "                        idx = int(k[4:]) - 1\n",
    "                        qual_map[idx] = flatten(v)\n",
    "                    elif k.startswith(\"ref\"):\n",
    "                        idx = int(k[3:]) - 1\n",
    "                        ref_map.setdefault(idx, []).append(flatten(v))\n",
    "                elif isinstance(arg, str) and arg.startswith(\"/\"):\n",
    "                    variants.append({\"ipa\": arg.strip(\"/\")})\n",
    "                elif isinstance(arg, list):\n",
    "                    for val in arg:\n",
    "                        if isinstance(val, str) and val.startswith(\"/\"):\n",
    "                            variants.append({\"ipa\": val.strip(\"/\")})\n",
    "\n",
    "            for i, var in enumerate(variants):\n",
    "                if i in qual_map:\n",
    "                    var[\"qualifier\"] = render_qualifier(qual_map[i])\n",
    "                if i in ref_map:\n",
    "                    var[\"refs\"] = ref_map[i]\n",
    "            current_ipa = {\"type\": \"ipa\", \"variants\": variants}\n",
    "            out.append(current_ipa)\n",
    "\n",
    "        elif isinstance(item, dict) and item.get(\"type\") == \"ref\":\n",
    "            if current_ipa and current_ipa[\"variants\"]:\n",
    "                current_ipa[\"variants\"][-1].setdefault(\"refs\", []).append(flatten(item[\"content\"]))\n",
    "\n",
    "    return out\n",
    "\n",
    "def parse_and_normalize(text):\n",
    "    tree = grammar.parse(text)\n",
    "    parsed = ParseTreeVisitor().visit(tree)\n",
    "    return json.dumps(normalize(parsed), indent=2, ensure_ascii=False)\n",
    "\n",
    "test_input = \"{{IPA|ga|/x/}}\"\n",
    "print(parse_and_normalize(test_input))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
