{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonetic rules, take 2\n",
    "\n",
    "> \"Second attempt at an earlier notebook\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- hidden: true\n",
    "- categories: [swedish, nst, lexicon, phonetic, rules]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier notebook is [here]({% post_url 2023-10-09-phonetic-rule-take-1 %})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ALPHABET = {\n",
    "    \"A\": [\"a\"],\n",
    "    \"B\": [\"be\", \"bé\"],\n",
    "    \"C\": [\"ce\", \"se\", \"sé\", \"cé\", \"si\", \"ci\"],\n",
    "    \"D\": [\"de\", \"dé\", \"di\"],\n",
    "    \"E\": [\"e\", \"é\"],\n",
    "    \"F\": [\"eff\", \"ef\"],\n",
    "    \"G\": [\"ge\", \"gé\", \"gi\"],\n",
    "    \"H\": [\"hå\", \"ho\"],\n",
    "    \"I\": [\"i\"],\n",
    "    \"J\": [\"ji\", \"gi\"],\n",
    "    \"K\": [\"kå\", \"ko\"],\n",
    "    \"L\": [\"ell\", \"el\"],\n",
    "    \"M\": [\"emm\", \"em\"],\n",
    "    \"N\": [\"enn\", \"en\"],\n",
    "    \"O\": [\"o\"],\n",
    "    \"P\": [\"pe\", \"pé\", \"pi\"],\n",
    "    \"Q\": [\"qu\"],\n",
    "    \"R\": [\"err\", \"er\", \"är\", \"ärr\"],\n",
    "    \"S\": [\"ess\", \"es\"],\n",
    "    \"T\": [\"te\", \"té\", \"ti\"],\n",
    "    \"U\": [\"u\"],\n",
    "    \"V\": [\"ve\", \"vé\", \"vi\"],\n",
    "    \"W\": [\"dubbelve\"],\n",
    "    \"X\": [\"ex\", \"ecz\", \"ecs\", \"eks\"],\n",
    "    \"Y\": [\"y\"],\n",
    "    \"Z\": [\"zäta\", \"säta\", \"seta\", \"zeta\"],\n",
    "    \"Å\": [\"å\"],\n",
    "    \"Ä\": [\"ä\"],\n",
    "    \"Ö\": [\"ö\"]\n",
    "}\n",
    "\n",
    "DIGITS = {\n",
    "    \"1\": [\"ett\"],\n",
    "    \"2\": [\"två\"],\n",
    "    \"3\": [\"tre\"],\n",
    "    \"4\": [\"fyra\"],\n",
    "    \"5\": [\"fem\"],\n",
    "    \"6\": [\"sex\"],\n",
    "    \"7\": [\"sju\"],\n",
    "    \"8\": [\"åtta\"],\n",
    "    \"9\": [\"nio\", \"ni\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = {k: sorted(v, key=len, reverse=True) for k,v in _ALPHABET.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2442206120015761721 1 487.78 0.32 essefu 1.0 <eps> ins\n",
    "2442206120015761721 1 488.26 0.16 fem 1.0 SfU5 sub\n",
    "```\n",
    "\n",
    "```\n",
    "2442206120015761721 1 490.6 0.519 tjugohundrafjorton 1.0 <eps> ins\n",
    "2442206120015761721 1 491.2 0.379 femton 1.0 <eps> ins\n",
    "2442206120015761721 1 491.74 0.779 etthundratjugofyra 1.0 2014/15:124 sub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALNUM = {**ALPHABET, **DIGITS}\n",
    "ALNUM_REGEX = {k: f\"({'|'.join(v)})\" for k,v in ALNUM.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "ACCEPT = [\n",
    "    (\"e\", \"ə\")\n",
    "]\n",
    "\n",
    "a = \"kamaren\"\n",
    "b = \"kamarən\"\n",
    "\n",
    "ok = False\n",
    "\n",
    "s = SequenceMatcher(None, a, b)\n",
    "for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "    if tag == \"replace\":\n",
    "        if (a[i1:i2], b[j1:j2]) in ACCEPT:\n",
    "            ok = True\n",
    "        else:\n",
    "            ok = False\n",
    "    elif tag == \"equal\":\n",
    "        ok = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer import phonemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'slʉt '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonemize(\"slut\", language='sv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sync_asr.utils.nst_lexicon import get_nst_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joregan/Playing/sync_asr\n",
      "Obtaining file:///Users/joregan/Playing/sync_asr\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytest in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from sync-asr==0.0.1) (8.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from sync-asr==0.0.1) (4.12.3)\n",
      "Requirement already satisfied: requests in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from sync-asr==0.0.1) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from beautifulsoup4->sync-asr==0.0.1) (2.5)\n",
      "Requirement already satisfied: iniconfig in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from pytest->sync-asr==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: packaging in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from pytest->sync-asr==0.0.1) (23.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.3.0 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from pytest->sync-asr==0.0.1) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from requests->sync-asr==0.0.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from requests->sync-asr==0.0.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from requests->sync-asr==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joregan/opt/anaconda3/envs/phonemizer/lib/python3.11/site-packages (from requests->sync-asr==0.0.1) (2024.2.2)\n",
      "Installing collected packages: sync-asr\n",
      "  Attempting uninstall: sync-asr\n",
      "    Found existing installation: sync-asr 0.0.1\n",
      "    Uninstalling sync-asr-0.0.1:\n",
      "      Successfully uninstalled sync-asr-0.0.1\n",
      "  Running setup.py develop for sync-asr\n",
      "Successfully installed sync-asr-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/joregan/Playing/sync_asr\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = get_nst_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for entry in lexicon:\n",
    "    if 'garbage' in entry and entry['garbage'] == 'GARB':\n",
    "        continue\n",
    "    else:\n",
    "        word = entry['orthography']\n",
    "        word = word.replace(\"_\", \" \")\n",
    "        if not word in dictionary:\n",
    "            dictionary[word] = set()\n",
    "        for translit in entry['transliterations']:\n",
    "            dictionary[word].add(translit['ipa'].replace(\".\", \"\").replace(\"¤\", \"\").replace(\"_\", \" \").replace(\"\\u0361\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSONANTS = \"bdfhjklmnprstvŋɕɖɡɧɭɳʂʈ\"\n",
    "VOWELS = \"aeiouyøɪɑɔɛɵʉʊʏ\"\n",
    "ACCENTS = \"²ˌˈ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'²ɪnte'}\n",
      "ɪntɛ \n"
     ]
    }
   ],
   "source": [
    "print(dictionary[\"inte\"])\n",
    "print(phonemize(\"inte\", language='sv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set()\n",
    "for entry in dictionary:\n",
    "    for pron in dictionary[entry]:\n",
    "        for char in pron:\n",
    "            characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = set()\n",
    "VS = VOWELS + ACCENTS + \"_\"\n",
    "for entry in dictionary:\n",
    "    for pron in dictionary[entry]:\n",
    "        for char in pron:\n",
    "            if char not in VS:\n",
    "                cons.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_accent(ipa):\n",
    "    accent = \"\"\n",
    "    output = \"\"\n",
    "    for character in ipa:\n",
    "        if character in ACCENTS:\n",
    "            accent = character\n",
    "        elif character in CONSONANTS:\n",
    "            output += character\n",
    "        elif character in VOWELS:\n",
    "            if accent != \"\":\n",
    "                output += accent\n",
    "                accent = \"\"\n",
    "            output += character\n",
    "        else:\n",
    "            output += character\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kr²atʊʂ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_accent(\"²kratʊʂ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "class Rule():\n",
    "    def __init__(self, match, replacement, rulename, example, on_accented = False, before_accented = False):\n",
    "        self.match = match\n",
    "        self.replacement = replacement\n",
    "        self.rulename = rulename\n",
    "        self.example = example\n",
    "        self.on_accented = on_accented\n",
    "        self.before_accented = before_accented\n",
    "\n",
    "    def apply(self, word):\n",
    "        if self.on_accented:\n",
    "            pattern = fr\"((?:[^²ˌˈ]){self.match}|^{self.match})\"\n",
    "        elif self.before_accented:\n",
    "            pattern = fr\"({self.match}(?:[^²ˌˈ]))\"\n",
    "        else:\n",
    "            word = re.sub(\"[²ˌˈ]\", \"\", word)\n",
    "            pattern = self.match\n",
    "        matches = [(m.start(), m.end()) for m in re.finditer(pattern, word)]\n",
    "        if self.on_accented:\n",
    "            tmp = []\n",
    "            for m in matches:\n",
    "                if m[1] - m[0] > len(self.match):\n",
    "                    tmp.append((m[1] - len(self.match), m[1]))\n",
    "                else:\n",
    "                    tmp.append(m)\n",
    "            matches = tmp\n",
    "\n",
    "        pieces = []\n",
    "        prev_end = 0\n",
    "        for piece in matches:\n",
    "            if piece[0] > 0:\n",
    "                pieces.append([word[prev_end:piece[0]]])\n",
    "            pieces.append([word[piece[0]:piece[1]], self.replacement])\n",
    "            prev_end = piece[1]\n",
    "        pieces.append([word[prev_end:]])\n",
    "\n",
    "        output = []\n",
    "        for part in itertools.product(*pieces):\n",
    "            output.append(\"\".join(part))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = Rule(\"nh\", \"n\", \"n → ∅ / _ h\", \"Stenholm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rule.apply(\"nhanhanha\") == ['nhanhanha',\n",
    " 'nhanhana',\n",
    " 'nhananha',\n",
    " 'nhanana',\n",
    " 'nanhanha',\n",
    " 'nanhana',\n",
    " 'nananha',\n",
    " 'nanana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basic idea with these is that they just apply anyway, because the input\n",
    "# is single words, but at some later processing stage we can validate\n",
    "class AssimilationRule(Rule):\n",
    "    def __init__(self, match, replacement, rulename, example, on_accented = False, before_accented = False, pre_context = \"\", post_context = \"\"):\n",
    "        super.__init__(match, replacement, rulename, example, on_accented, before_accented)\n",
    "        self.pre_context = pre_context\n",
    "        self.post_context = post_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor '__init__' requires a 'super' object but received a 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m GENERAL_STRESSED \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     Rule(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mə\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me → ə / [-stressed]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m     Rule(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mɛ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mə\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me → ə / [-stressed]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mAssimilationRule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh → ∅ / r # _\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhar han\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m AssimilationRule(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh → ∅ / n # _\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhan har\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m AssimilationRule(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr → ∅ / _ # [+consonant]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdär bilen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONSONANTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mAssimilationRule.__init__\u001b[0;34m(self, match, replacement, rulename, example, on_accented, before_accented, pre_context, post_context)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, match, replacement, rulename, example, on_accented \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, before_accented \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, pre_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, post_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrulename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_accented\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbefore_accented\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_context \u001b[38;5;241m=\u001b[39m pre_context\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_context \u001b[38;5;241m=\u001b[39m post_context\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor '__init__' requires a 'super' object but received a 'str'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "GENERAL_STRESSED = [\n",
    "    Rule(\"e\", \"ə\", \"e → ə / [-stressed]\", \"\", True),\n",
    "    Rule(\"ɛ\", \"ə\", \"e → ə / [-stressed]\", \"\", True)\n",
    "]\n",
    "\n",
    "AssimilationRule(\"r$\", \"\", \"h → ∅ / r # _\", \"har han\", False, False, \"\", \"h\")\n",
    "AssimilationRule(\"n$\", \"\", \"h → ∅ / n # _\", \"han har\", False, False, \"\", \"h\")\n",
    "AssimilationRule(\"r$\", \"\", \"r → ∅ / _ # [+consonant]\", \"där bilen\", False, False, \"\", f\"[{CONSONANTS}]\")\n",
    "\n",
    "Rule(\"[œɶeə]r\", \"r\", \"e → ∅ / _ r [+stressed]\", \"bero\", False, True)\n",
    "\t\n",
    "Rule(\"ɪntə\", \"ntə\", \"ɪ → ∅ / [+vowel] # _ n t ə\", \"ska inte\", False, False)\n",
    "Rule(\"ɪnte\", \"nte\", \"ɪ → ∅ / [+vowel] # _ n t e\", \"ska inte\", False, False)\n",
    "\n",
    "Rule(\"nh\", \"h\", \"n → ∅ / _ h\", \"Stenholm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syncasr_stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
