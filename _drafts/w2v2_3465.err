/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]
Device set to use cuda:0
Traceback (most recent call last):
  File "/nfs/tts2/home/joregan/run_w2v2.py", line 7, in <module>
    pipe = pipeline(model=MODEL, device=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 1230, in pipeline
    return pipeline_class(model=model, framework=framework, task=task, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py", line 216, in __init__
    super().__init__(model, tokenizer, feature_extractor, device=device, **kwargs)
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/transformers/pipelines/base.py", line 1044, in __init__
    self.model.to(self.device)
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4459, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1369, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 928, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 955, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/nfs/tts2/home/joregan/miniconda3/envs/hf/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in convert
    return t.to(
           ^^^^^
torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

