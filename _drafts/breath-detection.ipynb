{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bibtex\n",
    "@INPROCEEDINGS{szekely2019casting,\n",
    "  author={Székely, Éva and Henter, Gustav Eje and Gustafson, Joakim},\n",
    "  booktitle={Proc. ICASSP 2019}, \n",
    "  title={Casting to corpus: Segmenting and selecting spontaneous dialogue for {TTS} with a {CNN-LSTM} speaker-dependent breath detector}, \n",
    "  year={2019},\n",
    "  pages={6925-6929},\n",
    "  doi={10.1109/ICASSP.2019.8683846}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "helpers = requests.get(\"https://raw.githubusercontent.com/BirgerMoell/tmh/master/tmh/breath_detection/support_scripts/helpers.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(\"\\n\".join(helpers.text.split(\"\\n\")[83:94]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zcr_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorvec(inp, maxzcr=0.4,\n",
    "             low_slow=np.array([0., 255., 255.]),\n",
    "             low_fast=np.array([255., 255., 255.]),\n",
    "             high_slow=np.array([0., 0., 0.]),\n",
    "             high_fast=np.array([255., 0., 0.])):\n",
    "    spec, zcr = inp\n",
    "\n",
    "    spec_width = spec.shape[1]\n",
    "    zcr2 = np.interp(range(spec_width), \n",
    "                     np.linspace(0, spec_width, len(zcr)), \n",
    "                     zcr)\n",
    "\n",
    "    spec2 = np.abs(spec) / 80\n",
    "    outp = np.zeros((*spec2.shape, 3))\n",
    "    z = np.clip(zcr2 / maxzcr, 0, 1)\n",
    "\n",
    "    low = low_slow[:, np.newaxis] + (low_fast - low_slow)[:, np.newaxis] * z\n",
    "    high = high_slow[:, np.newaxis] + (high_fast - high_slow)[:, np.newaxis] * z\n",
    "\n",
    "    for k in range(3):\n",
    "        outp[:, :, k] = np.tile(low[k], (spec2.shape[0], 1)) + \\\n",
    "                        spec2 * np.tile(high[k] - low[k], (spec2.shape[0], 1))\n",
    "\n",
    "    outp /= 255\n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr_rate(wav_in, step=240, sz=960):\n",
    "    cross = np.abs(np.diff(np.sign(wav_in + 1e-8)))\n",
    "    cross = np.minimum(cross, 1)\n",
    "\n",
    "    steps = int((len(cross) - sz) / step)\n",
    "\n",
    "    zrate = np.array([np.mean(cross[i*step:i*step+sz]) for i in range(steps)])\n",
    "\n",
    "    return zrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def zcr_kernel(\n",
    "    wav_ptr, zrate_ptr,\n",
    "    step, sz, n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "\n",
    "    for i in range(block_start, block_start + BLOCK_SIZE):\n",
    "        if i < (n_elements - sz) // step:\n",
    "            sum_cross = 0.0\n",
    "            prev_sign = tl.sign(tl.load(wav_ptr + i * step) + 1e-8)\n",
    "            for j in range(1, sz):\n",
    "                curr_sign = tl.sign(tl.load(wav_ptr + i * step + j) + 1e-8)\n",
    "                sum_cross += tl.abs(curr_sign - prev_sign) / 2\n",
    "                prev_sign = curr_sign\n",
    "            zrate = sum_cross / sz\n",
    "            tl.store(zrate_ptr + i, zrate)\n",
    "\n",
    "def zcr_rate_gpu(wav_in, step=240, sz=960):\n",
    "    n_elements = wav_in.shape[0]\n",
    "    steps = (n_elements - sz) // step\n",
    "    zrate = torch.zeros(steps, dtype=torch.float32, device='cuda')\n",
    "\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    zcr_kernel[grid](wav_in, zrate, step, sz, n_elements, BLOCK_SIZE=1024)\n",
    "\n",
    "    return zrate\n",
    "\n",
    "# Load input wav and convert to tensor\n",
    "y, sr = torchaudio.load(input_root + input_file)\n",
    "wav_out = y[0].cuda()  # Assuming mono audio\n",
    "samples = len(wav_out) // (2 * sr)\n",
    "wav_in = wav_out[:2*sr*samples].reshape(samples, 2*sr)\n",
    "\n",
    "# Create mel-spectrogram\n",
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sr,\n",
    "    n_fft=sr // 50,\n",
    "    hop_length=(sr // 50) // 8,\n",
    "    n_mels=128\n",
    ").cuda()\n",
    "\n",
    "melspecs = mel_spec(wav_in.T).permute(1, 0, 2)\n",
    "\n",
    "# Calculate zero crossing rate\n",
    "zrates = torch.stack([zcr_rate_gpu(wav) for wav in wav_in])\n",
    "\n",
    "# Create zcr-colored melspectrograms\n",
    "def colorvec2_gpu(inp, maxzcr=0.4, low_slow=torch.tensor([0., 255., 255.]), \n",
    "                  low_fast=torch.tensor([255., 255., 255.]), \n",
    "                  high_slow=torch.tensor([0., 0., 0.]), \n",
    "                  high_fast=torch.tensor([255., 0., 0.])):\n",
    "    spec, zcr = inp\n",
    "    zcr2 = torch.nn.functional.interpolate(zcr.unsqueeze(0).unsqueeze(0), size=spec.shape[1], mode='linear', align_corners=False).squeeze()\n",
    "    spec2 = spec.abs() / 80\n",
    "    z = torch.clamp(zcr2 / maxzcr, 0, 1)\n",
    "    low = low_slow.unsqueeze(1) + (low_fast - low_slow).unsqueeze(1) * z\n",
    "    high = high_slow.unsqueeze(1) + (high_fast - high_slow).unsqueeze(1) * z\n",
    "    outp = torch.stack([\n",
    "        torch.tile(low[k], (spec2.shape[0], 1)) + spec2 * torch.tile(high[k] - low[k], (spec2.shape[0], 1))\n",
    "        for k in range(3)\n",
    "    ]).permute(1, 2, 0)\n",
    "    return outp / 255\n",
    "\n",
    "colspecs = [colorvec2_gpu((spec, zrate)) for spec, zrate in zip(melspecs, zrates)]\n",
    "x_complete = torch.stack(colspecs).float()\n",
    "\n",
    "# Convert back to CPU if needed\n",
    "x_complete = x_complete.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def create_melspec(wav_in, sr=None, n_fft=960, hop_length=120, n_mels=128):\n",
    "    if sr is None:\n",
    "        sr = min(48000, len(wav_in) // 2)\n",
    "        n_fft = sr // 50\n",
    "        hop_length = sr // 400\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=wav_in, \n",
    "        sr=sr, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length, \n",
    "        n_mels=n_mels,\n",
    "        power=1\n",
    "    )\n",
    "    log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    return log_S.astype(np.float32)\n",
    "\n",
    "def zcr_rate(wav_in, step=240, sz=960):\n",
    "    cross = np.abs(np.diff(np.sign(wav_in + 1e-8)))\n",
    "    cross = np.minimum(cross, 1)\n",
    "    steps = (len(cross) - sz) // step\n",
    "    return np.array([np.mean(cross[i*step:i*step+sz]) for i in range(steps)])\n",
    "\n",
    "def colorvec2(inp, maxzcr=0.4, low_slow=np.array([0., 255., 255.]), \n",
    "              low_fast=np.array([255., 255., 255.]), \n",
    "              high_slow=np.array([0., 0., 0.]), \n",
    "              high_fast=np.array([255., 0., 0.])):\n",
    "    spec, zcr = inp\n",
    "    zcr2 = np.interp(np.arange(spec.shape[1]), \n",
    "                     np.linspace(0, spec.shape[1], len(zcr)), zcr)\n",
    "    spec2 = np.abs(spec) / 80\n",
    "    z = np.clip(zcr2 / maxzcr, 0, 1)\n",
    "    low = low_slow[:, np.newaxis] + (low_fast - low_slow)[:, np.newaxis] * z\n",
    "    high = high_slow[:, np.newaxis] + (high_fast - high_slow)[:, np.newaxis] * z\n",
    "    outp = np.zeros((spec2.shape[0], spec2.shape[1], 3))\n",
    "    for k in range(3):\n",
    "        outp[:, :, k] = np.tile(low[k], (spec2.shape[0], 1)) + \\\n",
    "                        spec2 * np.tile(high[k] - low[k], (spec2.shape[0], 1))\n",
    "    return outp / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from codes.helpers import load_wav, create_melspec, zcr_rate, colorvec2\n",
    "\n",
    "# Load input wav and split into two second samples\n",
    "y = load_wav(input_root + input_file, sr=sr)\n",
    "wav_out = np.asarray(y[1])\n",
    "samples = len(wav_out) // (2 * sr)\n",
    "wav_in = np.reshape(wav_out[:(2*sr*samples)], (samples, 2*sr))\n",
    "\n",
    "# Create mel-spectrogram and calculate zero crossing rate\n",
    "pool = Pool()\n",
    "ins = [wav_in[r, :] for r in range(samples)]\n",
    "\n",
    "melspecs = pool.starmap(create_melspec, [(wav, sr) for wav in ins])\n",
    "zrates = pool.map(zcr_rate, ins)\n",
    "\n",
    "# Create zcr-colored melspectrograms\n",
    "colspecs = pool.starmap(colorvec2, zip(melspecs, zrates))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Convert to numpy array if needed\n",
    "x_complete = np.array(colspecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BreathAnalysisModel(nn.Module):\n",
    "    def __init__(self, timesteps, img_rows, img_cols, num_classes):\n",
    "        super(BreathAnalysisModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(5, 4))\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=(4, 1), stride=(4, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(6, 5))\n",
    "        self.lstm = nn.LSTM(input_size=self.calculate_lstm_input_size(img_rows, img_cols),\n",
    "                            hidden_size=8, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(16, num_classes)\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def calculate_lstm_input_size(self, img_rows, img_cols):\n",
    "        x = torch.randn(1, 3, img_rows, img_cols)\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, timesteps, channels, height, width)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size * self.timesteps, 3, x.size(3), x.size(4))\n",
    "        \n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(batch_size, self.timesteps, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def convert_keras_to_pytorch(h5_path, output_path):\n",
    "    # Load the Keras model weights\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        # Create a dictionary to store PyTorch state dict\n",
    "        state_dict = {}\n",
    "\n",
    "        # Iterate through layers in the H5 file\n",
    "        for layer_name in f.keys():\n",
    "            if isinstance(f[layer_name], h5py.Group):\n",
    "                for param_name in f[layer_name].keys():\n",
    "                    weight = f[layer_name][param_name][:]\n",
    "                    \n",
    "                    # Convert weights to PyTorch tensors\n",
    "                    if 'kernel' in param_name:\n",
    "                        if 'conv2d' in layer_name:\n",
    "                            weight = np.transpose(weight, (3, 2, 0, 1))\n",
    "                        elif 'dense' in layer_name:\n",
    "                            weight = np.transpose(weight)\n",
    "                    \n",
    "                    # Rename keys to PyTorch convention\n",
    "                    if 'kernel' in param_name:\n",
    "                        param_name = 'weight'\n",
    "                    elif 'gamma' in param_name:\n",
    "                        param_name = 'weight'\n",
    "                    elif 'beta' in param_name:\n",
    "                        param_name = 'bias'\n",
    "                    elif 'moving_mean' in param_name:\n",
    "                        param_name = 'running_mean'\n",
    "                    elif 'moving_variance' in param_name:\n",
    "                        param_name = 'running_var'\n",
    "                    \n",
    "                    # Special handling for LSTM weights\n",
    "                    if 'lstm' in layer_name:\n",
    "                        if 'kernel' in param_name:\n",
    "                            ih_weight = weight[:, :32]\n",
    "                            hh_weight = weight[:, 32:]\n",
    "                            state_dict[f'{layer_name}.weight_ih_l0'] = torch.FloatTensor(ih_weight)\n",
    "                            state_dict[f'{layer_name}.weight_hh_l0'] = torch.FloatTensor(hh_weight)\n",
    "                        elif 'bias' in param_name:\n",
    "                            state_dict[f'{layer_name}.bias_ih_l0'] = torch.FloatTensor(weight)\n",
    "                            state_dict[f'{layer_name}.bias_hh_l0'] = torch.FloatTensor(weight)\n",
    "                    else:\n",
    "                        state_dict[f'{layer_name}.{param_name}'] = torch.FloatTensor(weight)\n",
    "\n",
    "    # Save the state dict as a PyTorch checkpoint\n",
    "    torch.save(state_dict, output_path)\n",
    "    print(f\"PyTorch checkpoint saved to {output_path}\")\n",
    "\n",
    "# Usage\n",
    "h5_path = output_root + model_name + '.h5'\n",
    "output_path = os.path.splitext(h5_path)[0] + '.pth'\n",
    "convert_keras_to_pytorch(h5_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
