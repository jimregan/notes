{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":270049783,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get -qq update\n!apt-get -qq install -y hunspell git libhunspell-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:40:08.658153Z","iopub.execute_input":"2025-10-22T22:40:08.658479Z","iopub.status.idle":"2025-10-22T22:40:18.215454Z","shell.execute_reply.started":"2025-10-22T22:40:08.658448Z","shell.execute_reply":"2025-10-22T22:40:18.214167Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nSelecting previously unselected package libhunspell-dev:amd64.\n(Reading database ... 128735 files and directories currently installed.)\nPreparing to unpack .../libhunspell-dev_1.7.0-4build1_amd64.deb ...\nUnpacking libhunspell-dev:amd64 (1.7.0-4build1) ...\nSetting up libhunspell-dev:amd64 (1.7.0-4build1) ...\nProcessing triggers for man-db (2.10.2-1) ...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install hunspell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:40:28.460080Z","iopub.execute_input":"2025-10-22T22:40:28.460360Z","iopub.status.idle":"2025-10-22T22:40:34.734239Z","shell.execute_reply.started":"2025-10-22T22:40:28.460339Z","shell.execute_reply":"2025-10-22T22:40:34.732667Z"}},"outputs":[{"name":"stdout","text":"Collecting hunspell\n  Using cached hunspell-0.5.5.tar.gz (34 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: hunspell\n  Building wheel for hunspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for hunspell: filename=hunspell-0.5.5-cp311-cp311-linux_x86_64.whl size=66312 sha256=0b5000b26d6eb6e89d8cdd3e49a07e82d1beb9073da82a0aa180dad86c80c24c\n  Stored in directory: /root/.cache/pip/wheels/0b/41/b3/14ebfe8dfb3116e3f1ab55ff0db766d1ef033b6842ccc67e24\nSuccessfully built hunspell\nInstalling collected packages: hunspell\nSuccessfully installed hunspell-0.5.5\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!echo $PWD/dictionaries/dictionaries","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:37:25.069802Z","iopub.execute_input":"2025-10-22T22:37:25.070125Z","iopub.status.idle":"2025-10-22T22:37:25.193273Z","shell.execute_reply.started":"2025-10-22T22:37:25.070099Z","shell.execute_reply":"2025-10-22T22:37:25.192455Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/dictionaries/dictionaries\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!git clone https://github.com/wooorm/dictionaries\n!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:41:41.478172Z","iopub.execute_input":"2025-10-22T22:41:41.478471Z","iopub.status.idle":"2025-10-22T22:41:41.726056Z","shell.execute_reply.started":"2025-10-22T22:41:41.478447Z","shell.execute_reply":"2025-10-22T22:41:41.725042Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'dictionaries' already exists and is not an empty directory.\n/kaggle/working\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os, re, glob, pandas as pd, hunspell\n\nFILE_DIR = \"/kaggle/input/split-braxen-by-language\"\nDICT_ROOT = \"/kaggle/working/dictionaries/dictionaries\"  # as you showed\nOUT_TSV = \"hunspell_results.tsv\"\nMIN_ENTRIES = 1000\nSKIP_CODES = {\"afr\",\"asi\",\"aus\",\"sla\",\"mix\",\"fisa\"}\n\n# find index.aff/index.dic pairs under your DICT_ROOT\npairs = {}\nfor aff in glob.glob(os.path.join(DICT_ROOT, \"*\", \"index.aff\")):\n    d = os.path.dirname(aff)\n    dic = os.path.join(d, \"index.dic\")\n    code = os.path.basename(d)\n    if os.path.isfile(dic):\n        pairs[code] = (dic, aff)\n\nCODE2DICT = {\n    \"lat\":[\"la\"],\n    \"swe\":[\"sv\"],\n    \"nor\":[\"nb\",\"nn\"],\n    \"dan\":[\"da\"],\n    \"isl\":[\"is\"],\n    \"fin\":[\"fi\"],\n    \"est\":[\"et\"],\n    \"lav\":[\"lv\"],\n    \"lit\":[\"lt\"],\n    \"pol\":[\"pl\"],\n    \"cze\":[\"cs\"],\n    \"slk\":[\"sk\"],\n    \"slv\":[\"sl\"],\n    \"hrv\":[\"hr\"],\n    \"srp\":[\"sr-Latn\"],\n    \"bos\":[\"bs\"],\n    \"mkd\":[\"mk\"],\n    \"bul\":[\"bg\"],\n    \"ukr\":[\"uk\"],\n    \"rus\":[\"ru\"],\n    \"deu\":[\"de\"],\n    \"nld\":[\"nl\",\"dut\"],\n    \"eng\":[\"en\",\"en-GB\",\"en-CA\",\"en-AU\",\"en-ZA\"],\n    \"fre\":[\"fr\"],\n    \"ita\":[\"it\"],\n    \"spa\":[\"es\",\"es-MX\",\"es-AR\",\"es-CL\",\"es-ES\"],\n    \"por\":[\"pt\",\"pt-PT\"],\n    \"rom\":[\"ro\"],\n    \"hun\":[\"hu\"],\n    \"tur\":[\"tr\"],\n    \"gre\":[\"el\"],\n}\n\nWORD_RE = re.compile(r\"[^\\W\\d_][\\w’'\\-\\u2011\\u2013\\u2014]*\", re.UNICODE)\ndef tokenize(t): return WORD_RE.findall(t)\ndef read_text(p):\n    b = open(p,\"rb\").read()\n    try: return b.decode(\"utf-8\")\n    except UnicodeDecodeError: return b.decode(\"utf-8\", errors=\"ignore\")\ndef file_code(p):\n    b = os.path.basename(p)\n    return b[len(\"braxen-\"):-4] if b.startswith(\"braxen-\") else b\n\ndef load_hs(dict_codes):\n    for c in dict_codes:\n        if c in pairs:\n            dic, aff = pairs[c]\n            return hunspell.HunSpell(dic, aff), c\n    return None, None\n\nfiles = [os.path.join(FILE_DIR, f) for f in os.listdir(FILE_DIR) if f.startswith(\"braxen-\") and f.endswith(\".txt\")]\n\nfile_words, file_sizes = {}, {}\nfor p in files:\n    ws = set(tokenize(read_text(p)))\n    file_words[p] = ws\n    file_sizes[p] = len(ws)\n\ncandidates = []\nfor p, n in file_sizes.items():\n    code = file_code(p)\n    if n >= MIN_ENTRIES and code not in SKIP_CODES and code in CODE2DICT:\n        candidates.append(p)\n\nrows = []\nfor p in sorted(candidates):\n    code = file_code(p)\n    hs, used = load_hs(CODE2DICT[code])\n    if not hs:\n        print(f\"skip {code}: dict not found for {CODE2DICT[code]}\")\n        continue\n    words = sorted(file_words[p])\n    print(f\"{code}: {len(words)} tokens via {used}\")\n    for w in words:\n        if not w: \n            continue\n        if all(ord(ch) < 128 for ch in w) and len(w) < 2:\n            continue\n        wcheck = w.replace(\"ö\",\"ø\").replace(\"Ö\",\"Ø\") if code in {\"nor\",\"dan\"} else w\n        if hs.spell(wcheck):\n            rows.append((code, w, \"OK\", \"\"))\n        else:\n            sugs = \", \".join(hs.suggest(wcheck))\n            rows.append((code, w, \"MISS\", sugs))\n\ndf = pd.DataFrame(rows, columns=[\"file_code\",\"word\",\"status\",\"suggestions\"])\ndf.to_csv(OUT_TSV, sep=\"\\t\", index=False, encoding=\"utf-8\")\nprint(f\"Wrote {OUT_TSV} with {len(df):,} rows\")\ndf.head(20)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T23:13:43.182465Z","iopub.execute_input":"2025-10-22T23:13:43.182768Z"}},"outputs":[{"name":"stdout","text":"dan: 1941 tokens via da\n","output_type":"stream"}],"execution_count":null}]}