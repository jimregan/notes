{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gq4eL7Xj9psB"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'stanza (Python 3.10.17)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n stanza ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PHI6wzlqAk3t"
      },
      "outputs": [],
      "source": [
        "import urllib.parse, urllib.request, json, sys\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a-aFOYu_AoGu"
      },
      "outputs": [],
      "source": [
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "def standardise(text: str, lang: str = \"ga\"):\n",
        "    \"\"\"Return a list of (orig_tok, std_tok) pairs from Intergaelic.\"\"\"\n",
        "    data   = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    hdrs   = {\"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "              \"Accept\":        \"application/json\"}\n",
        "    req    = urllib.request.Request(STD_API, data, hdrs)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        return json.loads(resp.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a1WtCT_ZArRt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/var/folders/d9/cbkhg23x349_t692zq6yhcv00000gn/T/ipykernel_84400/1526126554.py\", line 1, in <module>\n",
            "    import stanza\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/__init__.py\", line 1, in <module>\n",
            "    from stanza.pipeline.core import DownloadMethod, Pipeline\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/pipeline/core.py\", line 17, in <module>\n",
            "    from stanza.models.common.doc import Document\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/models/common/doc.py\", line 17, in <module>\n",
            "    from stanza.models.common.utils import misc_to_space_after, space_after_to_misc, misc_to_space_before, space_before_to_misc\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/models/common/utils.py\", line 19, in <module>\n",
            "    import torch\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    # Let Stanza decide sentences & tokens\n",
        "    tokenize_pretokenized=True,\n",
        "    no_ssplit=True,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "knN_HAk5Auzx"
      },
      "outputs": [],
      "source": [
        "# Cell ▸ robust projection with multi‑word support\n",
        "# -----------------------------------------------\n",
        "from itertools import groupby\n",
        "from typing import List, Tuple\n",
        "\n",
        "def _split_std(std: str, orig: str) -> List[str]:\n",
        "    \"\"\"Return the token(s) that should feed Stanza for this pair.\"\"\"\n",
        "    if not std.strip():                         # e.g. Do → \"\"  → keep 'Do'\n",
        "        return [orig]\n",
        "    return std.split()                          # may yield 1‑N tokens\n",
        "\n",
        "def _sentences_from_pairs(pairs: List[Tuple[str, str]]):\n",
        "    \"\"\"Very light sentence splitter: keep everything up to . ! ?\"\"\"\n",
        "    sent, buf = [], []\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        parts = _split_std(std, orig)\n",
        "        for j, part in enumerate(parts):\n",
        "            buf.append((i, j, len(parts), orig, part))  # mapping entry\n",
        "            if part in {\".\", \"!\", \"?\"}:\n",
        "                sent.append(buf);  buf = []\n",
        "    if buf:\n",
        "        sent.append(buf)\n",
        "    return sent                                       # [[mapping …], …]\n",
        "\n",
        "def project_with_stanza(raw_text: str, lang: str = \"ga\") -> str:\n",
        "    # 1 ── standardise -------------------------------------------------------\n",
        "    pairs  = standardise(raw_text, lang)             # [(orig, std), …]\n",
        "\n",
        "    # 2 ── build *pre‑tokenised* input & a mapping table ---------------------\n",
        "    sents  = _sentences_from_pairs(pairs)            # list‑of‑sentences\n",
        "    pretok = [[m[4] for m in sent] for sent in sents]  # token strings only\n",
        "\n",
        "    # 3 ── parse with Stanza *pretok* mode -----------------------------------\n",
        "    doc = nlp(pretok)                                # same shape as `sents`\n",
        "\n",
        "    # 4 ── project back, keeping multi‑word tokens ---------------------------\n",
        "    conllu_lines = []\n",
        "    for sid, (sent_map, sent_doc) in enumerate(zip(sents, doc.sentences), 1):\n",
        "        # comment lines\n",
        "        raw_slice = [m[3]           for m in sent_map if m[1] == 0]     # first sub‑token per orig\n",
        "        std_slice = [m[4]           for m in sent_map]                  # every sub‑token\n",
        "        conllu_lines += [\n",
        "            f\"# sent_id = {sid}\",\n",
        "            f\"# text = {' '.join(raw_slice)}\",\n",
        "            f\"# text_standard = {' '.join(std_slice)}\",\n",
        "        ]\n",
        "\n",
        "        # token lines\n",
        "        widx = 0                                     # index in sent_doc.words\n",
        "        tid  = 1                                     # running token ID in CONLL‑U\n",
        "        for m in sent_map:\n",
        "            orig_i, sub_i, n_sub, orig_tok, std_tok = m\n",
        "            word = sent_doc.words[widx]\n",
        "\n",
        "            if sub_i == 0 and n_sub > 1:             # multi‑word‑token header\n",
        "                conllu_lines.append(f\"{tid}-{tid+n_sub-1}\\t{orig_tok}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\")\n",
        "\n",
        "            # choose FORM for the sub‑token\n",
        "            form = orig_tok if n_sub == 1 else std_tok\n",
        "\n",
        "            conllu_lines.append(\"\\t\".join([\n",
        "                str(tid),\n",
        "                form,\n",
        "                word.lemma or \"_\",\n",
        "                word.upos  or \"_\",\n",
        "                word.xpos  or \"_\",\n",
        "                word.feats or \"_\",\n",
        "                str(word.head) if word.head else \"_\",\n",
        "                word.deprel or \"_\",\n",
        "                \"_\",\n",
        "                \"_\",\n",
        "            ]))\n",
        "\n",
        "            widx += 1\n",
        "            tid  += 1\n",
        "        conllu_lines.append(\"\")                      # blank line between sents\n",
        "\n",
        "    return \"\\n\".join(conllu_lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ei2TN5Xy-gkI"
      },
      "outputs": [],
      "source": [
        "def run_parse(raw_text):\n",
        "    pairs      = standardise(raw_text, \"ga\")\n",
        "    sentences  = naive_sentences(pairs)\n",
        "    doc        = parse_standardised(sentences)\n",
        "    conllu_out = project(doc, sentences)\n",
        "    print(conllu_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I4bPGxj-qru",
        "outputId": "2d424120-19ff-403f-cf43-d3870a653ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# sent_id = 1\n",
            "# text = Páirc Uí Chaoimh\n",
            "# text_standard = Páirc Uí Chaoimh\n",
            "1\tPáirc\tpáirc\tNOUN\tNoun\tCase=Nom|Definite=Def|Gender=Fem|Number=Sing\t_\troot\t_\t_\n",
            "2\tUí\tuí\tPART\tPat\tPartType=Pat\t1\tnmod\t_\t_\n",
            "3\tChaoimh\tcaoimh\tPROPN\tNoun\tCase=Gen|Definite=Def|Form=Len|Gender=Masc|Number=Sing\t1\tflat:name\t_\t_\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(project_with_stanza(\"Páirc Uí Chaoimh\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1OjyfETtTT4",
        "outputId": "9c269b8c-1c12-423c-fff1-58fb6119fda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# sent_id = 1\n",
            "# text = do ól sé\n",
            "# text_standard = do ól sé\n",
            "1\tdo\tdo\tPART\tVb\tPartType=Vb\t2\tmark:prt\t_\t_\n",
            "2\tól\tól\tVERB\tVTI\tMood=Ind|Tense=Past\t_\troot\t_\t_\n",
            "3\tsé\tsé\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t2\tnsubj\t_\t_\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(project_with_stanza(\"do ól sé\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMtPVsZZ_Ma1",
        "outputId": "7639551a-0ab8-4b4e-f19f-42c9b21e6155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# sent_id = 1\n",
            "# text = Chuir sé ann a bhéil é .\n",
            "# text_standard = Chuir sé ann a bhéil é .\n",
            "1\tChuir\tcuir\tVERB\tVTI\tForm=Len|Mood=Ind|Tense=Past\t_\troot\t_\t_\n",
            "2\tsé\tsé\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t1\tnsubj\t_\t_\n",
            "3\tann\ti\tADP\tPrep\tGender=Masc|Number=Sing|Person=3\t1\txcomp:pred\t_\t_\n",
            "4\ta\ta\tDET\tDet\tGender=Masc|Number=Sing|Person=3|Poss=Yes\t5\tnmod:poss\t_\t_\n",
            "5\tbhéil\tbéil\tNOUN\tNoun\tCase=Nom|Definite=Def|Form=Len|Gender=Fem|Number=Sing\t1\tobj\t_\t_\n",
            "6\té\té\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t1\tobj\t_\t_\n",
            "7\t.\t.\tPUNCT\t.\t_\t1\tpunct\t_\t_\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(project_with_stanza(\"Chuir sé ann a bhéil é.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge2PdVWU3KBc",
        "outputId": "7a6cbd35-5b93-4e33-f587-22a289767679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# sent_id = 1\n",
            "# text = Chuir sé in uachtar\n",
            "# text_standard = Chuir sé in uachtar\n",
            "1\tChuir\tcuir\tVERB\tVTI\tForm=Len|Mood=Ind|Tense=Past\t_\troot\t_\t_\n",
            "2\tsé\tsé\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t1\tnsubj\t_\t_\n",
            "3\tin\ti\tADP\tSimp\t_\t4\tcase\t_\t_\n",
            "4\tuachtar\tuachtar\tNOUN\tNoun\tCase=Nom|Gender=Masc|Number=Sing\t1\tobl\t_\t_\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(project_with_stanza(\"Chuir sé in uachtar\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "deZchysp_xB0",
        "outputId": "9d33f2a6-5768-48c4-e478-5724d9fdec74"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'run_parse' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1a9bc6bf697a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D'fhan sé\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'run_parse' is not defined"
          ]
        }
      ],
      "source": [
        "run_parse(\"D'fhan sé\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R8NRyfoA2TW",
        "outputId": "d11c4e57-b538-4f10-8144-dc214946bbf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# sent_id = 1\n",
            "# text = Bu leis Baile ui Mún , áit fiche bó agus tarbh leobhtha .\n",
            "# text_standard = Ba leis Baile uí Mún , áit fiche bó agus tarbh leo .\n",
            "1\tBu\tis\tAUX\tCop\tTense=Past|VerbForm=Cop\t3\tcop\t_\t_\n",
            "2\tleis\tle\tADP\tSimp\t_\t3\tcase\t_\t_\n",
            "3\tBaile\tBaile\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t_\troot\t_\t_\n",
            "4\tui\tuí\tPART\tPat\tPartType=Pat\t3\tflat:name\t_\t_\n",
            "5\tMún\tMún\tPROPN\tNoun\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing\t4\tflat:name\t_\t_\n",
            "6\t,\t,\tPUNCT\tPunct\t_\t7\tpunct\t_\t_\n",
            "7\táit\táit\tNOUN\tNoun\tCase=Nom|Gender=Fem|Number=Sing\t3\tappos\t_\t_\n",
            "8\tfiche\tfiche\tNUM\tNum\tNumType=Card\t9\tnummod\t_\t_\n",
            "9\tbó\tbó\tNOUN\tNoun\tCase=Nom|Gender=Fem|Number=Sing\t7\tnmod\t_\t_\n",
            "10\tagus\tagus\tCCONJ\tCoord\t_\t11\tcc\t_\t_\n",
            "11\ttarbh\ttarbh\tNOUN\tNoun\tCase=Nom|Gender=Masc|Number=Sing\t3\tconj\t_\t_\n",
            "12\tleobhtha\tle\tADP\tPrep\tNumber=Plur|Person=3\t11\tobl:prep\t_\t_\n",
            "13\t.\t.\tPUNCT\t.\t_\t3\tpunct\t_\t_\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample = \"Do bhíodh longphort ag na Lochlannaigh anseo. D’éirigh an t‑árd‑rí.\"\n",
        "print(project_with_stanza(\"Bu leis Baile ui Mún, áit fiche bó agus tarbh leobhtha.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp_ga = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    # Let Stanza decide sentences & tokens\n",
        "    tokenize_pretokenized=False,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAGE = \"\"\"Maidin amháin, chuaigh gamal as Gúra amach chun\n",
        "connadh a bhailiú. Shiúil sé leis thar an machaire gur tháinig\n",
        "sé go dtí crann mór ológ cois abhann.\n",
        "\n",
        "Dhreap sé an crann agus shuigh sé faoi ar an ngéag ba mhó ar\n",
        "an gcrann. Nuair a bhraith sé ar a shocracht ar an ngéag,\n",
        "thosaigh sé á bualadh le tua.\n",
        "\n",
        "Ghabh an sagart thar bráid. D'fhéach sé in airde agus labhair\n",
        "leis an ngamal:\n",
        "\n",
        "\"A dheartháirín ó, cad atá ar bun agat?\" ar sé os ard. \"Ní mar\n",
        "sin a ghearrtar adhmad!\"\n",
        "\n",
        "\"Níl aon bhealach eile ann, a Athair,\" arsa an gamal.\n",
        "\"Caithfear an tua a ardú agus é a bhualadh anuas ar an\n",
        "adhmad!\"\n",
        "\n",
        "\"Ach tá tú i do shuí ar an ngéag atá á gearradh agat! Brisfidh\n",
        "an ghéag, agus nuair a bhrisfidh titfidh tusa go talamh agus\n",
        "marófar thú,” arsa an sagart.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/joregan/opt/anaconda3/envs/stanza/lib/python3.10/site-packages (2.2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnlp_ga\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPAGE\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/pipeline/tokenize_processor.py:104\u001b[0m, in \u001b[0;36mTokenizeProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# get dict data\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 104\u001b[0m     _, _, _, document \u001b[38;5;241m=\u001b[39m \u001b[43moutput_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43morig_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mno_ssplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_ssplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mpostprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# replace excessively long tokens with <UNK> to avoid downstream GPU memory issues in POS\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m document:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/models/tokenization/utils.py:324\u001b[0m, in \u001b[0;36moutput_predictions\u001b[0;34m(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen, orig_text, no_ssplit, use_regex_tokens, num_workers, postprocessor)\u001b[0m\n\u001b[1;32m    321\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    322\u001b[0m max_seqlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1000\u001b[39m, max_seqlen)\n\u001b[0;32m--> 324\u001b[0m all_preds, all_raw \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_regex_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m use_la_ittb_shorthand \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshorthand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mla_ittb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m skip_newline \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_newline\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/models/tokenization/utils.py:259\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(trainer, data_generator, batch_size, max_seqlen, use_regex_tokens, num_workers)\u001b[0m\n\u001b[1;32m    257\u001b[0m sorted_data \u001b[38;5;241m=\u001b[39m SortedDataset(data_generator)\n\u001b[1;32m    258\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m TorchDataLoader(sorted_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39msorted_data\u001b[38;5;241m.\u001b[39mcollate, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m    260\u001b[0m     num_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    261\u001b[0m     N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/stanza/lib/python3.10/site-packages/stanza/models/tokenization/data.py:426\u001b[0m, in \u001b[0;36mSortedDataset.collate\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(samples):\n\u001b[1;32m    425\u001b[0m     u_, l_, f_, r_ \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 426\u001b[0m     units[i, :\u001b[38;5;28mlen\u001b[39m(u_)] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     labels[i, :\u001b[38;5;28mlen\u001b[39m(l_)] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(l_)\n\u001b[1;32m    428\u001b[0m     features[i, :\u001b[38;5;28mlen\u001b[39m(f_), :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(f_)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "nlp_ga(PAGE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stanza",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
