{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parse pre-standard Irish via standardiser\n",
        "\n",
        "> \"Partial, incomplete\"\n",
        "\n",
        "- categories: [irish, ud, stanza]\n",
        "- branch: master\n",
        "- badges: true\n",
        "- hidden: true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gq4eL7Xj9psB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PHI6wzlqAk3t"
      },
      "outputs": [],
      "source": [
        "import urllib.parse, urllib.request, json, sys\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a-aFOYu_AoGu"
      },
      "outputs": [],
      "source": [
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "def standardise(text: str, lang: str = \"ga\"):\n",
        "    \"\"\"Return a list of (orig_tok, std_tok) pairs from Intergaelic.\"\"\"\n",
        "    data   = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    hdrs   = {\"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "              \"Accept\":        \"application/json\"}\n",
        "    req    = urllib.request.Request(STD_API, data, hdrs)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        return json.loads(resp.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a1WtCT_ZArRt"
      },
      "outputs": [],
      "source": [
        "stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    # Let Stanza decide sentences & tokens\n",
        "    tokenize_pretokenized=True,\n",
        "    no_ssplit=True,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "knN_HAk5Auzx"
      },
      "outputs": [],
      "source": [
        "from itertools import groupby\n",
        "from typing import List, Tuple\n",
        "\n",
        "def _split_std(std: str, orig: str) -> List[str]:\n",
        "    \"\"\"Return the token(s) that should feed Stanza for this pair.\"\"\"\n",
        "    if not std.strip():\n",
        "        return [orig]\n",
        "    return std.split()\n",
        "\n",
        "def _sentences_from_pairs(pairs: List[Tuple[str, str]]):\n",
        "    \"\"\"Very light sentence splitter: keep everything up to . ! ?\"\"\"\n",
        "    sent, buf = [], []\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        parts = _split_std(std, orig)\n",
        "        for j, part in enumerate(parts):\n",
        "            buf.append((i, j, len(parts), orig, part))\n",
        "            if part in {\".\", \"!\", \"?\"}:\n",
        "                sent.append(buf);  buf = []\n",
        "    if buf:\n",
        "        sent.append(buf)\n",
        "    return sent\n",
        "\n",
        "def project_with_stanza(raw_text: str, lang: str = \"ga\") -> str:\n",
        "    pairs  = standardise(raw_text, lang)\n",
        "\n",
        "    sents  = _sentences_from_pairs(pairs)\n",
        "    pretok = [[m[4] for m in sent] for sent in sents]\n",
        "\n",
        "    doc = nlp(pretok)\n",
        "\n",
        "    conllu_lines = []\n",
        "    for sid, (sent_map, sent_doc) in enumerate(zip(sents, doc.sentences), 1):\n",
        "        raw_slice = [m[3] for m in sent_map if m[1] == 0]\n",
        "        std_slice = [m[4] for m in sent_map]\n",
        "        conllu_lines += [\n",
        "            f\"# sent_id = {sid}\",\n",
        "            f\"# text = {' '.join(raw_slice)}\",\n",
        "            f\"# text_standard = {' '.join(std_slice)}\",\n",
        "        ]\n",
        "\n",
        "        # token lines\n",
        "        widx = 0\n",
        "        tid  = 1\n",
        "        for m in sent_map:\n",
        "            orig_i, sub_i, n_sub, orig_tok, std_tok = m\n",
        "            word = sent_doc.words[widx]\n",
        "\n",
        "            if sub_i == 0 and n_sub > 1:\n",
        "                conllu_lines.append(f\"{tid}-{tid+n_sub-1}\\t{orig_tok}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\")\n",
        "\n",
        "            form = orig_tok if n_sub == 1 else std_tok\n",
        "\n",
        "            conllu_lines.append(\"\\t\".join([\n",
        "                str(tid),\n",
        "                form,\n",
        "                word.lemma or \"_\",\n",
        "                word.upos  or \"_\",\n",
        "                word.xpos  or \"_\",\n",
        "                word.feats or \"_\",\n",
        "                str(word.head) if word.head else \"_\",\n",
        "                word.deprel or \"_\",\n",
        "                \"_\",\n",
        "                \"_\",\n",
        "            ]))\n",
        "\n",
        "            widx += 1\n",
        "            tid  += 1\n",
        "        conllu_lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(conllu_lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tYWPdPb286bi"
      },
      "outputs": [],
      "source": [
        "nlp_tok = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    tokenize_pretokenized=False,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aLotbbK19BRq"
      },
      "outputs": [],
      "source": [
        "pp = project_with_stanza(\"E-, ‘firing range’ a mbíonns acub agus é seo agus é siúd.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVf_cOHH3NP",
        "outputId": "411d5ac6-4327-483d-c9ec-4949e9a96da9"
      },
      "outputs": [],
      "source": [
        "lines = \"{:C}\".format(nlp_tok(raw)).split(\"\\n\")\n",
        "print(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYDAYg3OJrJG",
        "outputId": "be56da38-887d-4621-d499-2d1b4f9b5d3e"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIK_8uwOJztG",
        "outputId": "f64a62f5-c9d3-4514-9c3c-910710e6605a"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr tesseract-ocr-gle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fLBCzBqRP0j6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def read_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    image_array = np.frombuffer(response.content, np.uint8)\n",
        "    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OKgDKshVK-U9"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import cv2\n",
        "\n",
        "def extract_text_from_bbox(image_path, bbox, lang=\"gle\"):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Extract the region of interest\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    roi = image[y1:y2, x1:x2]\n",
        "\n",
        "    # Convert the ROI to grayscale\n",
        "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    text = pytesseract.image_to_string(gray, lang=lang)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "7Vfq6LamP5Ps"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import cv2\n",
        "from IPython.display import display, Image\n",
        "import io\n",
        "\n",
        "def extract_text_from_bbox_and_url(url, bbox, lang=\"gle\"):\n",
        "    image = read_image_from_url(url)\n",
        "\n",
        "    # Extract the region of interest\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    roi = image[y1:y2, x1:x2]\n",
        "\n",
        "    # Convert the ROI to grayscale\n",
        "    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    _, buffer = cv2.imencode('.png', roi)\n",
        "    io_buf = io.BytesIO(buffer)\n",
        "    display(Image(io_buf.getvalue()))\n",
        "\n",
        "    text = pytesseract.image_to_string(gray, lang=lang)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "bFb-zLtyLQe0"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_image_selector_from_url(url, selector):\n",
        "    req = requests.get(url)\n",
        "    assert req.status_code == 200, f\"Failed to fetch {url}\"\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    element = soup.select_one(selector)\n",
        "    if element:\n",
        "        return element['src']\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "vVI6NWufdgIh"
      },
      "outputs": [],
      "source": [
        "def get_image_from_data(url, selector, bbox_text):\n",
        "    bbox = [int(x) for x in bbox_text.split(\" \")]\n",
        "    img = get_image_selector_from_url(url, selector)\n",
        "    return extract_text_from_bbox_and_url(img, bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "quVuJ55fLzgR",
        "outputId": "9dc113d9-c5f1-403a-eb72-d59875031333"
      },
      "outputs": [],
      "source": [
        "b = get_image_from_data(\"https://www.leighleat.com/pages/1803\", \"#ajax-page-container > div > div:nth-child(2) > img\", \"297 681 725 742\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aU90T5bL9cO",
        "outputId": "14a5b699-0523-4f24-8d24-02c96bfd064f"
      },
      "outputs": [],
      "source": [
        "lines = \"{:C}\".format(nlp_tok(b)).split(\"\\n\")\n",
        "print(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJT7a_K5bYh9",
        "outputId": "e918fda1-c6f8-490c-8055-b72d4f755a5c"
      },
      "outputs": [],
      "source": [
        "cor = \"Nuair a thagann Brídín abhaile, ordaíonn a mamaí di í féin a ghlanadh.\"\n",
        "lines = \"{:C}\".format(nlp_tok(cor)).split(\"\\n\")\n",
        "print(\"\\n\".join(lines))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stanza",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
