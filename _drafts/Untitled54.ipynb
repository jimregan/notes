{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq4eL7Xj9psB",
        "outputId": "8b039ae2-d7a1-45d5-fa43-bdb88df1bec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "project_with_stanza.py\n",
        "\n",
        "Read pre‑standard Irish (or Scottish Gaelic/Manx) from STDIN, call the\n",
        "Intergaelic standardiser API, parse the *standardised* text with Stanza,\n",
        "and project the annotations back onto the original tokens.\n",
        "\n",
        "Usage\n",
        "-----\n",
        "  python project_with_stanza.py [ga|gd|gv] > out.conllu\n",
        "\"\"\"\n",
        "import sys\n",
        "import json\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "import stanza                   # pip install stanza\n",
        "from itertools import groupby   # for a very cheap sentence splitter\n",
        "\n",
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "##############################################################################\n",
        "# 1. call the standardiser ####################################################\n",
        "##############################################################################\n",
        "\n",
        "def standardise(text: str, lang: str):\n",
        "    \"\"\"\n",
        "    Return a list of (orig_tok, std_tok) pairs from the Intergaelic API.\n",
        "    \"\"\"\n",
        "    params = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "               \"Accept\":        \"application/json\"}\n",
        "    req = urllib.request.Request(STD_API, params, headers=headers)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        return json.loads(resp.read())          # → [[orig, std], …]\n",
        "\n",
        "##############################################################################\n",
        "# 2. (very rough!) sentence boundary detection ###############################\n",
        "##############################################################################\n",
        "\n",
        "def naive_sentences(pairs):\n",
        "    \"\"\"\n",
        "    Split the pair list into sentences whenever a token ends in . ! or ? .\n",
        "    Stanza will be run in *tokenise‑pretokenised* mode, so sentences are\n",
        "    lists‑of‑lists of *standardised* tokens.\n",
        "    \"\"\"\n",
        "    sentence, current = [], []\n",
        "    for orig, std in pairs:\n",
        "        current.append((orig, std))\n",
        "        if std.endswith((\".\", \"!\", \"?\")):\n",
        "            sentence.append(current)\n",
        "            current = []\n",
        "    if current:\n",
        "        sentence.append(current)\n",
        "    return sentence\n",
        "\n",
        "##############################################################################\n",
        "# 3. parse with Stanza (pretok = True) #######################################\n",
        "##############################################################################\n",
        "\n",
        "def parse_standardised(sentences):\n",
        "    \"\"\"\n",
        "    Run Stanza’s Irish pipeline on the already‑tokenised sentences and return\n",
        "    the resulting Document object.\n",
        "    \"\"\"\n",
        "    # Download once; afterwards this is a no‑op\n",
        "    stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "\n",
        "    nlp = stanza.Pipeline(\n",
        "        lang=\"ga\",\n",
        "        processors=\"tokenize,pos,lemma,depparse\",\n",
        "        tokenize_pretokenized=True,\n",
        "        verbose=False,\n",
        "    )\n",
        "    # Convert to *only* the standard tokens for the parser\n",
        "    std_sents = [[std for _orig, std in sent] for sent in sentences]\n",
        "    return nlp(std_sents)\n",
        "\n",
        "##############################################################################\n",
        "# 4. project the annotations back ############################################\n",
        "##############################################################################\n",
        "\n",
        "def project(doc, sentences):\n",
        "    \"\"\"\n",
        "    Yield CONLL‑U lines containing the original token but the Stanza annotation\n",
        "    (lemma, UPOS, feats, head, deprel, …) taken from the aligned standard token.\n",
        "    Assumes one‑to‑one alignment, which Scannell (2022) reports holds for ≈97 %\n",
        "    of tokens; multi‑word tokens and many‑to‑one cases need extra work.\n",
        "    \"\"\"\n",
        "    conllu_lines = []\n",
        "    sent_id = 1\n",
        "    for sent_pairs, sent_ann in zip(sentences, doc.sentences):\n",
        "        conllu_lines.append(f\"# sent_id = {sent_id}\")\n",
        "        conllu_lines.append(f\"# text = {' '.join(orig for orig, _ in sent_pairs)}\")\n",
        "        for i, (pair, word) in enumerate(zip(sent_pairs, sent_ann.words), start=1):\n",
        "            orig_tok, _ = pair\n",
        "            # CoNLL‑U columns: ID, FORM, LEMMA, UPOS, XPOS, FEATS,\n",
        "            #                  HEAD, DEPREL, DEPS, MISC\n",
        "            line = [\n",
        "                str(i),\n",
        "                orig_tok,\n",
        "                word.lemma or \"_\",\n",
        "                word.upos  or \"_\",\n",
        "                word.xpos  or \"_\",\n",
        "                word.feats or \"_\",\n",
        "                str(word.head) if word.head else \"_\",\n",
        "                word.deprel or \"_\",\n",
        "                \"_\",\n",
        "                \"_\",\n",
        "            ]\n",
        "            conllu_lines.append(\"\\t\".join(line))\n",
        "        conllu_lines.append(\"\")   # blank line between sentences\n",
        "        sent_id += 1\n",
        "    return \"\\n\".join(conllu_lines)\n",
        "\n",
        "##############################################################################\n",
        "# 5. driver ##################################################################\n",
        "##############################################################################\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) != 2 or sys.argv[1] not in {\"ga\", \"gd\", \"gv\"}:\n",
        "        sys.exit(\"Usage: python project_with_stanza.py [ga|gd|gv]\")\n",
        "\n",
        "    raw_text   = sys.stdin.read()\n",
        "    pairs      = standardise(raw_text, sys.argv[1])\n",
        "    sentences  = naive_sentences(pairs)\n",
        "    doc        = parse_standardised(sentences)\n",
        "    conllu_out = project(doc, sentences)\n",
        "    print(conllu_out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8Y2efOYa9ukB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}