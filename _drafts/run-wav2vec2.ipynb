{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2PhonemeCTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2CTCTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    " \n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    import torchaudio\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"audio\"] = speech_array[0].numpy()\n",
    "    batch[\"sampling_rate\"] = 16_000\n",
    "    #batch[\"target_text\"] = batch[\"sentence\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "_BASE = Path(\"/media/phonetics/asr_data_irish/audio/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSF = _BASE / \"mul_mo_sceal_fein\" / \"wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "msf_files = [str(f) for f in MSF.glob(\"*.wav\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"path\": msf_files,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca80dce6f5c4d9dbc4166b6ec4908e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3634 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(dataset[\"audio\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "#input_values = processor(msf_ds, return_tensors=\"pt\").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve logits\n",
    "with torch.no_grad():\n",
    "   logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
