{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LAo6u96zJ57"
      },
      "source": [
        "# Parse pre-standard Irish via standardiser\n",
        "\n",
        "> \"Partial, incomplete\"\n",
        "\n",
        "- categories: [irish, ud, stanza]\n",
        "- branch: master\n",
        "- badges: true\n",
        "- hidden: true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gq4eL7Xj9psB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PHI6wzlqAk3t"
      },
      "outputs": [],
      "source": [
        "import urllib.parse, urllib.request, json, sys\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a-aFOYu_AoGu"
      },
      "outputs": [],
      "source": [
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "def standardise(text: str, lang: str = \"ga\"):\n",
        "    \"\"\"Return a list of (orig_tok, std_tok) pairs from Intergaelic.\"\"\"\n",
        "    data   = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    hdrs   = {\"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "              \"Accept\":        \"application/json\"}\n",
        "    req    = urllib.request.Request(STD_API, data, hdrs)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        return json.loads(resp.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a1WtCT_ZArRt"
      },
      "outputs": [],
      "source": [
        "stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    # Let Stanza decide sentences & tokens\n",
        "    tokenize_pretokenized=True,\n",
        "    no_ssplit=True,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "knN_HAk5Auzx"
      },
      "outputs": [],
      "source": [
        "from itertools import groupby\n",
        "from typing import List, Tuple\n",
        "\n",
        "def _split_std(std: str, orig: str) -> List[str]:\n",
        "    \"\"\"Return the token(s) that should feed Stanza for this pair.\"\"\"\n",
        "    if not std.strip():\n",
        "        return [orig]\n",
        "    return std.split()\n",
        "\n",
        "def _sentences_from_pairs(pairs: List[Tuple[str, str]]):\n",
        "    \"\"\"Very light sentence splitter: keep everything up to . ! ?\"\"\"\n",
        "    sent, buf = [], []\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        parts = _split_std(std, orig)\n",
        "        for j, part in enumerate(parts):\n",
        "            buf.append((i, j, len(parts), orig, part))\n",
        "            if part in {\".\", \"!\", \"?\"}:\n",
        "                sent.append(buf);  buf = []\n",
        "    if buf:\n",
        "        sent.append(buf)\n",
        "    return sent\n",
        "\n",
        "def project_with_stanza(raw_text: str, lang: str = \"ga\") -> str:\n",
        "    pairs  = standardise(raw_text, lang)\n",
        "\n",
        "    sents  = _sentences_from_pairs(pairs)\n",
        "    pretok = [[m[4] for m in sent] for sent in sents]\n",
        "\n",
        "    doc = nlp(pretok)\n",
        "\n",
        "    conllu_lines = []\n",
        "    for sid, (sent_map, sent_doc) in enumerate(zip(sents, doc.sentences), 1):\n",
        "        raw_slice = [m[3] for m in sent_map if m[1] == 0]\n",
        "        std_slice = [m[4] for m in sent_map]\n",
        "        conllu_lines += [\n",
        "            f\"# sent_id = {sid}\",\n",
        "            f\"# text = {' '.join(raw_slice)}\",\n",
        "            f\"# text_standard = {' '.join(std_slice)}\",\n",
        "        ]\n",
        "\n",
        "        # token lines\n",
        "        widx = 0\n",
        "        tid  = 1\n",
        "        for m in sent_map:\n",
        "            orig_i, sub_i, n_sub, orig_tok, std_tok = m\n",
        "            word = sent_doc.words[widx]\n",
        "\n",
        "            if sub_i == 0 and n_sub > 1:\n",
        "                conllu_lines.append(f\"{tid}-{tid+n_sub-1}\\t{orig_tok}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\")\n",
        "\n",
        "            form = orig_tok if n_sub == 1 else std_tok\n",
        "\n",
        "            conllu_lines.append(\"\\t\".join([\n",
        "                str(tid),\n",
        "                form,\n",
        "                word.lemma or \"_\",\n",
        "                word.upos  or \"_\",\n",
        "                word.xpos  or \"_\",\n",
        "                word.feats or \"_\",\n",
        "                str(word.head) if word.head else \"_\",\n",
        "                word.deprel or \"_\",\n",
        "                \"_\",\n",
        "                \"_\",\n",
        "            ]))\n",
        "\n",
        "            widx += 1\n",
        "            tid  += 1\n",
        "        conllu_lines.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(conllu_lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tYWPdPb286bi"
      },
      "outputs": [],
      "source": [
        "nlp_tok = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    tokenize_pretokenized=False,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aLotbbK19BRq"
      },
      "outputs": [],
      "source": [
        "pp = project_with_stanza(\"E-, ‘firing range’ a mbíonns acub agus é seo agus é siúd.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4COAgpa3zeAc",
        "outputId": "affda03c-b3ef-45b3-87a9-790a65e10c0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sent_id = 1\n",
            "# text = E - , ‘ firing range ’ a mbíonns acub agus é seo agus é siúd .\n",
            "# text_standard = É - , ‘ firing range ’ a mbíonn acu agus é seo agus é siúd .\n",
            "1\tE\té\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t_\troot\t_\t_\n",
            "2\t-\t-\tPUNCT\t.\t_\t4\tpunct\t_\t_\n",
            "3\t,\t,\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "4\t‘\t‘\tX\tForeign\tForeign=Yes\t1\tparataxis\t_\t_\n",
            "5\tfiring\tfiring\tX\tForeign\tForeign=Yes\t4\tflat:foreign\t_\t_\n",
            "6\trange\trange\tX\tForeign\tForeign=Yes\t4\tflat:foreign\t_\t_\n",
            "7\t’\t’\tPUNCT\tPunct\t_\t9\tpunct\t_\t_\n",
            "8\ta\ta\tPART\tVb\tForm=Indirect|PartType=Vb|PronType=Rel\t9\tmark:prt\t_\t_\n",
            "9\tmbíonns\tbí\tVERB\tPresImp\tAspect=Hab|Form=Ecl|Mood=Ind|Tense=Pres\t4\tacl:relcl\t_\t_\n",
            "10\tacub\tag\tADP\tPrep\tNumber=Plur|Person=3\t9\tobl:prep\t_\t_\n",
            "11\tagus\tagus\tCCONJ\tCoord\t_\t12\tcc\t_\t_\n",
            "12\té\té\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t1\tconj\t_\t_\n",
            "13\tseo\tseo\tPRON\tDem\tPronType=Dem\t12\tdet\t_\t_\n",
            "14\tagus\tagus\tSCONJ\tSubord\t_\t15\tmark\t_\t_\n",
            "15\té\té\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t12\tconj\t_\t_\n",
            "16\tsiúd\tsiúd\tPRON\tDem\tPronType=Dem\t15\tdet\t_\t_\n",
            "17\t.\t.\tPUNCT\t.\t_\t1\tpunct\t_\t_\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = \"Eh, 'firing range' a mbíonns acub agus é seo agus é siúd.\""
      ],
      "metadata": {
        "id": "JEXxCkAF3zav"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVf_cOHH3NP",
        "outputId": "04063636-a87f-4500-807e-d170417f2fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text = Eh, 'firing range' a mbíonn acub agus é seo agus é siúd.\n",
            "# sent_id = 0\n",
            "1\tEh\tEh\tINTJ\tItj\t_\t0\troot\t_\tstart_char=0|end_char=2|SpaceAfter=No\n",
            "2\t,\t,\tPUNCT\tPunct\t_\t4\tpunct\t_\tstart_char=2|end_char=3\n",
            "3\t'\t'\tPUNCT\tPunct\t_\t4\tpunct\t_\tstart_char=4|end_char=5|SpaceAfter=No\n",
            "4\tfiring\tfiring\tX\tForeign\tForeign=Yes\t1\tparataxis\t_\tstart_char=5|end_char=11\n",
            "5\trange\trange\tX\tForeign\tForeign=Yes\t4\tflat:foreign\t_\tstart_char=12|end_char=17|SpaceAfter=No\n",
            "6\t'\t'\tPUNCT\tPunct\t_\t4\tpunct\t_\tstart_char=17|end_char=18\n",
            "7\ta\ta\tPART\tVb\tForm=Indirect|PartType=Vb|PronType=Rel\t8\tmark:prt\t_\tstart_char=19|end_char=20\n",
            "8\tmbíonn\tbí\tVERB\tPresImp\tAspect=Hab|Form=Ecl|Mood=Ind|Tense=Pres\t4\tcsubj:cleft\t_\tstart_char=21|end_char=27\n",
            "9\tacub\tag\tNOUN\tNoun\tCase=Nom|Gender=Masc|Number=Sing\t8\tnsubj\t_\tstart_char=28|end_char=32\n",
            "10\tagus\tagus\tCCONJ\tCoord\t_\t11\tcc\t_\tstart_char=33|end_char=37\n",
            "11\té\té\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t9\tconj\t_\tstart_char=38|end_char=39\n",
            "12\tseo\tseo\tPRON\tDem\tPronType=Dem\t11\tdet\t_\tstart_char=40|end_char=43\n",
            "13\tagus\tagus\tSCONJ\tSubord\t_\t14\tmark\t_\tstart_char=44|end_char=48\n",
            "14\té\té\tPRON\tPers\tGender=Masc|Number=Sing|Person=3\t8\tadvcl\t_\tstart_char=49|end_char=50\n",
            "15\tsiúd\tsiúd\tPRON\tDem\tPronType=Dem\t14\tdet\t_\tstart_char=51|end_char=55|SpaceAfter=No\n",
            "16\t.\t.\tPUNCT\t.\t_\t1\tpunct\t_\tstart_char=55|end_char=56|SpaceAfter=No\n"
          ]
        }
      ],
      "source": [
        "lines = \"{:C}\".format(nlp_tok(raw)).split(\"\\n\")\n",
        "print(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T659M3rr7a9h",
        "outputId": "778e89dc-ade0-4559-e7c3-322a7d3a33bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.10 [186 kB]\n",
            "Fetched 186 kB in 0s (436 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.10_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.10) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "aXfA-W8r8TWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dias.ie/wp-content/uploads/2014/03/track01.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJCg7wou8sjD",
        "outputId": "4d1db23b-68ae-42cb-9bd9-9b296bf7e6a0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-04 13:38:23--  https://www.dias.ie/wp-content/uploads/2014/03/track01.pdf\n",
            "Resolving www.dias.ie (www.dias.ie)... 160.6.22.11, 2001:770:60:22::11\n",
            "Connecting to www.dias.ie (www.dias.ie)|160.6.22.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52925 (52K) [application/pdf]\n",
            "Saving to: ‘track01.pdf’\n",
            "\n",
            "track01.pdf         100%[===================>]  51.68K   189KB/s    in 0.3s    \n",
            "\n",
            "2025-09-04 13:38:24 (189 KB/s) - ‘track01.pdf’ saved [52925/52925]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ph = 1262\n",
        "pw = 892\n",
        "top = 157\n",
        "left = 211\n",
        "bottom = 588 + 16\n",
        "right = 211 + 497\n",
        "\n",
        "bbox = (left, top, right, bottom)\n",
        "print(bbox)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vMmxXR99MP_",
        "outputId": "829e3118-e5e1-4d4a-986c-83364a204840"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(211, 157, 708, 604)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_poppler_box_scaled(\n",
        "    pdf_path,\n",
        "    page_idx,\n",
        "    left, top, right=None, bottom=None, width=None, height=None,\n",
        "    pw=None, ph=None  # Poppler page width/height from XML\n",
        "):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        page = pdf.pages[page_idx]\n",
        "        # PDF native dimensions in points\n",
        "        pdf_w, pdf_h = page.width, page.height\n",
        "\n",
        "        if pw is None or ph is None:\n",
        "            raise ValueError(\"Provide Poppler page width/height (pw, ph) from the XML.\")\n",
        "\n",
        "        # Scale factor Poppler used relative to PDF points\n",
        "        sx = pw / pdf_w\n",
        "        sy = ph / pdf_h\n",
        "        # They should be ~equal; use average to be safe\n",
        "        s = (sx + sy) / 2.0\n",
        "\n",
        "        # Normalize Poppler rectangle (top-left origin)\n",
        "        if right is None or bottom is None:\n",
        "            if width is None or height is None:\n",
        "                raise ValueError(\"Provide either (right,bottom) or (width,height).\")\n",
        "            right = left + width\n",
        "            bottom = top + height\n",
        "\n",
        "        # Ensure proper ordering\n",
        "        x0_pop, x1_pop = sorted((left, right))\n",
        "        y0_pop, y1_pop = sorted((top, bottom))\n",
        "\n",
        "        # Descale from Poppler space -> PDF points\n",
        "        x0_pt = x0_pop / s\n",
        "        x1_pt = x1_pop / s\n",
        "        y0_pt = y0_pop / s\n",
        "        y1_pt = y1_pop / s\n",
        "\n",
        "        # Convert Poppler top-left coords -> PDF bottom-left coords\n",
        "        pdf_top    = pdf_h - y0_pt\n",
        "        pdf_bottom = pdf_h - y1_pt\n",
        "\n",
        "        bbox = (x0_pt, pdf_bottom, x1_pt, pdf_top)  # (x0, bottom, x1, top)\n",
        "\n",
        "        # Optional: sanity clamp to page bounds to avoid ValueError\n",
        "        x0, btm, x1, top_ = bbox\n",
        "        x0 = max(0, min(x0, pdf_w))\n",
        "        x1 = max(0, min(x1, pdf_w))\n",
        "        btm = max(0, min(btm, pdf_h))\n",
        "        top_ = max(0, min(top_, pdf_h))\n",
        "        if x1 < x0 or top_ < btm:\n",
        "            return \"\"  # empty if fully out of bounds\n",
        "\n",
        "        bbox = (x0, btm, x1, top_)\n",
        "\n",
        "        return page.crop(bbox).extract_text() or \"\"\n"
      ],
      "metadata": {
        "id": "0EBCEa3f8ol-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_text_from_poppler_box_scaled(\"track01.pdf\", 1, top=209, left=211, right=708, bottom=588 + 16, ph=1262, pw=892)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9tVo7MoBE7Pt",
        "outputId": "16b65c2d-fd6a-45d5-a0c2-218693c5488a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "stanza",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}