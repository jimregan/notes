{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SegFeat on TIMIT\n",
    "\n",
    "> \"Exceeded Kaggle runtime limit\"\n",
    "\n",
    "- categories: [kaggle, segfeat, timit]\n",
    "- hidden: true\n",
    "- branch: master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original](https://www.kaggle.com/code/jimregan/train-segfeat-on-timit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-30T17:55:13.827439Z",
     "iopub.status.busy": "2026-01-30T17:55:13.827072Z",
     "iopub.status.idle": "2026-01-30T17:55:15.909638Z",
     "shell.execute_reply": "2026-01-30T17:55:15.908421Z",
     "shell.execute_reply.started": "2026-01-30T17:55:13.827397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/felixkreuk/SegFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:01:58.518841Z",
     "iopub.status.busy": "2026-01-30T18:01:58.518306Z",
     "iopub.status.idle": "2026-01-30T18:01:58.526968Z",
     "shell.execute_reply": "2026-01-30T18:01:58.525740Z",
     "shell.execute_reply.started": "2026-01-30T18:01:58.518799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:02:02.446113Z",
     "iopub.status.busy": "2026-01-30T18:02:02.445016Z",
     "iopub.status.idle": "2026-01-30T18:02:02.453468Z",
     "shell.execute_reply": "2026-01-30T18:02:02.452255Z",
     "shell.execute_reply.started": "2026-01-30T18:02:02.446072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile segfeat.patch\n",
    "diff --git a/dataloader.py b/dataloader.py\n",
    "index 44d6fba..f141162 100644\n",
    "--- a/dataloader.py\n",
    "+++ b/dataloader.py\n",
    "@@ -87,14 +87,14 @@ def extract_features(wav_file, hparams):\n",
    " \n",
    "     # extract mel-spectrogram\n",
    "     if hparams.feats == 'mel':\n",
    "-        spect = librosa.feature.melspectrogram(wav,\n",
    "+        spect = librosa.feature.melspectrogram(y=wav,\n",
    "                                                sr=sr,\n",
    "                                                n_fft=hparams.n_fft,\n",
    "                                                hop_length=hparams.hop_length,\n",
    "                                                n_mels=hparams.rnn_input_size)\n",
    "     # extract mfcc\n",
    "     elif hparams.feats == 'mfcc':\n",
    "-        spect = librosa.feature.mfcc(wav,\n",
    "+        spect = librosa.feature.mfcc(y=wav,\n",
    "                                      sr=sr,\n",
    "                                      n_fft=hparams.n_fft,\n",
    "                                      hop_length=hparams.hop_length,\n",
    "@@ -208,7 +208,7 @@ class WavPhnDataset(Dataset):\n",
    "         raise NotImplementedError\n",
    " \n",
    "     def process_file(self, wav_path):\n",
    "-        phn_path = wav_path.replace(\"wav\", \"phn\")\n",
    "+        phn_path = wav_path.replace(\"WAV\", \"PHN\")\n",
    " \n",
    "         # load audio\n",
    "         spect = extract_features(wav_path, self.hparams)\n",
    "@@ -235,7 +235,7 @@ class WavPhnDataset(Dataset):\n",
    " \n",
    "     def _make_dataset(self):\n",
    "         files = []\n",
    "-        wavs = list(iter_find_files(self.wav_path, \"*.wav\"))\n",
    "+        wavs = list(iter_find_files(self.wav_path, \"*.WAV\"))\n",
    "         if self.hparams.devrun:\n",
    "             wavs = wavs[:self.hparams.devrun_size]\n",
    " \n",
    "@@ -266,9 +266,9 @@ class TimitDataset(WavPhnDataset):\n",
    " \n",
    "     @staticmethod\n",
    "     def get_datasets(hparams):\n",
    "-        train_dataset = TimitDataset(join(hparams.wav_path, 'train'),\n",
    "+        train_dataset = TimitDataset(join(hparams.wav_path, 'TRAIN'),\n",
    "                                      hparams)\n",
    "-        test_dataset  = TimitDataset(join(hparams.wav_path, 'test'),\n",
    "+        test_dataset  = TimitDataset(join(hparams.wav_path, 'TEST'),\n",
    "                                      hparams)\n",
    " \n",
    "         train_len   = len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:13:31.200591Z",
     "iopub.status.busy": "2026-01-30T18:13:31.200143Z",
     "iopub.status.idle": "2026-01-30T18:13:31.207368Z",
     "shell.execute_reply": "2026-01-30T18:13:31.206013Z",
     "shell.execute_reply.started": "2026-01-30T18:13:31.200516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile lightning.patch\n",
    "diff --git a/main.py b/main.py\n",
    "index 62cbb2c..43845e4 100644\n",
    "--- a/main.py\n",
    "+++ b/main.py\n",
    "@@ -11,7 +11,7 @@ import torch.nn.functional as F\n",
    " from loguru import logger\n",
    " from pytorch_lightning import Trainer\n",
    " from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "-from pytorch_lightning.logging import TestTubeLogger\n",
    "+from pytorch_lightning.loggers import TensorBoardLogger\n",
    " from torch.backends import cudnn\n",
    " from torch.utils.data import DataLoader, Dataset\n",
    " \n",
    "@@ -44,15 +44,13 @@ def main(hparams):\n",
    "         mode='min'\n",
    "     )\n",
    " \n",
    "-    tt_logger = TestTubeLogger(\n",
    "+    tb_logger = TensorBoardLogger(\n",
    "         save_dir=hparams.run_dir,\n",
    "         name=\"lightning_logs\",\n",
    "-        debug=False,\n",
    "-        create_git_tag=False\n",
    "     )\n",
    " \n",
    "     checkpoint = ModelCheckpoint(\n",
    "-        filepath=model_save_path,\n",
    "+        dirpath=model_save_path,\n",
    "         save_top_k=1,\n",
    "         verbose=True,\n",
    "         monitor='val_f1_at_2',\n",
    "@@ -60,19 +58,17 @@ def main(hparams):\n",
    "     )\n",
    " \n",
    "     trainer = Trainer(\n",
    "-            logger=tt_logger,\n",
    "-            overfit_pct=hparams.overfit,\n",
    "+            logger=tb_logger,\n",
    "             check_val_every_n_epoch=1,\n",
    "             min_epochs=1,\n",
    "             max_epochs=hparams.epochs,\n",
    "-            nb_sanity_val_steps=4,\n",
    "-            checkpoint_callback=None,\n",
    "-            val_percent_check=hparams.val_percent_check,\n",
    "+            num_sanity_val_steps=4,\n",
    "+            callbacks=[early_stop, checkpoint],\n",
    "+            limit_val_batches=hparams.val_percent_check,\n",
    "             val_check_interval=hparams.val_check_interval,\n",
    "-            early_stop_callback=None,\n",
    "-            gpus=hparams.gpus,\n",
    "-            show_progress_bar=False,\n",
    "-            distributed_backend=None,\n",
    "+            devices=\"auto\",\n",
    "+            accelerator=\"auto\",\n",
    "+            enable_progress_bar=True,\n",
    "             )\n",
    " \n",
    "     if not hparams.test:\n",
    "diff --git a/model.py b/model.py\n",
    "index 12c3542..b0a8956 100644\n",
    "--- a/model.py\n",
    "+++ b/model.py\n",
    "@@ -138,7 +138,7 @@ class Segmentor(nn.Module):\n",
    "         results = {}\n",
    " \n",
    "         # feed through rnn\n",
    "-        x = nn.utils.rnn.pack_padded_sequence(x, length, batch_first=True, enforce_sorted=False)\n",
    "+        x = nn.utils.rnn.pack_padded_sequence(x, length.cpu(), batch_first=True, enforce_sorted=False)\n",
    "         rnn_out, _ = self.rnn(x)\n",
    "         rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "         rnn_cum = torch.cumsum(rnn_out, dim=1)\n",
    "diff --git a/solver.py b/solver.py\n",
    "index 46672db..5cf3c28 100644\n",
    "--- a/solver.py\n",
    "+++ b/solver.py\n",
    "@@ -19,7 +19,7 @@ from utils import PrecisionRecallMetricMultiple, StatsMeter\n",
    " class Solver(LightningModule):\n",
    "     def __init__(self, config):\n",
    "         super(Solver, self).__init__()\n",
    "-        self.hparams = config\n",
    "+        self.save_hyperparameters(config)\n",
    " \n",
    "         if config.dataset == \"timit\":\n",
    "             self.datasetClass = TimitDataset\n",
    "@@ -46,23 +46,23 @@ class Solver(LightningModule):\n",
    "                         'test':  StatsMeter()}\n",
    "         self._device = 'cuda' if config.cuda else 'cpu'\n",
    " \n",
    "+        self.validation_step_outputs = []\n",
    "+        self.test_step_outputs = []\n",
    "+\n",
    "         self.build_model()\n",
    "         logger.info(f\"running on {self._device}\")\n",
    "         logger.info(f\"rnn input size: {config.rnn_input_size}\")\n",
    "         logger.info(f\"{self.segmentor}\")\n",
    " \n",
    "-    @pl.data_loader\n",
    "     def train_dataloader(self):\n",
    "         self.train_loader = DataLoader(self.train_dataset,\n",
    "                                        batch_size=self.config.batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=collate_fn_padd,\n",
    "                                        num_workers=6)\n",
    "-        logger.info(f\"input shape: {self.train_dataset[0][0].shape}\")\n",
    "         logger.info(f\"training set length {len(self.train_dataset)}\")\n",
    "         return self.train_loader\n",
    " \n",
    "-    @pl.data_loader\n",
    "     def val_dataloader(self):\n",
    "         self.valid_loader = DataLoader(self.valid_dataset,\n",
    "                                        batch_size=self.config.batch_size,\n",
    "@@ -72,7 +72,6 @@ class Solver(LightningModule):\n",
    "         logger.info(f\"validation set length {len(self.valid_dataset)}\")\n",
    "         return self.valid_loader\n",
    " \n",
    "-    @pl.data_loader\n",
    "     def test_dataloader(self):\n",
    "         self.test_loader  = DataLoader(self.test_dataset,\n",
    "                                        batch_size=self.config.batch_size,\n",
    "@@ -200,8 +199,6 @@ class Solver(LightningModule):\n",
    " \n",
    "         for output in outputs:\n",
    "             loss = output[f'{prefix}_loss']\n",
    "-            if self.trainer.use_dp:\n",
    "-                loss = torch.mean(loss)\n",
    "             loss_mean += loss\n",
    " \n",
    "         loss_mean /= len(outputs)\n",
    "@@ -243,19 +240,28 @@ class Solver(LightningModule):\n",
    " \n",
    "         logger.info(f\"\\nEVAL {prefix} STATS:\\n{json.dumps(metrics, sort_keys=True, indent=4)}\\n\")\n",
    " \n",
    "-        return metrics\n",
    "+        for k, v in metrics.items():\n",
    "+            self.log(k, v, prog_bar=(k == f'{prefix}_f1_at_2'))\n",
    " \n",
    "     def validation_step(self, data_batch, batch_i):\n",
    "-        return self.generic_eval_step(data_batch, batch_i, 'val')\n",
    "+        out = self.generic_eval_step(data_batch, batch_i, 'val')\n",
    "+        self.validation_step_outputs.append(out)\n",
    "+        return out\n",
    " \n",
    "-    def validation_epoch_end(self, outputs):\n",
    "-        return self.generic_eval_end(outputs, 'val')\n",
    "+    def on_validation_epoch_end(self):\n",
    "+        outputs = self.validation_step_outputs\n",
    "+        self.generic_eval_end(outputs, 'val')\n",
    "+        self.validation_step_outputs.clear()\n",
    " \n",
    "     def test_step(self, data_batch, batch_i):\n",
    "-        return self.generic_eval_step(data_batch, batch_i, 'test')\n",
    "-\n",
    "-    def test_epoch_end(self, outputs):\n",
    "-        return self.generic_eval_end(outputs, 'test')\n",
    "+        out = self.generic_eval_step(data_batch, batch_i, 'test')\n",
    "+        self.test_step_outputs.append(out)\n",
    "+        return out\n",
    "+\n",
    "+    def on_test_epoch_end(self):\n",
    "+        outputs = self.test_step_outputs\n",
    "+        self.generic_eval_end(outputs, 'test')\n",
    "+        self.test_step_outputs.clear()\n",
    " \n",
    "     def configure_optimizers(self):\n",
    "         optimizer = {'adam':     torch.optim.Adam(self.segmentor.parameters(), lr=self.config.lr),\n",
    "diff --git a/utils.py b/utils.py\n",
    "index 599c482..c13444f 100644\n",
    "--- a/utils.py\n",
    "+++ b/utils.py\n",
    "@@ -37,12 +37,13 @@ class PrecisionRecallMetric:\n",
    "         for (y, yhat) in zip(batch_y, batch_yhat):\n",
    "             y, yhat = np.array(y), np.array(yhat)\n",
    "             y, yhat = y[1:-1], yhat[1:-1]\n",
    "-            for yhat_i in yhat:\n",
    "-                min_dist = np.abs(y - yhat_i).min()\n",
    "-                precision_counter += (min_dist <= self.tolerance)\n",
    "-            for y_i in y:\n",
    "-                min_dist = np.abs(yhat - y_i).min()\n",
    "-                recall_counter += (min_dist <= self.tolerance)\n",
    "+            if len(yhat) > 0 and len(y) > 0:\n",
    "+                for yhat_i in yhat:\n",
    "+                    min_dist = np.abs(y - yhat_i).min()\n",
    "+                    precision_counter += (min_dist <= self.tolerance)\n",
    "+                for y_i in y:\n",
    "+                    min_dist = np.abs(yhat - y_i).min()\n",
    "+                    recall_counter += (min_dist <= self.tolerance)\n",
    "             pred_counter += len(yhat)\n",
    "             gt_counter += len(y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:02:05.322730Z",
     "iopub.status.busy": "2026-01-30T18:02:05.321537Z",
     "iopub.status.idle": "2026-01-30T18:02:05.329288Z",
     "shell.execute_reply": "2026-01-30T18:02:05.327977Z",
     "shell.execute_reply.started": "2026-01-30T18:02:05.322694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd SegFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:02:14.302509Z",
     "iopub.status.busy": "2026-01-30T18:02:14.300948Z",
     "iopub.status.idle": "2026-01-30T18:02:14.424002Z",
     "shell.execute_reply": "2026-01-30T18:02:14.422048Z",
     "shell.execute_reply.started": "2026-01-30T18:02:14.302461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git apply ../segfeat.patch\n",
    "!git apply ../lightning.patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:07:17.963613Z",
     "iopub.status.busy": "2026-01-30T18:07:17.963188Z",
     "iopub.status.idle": "2026-01-30T18:07:17.972470Z",
     "shell.execute_reply": "2026-01-30T18:07:17.971283Z",
     "shell.execute_reply.started": "2026-01-30T18:07:17.963576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "torch\n",
    "torchaudio\n",
    "torchvision\n",
    "pytorch-lightning\n",
    "boltons\n",
    "loguru\n",
    "librosa\n",
    "numpy\n",
    "pandas\n",
    "soundfile\n",
    "tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:07:31.380531Z",
     "iopub.status.busy": "2026-01-30T18:07:31.379625Z",
     "iopub.status.idle": "2026-01-30T18:07:37.837236Z",
     "shell.execute_reply": "2026-01-30T18:07:37.835868Z",
     "shell.execute_reply.started": "2026-01-30T18:07:31.380495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:07:45.689867Z",
     "iopub.status.busy": "2026-01-30T18:07:45.689097Z",
     "iopub.status.idle": "2026-01-30T18:08:04.608668Z",
     "shell.execute_reply": "2026-01-30T18:08:04.607525Z",
     "shell.execute_reply.started": "2026-01-30T18:07:45.689824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python main.py --wav_path /kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data --dataset timit --delta_feats --dist_feats"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 212391,
     "sourceId": 471627,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
