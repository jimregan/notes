{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UsRchXMCGzV",
        "outputId": "20fbb2e9-992b-49eb-8355-b75903c3a021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
            "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.15.0 stanza-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import stanza\n",
        "\n",
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "# Injected pronouns (only these)\n",
        "PRON_FEATS: Dict[str, str] = {\n",
        "    \"mé\": \"Person=1|Number=Sing\",\n",
        "    \"tú\": \"Person=2|Number=Sing\",\n",
        "    \"muid\": \"Person=1|Number=Plur\",\n",
        "    \"sinn\": \"Person=1|Number=Plur\",\n",
        "    \"sibh\": \"Person=2|Number=Plur\",\n",
        "    \"siad\": \"Person=3|Number=Plur\",\n",
        "}\n",
        "PRON_FORMS = set(PRON_FEATS.keys())\n",
        "\n",
        "# Stanza setup\n",
        "stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    tokenize_pretokenized=True,\n",
        "    no_ssplit=True,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "\n",
        "def standardise(text: str, lang: str = \"ga\") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Return list of (orig_chunk, std_chunk) rewrite units from Intergaelic.\"\"\"\n",
        "    data = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    hdrs = {\"Content-Type\": \"application/x-www-form-urlencoded\", \"Accept\": \"application/json\"}\n",
        "    req = urllib.request.Request(STD_API, data, hdrs)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        return [tuple(x) for x in json.loads(resp.read())]\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Tok:\n",
        "    \"\"\"\n",
        "    A single OUTPUT token (orig_tok) aligned to a single STANZA token (std_tok).\n",
        "    If Intergaelic injected a pronoun (std has +1 token and last is PRON), it is stored\n",
        "    on the LAST token of the rewrite unit (inj_pron).\n",
        "    \"\"\"\n",
        "    orig_tok: str\n",
        "    std_tok: str\n",
        "    inj_pron: Optional[str] = None\n",
        "\n",
        "\n",
        "def expand_and_align(pairs: Sequence[Tuple[str, str]]) -> List[Tok]:\n",
        "    \"\"\"\n",
        "    Expand Intergaelic rewrite units into a flat token stream with strict alignment.\n",
        "\n",
        "    Allowed:\n",
        "      1) N→N by whitespace splitting: align positionally.\n",
        "      2) Injection: len(std_parts) == len(orig_parts) + 1 AND std_parts[-1] in PRON_FORMS.\n",
        "         Align shared prefix positionally; attach inj_pron to the LAST aligned token.\n",
        "\n",
        "    Everything else: raise (no guessing).\n",
        "    \"\"\"\n",
        "    out: List[Tok] = []\n",
        "\n",
        "    for i, (orig_chunk, std_chunk) in enumerate(pairs):\n",
        "        orig_parts = (orig_chunk or \"\").split()\n",
        "        std_parts = (std_chunk or \"\").split()\n",
        "\n",
        "        # Treat empty std as identity on the orig side (rare, but your earlier code did this)\n",
        "        if not std_parts and orig_parts:\n",
        "            out.extend(Tok(o, o) for o in orig_parts)\n",
        "            continue\n",
        "\n",
        "        if len(orig_parts) == len(std_parts):\n",
        "            out.extend(Tok(o, s) for o, s in zip(orig_parts, std_parts))\n",
        "            continue\n",
        "\n",
        "        if len(std_parts) == len(orig_parts) + 1 and std_parts[-1].lower() in PRON_FORMS:\n",
        "            inj = std_parts[-1].lower()\n",
        "            shared = std_parts[:-1]\n",
        "            if len(shared) != len(orig_parts):\n",
        "                raise ValueError(\n",
        "                    f\"Internal alignment error at pair {i}: orig={orig_chunk!r} std={std_chunk!r}\"\n",
        "                )\n",
        "            for j, (o, s) in enumerate(zip(orig_parts, shared)):\n",
        "                out.append(Tok(o, s, inj_pron=inj if j == len(orig_parts) - 1 else None))\n",
        "            continue\n",
        "\n",
        "        raise ValueError(\n",
        "            f\"Unsupported Intergaelic mapping at pair index {i}: \"\n",
        "            f\"orig={orig_chunk!r} ({len(orig_parts)} toks) std={std_chunk!r} ({len(std_parts)} toks)\"\n",
        "        )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_spaceafter(raw_text: str, orig_tokens: List[str]) -> List[bool]:\n",
        "    \"\"\"\n",
        "    True  => there WAS whitespace after this token in raw_text (so no SpaceAfter=No)\n",
        "    False => no whitespace after (emit SpaceAfter=No)\n",
        "\n",
        "    Monotonic substring alignment; raises if a token can't be located.\n",
        "    \"\"\"\n",
        "    flags: List[bool] = []\n",
        "    pos = 0\n",
        "    n = len(raw_text)\n",
        "\n",
        "    for i, tok in enumerate(orig_tokens):\n",
        "        # Skip whitespace before token\n",
        "        while pos < n and raw_text[pos].isspace():\n",
        "            pos += 1\n",
        "\n",
        "        # Prefer exact match at current position\n",
        "        if raw_text.startswith(tok, pos):\n",
        "            start = pos\n",
        "        else:\n",
        "            start = raw_text.find(tok, pos)\n",
        "            if start == -1:\n",
        "                raise ValueError(f\"Could not align token {i} {tok!r} near pos {pos}\")\n",
        "        end = start + len(tok)\n",
        "        pos = end\n",
        "\n",
        "        if pos >= n:\n",
        "            flags.append(True)  # end-of-text\n",
        "        else:\n",
        "            flags.append(raw_text[pos].isspace())\n",
        "\n",
        "    return flags\n",
        "\n",
        "\n",
        "def sentences_from_tokens(tokens: Sequence[Tok]) -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Sentence segmentation over the STANZA token stream:\n",
        "    end sentence at . ! ? on std_tok.\n",
        "    Returns sentences as lists of indices into `tokens`.\n",
        "    \"\"\"\n",
        "    sents: List[List[int]] = []\n",
        "    buf: List[int] = []\n",
        "    for i, t in enumerate(tokens):\n",
        "        buf.append(i)\n",
        "        if t.std_tok in {\".\", \"!\", \"?\"}:\n",
        "            sents.append(buf)\n",
        "            buf = []\n",
        "    if buf:\n",
        "        sents.append(buf)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def feats_to_dict(feats: str) -> Dict[str, str]:\n",
        "    if not feats or feats == \"_\":\n",
        "        return {}\n",
        "    out: Dict[str, str] = {}\n",
        "    for part in feats.split(\"|\"):\n",
        "        if \"=\" in part:\n",
        "            k, v = part.split(\"=\", 1)\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def dict_to_feats(d: Dict[str, str]) -> str:\n",
        "    if not d:\n",
        "        return \"_\"\n",
        "    return \"|\".join(f\"{k}={v}\" for k, v in sorted(d.items()))\n",
        "\n",
        "\n",
        "def merge_feats_preserve(base: str, add: str) -> str:\n",
        "    \"\"\"\n",
        "    Merge without overwriting existing keys (so we don't stomp on Stanza if it already\n",
        "    provided Person/Number).\n",
        "    \"\"\"\n",
        "    bd = feats_to_dict(base)\n",
        "    ad = feats_to_dict(add)\n",
        "    for k, v in ad.items():\n",
        "        bd.setdefault(k, v)\n",
        "    return dict_to_feats(bd)\n",
        "\n",
        "\n",
        "def merge_misc(*items: str) -> str:\n",
        "    parts: List[str] = []\n",
        "    for it in items:\n",
        "        if it and it != \"_\":\n",
        "            parts.append(it)\n",
        "    return \"_\" if not parts else \"|\".join(parts)\n",
        "\n",
        "\n",
        "def choose_rep_word(words, idxs: List[int]) -> int:\n",
        "    \"\"\"\n",
        "    Representative word for lemma/POS/feats/deprel/head among a group.\n",
        "    We only group when we *decide* to later; here each Tok is 1:1 with a stanza word.\n",
        "    Keep this for possible future extension; currently idxs will be length 1.\n",
        "    \"\"\"\n",
        "    for i in idxs:\n",
        "        if (words[i].upos or \"\") != \"PRON\":\n",
        "            return i\n",
        "    return idxs[0]\n",
        "\n",
        "\n",
        "def project_with_stanza(raw_text: str, lang: str = \"ga\") -> str:\n",
        "    \"\"\"\n",
        "    Outputs CoNLL-U:\n",
        "      - Tokenization = aligned original tokens (after expanding orig chunks by whitespace)\n",
        "      - Stanza run on aligned standardized tokens\n",
        "      - Injected pronoun (Intergaelic-only) contributes Person/Number to the LAST token\n",
        "        in the rewrite unit (Tok.inj_pron), never creates a token.\n",
        "      - SpaceAfter=No derived from raw_text spacing.\n",
        "    \"\"\"\n",
        "    pairs = standardise(raw_text, lang)\n",
        "    toks = expand_and_align(pairs)\n",
        "\n",
        "    orig_tokens = [t.orig_tok for t in toks]\n",
        "    spaceafter = compute_spaceafter(raw_text, orig_tokens)\n",
        "\n",
        "    sents = sentences_from_tokens(toks)\n",
        "    pretok: List[List[str]] = [[toks[i].std_tok for i in sent] for sent in sents]\n",
        "    doc = nlp(pretok)\n",
        "\n",
        "    out: List[str] = []\n",
        "    global_idx = 0  # index into toks\n",
        "\n",
        "    for sid, (sent_idxs, sent_doc) in enumerate(zip(sents, doc.sentences), 1):\n",
        "        raw_slice = [toks[i].orig_tok for i in sent_idxs]\n",
        "        std_slice = [toks[i].std_tok for i in sent_idxs]\n",
        "\n",
        "        out += [\n",
        "            f\"# sent_id = {sid}\",\n",
        "            f\"# text = {' '.join(raw_slice)}\",\n",
        "            f\"# text_standard = {' '.join(std_slice)}\",\n",
        "        ]\n",
        "\n",
        "        words = sent_doc.words  # 1 per pretokenized token in this sentence\n",
        "\n",
        "        # Map stanza word index (sentence-local) -> output token id (sentence-local)\n",
        "        # Here it's 1:1 by construction.\n",
        "        for widx, tok_i in enumerate(sent_idxs):\n",
        "            tid = widx + 1\n",
        "            t = toks[tok_i]\n",
        "            w = words[widx]\n",
        "\n",
        "            # Head remap: stanza head is 1-based within this sentence; 0=root\n",
        "            head_tid = w.head if (w.head is not None and w.head != 0) else 0\n",
        "\n",
        "            feats = w.feats or \"_\"\n",
        "            misc_parts: List[str] = []\n",
        "\n",
        "            if not spaceafter[tok_i]:\n",
        "                misc_parts.append(\"SpaceAfter=No\")\n",
        "\n",
        "            if t.inj_pron is not None:\n",
        "                # Guaranteed to be in PRON_FEATS by expand_and_align()\n",
        "                feats = merge_feats_preserve(feats, PRON_FEATS[t.inj_pron])\n",
        "                misc_parts.append(f\"InjPron={t.inj_pron}\")\n",
        "\n",
        "            misc = merge_misc(*misc_parts)\n",
        "\n",
        "            out.append(\"\\t\".join([\n",
        "                str(tid),\n",
        "                t.orig_tok or \"_\",\n",
        "                w.lemma or \"_\",\n",
        "                w.upos or \"_\",\n",
        "                w.xpos or \"_\",\n",
        "                feats,\n",
        "                str(head_tid),\n",
        "                w.deprel or \"_\",\n",
        "                \"_\",\n",
        "                misc,\n",
        "            ]))\n",
        "\n",
        "            global_idx += 1\n",
        "\n",
        "        out.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(out)\n"
      ],
      "metadata": {
        "id": "psn8ILaqCTlQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = project_with_stanza('Do leanadar ag \"seasamh a gcirt\" go dtí gur dhein Eoghan Rua Ó Néill, ag an mBeinn mBorb, gníomh díreach de shaghas an ghnímh a dhein driotháir a athar agus Aodh Rua Ó Dónaill ag Béal an Átha Buí deich mbliana agus daichead roimis sin.')"
      ],
      "metadata": {
        "id": "j4CqPq57C7ov"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEUHMBbIDgow",
        "outputId": "a81cb9a0-2124-4342-b5cb-561fa6111f96"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sent_id = 1\n",
            "# text = Do leanadar ag \" seasamh a gcirt \" go dtí gur dhein Eoghan Rua Ó Néill , ag an mBeinn mBorb , gníomh díreach de shaghas an ghnímh a dhein driotháir a athar agus Aodh Rua Ó Dónaill ag Béal an Átha Buí deich mbliana agus daichead roimis sin .\n",
            "# text_standard = Do leanadar ag \" seasamh a gcirt \" go dtí go ndearna Eoghan Rua Ó Néill , ag an mBinn mBorb , gníomh díreach de shaghas an ghnímh a rinne deartháir a athar agus Aodh Rua Ó Dónaill ag Béal an Átha Buí deich mbliana agus daichead roimhe sin .\n",
            "1\tDo\tdo\tPART\tVb\tPartType=Vb\t2\tmark:prt\t_\t_\n",
            "2\tleanadar\tlean\tVERB\tVTI\tMood=Ind|Number=Plur|Person=1|Tense=Past\t0\troot\t_\t_\n",
            "3\tag\tag\tADP\tSimp\t_\t5\tcase\t_\t_\n",
            "4\t\"\t\"\tPUNCT\tPunct\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "5\tseasamh\tseasamh\tNOUN\tNoun\tVerbForm=Inf\t2\txcomp\t_\t_\n",
            "6\ta\ta\tDET\tDet\tNumber=Plur|Person=3|Poss=Yes\t7\tnmod:poss\t_\t_\n",
            "7\tgcirt\tceirt\tNOUN\tNoun\tCase=Gen|Definite=Def|Form=Ecl|Gender=Fem|Number=Sing\t5\tnmod\t_\tSpaceAfter=No\n",
            "8\t\"\t\"\tPUNCT\tPunct\t_\t5\tpunct\t_\t_\n",
            "9\tgo\tgo\tADP\tCmpd\tPrepForm=Cmpd\t12\tmark\t_\t_\n",
            "10\tdtí\tdtí\tNOUN\tCmpd\tForm=Ecl|PrepForm=Cmpd\t9\tfixed\t_\t_\n",
            "11\tgur\tgo\tPART\tVb\tPartType=Cmpl\t12\tmark:prt\t_\t_\n",
            "12\tdhein\tdéan\tVERB\tVTI\tForm=Ecl|Mood=Ind|Tense=Past\t2\tccomp\t_\t_\n",
            "13\tEoghan\tEoghan\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t12\tnsubj\t_\t_\n",
            "14\tRua\tRua\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t13\tflat:name\t_\t_\n",
            "15\tÓ\tó\tPART\tPat\tPartType=Pat\t13\tflat:name\t_\t_\n",
            "16\tNéill\tNéill\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t13\tflat:name\t_\tSpaceAfter=No\n",
            "17\t,\t,\tPUNCT\tPunct\t_\t20\tpunct\t_\t_\n",
            "18\tag\tag\tADP\tSimp\t_\t20\tcase\t_\t_\n",
            "19\tan\tan\tDET\tArt\tDefinite=Def|Number=Sing|PronType=Art\t20\tdet\t_\t_\n",
            "20\tmBeinn\tbinn\tPROPN\tNoun\tCase=Nom|Definite=Def|Form=Ecl|Gender=Masc|Number=Sing\t13\tconj\t_\t_\n",
            "21\tmBorb\tborb\tPROPN\tNoun\tCase=Nom|Definite=Def|Form=Ecl|Gender=Masc|Number=Sing\t20\tflat:foreign\t_\tSpaceAfter=No\n",
            "22\t,\t,\tPUNCT\tPunct\t_\t23\tpunct\t_\t_\n",
            "23\tgníomh\tgníomh\tNOUN\tNoun\tCase=Nom|Gender=Masc|Number=Sing\t12\tobj\t_\t_\n",
            "24\tdíreach\tdíreach\tADJ\tAdj\tCase=Nom|Gender=Masc|Number=Sing\t23\tamod\t_\t_\n",
            "25\tde\tde\tADP\tSimp\t_\t26\tcase\t_\t_\n",
            "26\tshaghas\tsaghas\tNOUN\tNoun\tCase=Nom|Definite=Def|Form=Len|Gender=Masc|Number=Sing\t23\tnmod\t_\t_\n",
            "27\tan\tan\tDET\tArt\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing|PronType=Art\t28\tdet\t_\t_\n",
            "28\tghnímh\tgníomh\tNOUN\tNoun\tCase=Gen|Definite=Def|Form=Len|Gender=Masc|Number=Sing\t26\tnmod\t_\t_\n",
            "29\ta\ta\tPART\tVb\tForm=Direct|PartType=Vb|PronType=Rel\t30\tobj\t_\t_\n",
            "30\tdhein\tdéan\tVERB\tVTI\tMood=Ind|Tense=Past\t23\tacl:relcl\t_\t_\n",
            "31\tdriotháir\tdeartháir\tNOUN\tNoun\tCase=Nom|Definite=Def|Gender=Masc|Number=Sing\t30\tnsubj\t_\t_\n",
            "32\ta\ta\tDET\tDet\tGender=Masc|Number=Sing|Person=3|Poss=Yes\t33\tnmod:poss\t_\t_\n",
            "33\tathar\tathair\tNOUN\tNoun\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing\t31\tnmod\t_\t_\n",
            "34\tagus\tagus\tCCONJ\tCoord\t_\t35\tcc\t_\t_\n",
            "35\tAodh\tAodh\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t31\tconj\t_\t_\n",
            "36\tRua\tRua\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t35\tflat:name\t_\t_\n",
            "37\tÓ\tó\tPART\tPat\tPartType=Pat\t35\tflat:name\t_\t_\n",
            "38\tDónaill\tDónaill\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t35\tflat:name\t_\t_\n",
            "39\tag\tag\tADP\tSimp\t_\t40\tcase\t_\t_\n",
            "40\tBéal\tBéal\tNOUN\tNoun\tCase=Nom|Definite=Def|Gender=Masc|Number=Sing\t30\tobl\t_\t_\n",
            "41\tan\tan\tDET\tArt\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing|PronType=Art\t42\tdet\t_\t_\n",
            "42\tÁtha\táth\tPROPN\tNoun\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing\t40\tnmod\t_\t_\n",
            "43\tBuí\tbuí\tADJ\tAdj\tCase=Nom|Gender=Masc|Number=Sing\t42\tamod\t_\t_\n",
            "44\tdeich\tdeich\tNUM\tNum\tNumType=Card\t45\tnummod\t_\t_\n",
            "45\tmbliana\tbliain\tNOUN\tNoun\tCase=Nom|Form=Ecl|Gender=Fem|Number=Plur\t30\tobl:tmod\t_\t_\n",
            "46\tagus\tagus\tCCONJ\tCoord\t_\t47\tcc\t_\t_\n",
            "47\tdaichead\tdaichead\tNUM\tNum\tNumType=Card\t45\tconj\t_\t_\n",
            "48\troimis\troimh\tADP\tPrep\tGender=Masc|Number=Sing|Person=3\t49\tcase\t_\t_\n",
            "49\tsin\tsin\tPRON\tDem\tPronType=Dem\t47\tnmod\t_\tSpaceAfter=No\n",
            "50\t.\t.\tPUNCT\t.\t_\t2\tpunct\t_\t_\n",
            "\n"
          ]
        }
      ]
    }
  ]
}