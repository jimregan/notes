{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'ɖ': 0, 'ɭ': 1, 'ɳ': 2, 'ʂ': 3, 'ʈ': 4, 'a': 5, 'ɑː': 6, 'b': 7, 'd': 8, 'e': 9, 'ə': 10, 'eː': 11, 'f': 12, 'ɡ': 13, 'h': 14, 'ɪ': 15, 'iː': 16, 'j': 17, 'k': 18, 'l': 19, 'm': 20, 'n': 21, 'ŋ': 22, 'ʊ': 23, 'uː': 24, 'p': 25, 'r': 26, 's': 27, 'ɧ': 28, 't': 29, 'ɕ': 30, 'ɵ': 31, 'ʉː': 32, 'v': 33, 'ʏ': 34, 'yː': 35, 'ɛ': 36, 'æː': 37, 'æ': 38, 'ɛː': 39, 'œ': 40, 'œ̞ː': 41, 'œ̞': 42, 'øː': 43, 'ɔ': 44, 'oː': 45, '|': 46, '<v>': 47, '~': 48, '<unk>': 49}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongestMatchTokenizer:\n",
    "    def __init__(self, vocab, unk_token=\"<unk>\"):\n",
    "        self.vocab = vocab  # Dictionary: token -> ID\n",
    "        self.unk_token = unk_token  # Handle unknown tokens\n",
    "        self.unk_id = vocab.get(unk_token, len(vocab))  # Default unknown token ID\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Tokenize text using longest-match-first approach.\"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            match = None\n",
    "            match_token = None\n",
    "\n",
    "            # Try to match the longest possible token\n",
    "            for j in range(len(text), i, -1):\n",
    "                sub = text[i:j]\n",
    "                if sub in self.vocab:\n",
    "                    match = sub\n",
    "                    match_token = self.vocab[sub]\n",
    "                    break  # Longest match found\n",
    "\n",
    "            if match:\n",
    "                tokens.append((match, match_token))\n",
    "                i += len(match)  # Move forward in the text\n",
    "            else:\n",
    "                tokens.append((self.unk_token, self.unk_id))\n",
    "                i += 1  # Move forward by 1 character if no match\n",
    "\n",
    "        tokenized_str = \" \".join([t[0] for t in tokens])\n",
    "        token_ids = [t[1] for t in tokens]\n",
    "\n",
    "        return tokenized_str, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongestMatchTokenizer(vocab=vocab, unk_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"apstrakʂuːn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a p s t r a k ʂ uː n', [5, 25, 27, 29, 26, 5, 18, 3, 24, 21])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded = tokenizer.encode(text)\n",
    "# \" \".join(encoded.tokens)\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ɖɭɳʂʈ aɑː bd eə eːfɡ hɪiː j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting panphon\n",
      "  Downloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /Users/joregan/opt/anaconda3/envs/ctcseg/lib/python3.10/site-packages (from panphon) (2022.3.15)\n",
      "Collecting unicodecsv\n",
      "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
      "Collecting munkres\n",
      "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/joregan/opt/anaconda3/envs/ctcseg/lib/python3.10/site-packages/PyYAML-6.0-py3.10-macosx-10.9-x86_64.egg (from panphon) (6.0)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.8.1-cp310-cp310-macosx_10_9_x86_64.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/joregan/opt/anaconda3/envs/ctcseg/lib/python3.10/site-packages (from panphon) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.2 in /Users/joregan/opt/anaconda3/envs/ctcseg/lib/python3.10/site-packages (from panphon) (1.26.4)\n",
      "Building wheels for collected packages: unicodecsv\n",
      "  Building wheel for unicodecsv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10768 sha256=b2ba466b3ff3319ff1e7e73cb98bd7ed6722ddd96a30cff6482edd296917169d\n",
      "  Stored in directory: /Users/joregan/Library/Caches/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
      "Successfully built unicodecsv\n",
      "Installing collected packages: unicodecsv, munkres, editdistance, panphon\n",
      "Successfully installed editdistance-0.8.1 munkres-1.1.4 panphon-0.21.2 unicodecsv-0.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install panphon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'ɛːɡandə'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panphon.segment import Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panphon\n",
    "ft = panphon.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, -back, -round, -velaric, -tense, +long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, -ant, -cor, 0distr, -lab, +hi, -lo, +back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, +son, +cons, -cont, -delrel, -lat, +nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, +back, -round, -velaric, -tense, -long, 0hitone, 0hireg]>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_fts(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, +ant, -cor, 0distr, +lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, -lo, -back, +round, -velaric, -tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, +son, +cons, +cont, 0delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, 0hi, 0lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, -ant, -cor, 0distr, -lab, +hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, +hi, -lo, +back, +round, -velaric, -tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, -voi, -sg, -cg, -ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_fts(\"fœ̞ːrjʊʈ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, -son, +cons, -cont, -delrel, -lat, -nas, -strid, +voi, -sg, -cg, +ant, -cor, 0distr, +lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_fts(\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Segment [-syl, -son, +cons, +cont, +delrel, -lat, -nas, -strid, -voi, -sg, -cg, -ant, +cor, +distr, -lab, +hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>,\n",
       " <Segment [-syl, -son, +cons, +cont, -delrel, -lat, -nas, +strid, -voi, -sg, -cg, -ant, +cor, -distr, -lab, -hi, -lo, -back, -round, -velaric, 0tense, -long, 0hitone, 0hireg]>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_fts(\"ɧʂ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctcseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
