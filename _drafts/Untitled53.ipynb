{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download sv_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "811Y3PSCH9Ho",
        "outputId": "a640a09f-b735-4fd7-beb2-ce1714d649cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sv-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/sv_core_news_sm-3.8.0/sv_core_news_sm-3.8.0-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sv-core-news-sm\n",
            "Successfully installed sv-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('sv_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdGBjOxmHpHG",
        "outputId": "62cd05de-7e6d-49cf-dba7-1b21cc8770a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equivalent: False\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "nlp = spacy.load(\"sv_core_news_sm\")\n",
        "\n",
        "# Example rule DSL\n",
        "dsl_rules = [\n",
        "    {\n",
        "        \"name\": \"den-har + definite noun -> noun\",\n",
        "        \"pattern\": [\n",
        "            {\"LEMMA\": {\"in\": [\"den\", \"det\"]}, \"POS\": \"DET\"},\n",
        "            {\"TEXT\": \"här\", \"POS\": \"ADV\"},\n",
        "            {\"DEP\": \"nmod\", \"POS\": \"NOUN\", \"MORPH\": {\"Definite\": \"Def\"}}\n",
        "        ],\n",
        "        \"action\": \"collapse_to\",\n",
        "        \"head\": 2\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"har + supine -> past\",\n",
        "        \"pattern\": [\n",
        "            {\"LEMMA\": \"ha\", \"POS\": \"AUX\"},\n",
        "            {\"DEP\": \"xcomp\", \"POS\": \"VERB\", \"MORPH\": {\"VerbForm\": \"Sup\"}}\n",
        "        ],\n",
        "        \"action\": \"transform_verb\",\n",
        "        \"head\": 1,\n",
        "        \"transform\": {\n",
        "            \"lemma_map\": {\n",
        "                \"suttit\": \"satt\",\n",
        "                \"gått\": \"gick\",\n",
        "                \"kommit\": \"kom\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def apply_rewrite_rules(token, rules):\n",
        "    for rule in rules:\n",
        "        match = match_pattern(token, rule['pattern'])\n",
        "        if match:\n",
        "            if rule['action'] == 'collapse_to':\n",
        "                return build_tree(match[rule['head']], rules)\n",
        "            elif rule['action'] == 'transform_verb':\n",
        "                head_token = match[rule['head']]\n",
        "                lemma_map = rule.get('transform', {}).get('lemma_map', {})\n",
        "                return {\n",
        "                    'lemma': lemma_map.get(head_token.lemma_, head_token.lemma_),\n",
        "                    'dep': head_token.dep_,\n",
        "                    'pos': head_token.pos_,\n",
        "                    'children': sorted([\n",
        "                        build_tree(c, rules) for c in head_token.children if c not in match\n",
        "                    ], key=lambda x: (x['dep'], x['lemma']))\n",
        "                }\n",
        "    return None\n",
        "\n",
        "def match_pattern(token, pattern):\n",
        "    matched = []\n",
        "    siblings = [token] + list(token.children)\n",
        "    for rule_token in pattern:\n",
        "        for sib in siblings:\n",
        "            if token_matches(sib, rule_token) and sib not in matched:\n",
        "                matched.append(sib)\n",
        "                break\n",
        "        else:\n",
        "            return None\n",
        "    return matched\n",
        "\n",
        "def token_matches(token, rule):\n",
        "    for key, value in rule.items():\n",
        "        if key == \"LEMMA\" and not match_value(token.lemma_, value): return False\n",
        "        if key == \"TEXT\" and not match_value(token.text, value): return False\n",
        "        if key == \"POS\" and token.pos_ != value: return False\n",
        "        if key == \"DEP\" and token.dep_ != value: return False\n",
        "        if key == \"MORPH\":\n",
        "            for morph_key, morph_val in value.items():\n",
        "                if token.morph.get(morph_key) != [morph_val]:\n",
        "                    return False\n",
        "    return True\n",
        "\n",
        "def match_value(val, cond):\n",
        "    if isinstance(cond, dict):\n",
        "        if \"in\" in cond: return val in cond[\"in\"]\n",
        "    else:\n",
        "        return val == cond\n",
        "\n",
        "def build_tree(token, rules):\n",
        "    # Try applying rewrite rules\n",
        "    rewritten = apply_rewrite_rules(token, rules)\n",
        "    if rewritten:\n",
        "        return rewritten\n",
        "\n",
        "    if token.pos_ == \"PUNCT\":\n",
        "        return None\n",
        "\n",
        "    children = filter(None, [build_tree(child, rules) for child in token.children])\n",
        "    return {\n",
        "        'lemma': token.lemma_.lower(),\n",
        "        'dep': token.dep_,\n",
        "        'pos': token.pos_,\n",
        "        'children': sorted(children, key=lambda x: (x['dep'], x['lemma']))\n",
        "    }\n",
        "\n",
        "def get_root_trees(doc):\n",
        "    roots = [token for token in doc if token.head == token]\n",
        "    return sorted([\n",
        "        build_tree(root, dsl_rules) for root in roots if root.pos_ != \"PUNCT\"\n",
        "    ], key=lambda x: (x['dep'], x['lemma']))\n",
        "\n",
        "def compare_trees(t1, t2):\n",
        "    if t1['lemma'] != t2['lemma'] or t1['dep'] != t2['dep'] or t1['pos'] != t2['pos']:\n",
        "        return False\n",
        "    if len(t1['children']) != len(t2['children']):\n",
        "        return False\n",
        "    return all(compare_trees(c1, c2) for c1, c2 in zip(t1['children'], t2['children']))\n",
        "\n",
        "def are_equivalent(sent1, sent2):\n",
        "    doc1 = nlp(sent1)\n",
        "    doc2 = nlp(sent2)\n",
        "    trees1 = get_root_trees(doc1)\n",
        "    trees2 = get_root_trees(doc2)\n",
        "    if len(trees1) != len(trees2):\n",
        "        return False\n",
        "    return all(compare_trees(t1, t2) for t1, t2 in zip(trees1, trees2))\n",
        "\n",
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "s1 = \"Katten satt på mattan.\"\n",
        "s2 = \"Den här katten har suttit på mattan.\"\n",
        "print(\"Equivalent:\", are_equivalent(s1, s2))\n"
      ]
    }
  ]
}