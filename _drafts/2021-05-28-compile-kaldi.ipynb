{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302ac70d",
   "metadata": {},
   "source": [
    "# Compiling Kaldi on Kaggle\n",
    "\n",
    "> \"Kaldi is a bit of a beast to install\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- comments: true\n",
    "- hidden: true\n",
    "- categories: [kaggle, kaldi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-twenty",
   "metadata": {
    "papermill": {
     "duration": 0.016395,
     "end_time": "2021-06-04T08:23:35.235925",
     "exception": false,
     "start_time": "2021-06-04T08:23:35.219530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Kaldi is a bit of a beast to install. I tried to get around it by [extracting files from the official docker image](https://www.kaggle.com/jimregan/extract-prebuilt-kaldi-from-docker): tl;dr, it works fine for the tools that run on CPU, but the GPU-based tools depend on CUDA 10.0, while Kaggle's GPU images use CUDA 11.0.\n",
    "\n",
    "I'm building this in `/opt` to match the docker extraction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-marker",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-04T08:23:35.278805Z",
     "iopub.status.busy": "2021-06-04T08:23:35.277960Z",
     "iopub.status.idle": "2021-06-04T08:23:35.281840Z",
     "shell.execute_reply": "2021-06-04T08:23:35.282248Z",
     "shell.execute_reply.started": "2021-05-28T13:47:35.941509Z"
    },
    "papermill": {
     "duration": 0.030837,
     "end_time": "2021-06-04T08:23:35.282449",
     "exception": false,
     "start_time": "2021-06-04T08:23:35.251612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-chosen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:23:35.318763Z",
     "iopub.status.busy": "2021-06-04T08:23:35.318232Z",
     "iopub.status.idle": "2021-06-04T08:23:46.166580Z",
     "shell.execute_reply": "2021-06-04T08:23:46.165753Z",
     "shell.execute_reply.started": "2021-05-28T13:47:35.952853Z"
    },
    "papermill": {
     "duration": 10.867655,
     "end_time": "2021-06-04T08:23:46.166731",
     "exception": false,
     "start_time": "2021-06-04T08:23:35.299076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/kaldi-asr/kaldi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-title",
   "metadata": {
    "papermill": {
     "duration": 0.039878,
     "end_time": "2021-06-04T08:23:46.247274",
     "exception": false,
     "start_time": "2021-06-04T08:23:46.207396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I'm installing cudatoolkit to make sure it's there, and the same version as the one in the gpu docker image, because issues with that are why I'm compiling this in the first place. You can quite possibly get away with not running this step, but I'm not taking a chance on it. If you are going to re-run this, make sure to check the [gpu docker image](https://github.com/Kaggle/docker-python/blob/main/gpu.Dockerfile) to match the version for cudatoolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-crest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:23:46.331706Z",
     "iopub.status.busy": "2021-06-04T08:23:46.330847Z",
     "iopub.status.idle": "2021-06-04T08:23:46.333428Z",
     "shell.execute_reply": "2021-06-04T08:23:46.332926Z",
     "shell.execute_reply.started": "2021-05-28T14:25:34.509895Z"
    },
    "papermill": {
     "duration": 0.045827,
     "end_time": "2021-06-04T08:23:46.333615",
     "exception": false,
     "start_time": "2021-06-04T08:23:46.287788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!conda install cudatoolkit=11.0 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-reflection",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:23:46.420282Z",
     "iopub.status.busy": "2021-06-04T08:23:46.419543Z",
     "iopub.status.idle": "2021-06-04T08:50:52.201686Z",
     "shell.execute_reply": "2021-06-04T08:50:52.201213Z"
    },
    "papermill": {
     "duration": 1625.828286,
     "end_time": "2021-06-04T08:50:52.201824",
     "exception": false,
     "start_time": "2021-06-04T08:23:46.373538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda install cudatoolkit-dev=11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-reducing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:50:58.286864Z",
     "iopub.status.busy": "2021-06-04T08:50:58.286001Z",
     "iopub.status.idle": "2021-06-04T08:51:04.672709Z",
     "shell.execute_reply": "2021-06-04T08:51:04.672246Z",
     "shell.execute_reply.started": "2021-05-28T13:49:09.908892Z"
    },
    "papermill": {
     "duration": 9.540146,
     "end_time": "2021-06-04T08:51:04.672841",
     "exception": false,
     "start_time": "2021-06-04T08:50:55.132695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get -y install liblapack-dev sox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-toronto",
   "metadata": {
    "papermill": {
     "duration": 3.147347,
     "end_time": "2021-06-04T08:51:10.802401",
     "exception": false,
     "start_time": "2021-06-04T08:51:07.655054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First step is to follow the instructions in `kaldi/tools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-pipeline",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:51:16.647632Z",
     "iopub.status.busy": "2021-06-04T08:51:16.646705Z",
     "iopub.status.idle": "2021-06-04T08:51:16.650762Z",
     "shell.execute_reply": "2021-06-04T08:51:16.650298Z",
     "shell.execute_reply.started": "2021-05-28T13:50:58.598626Z"
    },
    "papermill": {
     "duration": 2.910771,
     "end_time": "2021-06-04T08:51:16.650885",
     "exception": false,
     "start_time": "2021-06-04T08:51:13.740114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /opt/kaldi/tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-makeup",
   "metadata": {
    "papermill": {
     "duration": 3.117759,
     "end_time": "2021-06-04T08:51:22.721011",
     "exception": false,
     "start_time": "2021-06-04T08:51:19.603252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `INSTALL` file says to run `extras/check_dependencies.sh`, fix anything that's missing, and then run `make`. It's unlikely that new dependencies will be added (Kaldi more in maintenance mode than active development), but just in case, uncomment the line in the next cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-environment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:51:29.032282Z",
     "iopub.status.busy": "2021-06-04T08:51:29.031423Z",
     "iopub.status.idle": "2021-06-04T08:51:29.034014Z",
     "shell.execute_reply": "2021-06-04T08:51:29.033508Z"
    },
    "papermill": {
     "duration": 2.936167,
     "end_time": "2021-06-04T08:51:29.034128",
     "exception": false,
     "start_time": "2021-06-04T08:51:26.097961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat INSTALL\n",
    "#!./extras/check_dependencies.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-breath",
   "metadata": {
    "papermill": {
     "duration": 2.891237,
     "end_time": "2021-06-04T08:51:35.096898",
     "exception": false,
     "start_time": "2021-06-04T08:51:32.205661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is what I was missing from the `check_dependencies` check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-buffer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:51:41.015330Z",
     "iopub.status.busy": "2021-06-04T08:51:41.012188Z",
     "iopub.status.idle": "2021-06-04T08:51:46.710411Z",
     "shell.execute_reply": "2021-06-04T08:51:46.712450Z",
     "shell.execute_reply.started": "2021-05-28T13:53:40.699509Z"
    },
    "papermill": {
     "duration": 8.650724,
     "end_time": "2021-06-04T08:51:46.712731",
     "exception": false,
     "start_time": "2021-06-04T08:51:38.062007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get -y install automake autoconf gfortran subversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-decline",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T08:51:52.568448Z",
     "iopub.status.busy": "2021-06-04T08:51:52.564459Z",
     "iopub.status.idle": "2021-06-04T09:17:47.217862Z",
     "shell.execute_reply": "2021-06-04T09:17:47.217335Z"
    },
    "papermill": {
     "duration": 1557.60253,
     "end_time": "2021-06-04T09:17:47.218027",
     "exception": false,
     "start_time": "2021-06-04T08:51:49.615497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-superintendent",
   "metadata": {
    "papermill": {
     "duration": 2.952544,
     "end_time": "2021-06-04T09:17:53.076804",
     "exception": false,
     "start_time": "2021-06-04T09:17:50.124260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next,we need a maths library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-banana",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:17:59.244223Z",
     "iopub.status.busy": "2021-06-04T09:17:59.243508Z",
     "iopub.status.idle": "2021-06-04T09:27:21.307960Z",
     "shell.execute_reply": "2021-06-04T09:27:21.306938Z"
    },
    "papermill": {
     "duration": 565.097735,
     "end_time": "2021-06-04T09:27:21.308127",
     "exception": false,
     "start_time": "2021-06-04T09:17:56.210392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!extras/install_openblas.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hollywood",
   "metadata": {
    "papermill": {
     "duration": 3.189448,
     "end_time": "2021-06-04T09:27:27.437329",
     "exception": false,
     "start_time": "2021-06-04T09:27:24.247881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now is the time to build any of the optional dependencies you might want. I want phonetisaurus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-pennsylvania",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:27:33.366252Z",
     "iopub.status.busy": "2021-06-04T09:27:33.362007Z",
     "iopub.status.idle": "2021-06-04T09:29:51.411384Z",
     "shell.execute_reply": "2021-06-04T09:29:51.410916Z"
    },
    "papermill": {
     "duration": 141.038571,
     "end_time": "2021-06-04T09:29:51.411556",
     "exception": false,
     "start_time": "2021-06-04T09:27:30.372985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!bash extras/install_phonetisaurus.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-carolina",
   "metadata": {
    "papermill": {
     "duration": 2.944764,
     "end_time": "2021-06-04T09:29:57.261802",
     "exception": false,
     "start_time": "2021-06-04T09:29:54.317038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I had problems with `phonetisaurus-apply` from the branch installed by `install_phonetisaurus.sh`, so I'm replacing it with an updated version from the phonetisaurus repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-judgment",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-06-04T09:30:03.842833Z",
     "iopub.status.busy": "2021-06-04T09:30:03.841712Z",
     "iopub.status.idle": "2021-06-04T09:30:03.846221Z",
     "shell.execute_reply": "2021-06-04T09:30:03.845824Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 3.495684,
     "end_time": "2021-06-04T09:30:03.846334",
     "exception": false,
     "start_time": "2021-06-04T09:30:00.350650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile /opt/kaldi/tools/phonetisaurus-g2p/src/scripts/phonetisaurus-apply\n",
    "#!/usr/bin/env python\n",
    "# -*- mode: python; coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os, logging, subprocess, time, re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import tempfile\n",
    "\n",
    "class G2PModelTester () :\n",
    "    \"\"\"G2P Model training wrapper class.\n",
    "\n",
    "    Phonetisaurus G2P modeling training wrapper class.\n",
    "    This wraps the alignment, joint n-gram training, and ARPA to\n",
    "    WFST conversion steps into one command.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, model, **kwargs) :\n",
    "        self.model = model\n",
    "        self.lexicon_file = kwargs.get (\"lexicon\", None)\n",
    "        self.nbest = kwargs.get (\"nbest\", 1)\n",
    "        self.thresh = kwargs.get (\"thresh\", 99)\n",
    "        self.beam = kwargs.get (\"beam\", 10000)\n",
    "        self.greedy = kwargs.get (\"greedy\", False)\n",
    "        self.accumulate = kwargs.get (\"accumulate\", False)\n",
    "        self.pmass = kwargs.get (\"pmass\", 0.0)\n",
    "        self.probs = kwargs.get (\"probs\", False)\n",
    "        self.verbose = kwargs.get (\"verbose\", False)\n",
    "        self.logger = self.setupLogger ()\n",
    "\n",
    "    def setupLogger (self) :\n",
    "        \"\"\"Setup the logger and logging level.\n",
    "\n",
    "        Setup the logger and logging level.  We only support\n",
    "        verbose and non-verbose mode.\n",
    "\n",
    "        Args:\n",
    "            verbose (bool): Verbose mode, or not.\n",
    "\n",
    "        Returns:\n",
    "            Logger: A configured logger instance.\n",
    "        \"\"\"\n",
    "\n",
    "        level = logging.DEBUG if self.verbose else logging.INFO\n",
    "        logging.basicConfig (\n",
    "            level=level,\n",
    "            format=\"\\033[94m%(levelname)s:%(name)s:\"\\\n",
    "            \"%(asctime)s\\033[0m:  %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "\n",
    "        return logging.getLogger (\"phonetisaurus-apply\")\n",
    "\n",
    "    def _loadLexicon (self) :\n",
    "        \"\"\"Load the lexicon from a file.\n",
    "\n",
    "        Load the reference lexicon from a file, and store it\n",
    "        in a defaultdict (list).\n",
    "        \"\"\"\n",
    "\n",
    "        _lexicon = defaultdict (list)\n",
    "        if not self.lexicon_file :\n",
    "            return _lexicon\n",
    "\n",
    "        self.logger.debug (\"Loading lexicon from file...\")\n",
    "        with open (self.lexicon_file, \"r\") as ifp :\n",
    "            for line in ifp :\n",
    "                # py2py3 compatbility,\n",
    "                if sys.version_info[0] < 3:\n",
    "                    line = line.decode(\"utf8\").strip ()\n",
    "                else:\n",
    "                    line = line.strip ()\n",
    "                word, pron = re.split (r\"\\t\", line, 1)\n",
    "                _lexicon [word].append (pron)\n",
    "\n",
    "        return _lexicon\n",
    "\n",
    "    def checkPhonetisaurusConfig (self) :\n",
    "        \"\"\"Run some basic checks before training.\n",
    "\n",
    "        Run some basic checks regarding the $PATH, environment,\n",
    "        and provided data before starting training.\n",
    "\n",
    "        Raises:\n",
    "            EnvironmentError: raised if binaries are not found.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug (\"Checking command configuration...\")\n",
    "        for program in [\"phonetisaurus-g2pfst\"] :\n",
    "            if not self.which (program) :\n",
    "                raise EnvironmentError(\"Phonetisaurus command, '{0}', \"\\\n",
    "                    \"not found in path.\".format (program))\n",
    "\n",
    "        if self.lexicon_file and not os.path.exists (self.lexicon_file) :\n",
    "            self.logger.error (\"Could not find provided lexicon file.\")\n",
    "            sys.exit (1)\n",
    "\n",
    "        for key,val in sorted (vars (self).items ()) :\n",
    "            self.logger.debug (u\"{0}:  {1}\".format (key, val))\n",
    "\n",
    "        self.lexicon = self._loadLexicon ()\n",
    "\n",
    "        return\n",
    "\n",
    "    def which (self, program) :\n",
    "        \"\"\"Basic 'which' implementation for python.\n",
    "\n",
    "        Basic 'which' implementation for python from stackoverflow:\n",
    "          * https://stackoverflow.com/a/377028/6739158\n",
    "\n",
    "        Args:\n",
    "            program (str): The program name to search the $PATH for.\n",
    "\n",
    "        Returns:\n",
    "            path/None: The path to the executable, or None.\n",
    "        \"\"\"\n",
    "\n",
    "        def is_exe (fpath) :\n",
    "            return os.path.isfile (fpath) and os.access (fpath, os.X_OK)\n",
    "\n",
    "        fpath, fname = os.path.split (program)\n",
    "        if fpath:\n",
    "            if is_exe (program):\n",
    "                return program\n",
    "        else:\n",
    "            for path in os.environ[\"PATH\"].split (os.pathsep) :\n",
    "                path = path.strip ('\"')\n",
    "                exe_file = os.path.join (path, program)\n",
    "                if is_exe (exe_file):\n",
    "                    return exe_file\n",
    "\n",
    "        return None\n",
    "\n",
    "    def makeG2PCommand (self, word_list) :\n",
    "        \"\"\"Build the G2P command.\n",
    "\n",
    "        Build the G2P command from the provided arguments.\n",
    "\n",
    "        Returns:\n",
    "            list: The command in subprocess list format.\n",
    "        \"\"\"\n",
    "\n",
    "        command = [\n",
    "            u\"phonetisaurus-g2pfst\",\n",
    "            u\"--model={0}\".format (self.model),\n",
    "            u\"--nbest={0}\".format (self.nbest),\n",
    "            u\"--beam={0}\".format (self.beam),\n",
    "            u\"--thresh={0}\".format (self.thresh),\n",
    "            u\"--accumulate={0}\".format (str (self.accumulate).lower ()),\n",
    "            u\"--pmass={0}\".format (self.pmass),\n",
    "            u\"--nlog_probs={0}\".format (str(not self.probs).lower ()),\n",
    "            u\"--wordlist={0}\".format (word_list)\n",
    "        ]\n",
    "\n",
    "        self.logger.debug (u\" \".join (command))\n",
    "\n",
    "        return command\n",
    "\n",
    "    def runG2PCommand (self, word_list_file) :\n",
    "        \"\"\"Generate and run the actual G2P command.\n",
    "\n",
    "        Generate and run the actual G2P command.  Each synthesized\n",
    "        entry will be yielded back on-the-fly via the subprocess\n",
    "        stdout readline method.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        g2p_command = self.makeG2PCommand (word_list_file)\n",
    "\n",
    "        self.logger.debug (\"Applying G2P model...\")\n",
    "\n",
    "        with open (os.devnull, \"w\") as devnull :\n",
    "            proc = subprocess.Popen (\n",
    "                g2p_command,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=devnull if not self.verbose else None\n",
    "            )\n",
    "\n",
    "            for line in proc.stdout :\n",
    "                parts = re.split (r\"\\t\", line.decode (\"utf8\").strip ())\n",
    "                if not len (parts) == 3 :\n",
    "                    self.logger.warning (\n",
    "                        u\"No pronunciation for word: '{0}'\".format (parts [0])\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                yield parts\n",
    "\n",
    "        return\n",
    "\n",
    "    def applyG2POnly (self, word_list_file) :\n",
    "        \"\"\"Apply the G2P model to a word list.\n",
    "\n",
    "        Apply the G2P model to a word list.  No filtering or application\n",
    "        of a reference lexicon is used here.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        for word, score, pron in self.runG2PCommand (word_list_file) :\n",
    "            line = u\"\"\n",
    "            if self.verbose :\n",
    "                line = u\"{0}\\t{1:.2f}\\t{2}\".format (\n",
    "                    word, float (score), pron\n",
    "                )\n",
    "            else :\n",
    "                line = u\"{0}\\t{1}\".format (word, pron)\n",
    "            # py2py3 compatbility,\n",
    "            if sys.version_info[0] < 3:\n",
    "                print (line.encode (\"utf8\"))\n",
    "            else :\n",
    "                print (line)\n",
    "\n",
    "        return\n",
    "\n",
    "    def applyG2PWithLexicon (self, word_list_file) :\n",
    "        \"\"\"Apply the G2P model to a word list, combined with lexicon.\n",
    "\n",
    "        Apply the G2P model to a word list, but combine this with\n",
    "        a reference lexicon.  Words for which a reference entry exists\n",
    "        will not be sent to the G2P, unless the additional '--greedy'\n",
    "        flag is set to True.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        target_lexicon = defaultdict (list)\n",
    "        tmpwordlist = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "\n",
    "        #First, find any words in the target list for which we already\n",
    "        # have a canonical pronunciation in the reference lexicon.\n",
    "        with open (word_list_file, \"r\") as ifp :\n",
    "            for word in ifp :\n",
    "                # py2py3 compatbility,\n",
    "                if sys.version_info[0] < 3:\n",
    "                    word = word.decode (\"utf8\").strip ()\n",
    "                else:\n",
    "                    word = word.strip () # already in 'utf8'.\n",
    "                if word in self.lexicon :\n",
    "                    target_lexicon [word] = [(0.0,pron)\n",
    "                                             for pron in self.lexicon [word]]\n",
    "                    #In greedy mode we still send words to the G2P, even\n",
    "                    # if we have canonical entries in the reference lexicon.\n",
    "                    if self.greedy :\n",
    "                        print (word.encode (\"utf8\"), file=tmpwordlist)\n",
    "                else :\n",
    "                    # py2py3 compatbility,\n",
    "                    if sys.version_info[0] < 3:\n",
    "                        print (word.encode (\"utf8\"), file=tmpwordlist)\n",
    "                    else:\n",
    "                        print (word, file=tmpwordlist)\n",
    "        tmpwordlist.close ()\n",
    "\n",
    "        #Second, iterate through the G2P output, and filter against\n",
    "        # any possible duplicates previously found in the reference lexicon.\n",
    "        for word, score, pron in self.runG2PCommand (tmpwordlist.name) :\n",
    "            prons = set ([p for s,p in target_lexicon [word]])\n",
    "            if pron in prons :\n",
    "                continue\n",
    "            target_lexicon [word].append ((score, pron))\n",
    "\n",
    "        #Finally, sort everything that is left and print it.\n",
    "        for word in sorted (target_lexicon.keys ()) :\n",
    "            for score, pron in target_lexicon [word] :\n",
    "                line = u\"\"\n",
    "                if self.verbose :\n",
    "                    line = u\"{0}\\t{1:.2f}\\t{2}\".format (\n",
    "                        word, float (score), pron\n",
    "                    )\n",
    "                else :\n",
    "                    line = u\"{0}\\t{1}\".format (word, pron)\n",
    "                # py2py3 compatbility,\n",
    "                if sys.version_info[0] < 3:\n",
    "                    print (line.encode (\"utf8\"))\n",
    "                else :\n",
    "                    print (line)\n",
    "\n",
    "        os.unlink (tmpwordlist.name)\n",
    "        return\n",
    "\n",
    "    def ApplyG2PModel (self, word_list_file) :\n",
    "        \"\"\"Apply the G2P model to a word list.\n",
    "\n",
    "        Apply the G2P model to a word list.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        self.checkPhonetisaurusConfig ()\n",
    "\n",
    "        if not os.path.exists (word_list_file) \\\n",
    "           or not os.path.isfile (word_list_file) :\n",
    "            raise IOError(\"Word list file not found.\")\n",
    "\n",
    "        if len (self.lexicon) == 0 :\n",
    "            self.applyG2POnly (word_list_file)\n",
    "        else :\n",
    "            self.applyG2PWithLexicon (word_list_file)\n",
    "\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    import sys, argparse\n",
    "\n",
    "    example = \"{0} --model train/model.fst --word test\".format (sys.argv [0])\n",
    "\n",
    "    parser  = argparse.ArgumentParser (description=example)\n",
    "    parser.add_argument (\"--model\", \"-m\", help=\"Phonetisaurus G2P fst model.\",\n",
    "                         required=True)\n",
    "    parser.add_argument (\"--lexicon\", \"-l\", help=\"Optional reference lexicon.\",\n",
    "                         required=False)\n",
    "    parser.add_argument (\"--nbest\", \"-n\", help=\"Maximum number of hypotheses \"\n",
    "                         \"to produce.  Overridden if --pmass is set.\",\n",
    "                         default=1, type=int)\n",
    "    parser.add_argument (\"--beam\", \"-b\", help=\"Search 'beam'.\",\n",
    "                         default=10000, type=int)\n",
    "    parser.add_argument (\"--thresh\", \"-t\", help=\"Pruning threshold for n-best.\",\n",
    "                         default=99.0, type=float)\n",
    "    parser.add_argument (\"--greedy\", \"-g\", help=\"Use the G2P even if a \"\n",
    "                         \"reference lexicon has been provided.\", default=False,\n",
    "                         action=\"store_true\")\n",
    "    parser.add_argument (\"--accumulate\", \"-a\", help=\"Accumulate probabilities \"\n",
    "                         \"across unique pronunciations.\", default=False,\n",
    "                         action=\"store_true\")\n",
    "    parser.add_argument (\"--pmass\", \"-p\", help=\"Select the maximum number of \"\n",
    "                         \"hypotheses summing to P total mass for a word.\",\n",
    "                         default=0.0, type=float)\n",
    "    parser.add_argument (\"--probs\", \"-pr\", help=\"Print exp(-val) \"\n",
    "                         \"instead of default -log values.\", default=False,\n",
    "                         action=\"store_true\")\n",
    "    parser.add_argument (\"--word_list\", \"-wl\", help=\"Input word or word list to apply \"\n",
    "                        \"G2P model to.\", type=str)\n",
    "\n",
    "    parser.add_argument (\"--verbose\", \"-v\", help=\"Verbose mode.\",\n",
    "                         default=False, action=\"store_true\")\n",
    "    args = parser.parse_args ()\n",
    "\n",
    "    tester = G2PModelTester (\n",
    "        args.model,\n",
    "        **{key:val for key,val in args.__dict__.items ()\n",
    "           if not key in [\"model\",\"word_list\"]}\n",
    "    )\n",
    "\n",
    "    tester.ApplyG2PModel (args.word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-increase",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:30:09.744666Z",
     "iopub.status.busy": "2021-06-04T09:30:09.743828Z",
     "iopub.status.idle": "2021-06-04T09:30:09.747357Z",
     "shell.execute_reply": "2021-06-04T09:30:09.747933Z"
    },
    "papermill": {
     "duration": 2.98504,
     "end_time": "2021-06-04T09:30:09.748118",
     "exception": false,
     "start_time": "2021-06-04T09:30:06.763078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /opt/kaldi/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-tribute",
   "metadata": {
    "papermill": {
     "duration": 2.963149,
     "end_time": "2021-06-04T09:30:15.876435",
     "exception": false,
     "start_time": "2021-06-04T09:30:12.913286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Be sure to uncomment and run the next cell, to make sure nothing's changed; it's unlikely that anything will have changed, though. The following cell does configure, make depend, and make, which is all that the current INSTALL says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-bachelor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:30:21.746393Z",
     "iopub.status.busy": "2021-06-04T09:30:21.745583Z",
     "iopub.status.idle": "2021-06-04T09:30:21.748267Z",
     "shell.execute_reply": "2021-06-04T09:30:21.747822Z"
    },
    "papermill": {
     "duration": 2.937307,
     "end_time": "2021-06-04T09:30:21.748379",
     "exception": false,
     "start_time": "2021-06-04T09:30:18.811072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-specialist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:30:27.838242Z",
     "iopub.status.busy": "2021-06-04T09:30:27.837536Z",
     "iopub.status.idle": "2021-06-04T09:38:02.180263Z",
     "shell.execute_reply": "2021-06-04T09:38:02.179380Z"
    },
    "papermill": {
     "duration": 457.296845,
     "end_time": "2021-06-04T09:38:02.180407",
     "exception": false,
     "start_time": "2021-06-04T09:30:24.883562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./configure --shared --use-cuda --mathlib=OPENBLAS\n",
    "!make depend -j 8\n",
    "!make -j 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-tokyo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:38:08.559918Z",
     "iopub.status.busy": "2021-06-04T09:38:08.559002Z",
     "iopub.status.idle": "2021-06-04T09:38:08.562909Z",
     "shell.execute_reply": "2021-06-04T09:38:08.563490Z"
    },
    "papermill": {
     "duration": 3.091006,
     "end_time": "2021-06-04T09:38:08.563672",
     "exception": false,
     "start_time": "2021-06-04T09:38:05.472666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-standard",
   "metadata": {
    "papermill": {
     "duration": 3.061115,
     "end_time": "2021-06-04T09:38:14.722018",
     "exception": false,
     "start_time": "2021-06-04T09:38:11.660903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Second last step, clean up the object files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-tobago",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:38:21.157897Z",
     "iopub.status.busy": "2021-06-04T09:38:21.153467Z",
     "iopub.status.idle": "2021-06-04T09:38:30.121864Z",
     "shell.execute_reply": "2021-06-04T09:38:30.121341Z"
    },
    "papermill": {
     "duration": 12.094722,
     "end_time": "2021-06-04T09:38:30.122003",
     "exception": false,
     "start_time": "2021-06-04T09:38:18.027281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!find /opt/kaldi  -type f \\( -name \"*.o\" -o -name \"*.la\" -o -name \"*.a\" \\) -exec rm {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-portable",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-04T09:38:36.276825Z",
     "iopub.status.busy": "2021-06-04T09:38:36.272431Z",
     "iopub.status.idle": "2021-06-04T09:38:41.548432Z",
     "shell.execute_reply": "2021-06-04T09:38:41.548007Z"
    },
    "papermill": {
     "duration": 8.347305,
     "end_time": "2021-06-04T09:38:41.548580",
     "exception": false,
     "start_time": "2021-06-04T09:38:33.201275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar cvf /kaggle/working/kaldi.tar kaldi/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4520.054953,
   "end_time": "2021-06-04T09:38:48.639311",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-04T08:23:28.584358",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
