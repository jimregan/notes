{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv(filename):\n",
    "    data = []\n",
    "    with open(filename) as inf:\n",
    "        for line in inf.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            data.append({\n",
    "                \"start\": float(parts[0]),\n",
    "                \"end\": float(parts[1]),\n",
    "                \"word\": parts[2]\n",
    "            })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_tsv_data(data, start, end):\n",
    "    ret = []\n",
    "    for datum in data:\n",
    "        if type(datum[\"start\"]) is str:\n",
    "            datum[\"start\"] = float(datum[\"start\"])\n",
    "        if type(datum[\"end\"]) is str:\n",
    "            datum[\"end\"] = float(datum[\"end\"])\n",
    "        if datum[\"start\"] >= start and datum[\"end\"] <= end:\n",
    "            ret.append(datum)\n",
    "        elif datum[\"end\"] > end:\n",
    "            return ret\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def norm_spaces(text):\n",
    "    return re.sub(\"  +\", \" \", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = norm_spaces(text)\n",
    "    return \" \".join([x.lower().strip(\".,;?!\") for x in text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TSVS = Path(\"/Users/joregan/Playing/hsi/word_annotations/\")\n",
    "JSON = Path(\"/Users/joregan/Playing/merged_annotations/\")\n",
    "OUTP = Path(\"/Users/joregan/Playing/timed_annotations/\")\n",
    "if not OUTP.is_dir():\n",
    "    OUTP.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(needle, haystack, checkpos=True):\n",
    "    ret = []\n",
    "    nwords = [x.lower().strip(\",?.;:()\") for x in needle.split(\" \")]\n",
    "    hwords = [x.lower().strip(\",?.;:\") for x in haystack.split(\" \")]\n",
    "    nwordspos = nwords[:-1] + [f\"{nwords[-1]}'s\"]\n",
    "    nlen = len(nwords)\n",
    "\n",
    "    for i in range(len(hwords)):\n",
    "        if hwords[i:i+nlen] == nwords:\n",
    "            ret.append((i, i+nlen))\n",
    "        elif checkpos and hwords[i:i+nlen] == nwordspos:\n",
    "            ret.append((i, i+nlen))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text2(text):\n",
    "    nums = {\n",
    "        \"60\": \"sixty\",\n",
    "        \"1\": \"one\",\n",
    "        \"20th\": \"twentieth\",\n",
    "        \"9th\": \"ninth\",\n",
    "        \"5\": \"five\"\n",
    "    }\n",
    "    text = norm_spaces(text)\n",
    "    words = [x.lower().strip(\".,;?!\") for x in text.split(\" \")]\n",
    "    ret = []\n",
    "    for word in words:\n",
    "        if word.startswith(\"[\") and word.endswith(\"]\"):\n",
    "            continue\n",
    "        elif word.startswith(\"{\") and word.endswith(\"}\"):\n",
    "            continue\n",
    "        word = nums.get(word, word)\n",
    "        word = word.replace(\".\", \" \").replace(\",\", \" \")\n",
    "        ret.append(word)\n",
    "    return \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsv_for_segment(segment, tsv_data, filename=None, segment_id=None):\n",
    "    assert \"general\" in segment, \"Missing key 'general'\"\n",
    "    assert \"start\" in segment[\"general\"], \"Missing key 'start'\"\n",
    "    assert \"end\" in segment[\"general\"], \"Missing key 'end'\"\n",
    "\n",
    "    start = segment[\"general\"][\"start\"]\n",
    "    end = segment[\"general\"][\"end\"]\n",
    "\n",
    "    tsv = slice_tsv_data(tsv_data, start, end)\n",
    "    tsv_words = \" \".join([x[\"word\"] for x in tsv])\n",
    "\n",
    "    if segment[\"snippet\"] != tsv_words:\n",
    "        cleaned_snippet = clean_text2(segment[\"snippet\"])\n",
    "        cleaned_text = clean_text2(tsv_words)\n",
    "\n",
    "        if cleaned_snippet not in cleaned_text:\n",
    "            if filename is not None and segment_id is not None:\n",
    "                print(f\"{filename}\\t{segment_id}\\t{segment['snippet']}\\t{tsv_words}\")\n",
    "            else:\n",
    "                print(\"🙀 mismatch:\", \"🖇️\", segment[\"snippet\"], \"🎧\", tsv_words, cleaned_text.find(cleaned_snippet))\n",
    "            return []\n",
    "        else:\n",
    "            idxes = get_indices(cleaned_snippet, cleaned_text)\n",
    "            assert len(idxes) == 1\n",
    "            tsv = tsv[idxes[0][0]:idxes[0][1]]\n",
    "            tsv_words = \" \".join([x[\"word\"] for x in tsv])\n",
    "            cleaned_text = clean_text(tsv_words)\n",
    "            assert cleaned_snippet == cleaned_text, f\"🖇️ {cleaned_snippet} 🎧 {cleaned_text}\"\n",
    "    return tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_skippable(segment, strict=True):\n",
    "    skippables = [\"conversation_generic\"]\n",
    "    if strict:\n",
    "        skippables += [\"reference_imaginary\"]\n",
    "    if not \"topic_name\" in segment[\"high_level\"]:\n",
    "        if \"current_topic\" in segment[\"high_level\"]:\n",
    "            segment[\"high_level\"][\"topic_name\"] = segment[\"high_level\"][\"current_topic\"]\n",
    "            del(segment[\"high_level\"][\"current_topic\"])\n",
    "    if segment[\"high_level\"][\"topic_name\"] == \"reference_unreal\":\n",
    "        segment[\"high_level\"][\"topic_name\"] = \"reference_imaginary\"\n",
    "    if segment[\"high_level\"][\"topic_name\"] in skippables:\n",
    "        return True\n",
    "    elif segment[\"low_level\"][\"resolved_references\"] == {}:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that weird waste bin [(1, 5)]\n",
    "# that [(1, 2), (8, 9)]\n",
    "def skip_overlapped_index(a, b):\n",
    "    if a[0] >= b[0] and a[1] <= b[1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "assert skip_overlapped_index((1, 2), (1, 5)) == True\n",
    "assert skip_overlapped_index((1, 5), (1, 2)) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_manual_index(indices, manual):\n",
    "    ret = []\n",
    "    for index in indices:\n",
    "        if index[0] in manual:\n",
    "            ret.append(index)\n",
    "    return ret\n",
    "\n",
    "assert prune_manual_index([(1, 3), (5, 7)], [1]) == [(1, 3)]\n",
    "assert prune_manual_index([(1, 3), (5, 7)], [1, 5]) == [(1, 3), (5, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will lose all faith in humanity if there isn't a less idiotic way to do this\n",
    "def prune_dict_for_overlap(segments):\n",
    "    if len(segments.keys()) == 1:\n",
    "        return segments\n",
    "    for segment in segments:\n",
    "        pruned = set()\n",
    "        for seg2 in segments:\n",
    "            if segment != seg2:\n",
    "                for a in segments[segment]:\n",
    "                    for b in segments[seg2]:\n",
    "                        if skip_overlapped_index(a, b):\n",
    "                            if a in pruned:\n",
    "                                pruned.remove(a)\n",
    "                        else:\n",
    "                            pruned.add(a)\n",
    "        segments[segment] = list(pruned)\n",
    "    return segments\n",
    "\n",
    "test = {\n",
    "    \"1\": [(1, 3), (5, 7)],\n",
    "    \"2\": [(9, 11)],\n",
    "    \"3\": [(1, 4)]\n",
    "}\n",
    "exp = {\n",
    "    \"1\": [(5, 7)],\n",
    "    \"2\": [(9, 11)],\n",
    "    \"3\": [(1, 4)]\n",
    "}\n",
    "assert prune_dict_for_overlap(test) == exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(segment, tsv_data, filename=None, segment_id=None):\n",
    "    if is_skippable(segment):\n",
    "        return\n",
    "    tsv = get_tsv_for_segment(segment, tsv_data, filename, segment_id)\n",
    "    references = segment[\"low_level\"][\"resolved_references\"]\n",
    "    manual_idx = segment[\"low_level\"].get(\"resolved_references_indices\", {})\n",
    "\n",
    "    # these are ordered. Kinda.\n",
    "    indices = {}\n",
    "    for ref in references:\n",
    "        indices[ref] = get_indices(ref, segment[\"snippet\"])\n",
    "        if ref in manual_idx:\n",
    "            indices[ref] = prune_manual_index(indices[ref], manual_idx[ref])\n",
    "    indices = prune_dict_for_overlap(indices)\n",
    "    reftimes = []\n",
    "    for ref in references:\n",
    "        for index in indices[ref]:\n",
    "            seq = tsv[index[0]:index[1]]\n",
    "            if seq == []:\n",
    "                continue\n",
    "            start = seq[0][\"start\"]\n",
    "            end = seq[-1][\"end\"]\n",
    "            reftimes.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": ref,\n",
    "                \"reference\": references[ref]\n",
    "            })\n",
    "    segment[\"low_level\"][\"reference_times\"] = reftimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi_6_0718_227_002\t63\tLike if someone wants to walk around your, your, your sofa,it's, it's going to be in their way, right?\tLike if someone wants to walk around your, your, your sofa, it's, it's gonna be in their way, right?\n",
      "hsi_5_0718_209_003\t7\tThe desk, you can't have that here because it's ruining the whole atmosphere.\tThe desk, you you can't have that here because it's eh ruining the whole atmosphere.\n",
      "hsi_6_0718_222_001\t47\tuh that that\t\n",
      "hsi_6_0718_222_001\t87\tit's\t\n",
      "hsi_5_0718_210_001\t17\teh, So it's brought here by a friend as just a little, little cutling.\teh, So it's brought here by a friend as just a little, little cutting..\n",
      "hsi_5_0718_210_001\t18\tAnd now it looks like that.\tAnd now it's looks like that.\n",
      "hsi_5_0718_210_001\t49\tBecause, you know, every time I was with my... We didn't watch much TV in those days because I'm 60 years old.\tBecause, you know, every time I was with my... We didn't watch much eh TV in those days because I'm sixty years old.\n",
      "hsi_5_0718_210_001\t114\tThey're (fr-) from Germany, actually.\tThey're fr- from Germany, actually.\n",
      "hsi_7_0719_227_003\t13\tBut then if you want to to remove some of the pieces, I think ah the the two ah pieces, the one the red one and the brown one there, eh those are the one down there.\tBut then if you want to to remove some of the pieces, I think ah the the two ah pieces, the one the red one and the brown one there, those are eh the one down there.\n",
      "hsi_4_0717_211_003\t36\tThe sofa table and the rug.\tThe sofa table and the the rug,\n",
      "hsi_4_0717_211_003\t41\tCuz then you will get light shining through and also the light will be nice looking in the vases.\tCuz then you will get light shining through and also the light will be nice looking in the voses.\n",
      "hsi_4_0717_211_003\t42\tSo the colors would pop out.\tSo you the colors would pop out.\n",
      "hsi_4_0717_211_003\t64\tYou're welcome. Good luck with your apartment and get rid of the teddy bear.\tYou're welcome. Good luck with your apartment and get rid of the T. Teddy bear. Get rid of\n",
      "hsi_3_0715_210_010\t15\tAnd eh, you [loud click noise] know that lamp in the corner?\tAnd eh, you know that lamp in the corner?\n",
      "hsi_3_0715_210_010\t87\tIt was someone before that ehh... that it, it belonged to this apartment before.\tIt was someone before that eh... that it, it belonged to this apartment before.\n",
      "hsi_3_0715_210_010\t89\tAnd then they had a [laugh] dog - that a they dog.\tAnd then they had a dog that a they dog.\n",
      "hsi_6_0718_211_001\t6\tdon't mind these boxes here.\tYeah, because I'm still, I'm still moving in.\n",
      "hsi_7_0719_210_001\t10\tYeah, I mean, they all have my, you know, a little bit of a piece of my heart.\tYeah, I mean, they all have my, you know, little bit of a piece of my heart.\n",
      "hsi_3_0715_227_002\t12\tCrumbles\t\n",
      "hsi_3_0715_209_008\t8\tI know it's hard to clean up, eh, because it's with the [loud click noise] carbon from from winter and everything.\tI know it's hard to clean up, eh, because it's with the carbon from from winter and everything.\n",
      "hsi_3_0715_209_008\t31\tThis [loud locking noise] this pillow.\tOh, this this pillow.\n",
      "hsi_3_0715_209_008\t42\tDid you move the rug like in front of the...\tI see I see did you it's it seems like did you did you move the carpet like in front of the\n",
      "hsi_3_0715_209_008\t45\tIt's lighter here in the sides and darker over there so this is eeh.\tIt's lighter here in the the sides and darker over there so this is eeh\n",
      "hsi_3_0715_210_011\t22\tand so I think this this place so yeah if we have then we need to put the the couch a little bit here further was but that's no problem because there's space here and, ehh, yeah okay you can put it on the wall but then maybe you should you should change this this furniture\tand so I think this this place so yeah if we have then we need to put the the couch a little bit here further was but that's no problem because there's space here and, eh, yeah okay you can put it on the wall but then maybe you should you should change this this furniture\n",
      "hsi_3_0715_210_011\t23\tso you have a, a, a longer one because if you have a big TV and have just one it will be strange\tso you have a, a, a longer one because if you have a big TV and have just once it will be strange\n",
      "hsi_3_0715_210_011\t48\tThat, that ehh, that exists.\tThat, that eh, that exists.\n",
      "hsi_3_0715_210_011\t69\tIt will be ve, very beautiful because you have this lamp there.\tIt will be ve-, very beautiful because you have this lamp there.\n",
      "hsi_4_0717_209_001\t46\tDo you have another vase?\tDo you have another base?\n",
      "hsi_4_0717_209_001\t63\tAnd I also have a fireplace, but there's no fire in it.\tAnd I also have fireplace, but there's no fire in it.\n",
      "hsi_7_0719_209_003\t31\tYeah, IYeah, I I wouldn't I wouldn't put like the laptop close by to the s- to the statues because, in that case you would be uh moving the laptop or just working there and then... I mean you're really close to the statues and then maybe some ah accident happens and then you break some of uh the statues. So I I wouldn't do that.\tYeah, I I wouldn't I wouldn't put like the laptop close by to the s- to the statues because, in that case you would be uh moving the laptop or just working there and then... I mean you're really close to the statues and then maybe some ah accident happens and then you break some of uh the statues. So I I wouldn't do that.\n",
      "hsi_4_0717_211_002\t6\tySo uh where... Did you clean the rug? Eh? Yeah, of course. Okay. Good.\tSo uh where... Did you clean the rug? Eh? Yeah, of course. Okay. Good.\n",
      "hsi_4_0717_211_002\t19\tDon't put keys on the table. It's very bad luck.\tIt's very much bad luck. Yes.\n",
      "hsi_4_0717_227_004\t4\tthe couch should be turned this way\t\n",
      "hsi_4_0717_227_004\t8\tthese are doors so you can slide them out\t\n",
      "hsi_4_0717_227_004\t10\tno it's not\t\n",
      "hsi_4_0717_227_004\t11\tit's slide sliding doors\t\n",
      "hsi_4_0717_227_004\t12\tand have you seen my super big painting\t\n",
      "hsi_4_0717_227_004\t14\tno, it's it's eh they're from the same collection\t\n",
      "hsi_4_0717_227_004\t15\tmaybe I should put them next to each other instead\t\n",
      "hsi_4_0717_227_004\t16\tso you can see, maybe they should be on that wall and just be like next to each other\t\n",
      "hsi_4_0717_227_004\t17\tmaybe on one on eah side of the chandelier\t\n",
      "hsi_4_0717_227_004\t20\toh I don't like the blue pillows, put them away\t\n",
      "hsi_4_0717_227_004\t21\tjust no pillows, there's it's like nothing else in this room has that color\t\n",
      "hsi_4_0717_227_004\t27\twell both the the man the wooden man and the lion there are from Africa like ancient African art\t\n",
      "hsi_4_0717_227_004\t28\tI got them there when I was there and it was an\t\n",
      "hsi_4_0717_227_004\t29\tan auction, it was like first auction, it was pretty upsetting and I won them!\t\n",
      "hsi_4_0717_227_004\t32\tshe works a lot with woman's bodies with different shapes and\t\n",
      "hsi_4_0717_227_004\t33\tdifferent poses and such and that feels good to buy so\t\n",
      "hsi_4_0717_227_004\t39\tthat's why [spn]\t\n",
      "hsi_4_0717_227_004\t51\tfeel bright or whatever\t\n",
      "hsi_4_0717_210_001\t47\tIt's a that wouldn't be, doesn't belong here really, but.\tIt's something that wouldn't be, doesn't belong here really, but.\n",
      "hsi_4_0717_210_001\t83\tAnd it's so nice with the foot the coosh foot cushion.\tAnd it's so nice with the fffoot the coosh foot cushion.\n",
      "hsi_4_0717_211_001\t79\tAnd the, what's that red thing? I mean, it looks like a credit card but I don't think it is.\tAnd the, what's that red thing? I mean the looks like a credit card but I don't think it is.\n",
      "hsi_3_0715_227_001\t21\tBut it's a little bit low though, so (breath out) to get up sometimes when you're sleepy is a little bit hard.\tBut it's a little bit low though, so to get up sometimes when you're sleepy is a little bit hard.\n",
      "hsi_6_0718_211_002\t14\tBecause those are, those look like bars, right?\tIt's conveying the exact opposite of what you want.\n",
      "hsi_6_0718_211_002\t25\tIt's false kids.\tit's, It's your kids.\n",
      "hsi_6_0718_211_002\t136\tEither you have a new television, like those flat ones you put on the wall\tEither you have a new television, like those those flat ones you put on the wall, or you just get rid of it.\n",
      "hsi_6_0718_210_002\t18\tThose guys, they also have a clay pot, but then that one has a stone pot.\tThose guys, they also have a clay pot, but then that one h has a stone pot.\n",
      "hsi_6_0718_210_002\t33\tYeah, is that why you also replaced, uh, my my pillow right there?\tYeah, is that why you also replaced, uh, my uh my pillow right there?\n",
      "hsi_6_0718_210_002\t102\tproduct I mean, look at this.\tproduct I mean, look at that this.\n",
      "hsi_6_0718_210_002\t121\tit's obviously not what was there before, right?\tit's obviously\n",
      "hsi_6_0718_209_001\t95\tAnd around the turn of the 9th, uh, 20th century, yes, they actually captured a woman who had a similar figure to that and brought her around Europe.\tAnd around the uh the turn of the the ninth, uh, twentieth century, yes, um they actually they captured a woman uh who had a similar figure to that uh and brought her around Europe.\n",
      "hsi_3_0715_209_006\t18\tAnd ehh, the other painting is from South America, actually.\tAnd eh, the other painting is from South America, actually.\n",
      "hsi_3_0715_209_006\t30\tIt looks like a, a, a person looking down, like a face looking down.\tIt's looks like a, a, a person looking down, like a face looking down.\n",
      "hsi_3_0715_209_006\t43\tThose two women, ehh, they are from two different ehh, parts of the world.\tThose two women, eh, they are from two different eh, parts of the world.\n",
      "hsi_3_0715_209_006\t44\tThe black one, she's from, eh, the eh, Tromso in the south of Norway, really, really high up in the north.\tThe black one, she's from, uh eh, the eh, Tr- Tromsø in the south of of Norway, really, really high up in the north.\n",
      "hsi_5_0718_211_002\t34\tAnd you see, there are there.\tAnd you see, there are three.\n",
      "hsi_5_0718_210_002\t32\tNo, because I thought if you had told me we could have removed some of them.\tNo, because I thought\n",
      "hsi_5_0718_209_001\t1\tWhat do you call this one in Swedish eh or in English, fauteuil\tWhat you call this one in Swedish eh or in English, fåtölj?\n",
      "hsi_4_0717_227_002\t5\tWell, I'm thinking maybe (we'll) we should just turn this, the big couch, turn it this way, have these as they are.\tWell, I'm thinking maybe we'll we should just turn this, the big couch, turn it this way, have these as they are.\n",
      "hsi_7_0719_211_004\t48\tAnd... there is one vase missing there.\tAnd... there is one base missing there.\n",
      "hsi_5_0718_211_003\t12\tBut I I think you should have something that was about this high, 20 centimeters.\tBut I I think you should have something that was about this high, twenty centimeters.\n",
      "hsi_6_0718_227_001\t28\toYeah, cause like what I do is like I have it facing me and I, I take one of the chairs and I sit at the head of the table and I watch mukbang videos.\tYeah, cause like what I do is like I have it facing me and I, I take one of the chairs and I sit at the head of the table and I watch mukbang videos.\n",
      "hsi_6_0718_227_001\t88\tit's\t\n"
     ]
    }
   ],
   "source": [
    "for jsonfile in JSON.glob(\"*.json\"):\n",
    "    base = jsonfile.stem\n",
    "    with open(jsonfile) as jsf:\n",
    "        data = json.load(jsf)\n",
    "    rawtsv = load_tsv(str(TSVS / f\"{base}_main.tsv\"))\n",
    "    outfile = OUTP / f\"{base}.json\"\n",
    "    for seg in data:\n",
    "        process_segment(data[seg], rawtsv, base, seg)\n",
    "    with open(str(outfile), 'w') as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joregan/Playing/merged_annotations/hsi_6_0718_227_001.json\n"
     ]
    }
   ],
   "source": [
    "print(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv      \n",
    "    \n",
    "def update_json_snippets_from_csv(json_path, csv_path, output_path):\n",
    "    # Load JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Load CSV and store snippets by (start, end)\n",
    "    csv_snippets = {}\n",
    "    with open(csv_path, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row) != 3:\n",
    "                continue\n",
    "            try:\n",
    "                start = round(float(row[0]), 3)\n",
    "                end = round(float(row[1]), 3)\n",
    "                snippet = row[2]\n",
    "                csv_snippets[(start, end)] = snippet\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Replace JSON snippets based on matching start/end\n",
    "    for entry in json_data.values():\n",
    "        start = round(entry['general']['start'], 3)\n",
    "        end = round(entry['general']['end'], 3)\n",
    "        if (start, end) in csv_snippets:\n",
    "            entry['snippet'] = csv_snippets[(start, end)]\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    print(f\"Updated JSON saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
