{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv(filename):\n",
    "    data = []\n",
    "    with open(filename) as inf:\n",
    "        for line in inf.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            data.append({\n",
    "                \"start\": float(parts[0]),\n",
    "                \"end\": float(parts[1]),\n",
    "                \"word\": parts[2]\n",
    "            })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_tsv_data(data, start, end):\n",
    "    ret = []\n",
    "    for datum in data:\n",
    "        if type(datum[\"start\"]) is str:\n",
    "            datum[\"start\"] = float(datum[\"start\"])\n",
    "        if type(datum[\"end\"]) is str:\n",
    "            datum[\"end\"] = float(datum[\"end\"])\n",
    "        if datum[\"start\"] >= start and datum[\"end\"] <= end:\n",
    "            ret.append(datum)\n",
    "        elif datum[\"end\"] > end:\n",
    "            return ret\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def norm_spaces(text):\n",
    "    return re.sub(\"  +\", \" \", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = norm_spaces(text)\n",
    "    return \" \".join([x.lower().strip(\".,;?!\") for x in text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TSVS = Path(\"/Users/joregan/Playing/hsi/word_annotations/\")\n",
    "JSON = Path(\"/Users/joregan/Playing/merged_annotations/\")\n",
    "OUTP = Path(\"/Users/joregan/Playing/timed_annotations/\")\n",
    "if not OUTP.is_dir():\n",
    "    OUTP.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(needle, haystack, checkpos=True):\n",
    "    ret = []\n",
    "    nwords = [x.lower().strip(\",?.;:()\") for x in needle.split(\" \")]\n",
    "    hwords = [x.lower().strip(\",?.;:\") for x in haystack.split(\" \")]\n",
    "    nwordspos = nwords[:-1] + [f\"{nwords[-1]}'s\"]\n",
    "    nlen = len(nwords)\n",
    "\n",
    "    for i in range(len(hwords)):\n",
    "        if hwords[i:i+nlen] == nwords:\n",
    "            ret.append((i, i+nlen))\n",
    "        elif checkpos and hwords[i:i+nlen] == nwordspos:\n",
    "            ret.append((i, i+nlen))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text2(text):\n",
    "    nums = {\n",
    "        \"60\": \"sixty\",\n",
    "        \"1\": \"one\",\n",
    "        \"20th\": \"twentieth\",\n",
    "        \"9th\": \"ninth\",\n",
    "        \"5\": \"five\"\n",
    "    }\n",
    "    text = norm_spaces(text)\n",
    "    words = [x.lower().strip(\".,;?!\") for x in text.split(\" \")]\n",
    "    ret = []\n",
    "    for word in words:\n",
    "        if word.startswith(\"[\") and word.endswith(\"]\"):\n",
    "            continue\n",
    "        elif word.startswith(\"{\") and word.endswith(\"}\"):\n",
    "            continue\n",
    "        word = nums.get(word, word)\n",
    "        word = word.replace(\".\", \" \").replace(\",\", \" \")\n",
    "        ret.append(word)\n",
    "    return \" \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL = \"\"\"\n",
    "hsi_5_0718_210_001\t17\n",
    "hsi_5_0718_210_001\t18\n",
    "hsi_5_0718_210_001\t114\n",
    "hsi_4_0717_211_003\t36\n",
    "hsi_4_0717_211_003\t42\n",
    "hsi_3_0715_210_010\t89\n",
    "hsi_3_0715_209_008\t31\n",
    "hsi_3_0715_210_011\t48\n",
    "hsi_4_0717_211_002\t6\n",
    "hsi_5_0718_210_001\t49\n",
    "hsi_5_0718_209_003\t7\n",
    "hsi_6_0718_227_002\t63\n",
    "hsi_5_0718_209_001\t1\n",
    "hsi_6_0718_210_002\t102\n",
    "hsi_6_0718_210_002\t33\n",
    "hsi_6_0718_210_002\t18\n",
    "hsi_6_0718_209_001\t95\n",
    "hsi_3_0715_209_006\t18\n",
    "hsi_3_0715_227_001\t21\n",
    "hsi_4_0717_210_001\t47\n",
    "hsi_3_0715_210_010\t87\n",
    "hsi_3_0715_210_010\t15\n",
    "hsi_3_0715_209_006\t30\n",
    "hsi_3_0715_209_006\t43\n",
    "hsi_6_0718_211_002\t14\n",
    "\"\"\"\n",
    "\n",
    "manual_segments = {}\n",
    "for line in MANUAL.split(\"\\n\"):\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    parts = line.split(\"\\t\")\n",
    "    if not parts[0] in manual_segments:\n",
    "        manual_segments[parts[0]] = []\n",
    "    manual_segments[parts[0]].append(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsv_for_segment(segment, tsv_data, filename=None, segment_id=None):\n",
    "    assert \"general\" in segment, \"Missing key 'general'\"\n",
    "    assert \"start\" in segment[\"general\"], \"Missing key 'start'\"\n",
    "    assert \"end\" in segment[\"general\"], \"Missing key 'end'\"\n",
    "\n",
    "    start = segment[\"general\"][\"start\"]\n",
    "    end = segment[\"general\"][\"end\"]\n",
    "\n",
    "    tsv = slice_tsv_data(tsv_data, start, end)\n",
    "    tsv_words = \" \".join([x[\"word\"] for x in tsv])\n",
    "\n",
    "    if filename and filename in manual_segments and segment_id and segment_id in manual_segments[filename]:\n",
    "        return tsv\n",
    "\n",
    "    if segment[\"snippet\"] != tsv_words:\n",
    "        cleaned_snippet = clean_text2(segment[\"snippet\"])\n",
    "        cleaned_text = clean_text2(tsv_words)\n",
    "\n",
    "        if cleaned_snippet not in cleaned_text:\n",
    "            if filename is not None and segment_id is not None:\n",
    "                print(f\"{filename}\\t{segment_id}\\t{segment['snippet']}\\t{tsv_words}\")\n",
    "            else:\n",
    "                print(\"ğŸ™€ mismatch:\", \"ğŸ–‡ï¸\", segment[\"snippet\"], \"ğŸ§\", tsv_words, cleaned_text.find(cleaned_snippet))\n",
    "            return []\n",
    "        else:\n",
    "            idxes = get_indices(cleaned_snippet, cleaned_text)\n",
    "            assert len(idxes) == 1\n",
    "            tsv = tsv[idxes[0][0]:idxes[0][1]]\n",
    "            tsv_words = \" \".join([x[\"word\"] for x in tsv])\n",
    "            cleaned_text = clean_text(tsv_words)\n",
    "            assert cleaned_snippet == cleaned_text, f\"ğŸ–‡ï¸ {cleaned_snippet} ğŸ§ {cleaned_text}\"\n",
    "    return tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_skippable(segment, strict=True):\n",
    "    skippables = [\"conversation_generic\"]\n",
    "    if strict:\n",
    "        skippables += [\"reference_imaginary\"]\n",
    "    if not \"topic_name\" in segment[\"high_level\"]:\n",
    "        if \"current_topic\" in segment[\"high_level\"]:\n",
    "            segment[\"high_level\"][\"topic_name\"] = segment[\"high_level\"][\"current_topic\"]\n",
    "            del(segment[\"high_level\"][\"current_topic\"])\n",
    "    if segment[\"high_level\"][\"topic_name\"] == \"reference_unreal\":\n",
    "        segment[\"high_level\"][\"topic_name\"] = \"reference_imaginary\"\n",
    "    if segment[\"high_level\"][\"topic_name\"] in skippables:\n",
    "        return True\n",
    "    elif segment[\"low_level\"][\"resolved_references\"] == {}:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that weird waste bin [(1, 5)]\n",
    "# that [(1, 2), (8, 9)]\n",
    "def skip_overlapped_index(a, b):\n",
    "    if a[0] >= b[0] and a[1] <= b[1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "assert skip_overlapped_index((1, 2), (1, 5)) == True\n",
    "assert skip_overlapped_index((1, 5), (1, 2)) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_manual_index(indices, manual):\n",
    "    ret = []\n",
    "    for index in indices:\n",
    "        if index[0] in manual:\n",
    "            ret.append(index)\n",
    "    return ret\n",
    "\n",
    "assert prune_manual_index([(1, 3), (5, 7)], [1]) == [(1, 3)]\n",
    "assert prune_manual_index([(1, 3), (5, 7)], [1, 5]) == [(1, 3), (5, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will lose all faith in humanity if there isn't a less idiotic way to do this\n",
    "def prune_dict_for_overlap(segments):\n",
    "    if len(segments.keys()) == 1:\n",
    "        return segments\n",
    "    for segment in segments:\n",
    "        pruned = set()\n",
    "        for seg2 in segments:\n",
    "            if segment != seg2:\n",
    "                for a in segments[segment]:\n",
    "                    for b in segments[seg2]:\n",
    "                        if skip_overlapped_index(a, b):\n",
    "                            if a in pruned:\n",
    "                                pruned.remove(a)\n",
    "                        else:\n",
    "                            pruned.add(a)\n",
    "        segments[segment] = list(pruned)\n",
    "    return segments\n",
    "\n",
    "test = {\n",
    "    \"1\": [(1, 3), (5, 7)],\n",
    "    \"2\": [(9, 11)],\n",
    "    \"3\": [(1, 4)]\n",
    "}\n",
    "exp = {\n",
    "    \"1\": [(5, 7)],\n",
    "    \"2\": [(9, 11)],\n",
    "    \"3\": [(1, 4)]\n",
    "}\n",
    "assert prune_dict_for_overlap(test) == exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segment(segment, tsv_data, filename=None, segment_id=None):\n",
    "    if is_skippable(segment):\n",
    "        return\n",
    "    tsv = get_tsv_for_segment(segment, tsv_data, filename, segment_id)\n",
    "    references = segment[\"low_level\"][\"resolved_references\"]\n",
    "    manual_idx = segment[\"low_level\"].get(\"resolved_references_indices\", {})\n",
    "\n",
    "    # these are ordered. Kinda.\n",
    "    indices = {}\n",
    "    for ref in references:\n",
    "        indices[ref] = get_indices(ref, segment[\"snippet\"])\n",
    "        if ref in manual_idx:\n",
    "            indices[ref] = prune_manual_index(indices[ref], manual_idx[ref])\n",
    "    indices = prune_dict_for_overlap(indices)\n",
    "    reftimes = []\n",
    "    for ref in references:\n",
    "        for index in indices[ref]:\n",
    "            seq = tsv[index[0]:index[1]]\n",
    "            if seq == []:\n",
    "                continue\n",
    "            start = seq[0][\"start\"]\n",
    "            end = seq[-1][\"end\"]\n",
    "            reftimes.append({\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"text\": ref,\n",
    "                \"reference\": references[ref]\n",
    "            })\n",
    "    segment[\"low_level\"][\"reference_times\"] = reftimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi_4_0717_227_004\t4\tthe couch should be turned this way\t\n",
      "hsi_4_0717_227_004\t8\tthese are doors so you can slide them out\t\n",
      "hsi_4_0717_227_004\t10\tno it's not\t\n",
      "hsi_4_0717_227_004\t11\tit's slide sliding doors\t\n",
      "hsi_4_0717_227_004\t12\tand have you seen my super big painting\t\n",
      "hsi_4_0717_227_004\t14\tno, it's it's eh they're from the same collection\t\n",
      "hsi_4_0717_227_004\t15\tmaybe I should put them next to each other instead\t\n",
      "hsi_4_0717_227_004\t16\tso you can see, maybe they should be on that wall and just be like next to each other\t\n",
      "hsi_4_0717_227_004\t17\tmaybe on one on eah side of the chandelier\t\n",
      "hsi_4_0717_227_004\t20\toh I don't like the blue pillows, put them away\t\n",
      "hsi_4_0717_227_004\t21\tjust no pillows, there's it's like nothing else in this room has that color\t\n",
      "hsi_4_0717_227_004\t27\twell both the the man the wooden man and the lion there are from Africa like ancient African art\t\n",
      "hsi_4_0717_227_004\t28\tI got them there when I was there and it was an\t\n",
      "hsi_4_0717_227_004\t29\tan auction, it was like first auction, it was pretty upsetting and I won them!\t\n",
      "hsi_4_0717_227_004\t32\tshe works a lot with woman's bodies with different shapes and\t\n",
      "hsi_4_0717_227_004\t33\tdifferent poses and such and that feels good to buy so\t\n",
      "hsi_4_0717_227_004\t39\tthat's why [spn]\t\n",
      "hsi_4_0717_227_004\t51\tfeel bright or whatever\t\n"
     ]
    }
   ],
   "source": [
    "for jsonfile in JSON.glob(\"*.json\"):\n",
    "    base = jsonfile.stem\n",
    "    with open(jsonfile) as jsf:\n",
    "        data = json.load(jsf)\n",
    "    rawtsv = load_tsv(str(TSVS / f\"{base}_main.tsv\"))\n",
    "    outfile = OUTP / f\"{base}.json\"\n",
    "    for seg in data:\n",
    "        process_segment(data[seg], rawtsv, base, seg)\n",
    "    with open(str(outfile), 'w') as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joregan/Playing/merged_annotations/hsi_6_0718_227_001.json\n"
     ]
    }
   ],
   "source": [
    "print(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv      \n",
    "    \n",
    "def update_json_snippets_from_csv(json_path, csv_path, output_path):\n",
    "    # Load JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Load CSV and store snippets by (start, end)\n",
    "    csv_snippets = {}\n",
    "    with open(csv_path, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row) != 3:\n",
    "                continue\n",
    "            try:\n",
    "                start = round(float(row[0]), 3)\n",
    "                end = round(float(row[1]), 3)\n",
    "                snippet = row[2]\n",
    "                csv_snippets[(start, end)] = snippet\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Replace JSON snippets based on matching start/end\n",
    "    for entry in json_data.values():\n",
    "        start = round(entry['general']['start'], 3)\n",
    "        end = round(entry['general']['end'], 3)\n",
    "        if (start, end) in csv_snippets:\n",
    "            entry['snippet'] = csv_snippets[(start, end)]\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "\n",
    "    print(f\"Updated JSON saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
