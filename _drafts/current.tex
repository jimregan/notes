\documentclass{Interspeech}

% 2023-10-21 modified by Simon King (Simon.King@ed.ac.uk)  
% 2024-01 modified by TPC Chairs of Interspeech 2024  
% 2024-10 modified by Antoine Serrurier for Interspeech 2025
% 2024-12 modified by TPC Chairs of Interspeech 2025

% **************************************
% *    DOUBLE-BLIND REVIEW SETTINGS    *
% **************************************
% Comment out \interspeechcameraready when submitting the 
% paper for review.
% If your paper is accepted, uncomment this to produce the
%  'camera ready' version to submit for publication.

% \interspeechcameraready 


% **************************************
% *                                    *
% *      STOP !   DO NOT DELETE !      *
% *          READ THIS FIRST           *
% *                                    *
% * This template also includes        *
% * important INSTRUCTIONS that you    *
% * must follow when preparing your    *
% * paper. Read it BEFORE replacing    *
% * the content with your own work.    *
% **************************************

% title here must exactly match the title entered into the paper submission system
\title{Challenges in creating a phonetic corpus of Swedish based on parliamentary speeches}

% the order of authors here must exactly match the order entered into the paper submission system
% note that the COMPLETE list of authors MUST be entered into the paper submission system at the outset, including when submitting your manuscript for double-blind review
\author[affiliation={1}]{Jim}{O'Regan}
\author[affiliation={1}]{Jens}{Edlund}

%The maximum number of authors in the author list is 20. If the number of contributing authors is more than this, they should be listed in a footnote or the acknowledgement section.

% if you have too many addresses to fit within the available space, try removing the "\\" newlines
\affiliation{Division of Speech, Music and Hearing}{KTH Royal Institute of Technology}{Sweden}
\email{joregan@kth.se, edlund@speech.kth.se}
\keywords{speech recognition, human-computer interaction, computational paralinguistics}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\usepackage{comment}

\begin{document}

\maketitle

% the abstract here must exactly match the abstract entered into the paper submission system
\begin{abstract}
Parliamentary speeches are ...
Challenging because...

In this document, we describe ongoing work towards the creation of a phonetically transcribed corpus of spoken Swedish, with aims towards
creating a pronunciation dictionary that takes into account dialectal
variation. Using speeches from the Swedish Riksdag (Parliament), we use the output of a phonetic recognition system to validate pronunciations for a variety of Swedish dialects.
\end{abstract}




\section{Introduction}

Modern speech technology development is increasingly reliant on large-scale spoken corpora that capture the complexities of natural speech. These corpora serve as foundational resources for linguistic research, automatic speech recognition (ASR), text-to-speech (TTS) systems, and various applications in human-computer interaction. 

Speech technology is increasingly shifting away from the explicit use of phonetic annotation; as larger amounts of data become available, the ability to learn directly from text increases, while too does the potential cost of phonetic annotation for such large speech datasets: the Multilingual LibriSpeech (MLS) dataset~\cite{pratap20_interspeech} includes over 32K hours of English and 4.5K hours for other languages under an open license, while the more recent Emilia dataset~\cite{he2024emilia} includes 101K hours across six languages, albeit not under an open license.

Large phonetically annotated corpora are also becoming available, by applying automatic phonetic transcription to existing large speech datasets. IPAPACK~\cite{zhu-etal-2024-taste} offers over 1000 hours across 115 languages, while VoxCommunis~\cite{ahn-chodroff-2022-voxcommunis} offers has around 800 hours across 36 languages, and has been used as the basis for the descriptive analysis of language-specific vowel systems.

These efforts have concentrated on phonetic annotation of read speech, primarily from datasets created as training data for ASR, where the use of forced alignment is feasible. More spontaneous speech poses somewhat more of a challenge.

\subsection{Riksdag}

This paper focuses on the development of an annotated corpus derived from speeches delivered in the Swedish Riksdag (Parliament).

The official transcripts comprise the official record of debates in Riksdag, to which are supplemented recordings of the sessions.
Parliamentary speech, at least in the Swedish case, exists on a continuum between read and spontaneous speech: the official transcripts are prepared from a version of the speech to be read that was filed in advance, but the speaker may diverge from this script in reaction to other events, or based on their own speaking style. Additionally, the transcribers have a style guide that they follow to ensure consistency in the official transcripts.

Representatives to Riksdag come from all parts of Sweden, and tend overwhelmingly to speak using their own dialects. Riksdag recordings are therefore a valuable resource for Swedish dialects, with the notable exception of Finland Swedish.

Although under Swedish law voice recordings are considered personal data, the considerations of public interest in matters of government supersede the usual protections. Similarly, the drive towards open government data means that the speeches and their transcripts are provided to the public under terms similar to, and compatible with the Creative Commons Attribution license, allowing for datasets created using them to be distributed as open data.

In our initial phase, we concentrate on the speeches delivered in the period from 2012 to 2022, from which we located 5,925 hours of raw video via the Riksdag API. In addition to the videos provided since 2022, we have downloaded videos from the previous 10 years, which await processing. In addition to the 20+ years of video made available through Riksdag's website and API, we have been provided with archival material that was digitized from video and audio tapes with recordings dating back to 1966.

\subsection{Aims}

While our ultimate aim is to have a corpus suitable for use in speech science in the broadest sense, our initial aims are to enhance speech technology. Our immediate use case is to create a corpus-based pronunciation dictionary for use in speech technology, particularly in text to speech (TTS).

Like other types of dictionary, pronunciation dictionaries are difficult and expensive to create manually, and where they exist, they typically tend to be restricted to one or two standard dialects.

Languages with an alphabetic script typically follow a known set of rules, and grapheme-to-phoneme (G2P) conversion can automate the creation of pronunciation dictionaries, though how effectively depends on the orthography of the language in question, and dictionaries for technological purposes typically require pronunciations for words that do not follow the rules of the language proper, such as foreign names. Swedish surnames in particular often tend to have multiple variants that adhere to earlier versions of orthographic standards.\cite{gustafson1996swedish}

These rules can be augmented in stages to handle dialectal variation, assimilation to adjoining words, adjustments in speaking rate, etc., but the interactions between these rules can lead to a large number of possible candidate pronunciations, not all of them realistic: for example, different dialects may handle speaking rates differently.

Corpus-based pronunciation dictionaries are quite rare, and when they do exist, they are somewhat limited.

The Current British English (CUBE)\footnote{\href{http://seas.elte.hu/cube/}{http://seas.elte.hu/cube/}} includes a single example per word, via Youglish\footnote{\href{https://youglish.com/}{https://youglish.com/}}, a site which indexes transcribed videos (text only) to provide pronunciation examples for learners. CUBE provides a phonetic transcription that matches the pronunciation given in that particular example. Youglish is not limited to English, but does not itself offer phonetic transcripts.

``Ordbok över Finlands svenska folkmål'' \textit{(Dictionary of Finland Swedish Vernacular)}\footnote{\href{https://kaino.kotus.fi/fo/}{https://kaino.kotus.fi/fo/}} which includes pronunciations by dialect, where available, with a phonetic transcription of example sentences that have been transcribed from field recordings. In its current form, however, there is no reference to the source audio from which the transcription was made.

Wiktionary, the crowd-sourced dictionary companion to Wikipedia, contains many entries with pronunciation information, but because of its nature, the pronunciation information can vary quite a lot in quality. For languages with regular orthographies, transcriptions are often generated using custom modules for grapheme-to-phoneme conversion. For languages with less regular orthographies, such as Swedish, entries must be added in full. Individuals or groups often serve as editorial oversight for particular languages, but there is no guarantee of consistency.

\subsection{Data}

In this section we describe the training data for our phonetic recognition model, and the dictionaries we used in search of authoritative pronunciations.

\subsubsection{Waxholm}
To train a phonetic recognition system, we used the data created in the Waxholm~\cite{bertenstam1995waxholm} project, a project to create a dialog system to provide information on boat traffic in the Stockholm archipelago.

Speech recognition was not integrated into the original demonstrator application, which was replaced with Wizard-of-Oz techniques; participants were aware of the wizard~\cite{blomberg1993waxholm}. All utterances are accompanied by a first-pass phonetic transcription, produced by the same lexical components employed in the text-to-speech component of the system. A second-pass transcription was created by trained phoneticians, who refined the first pass to more exactly match the utterances. Although most of the sentences were the result of interactions with the system, as a means of collecting data for the speech recognition component, an additional number of phonetically rich sentences were read by the participants, to ensure coverage of the phoneset to be employed. Both the orthographic and phonetic transcriptions include annotations of non-speech sounds, such as filled pauses, breaths, coughs, and hesitation noises.

The audio was recorded using Wavesurfer~\cite{sjolander2000wavesurfer} and saved in the ``\texttt{smp}'' format. We converted the audio to wav by replacing the header and swapping the bytes of each 16-bit sample. No further processing was applied to the audio. In total, there is slightly more than 2 hours of transcribed audio.

\subsubsection{Dictionaries}

We originally investigated different dictionaries for use in forced alignment; although the Montreal forced aligner (MFA)~\cite{mcauliffe17_interspeech} has a pretrained acoustic model and accompanying dictionary for Swedish, the pronunciations were extremely inconsistent, and the decision to approximate Swedish pitch through the use of tones results in an essentially random attachment of 1 or 2 IPA pitch marks, which was impossible to predict in a way that suited the acoustic model.

WikiPron~\cite{lee-etal-2020-massively} is a multilingual pronunciation dictionary created by scraping Wiktionary. Although some attempts are made for some languages to ensure consistency, not all languages benefit from it. WikiPron, because of its availability and inclusion of dialectal forms, was our initial choice of lexicon. However, when looking at the top 10 pairs of word/transcription, we noticed that the word ``ock'' (\textit{and}) was missing, because the most frequently occurring pronunciation was absent from Wiktionary. On further inspection, we found that this was the case for a number of other high frequency words.

``Folkets lexicon'' (\textit{The People's Dictionary})\footnote{\url{https://folkets-lexikon.csc.kth.se/folkets/folkets.html}} is an open-content crowd-sourced bilingual dictionary for both English-Swedish and Swedish-English, based on a pair of dictionaries originally published by Språkrådet (The Language Council of Sweden). The dictionaries were designed for use by learners, and include pronunciation information for a large number of the original entries.

``Braxen''~\cite{tannander2025braxen} is a pronunciation dictionary intended for text-to-speech. It is developed by MTM, the Swedish Agency for Accessible Media, for use in creating audiobooks for the visually impaired, so it is under active development. As a TTS lexicon, it shares many of the same conventions as the Waxholm data, with an even more specific phone set and has a broader scope of what it includes, including names. Braxen will be our primary lexicon going forward, but there is benefit to be had from having multiple references to external sources that offer authoritative transcriptions.

\section{Method}

\subsection{ASR}

Because the official transcripts are not always an exact representation of the speech that was delivered, we used speech recognition on the recordings of each speech to find matching sentences, using the model described by~\cite{malmstenHearingVoicesNational2022}, based on wav2vec 2~\cite{baevski2020wav2vec2}.

Although Whisper~\cite{radford23whisper} gave impressive output, we strongly suspect that Riksdag speeches were included in the training data. By transcriber's convention, all debates start with the word ``tack'' \textit{(thank you)}, in reality the vast majority of speakers commence by saying ``tack herr/fru talman'' \textit{(thank you mister/madam speaker)}; these phrases are frequently reduced to ``tack'' by Whisper, wherever they appear. Additionally, the implementation of wav2vec in the Transformers library is able to produce word-level time stamps; Whisper's time stamps are unpredictable at best.

In addition to simple matches, we ran a set of filters in stages on the unmatched portions to account for a number of phenomena, such as pairs of sentences being joined, denormalization (numbers, abbreviations, etc.), alternate spellings, predictable recognition errors, and so on. Each extracted sentence is accompanied by metadata that lists which filters were applied.

\subsection{Phonetic recognition}

To create phonetic transcriptions, we fine-tuned the same model on the phonetic transcriptions of the Waxholm dataset, which we aligned with the validated sentences using timestamps.

The resulting pairs of orthographic words and their transcriptions were then compared against a reference pronunciation, possibly generated and/or expanded using a variety of rule sets to account for assimilation, speaking rate, and dialect. Words are annotated with a list of identifiers for the rules that were applied, for later use in searching the corpus: where, for example, /n/ is assimilated to /m/ before a labial, for many purposes this can be considered evidence of /n/, and we wish to make our corpus searchable on this basis, if the user wishes.

\section{Ongoing Work and Current Challenges}

%The development of a phonetic corpus from Swedish parliamentary speech is an iterative process that presents continuous challenges and opportunities for refinement. As the project progresses, several key areas have been identified for ongoing research and future development, particularly in enhancing phonetic recognizer accuracy, refining data curation methods, and expanding the corpus to support broader linguistic and technological applications.

\subsection{Older recordings}

Our work to date has concentrated on the recordings made from 2012 to 2022; almost 50 years of earlier recordings are available. The older recordings have been digitized from audio and video tape recordings, which do not completely overlap. Rather than waste tape by changing on a daily basis, the tapes were changed as they ran out, leading to gaps in the recordings. Where both recordings are available, the gaps tend not to coincide because of the differing capacities of the tapes, but only audio is available for the earliest recordings.

These recordings are of particular interest because of the scope for diachronic investigation into phonetic change. The Swedish `damped' /i/ (see, e.g., ~\cite{bjorsten1999swedish}) is thought to have started to enter the Stockholm dialect in the 1990s, and we have found video of at least one speaker who uses both `damped' and pre-damped variants of /i/.

%In the earliest recordings, only a selection of speeches were recorded, and the recordings quite often cut off mid-sentence

\subsection{Increasing transcript alignments}

Although we have made attempts to match the ASR output with the official transcripts, a number of possibilities remain to us to expand the amount of alignments.

A large number of sentences in the data differ only in the placement of a phrase: for example, what appears at the end of a sentence in the transcript might have been read at the start of the same sentence. We are investigating the use of dependency parsing to locate such cases, as they ought to appear as different serialisations of the same tree.

[MORE]

\subsection{Enhancing Phonetic Recognizer Accuracy}

The phonetic recognizer has demonstrated surprisingly strong performance in spite of the limited amount of training data available but continues to face challenges, particularly in dialectal variation and  short-duration phones. Our ongoing efforts focus on targeted improvements in these areas:

\subsubsection{Dialectal Diversity}

One of the primary weaknesses is the recognizer's difficulty in handling dialectal features, particularly the uvular varieties of /r/. The Waxholm data had a very limited number of speakers likely to have produced it, we expect that it will not take a large amount of manual annotation to address this shortcoming.

The bulk of dialectal differences can be addressed through rules; either by encoding them from phonetic descriptions of the dialect (which often use the Stockholm dialect as a point of reference) or through deriving the rules automatically.

%We are actively working to expand the training dataset with speech samples from a wider range of dialects to improve coverage and reduce systematic errors.

\subsubsection{Short-duration sounds}

A second phonetic recognition pass is being planned using MFA. One limitation of the model we used is that the large size of the context window can sometimes cause short sounds to be discarded. Older methods, such as those used in MFA, use a shorter window and are less prone to discarding such sounds.

MFA also provides more accurate phone-level time stamps which is important for other uses and enhancements of the corpus, such as determining speech rate, and making use of pitch extraction to determine accent.

We plan to train MFA-based aligners on different subsets of the data, to determine if there are gains to be made through the use of dialect- or even speaker-specific aligners.

Having phone-level timings also offers the possibility of using Edyson~\cite{fallgren2021human} for rapid annotation of phones.

\subsection{Pitch extraction}

Swedish is a language with phonemic pitch accent. To estimate word pitch, we intend to use pitch extraction intersected with vowel timings as a means of determining the accent of the word.


\subsection{Grapheme to phoneme}

No dictionary can include every word, and grapheme to phoneme conversion (G2P) is often used to expand dictionaries. As all sources of provenance are being tagged in the corpus, we plan to adopt a tagging convention for G2P to exclude their output by default, as we expect a large number of false starts and other non-words might otherwise be included.

As with the forced aligners, we think there may be potential gains from training dialect- or ideolect-specific G2P models.

\subsection{Corpus search}

We plan a search interface inspired by the corpus search software for the Buckeye Corpus~\cite{pitt2007buckeye}. Taking into account text search, we intend that in addition to the output of the phonetic recognizer, the user should have the option to search for a normalized form of the phonetic representation, such as the underlying form when assimilation has taken place, in a manner akin to stemming or lemmatization in text search.


[EXAMPLES! CHECK THE SCOFF]



\section{Acknowledgements}
%Acknowledgement should only be included in the camera-ready version, not in the version submitted for review. The 5th page is reserved exclusively for acknowledgements and  references. No other content must appear on the 5th page. Appendices, if any, must be within the first 4 pages. The acknowledgments and references may start on an earlier page, if there is space.

% \ifinterspeechfinal
%      The Interspeech 2025 organisers
% \else
%      The authors
% \fi
% would like to thank ISCA and the organising committees of past Interspeech conferences for their help and for kindly providing the previous version of this template.


\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
