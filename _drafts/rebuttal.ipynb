{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb8dbe0",
   "metadata": {},
   "source": [
    "# Reviewer QkfD\n",
    "\n",
    "First of all, we would like to express our appreciation for your review, in particular, for the wonderful summary of our work.\n",
    "\n",
    "**Q1. About using a simulated environment**\n",
    "\n",
    "This is an excellent point regarding the sim-to-real gap.  All vision components are pre-trained on massive, real-world image datasets, not on AI2-THOR. Therefore, their core visual grounding capabilities are not tied to the simulator.  If anything, we expect the out-of-domain images from the simulator to have worse performance than real-world data. We will add a discussion of this limitation and our reasoning for the approach's likely generalizability to the final manuscript.\n",
    "\n",
    "**Q2. About the grounding component in the two stage method**\n",
    "\n",
    " We thank the reviewer for this insightful question. Our two-stage pipeline employs a deliberate 'divide and conquer' strategy: first, an LLM (GPT-4o) resolves conversational ambiguity using the text-only dialogue history. Second, a specialized VLM (Florence-2) performs the visual grounding by localizing this now-unambiguous phrase in the image.\n",
    "\n",
    " We chose models like Florence-2 because they excel at this open-vocabulary localization. In contrast, many popular VLMs are designed for VQA/captioning and lack this crucial capability, a limitation confirmed in our initial tests where they failed to resolve any pronominal references. For example using GPT-4o failed on grounding tasks (outputting bounding boxes) and only the specialized, fine-tuned GroundingGPT was able to do grounding. We will make these considerations more clear in the paper.\n",
    "\n",
    " **Q3. About the use of active learning in the annotation**\n",
    "\n",
    "The use of crowd-sourcing for verification of annotations is one important step, which we have piloted already with the experiments in this paper. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba826747",
   "metadata": {},
   "source": [
    "# Reviewer wGYR\n",
    "\n",
    "**Q1. About evaluating on more vision-language models**\n",
    "\n",
    "This is an important point, which was also raised by reviewer 2mrB: we agree that expanding VLM evaluation is valuable. While our task requires models that output coordinates—limiting us to options like Florence-2 and GroundingGPT—we recognize that GLIP and Grounded-SAM are strong candidates for this setting. We intend to include evaluations of both in the final version to address this feedback and offer a broader perspective on grounding performance.\n",
    "\n",
    "**Q2. About more details on prompting and utterance history**\n",
    "\n",
    "We have provided prompts for GPT-4o and GroundingGPT in the supplementary material. “information.md” contains a link to an excerpt of the dataset showing dataset format, including how we provide scene context (DOUBLE CHECK). We will add examples of intermediate representation of the 2-stage approach. About the utterance history: TBA (JIM).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
