{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7883f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_JSON = \"\"\"\n",
    "{\"\\u0251\": 1, \"\\u00e6\": 2, \"\\u028c\": 3, \"a\\u028a\": 4, \"\\u0259\": 5, \"\\u025d\": 6, \"a\\u026a\": 7, \"b\": 8, \"\\u02a7\": 9, \"d\": 10, \"\\u00f0\": 11, \"\\u027e\": 12, \"\\u025b\": 13, \"l\\u0329\": 14, \"m\\u0329\": 15, \"n\\u0329\": 16, \"\\u014b\\u0329\": 17, \"\\u025a\": 18, \"e\\u026a\": 19, \"f\": 20, \"g\": 21, \" \": 22, \"h\": 24, \"\\u026a\": 26, \"i\\u02d0\": 27, \"\\u02a4\": 28, \"k\": 29, \"l\": 30, \"m\": 31, \"n\": 32, \"\\u014b\": 33, \"\\u027e\\u0303\": 34, \"o\\u028a\": 35, \"\\u0254\\u026a\": 36, \"p\": 37, \"ʔ\": 38, \"\\u0279\": 39, \"s\": 40, \"\\u0283\": 41, \"t\": 42, \"\\u03b8\": 43, \"\\u028a\": 44, \"u\\u02d0\": 45, \"v\": 46, \"w\": 47, \"j\": 48, \"z\": 49, \"\\u0292\": 50, \"|\": 0, \"[UNK]\": 51, \"[PAD]\": 52}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "437b6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "TOKENS = json.loads(TOK_JSON)\n",
    "TOKLIST = [\"\\[UNK\\]\"]\n",
    "for key in TOKENS:\n",
    "    if \"[\" in key or key == \"|\" or key == \" \":\n",
    "        continue\n",
    "    TOKLIST.append(key)\n",
    "# sort TOKLIST by length, longest first\n",
    "TOKLIST.sort(key=len, reverse=True)\n",
    "TOK_REGEX_INNER = \"|\".join(TOKLIST)\n",
    "TOK_REGEX = fr\"({TOK_REGEX_INNER})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32d1a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenise(text):\n",
    "    tokens = []\n",
    "    text = text.strip()\n",
    "    while text:\n",
    "        match = re.match(TOK_REGEX, text)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not match token in text: {text}\")\n",
    "        token = match.group(0)\n",
    "        if token == \"[UNK]\":\n",
    "            tokens.append(\"ɪ\")\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "        text = text[len(token):].strip()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44eba8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TIMIT = \"doʊn t biːɪf ɹeɪʔ l[UNK]ɾl̩wʌn ð[UNK]biːs kæn t[UNK]ɚʔʧuːwɑlʔaɪmɚɹaʊn dɪnðɛnhiːt[UNK]tɪz kæp ʔoʊvɝhɪz lɛf tʔiːɝ ʔɪnʃʊk[UNK]z k lʌb ʔɪʔðɪp ɹ[UNK]n ʔ s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3881116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " 'oʊ',\n",
       " 'n',\n",
       " 't',\n",
       " 'b',\n",
       " 'iː',\n",
       " 'ɪ',\n",
       " 'f',\n",
       " 'ɹ',\n",
       " 'eɪ',\n",
       " 'ʔ',\n",
       " 'l',\n",
       " 'ɪ',\n",
       " 'ɾ',\n",
       " 'l̩',\n",
       " 'w',\n",
       " 'ʌ',\n",
       " 'n',\n",
       " 'ð',\n",
       " 'ɪ',\n",
       " 'b',\n",
       " 'iː',\n",
       " 's',\n",
       " 'k',\n",
       " 'æ',\n",
       " 'n',\n",
       " 't',\n",
       " 'ɪ',\n",
       " 'ɚ',\n",
       " 'ʔ',\n",
       " 'ʧ',\n",
       " 'uː',\n",
       " 'w',\n",
       " 'ɑ',\n",
       " 'l',\n",
       " 'ʔ',\n",
       " 'aɪ',\n",
       " 'm',\n",
       " 'ɚ',\n",
       " 'ɹ',\n",
       " 'aʊ',\n",
       " 'n',\n",
       " 'd',\n",
       " 'ɪ',\n",
       " 'n',\n",
       " 'ð',\n",
       " 'ɛ',\n",
       " 'n',\n",
       " 'h',\n",
       " 'iː',\n",
       " 't',\n",
       " 'ɪ',\n",
       " 't',\n",
       " 'ɪ',\n",
       " 'z',\n",
       " 'k',\n",
       " 'æ',\n",
       " 'p',\n",
       " 'ʔ',\n",
       " 'oʊ',\n",
       " 'v',\n",
       " 'ɝ',\n",
       " 'h',\n",
       " 'ɪ',\n",
       " 'z',\n",
       " 'l',\n",
       " 'ɛ',\n",
       " 'f',\n",
       " 't',\n",
       " 'ʔ',\n",
       " 'iː',\n",
       " 'ɝ',\n",
       " 'ʔ',\n",
       " 'ɪ',\n",
       " 'n',\n",
       " 'ʃ',\n",
       " 'ʊ',\n",
       " 'k',\n",
       " 'ɪ',\n",
       " 'z',\n",
       " 'k',\n",
       " 'l',\n",
       " 'ʌ',\n",
       " 'b',\n",
       " 'ʔ',\n",
       " 'ɪ',\n",
       " 'ʔ',\n",
       " 'ð',\n",
       " 'ɪ',\n",
       " 'p',\n",
       " 'ɹ',\n",
       " 'ɪ',\n",
       " 'n',\n",
       " 'ʔ',\n",
       " 's']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise(SAMPLE_TIMIT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
