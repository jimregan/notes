{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UsRchXMCGzV",
        "outputId": "20fbb2e9-992b-49eb-8355-b75903c3a021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
            "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.15.0 stanza-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import stanza\n",
        "\n",
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "PRON_FEATS: Dict[str, str] = {\n",
        "    \"mé\": \"Person=1|Number=Sing\",\n",
        "    \"tú\": \"Person=2|Number=Sing\",\n",
        "    \"muid\": \"Person=1|Number=Plur\",\n",
        "    \"sinn\": \"Person=1|Number=Plur\",\n",
        "    \"sibh\": \"Person=2|Number=Plur\",\n",
        "    \"siad\": \"Person=3|Number=Plur\",\n",
        "}\n",
        "PRON_FORMS = set(PRON_FEATS.keys())\n",
        "\n",
        "# One-time download is idempotent; keep it here if you run as a script.\n",
        "stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "\n",
        "# We supply tokens & sentences. Stanza won't retokenize.\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    tokenize_pretokenized=True,\n",
        "    no_ssplit=True,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "\n",
        "def standardise(text: str, lang: str = \"ga\") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Return list of (orig_tok, std_tok) pairs from Intergaelic, with hard invariants.\"\"\"\n",
        "    data = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    hdrs = {\"Content-Type\": \"application/x-www-form-urlencoded\", \"Accept\": \"application/json\"}\n",
        "    req = urllib.request.Request(STD_API, data, hdrs)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        pairs = [tuple(x) for x in json.loads(resp.read())]\n",
        "\n",
        "    # Hard invariant: if std splits, it's exactly \"<X> <PRON>\" where PRON is in PRON_FORMS.\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        parts = std.split()\n",
        "        if len(parts) > 1:\n",
        "            if len(parts) != 2 or parts[1].lower() not in PRON_FORMS:\n",
        "                raise ValueError(\n",
        "                    f\"Unexpected multi-token Intergaelic output at index {i}: \"\n",
        "                    f\"orig={orig!r}, std={std!r}\"\n",
        "                )\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def _feats_to_dict(feats: str) -> Dict[str, str]:\n",
        "    if not feats or feats == \"_\":\n",
        "        return {}\n",
        "    out: Dict[str, str] = {}\n",
        "    for part in feats.split(\"|\"):\n",
        "        if \"=\" in part:\n",
        "            k, v = part.split(\"=\", 1)\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def _dict_to_feats(d: Dict[str, str]) -> str:\n",
        "    if not d:\n",
        "        return \"_\"\n",
        "    return \"|\".join(f\"{k}={v}\" for k, v in sorted(d.items()))\n",
        "\n",
        "\n",
        "def _merge_feats_preserve(base: str, add: str) -> str:\n",
        "    \"\"\"\n",
        "    Merge features from `add` into `base` but DO NOT overwrite existing keys\n",
        "    (especially important for Person/Number if Stanza already provided them).\n",
        "    \"\"\"\n",
        "    bd = _feats_to_dict(base)\n",
        "    ad = _feats_to_dict(add)\n",
        "    for k, v in ad.items():\n",
        "        bd.setdefault(k, v)\n",
        "    return _dict_to_feats(bd)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MapItem:\n",
        "    orig_i: int       # index in pairs\n",
        "    sub_i: int        # subtoken index within that orig token\n",
        "    n_sub: int        # number of subtokens for that orig token\n",
        "    orig_tok: str     # original surface token\n",
        "    std_tok: str      # standardized token fed to stanza\n",
        "\n",
        "\n",
        "def _split_std(std: str, orig: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    For Stanza input:\n",
        "    - if std is empty/whitespace: fall back to original token\n",
        "    - else: whitespace split\n",
        "    \"\"\"\n",
        "    if not std.strip():\n",
        "        return [orig]\n",
        "    return std.split()\n",
        "\n",
        "\n",
        "def _sentences_from_pairs(pairs: Sequence[Tuple[str, str]]) -> List[List[MapItem]]:\n",
        "    \"\"\"Light sentence splitter on standardized stream: end on . ! ?\"\"\"\n",
        "    sents: List[List[MapItem]] = []\n",
        "    buf: List[MapItem] = []\n",
        "\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        parts = _split_std(std, orig)\n",
        "        n = len(parts)\n",
        "        for j, part in enumerate(parts):\n",
        "            buf.append(MapItem(i, j, n, orig, part))\n",
        "            if part in {\".\", \"!\", \"?\"}:\n",
        "                sents.append(buf)\n",
        "                buf = []\n",
        "\n",
        "    if buf:\n",
        "        sents.append(buf)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def _choose_representative_word(words, idxs: List[int]) -> int:\n",
        "    \"\"\"\n",
        "    Choose which stanza word represents the original token.\n",
        "    Since any split is guaranteed to be \"<X> <PRON>\", we prefer non-PRON.\n",
        "    \"\"\"\n",
        "    for i in idxs:\n",
        "        if (words[i].upos or \"\") != \"PRON\":\n",
        "            return i\n",
        "    return idxs[0]\n",
        "\n",
        "\n",
        "def project_with_stanza(raw_text: str, lang: str = \"ga\") -> str:\n",
        "    \"\"\"\n",
        "    One CoNLL-U token per ORIGINAL token.\n",
        "    Intergaelic-injected pronouns never become tokens; they only contribute Person/Number\n",
        "    to the original token's FEATS (when a split is present).\n",
        "    \"\"\"\n",
        "    pairs = standardise(raw_text, lang)\n",
        "    sents = _sentences_from_pairs(pairs)\n",
        "\n",
        "    pretok: List[List[str]] = [[m.std_tok for m in sent] for sent in sents]\n",
        "    doc = nlp(pretok)\n",
        "\n",
        "    out: List[str] = []\n",
        "\n",
        "    for sid, (sent_map, sent_doc) in enumerate(zip(sents, doc.sentences), 1):\n",
        "        raw_slice = [m.orig_tok for m in sent_map if m.sub_i == 0]\n",
        "        std_slice = [m.std_tok for m in sent_map]\n",
        "        out += [\n",
        "            f\"# sent_id = {sid}\",\n",
        "            f\"# text = {' '.join(raw_slice)}\",\n",
        "            f\"# text_standard = {' '.join(std_slice)}\",\n",
        "        ]\n",
        "\n",
        "        words = sent_doc.words  # aligned 1:1 with pretok tokens\n",
        "\n",
        "        # Original token order in this sentence (unique orig_i, in appearance order)\n",
        "        orig_keys: List[int] = []\n",
        "        for m in sent_map:\n",
        "            if not orig_keys or orig_keys[-1] != m.orig_i:\n",
        "                orig_keys.append(m.orig_i)\n",
        "\n",
        "        # Map orig_i -> sentence-local CoNLL id (1..N)\n",
        "        orig_i_to_tid = {orig_i: k + 1 for k, orig_i in enumerate(orig_keys)}\n",
        "\n",
        "        # Map orig_i -> list of stanza word indices (0-based) that came from it\n",
        "        orig_i_to_widxs: Dict[int, List[int]] = {orig_i: [] for orig_i in orig_keys}\n",
        "        for widx, m in enumerate(sent_map):\n",
        "            orig_i_to_widxs[m.orig_i].append(widx)\n",
        "\n",
        "        # For head remapping: stanza word index -> original token id\n",
        "        widx_to_tid: Dict[int, int] = {}\n",
        "        for orig_i, widxs in orig_i_to_widxs.items():\n",
        "            tid = orig_i_to_tid[orig_i]\n",
        "            for widx in widxs:\n",
        "                widx_to_tid[widx] = tid\n",
        "\n",
        "        for orig_i in orig_keys:\n",
        "            tid = orig_i_to_tid[orig_i]\n",
        "            widxs = orig_i_to_widxs[orig_i]\n",
        "\n",
        "            rep_widx = _choose_representative_word(words, widxs)\n",
        "            rep = words[rep_widx]\n",
        "\n",
        "            # Remap head from stanza-word index space to original-token id space\n",
        "            if rep.head and rep.head != 0:\n",
        "                head_widx0 = rep.head - 1\n",
        "                head_tid = widx_to_tid.get(head_widx0, 0)\n",
        "            else:\n",
        "                head_tid = 0\n",
        "\n",
        "            form = pairs[orig_i][0] or \"_\"\n",
        "            feats = rep.feats or \"_\"\n",
        "\n",
        "            # If Intergaelic split this token, the 2nd part is guaranteed pronoun by invariant.\n",
        "            std = pairs[orig_i][1] or \"\"\n",
        "            parts = std.split()\n",
        "            misc = \"_\"\n",
        "\n",
        "            if len(parts) == 2:\n",
        "                inj_pron = parts[1].lower()\n",
        "                # invariant guarantees it exists in PRON_FEATS\n",
        "                feats = _merge_feats_preserve(feats, PRON_FEATS[inj_pron])\n",
        "                misc = f\"StdSplit={parts[0]}|{parts[1]}\"\n",
        "\n",
        "            out.append(\"\\t\".join([\n",
        "                str(tid),\n",
        "                form,\n",
        "                rep.lemma or \"_\",\n",
        "                rep.upos or \"_\",\n",
        "                rep.xpos or \"_\",\n",
        "                feats,\n",
        "                str(head_tid),\n",
        "                rep.deprel or \"_\",\n",
        "                \"_\",\n",
        "                misc,\n",
        "            ]))\n",
        "\n",
        "        out.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(out)"
      ],
      "metadata": {
        "id": "psn8ILaqCTlQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}