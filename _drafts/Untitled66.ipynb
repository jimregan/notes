{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UsRchXMCGzV",
        "outputId": "20fbb2e9-992b-49eb-8355-b75903c3a021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
            "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.15.0 stanza-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Sequence, Tuple\n",
        "\n",
        "import stanza\n",
        "\n",
        "STD_API = \"https://cadhan.com/api/intergaelic/3.0\"\n",
        "\n",
        "PRON_FEATS: Dict[str, str] = {\n",
        "    \"mé\": \"Person=1|Number=Sing\",\n",
        "    \"tú\": \"Person=2|Number=Sing\",\n",
        "    \"muid\": \"Person=1|Number=Plur\",\n",
        "    \"sinn\": \"Person=1|Number=Plur\",\n",
        "    \"sibh\": \"Person=2|Number=Plur\",\n",
        "    \"siad\": \"Person=3|Number=Plur\",\n",
        "}\n",
        "PRON_FORMS = set(PRON_FEATS.keys())\n",
        "\n",
        "# Download once (idempotent)\n",
        "stanza.download(\"ga\", processors=\"tokenize,pos,lemma,depparse\", verbose=False)\n",
        "\n",
        "# We supply tokens + sentence breaks\n",
        "nlp = stanza.Pipeline(\n",
        "    lang=\"ga\",\n",
        "    processors=\"tokenize,pos,lemma,depparse\",\n",
        "    tokenize_pretokenized=True,\n",
        "    no_ssplit=True,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "\n",
        "def standardise(text: str, lang: str = \"ga\") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Intergaelic pairs with a strict check ONLY for 1→2 splits.\"\"\"\n",
        "    data = urllib.parse.urlencode({\"foinse\": lang, \"teacs\": text}).encode()\n",
        "    hdrs = {\"Content-Type\": \"application/x-www-form-urlencoded\", \"Accept\": \"application/json\"}\n",
        "    req = urllib.request.Request(STD_API, data, hdrs)\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        pairs = [tuple(x) for x in json.loads(resp.read())]\n",
        "\n",
        "    # If (orig is 1 token) and (std is 2 tokens), then std[1] MUST be one of our pronouns.\n",
        "    # Anything else (2→2, 2→1, 3→2, etc.) is allowed.\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        o = orig.split()\n",
        "        s = (std or \"\").split()\n",
        "        if len(o) == 1 and len(s) == 2 and s[1].lower() not in PRON_FORMS:\n",
        "            raise ValueError(\n",
        "                f\"Unexpected 1→2 Intergaelic output at index {i}: orig={orig!r}, std={std!r}\"\n",
        "            )\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def compute_spaceafter(raw_text: str, orig_tokens: List[str]) -> List[bool]:\n",
        "    \"\"\"\n",
        "    True  => there WAS whitespace after this token in raw_text (no SpaceAfter=No needed)\n",
        "    False => no whitespace after (emit SpaceAfter=No)\n",
        "    Aligns monotically by substring search; raises if alignment fails.\n",
        "    \"\"\"\n",
        "    flags: List[bool] = []\n",
        "    pos = 0\n",
        "    n = len(raw_text)\n",
        "\n",
        "    for i, tok in enumerate(orig_tokens):\n",
        "        # Skip whitespace before token\n",
        "        while pos < n and raw_text[pos].isspace():\n",
        "            pos += 1\n",
        "\n",
        "        # Exact at current position preferred\n",
        "        if raw_text.startswith(tok, pos):\n",
        "            start = pos\n",
        "        else:\n",
        "            start = raw_text.find(tok, pos)\n",
        "            if start == -1:\n",
        "                raise ValueError(f\"Could not align token {i} {tok!r} near pos {pos}\")\n",
        "        end = start + len(tok)\n",
        "        pos = end\n",
        "\n",
        "        if pos >= n:\n",
        "            flags.append(True)  # end-of-text\n",
        "        else:\n",
        "            flags.append(raw_text[pos].isspace())\n",
        "\n",
        "    return flags\n",
        "\n",
        "\n",
        "def feats_to_dict(feats: str) -> Dict[str, str]:\n",
        "    if not feats or feats == \"_\":\n",
        "        return {}\n",
        "    out: Dict[str, str] = {}\n",
        "    for part in feats.split(\"|\"):\n",
        "        if \"=\" in part:\n",
        "            k, v = part.split(\"=\", 1)\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def dict_to_feats(d: Dict[str, str]) -> str:\n",
        "    if not d:\n",
        "        return \"_\"\n",
        "    return \"|\".join(f\"{k}={v}\" for k, v in sorted(d.items()))\n",
        "\n",
        "\n",
        "def merge_feats_preserve(base: str, add: str) -> str:\n",
        "    \"\"\"Merge without overwriting existing keys.\"\"\"\n",
        "    bd = feats_to_dict(base)\n",
        "    ad = feats_to_dict(add)\n",
        "    for k, v in ad.items():\n",
        "        bd.setdefault(k, v)\n",
        "    return dict_to_feats(bd)\n",
        "\n",
        "\n",
        "def merge_misc(*items: str) -> str:\n",
        "    parts: List[str] = []\n",
        "    for it in items:\n",
        "        if it and it != \"_\":\n",
        "            parts.append(it)\n",
        "    return \"_\" if not parts else \"|\".join(parts)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MapItem:\n",
        "    orig_i: int\n",
        "    sub_i: int\n",
        "    n_sub: int\n",
        "    orig_tok: str\n",
        "    std_tok: str\n",
        "\n",
        "\n",
        "def split_std(std: str, orig: str) -> List[str]:\n",
        "    if not (std or \"\").strip():\n",
        "        return [orig]\n",
        "    return (std or \"\").split()\n",
        "\n",
        "\n",
        "def sentences_from_pairs(pairs: Sequence[Tuple[str, str]]) -> List[List[MapItem]]:\n",
        "    \"\"\"End a sentence when standardized token is . ! ?\"\"\"\n",
        "    sents: List[List[MapItem]] = []\n",
        "    buf: List[MapItem] = []\n",
        "    for i, (orig, std) in enumerate(pairs):\n",
        "        parts = split_std(std, orig)\n",
        "        n = len(parts)\n",
        "        for j, part in enumerate(parts):\n",
        "            buf.append(MapItem(i, j, n, orig, part))\n",
        "            if part in {\".\", \"!\", \"?\"}:\n",
        "                sents.append(buf)\n",
        "                buf = []\n",
        "    if buf:\n",
        "        sents.append(buf)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def choose_rep_word(words, idxs: List[int]) -> int:\n",
        "    \"\"\"\n",
        "    Representative word for lemma/POS/feats:\n",
        "    prefer non-PRON when there is a 1→2 split (X PRON).\n",
        "    \"\"\"\n",
        "    for i in idxs:\n",
        "        if (words[i].upos or \"\") != \"PRON\":\n",
        "            return i\n",
        "    return idxs[0]\n",
        "\n",
        "\n",
        "def project_with_stanza(raw_text: str, lang: str = \"ga\") -> str:\n",
        "    pairs = standardise(raw_text, lang)\n",
        "    orig_tokens = [o for (o, _) in pairs]\n",
        "    spaceafter = compute_spaceafter(raw_text, orig_tokens)\n",
        "\n",
        "    sents = sentences_from_pairs(pairs)\n",
        "    pretok: List[List[str]] = [[m.std_tok for m in sent] for sent in sents]\n",
        "    doc = nlp(pretok)\n",
        "\n",
        "    out: List[str] = []\n",
        "\n",
        "    for sid, (sent_map, sent_doc) in enumerate(zip(sents, doc.sentences), 1):\n",
        "        raw_slice = [m.orig_tok for m in sent_map if m.sub_i == 0]\n",
        "        std_slice = [m.std_tok for m in sent_map]\n",
        "        out += [\n",
        "            f\"# sent_id = {sid}\",\n",
        "            f\"# text = {' '.join(raw_slice)}\",\n",
        "            f\"# text_standard = {' '.join(std_slice)}\",\n",
        "        ]\n",
        "\n",
        "        words = sent_doc.words\n",
        "\n",
        "        # Unique orig indices in this sentence, in order\n",
        "        orig_keys: List[int] = []\n",
        "        for m in sent_map:\n",
        "            if not orig_keys or orig_keys[-1] != m.orig_i:\n",
        "                orig_keys.append(m.orig_i)\n",
        "\n",
        "        # orig_i -> sentence-local token id\n",
        "        orig_i_to_tid = {orig_i: k + 1 for k, orig_i in enumerate(orig_keys)}\n",
        "\n",
        "        # orig_i -> stanza word indices in this sentence\n",
        "        orig_i_to_widxs: Dict[int, List[int]] = {orig_i: [] for orig_i in orig_keys}\n",
        "        for widx, m in enumerate(sent_map):\n",
        "            orig_i_to_widxs[m.orig_i].append(widx)\n",
        "\n",
        "        # stanza word index -> orig token id (for head remap)\n",
        "        widx_to_tid: Dict[int, int] = {}\n",
        "        for orig_i, widxs in orig_i_to_widxs.items():\n",
        "            tid = orig_i_to_tid[orig_i]\n",
        "            for widx in widxs:\n",
        "                widx_to_tid[widx] = tid\n",
        "\n",
        "        # Emit one CoNLL-U token per orig_i\n",
        "        for orig_i in orig_keys:\n",
        "            tid = orig_i_to_tid[orig_i]\n",
        "            widxs = orig_i_to_widxs[orig_i]\n",
        "\n",
        "            rep_widx = choose_rep_word(words, widxs)\n",
        "            rep = words[rep_widx]\n",
        "\n",
        "            # Head remap: stanza is 1-based, 0=root\n",
        "            if rep.head and rep.head != 0:\n",
        "                head_widx0 = rep.head - 1\n",
        "                head_tid = widx_to_tid.get(head_widx0, 0)\n",
        "            else:\n",
        "                head_tid = 0\n",
        "\n",
        "            form = pairs[orig_i][0] or \"_\"\n",
        "            feats = rep.feats or \"_\"\n",
        "\n",
        "            misc_parts: List[str] = []\n",
        "            if not spaceafter[orig_i]:\n",
        "                misc_parts.append(\"SpaceAfter=No\")\n",
        "\n",
        "            # Inject Person/Number only for 1→2 splits (orig single token, std two tokens)\n",
        "            orig_parts = (pairs[orig_i][0] or \"\").split()\n",
        "            std_parts = (pairs[orig_i][1] or \"\").split()\n",
        "            if len(orig_parts) == 1 and len(std_parts) == 2:\n",
        "                inj_pron = std_parts[1].lower()\n",
        "                # guaranteed by standardise() invariant\n",
        "                feats = merge_feats_preserve(feats, PRON_FEATS[inj_pron])\n",
        "                misc_parts.append(f\"StdSplit={std_parts[0]}|{std_parts[1]}\")\n",
        "\n",
        "            misc = merge_misc(*misc_parts)\n",
        "\n",
        "            out.append(\"\\t\".join([\n",
        "                str(tid),\n",
        "                form,\n",
        "                rep.lemma or \"_\",\n",
        "                rep.upos or \"_\",\n",
        "                rep.xpos or \"_\",\n",
        "                feats,\n",
        "                str(head_tid),\n",
        "                rep.deprel or \"_\",\n",
        "                \"_\",\n",
        "                misc,\n",
        "            ]))\n",
        "\n",
        "        out.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(out)"
      ],
      "metadata": {
        "id": "psn8ILaqCTlQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = project_with_stanza('Do leanadar ag \"seasamh a gcirt\" go dtí gur dhein Eoghan Rua Ó Néill, ag an mBeinn mBorb, gníomh díreach de shaghas an ghnímh a dhein driotháir a athar agus Aodh Rua Ó Dónaill ag Béal an Átha Buí deich mbliana agus daichead roimis sin.')"
      ],
      "metadata": {
        "id": "j4CqPq57C7ov"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEUHMBbIDgow",
        "outputId": "765599ee-d242-4d97-d798-21443d1fd784"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sent_id = 1\n",
            "# text = Do leanadar ag \" seasamh a gcirt \" go dtí gur dhein Eoghan Rua Ó Néill , ag an mBeinn mBorb , gníomh díreach de shaghas an ghnímh a dhein driotháir a athar agus Aodh Rua Ó Dónaill ag Béal an Átha Buí deich mbliana agus daichead roimis sin .\n",
            "# text_standard = Do leanadar ag \" seasamh a gcirt \" go dtí go ndearna Eoghan Rua Ó Néill , ag an mBinn mBorb , gníomh díreach de shaghas an ghnímh a rinne deartháir a athar agus Aodh Rua Ó Dónaill ag Béal an Átha Buí deich mbliana agus daichead roimhe sin .\n",
            "1\tDo\tdo\tPART\tVb\tPartType=Vb\t2\tmark:prt\t_\t_\n",
            "2\tleanadar\tlean\tVERB\tVTI\tMood=Ind|Number=Plur|Person=1|Tense=Past\t0\troot\t_\t_\n",
            "3\tag\tag\tADP\tSimp\t_\t5\tcase\t_\t_\n",
            "4\t\"\t\"\tPUNCT\tPunct\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "5\tseasamh\tseasamh\tNOUN\tNoun\tVerbForm=Inf\t2\txcomp\t_\t_\n",
            "6\ta\ta\tDET\tDet\tNumber=Plur|Person=3|Poss=Yes\t7\tnmod:poss\t_\t_\n",
            "7\tgcirt\tceirt\tNOUN\tNoun\tCase=Gen|Definite=Def|Form=Ecl|Gender=Fem|Number=Sing\t5\tnmod\t_\tSpaceAfter=No\n",
            "8\t\"\t\"\tPUNCT\tPunct\t_\t5\tpunct\t_\t_\n",
            "9\tgo\tgo\tADP\tCmpd\tPrepForm=Cmpd\t11\tmark\t_\t_\n",
            "10\tdtí\tdtí\tNOUN\tCmpd\tForm=Ecl|PrepForm=Cmpd\t9\tfixed\t_\t_\n",
            "11\tgur dhein\tgo\tPART\tVb\tPartType=Cmpl\t11\tmark:prt\t_\t_\n",
            "12\tEoghan\tEoghan\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t11\tnsubj\t_\t_\n",
            "13\tRua\tRua\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t12\tflat:name\t_\t_\n",
            "14\tÓ\tó\tPART\tPat\tPartType=Pat\t12\tflat:name\t_\t_\n",
            "15\tNéill\tNéill\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t12\tflat:name\t_\tSpaceAfter=No\n",
            "16\t,\t,\tPUNCT\tPunct\t_\t19\tpunct\t_\t_\n",
            "17\tag\tag\tADP\tSimp\t_\t19\tcase\t_\t_\n",
            "18\tan\tan\tDET\tArt\tDefinite=Def|Number=Sing|PronType=Art\t19\tdet\t_\t_\n",
            "19\tmBeinn\tbinn\tPROPN\tNoun\tCase=Nom|Definite=Def|Form=Ecl|Gender=Masc|Number=Sing\t12\tconj\t_\t_\n",
            "20\tmBorb\tborb\tPROPN\tNoun\tCase=Nom|Definite=Def|Form=Ecl|Gender=Masc|Number=Sing\t19\tflat:foreign\t_\tSpaceAfter=No\n",
            "21\t,\t,\tPUNCT\tPunct\t_\t22\tpunct\t_\t_\n",
            "22\tgníomh\tgníomh\tNOUN\tNoun\tCase=Nom|Gender=Masc|Number=Sing\t11\tobj\t_\t_\n",
            "23\tdíreach\tdíreach\tADJ\tAdj\tCase=Nom|Gender=Masc|Number=Sing\t22\tamod\t_\t_\n",
            "24\tde\tde\tADP\tSimp\t_\t25\tcase\t_\t_\n",
            "25\tshaghas\tsaghas\tNOUN\tNoun\tCase=Nom|Definite=Def|Form=Len|Gender=Masc|Number=Sing\t22\tnmod\t_\t_\n",
            "26\tan\tan\tDET\tArt\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing|PronType=Art\t27\tdet\t_\t_\n",
            "27\tghnímh\tgníomh\tNOUN\tNoun\tCase=Gen|Definite=Def|Form=Len|Gender=Masc|Number=Sing\t25\tnmod\t_\t_\n",
            "28\ta\ta\tPART\tVb\tForm=Direct|PartType=Vb|PronType=Rel\t29\tobj\t_\t_\n",
            "29\tdhein\tdéan\tVERB\tVTI\tMood=Ind|Tense=Past\t22\tacl:relcl\t_\t_\n",
            "30\tdriotháir\tdeartháir\tNOUN\tNoun\tCase=Nom|Definite=Def|Gender=Masc|Number=Sing\t29\tnsubj\t_\t_\n",
            "31\ta\ta\tDET\tDet\tGender=Masc|Number=Sing|Person=3|Poss=Yes\t32\tnmod:poss\t_\t_\n",
            "32\tathar\tathair\tNOUN\tNoun\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing\t30\tnmod\t_\t_\n",
            "33\tagus\tagus\tCCONJ\tCoord\t_\t34\tcc\t_\t_\n",
            "34\tAodh\tAodh\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t30\tconj\t_\t_\n",
            "35\tRua\tRua\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t34\tflat:name\t_\t_\n",
            "36\tÓ\tó\tPART\tPat\tPartType=Pat\t34\tflat:name\t_\t_\n",
            "37\tDónaill\tDónaill\tPROPN\tNoun\tDefinite=Def|Gender=Masc|Number=Sing\t34\tflat:name\t_\t_\n",
            "38\tag\tag\tADP\tSimp\t_\t39\tcase\t_\t_\n",
            "39\tBéal\tBéal\tNOUN\tNoun\tCase=Nom|Definite=Def|Gender=Masc|Number=Sing\t29\tobl\t_\t_\n",
            "40\tan\tan\tDET\tArt\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing|PronType=Art\t41\tdet\t_\t_\n",
            "41\tÁtha\táth\tPROPN\tNoun\tCase=Gen|Definite=Def|Gender=Masc|Number=Sing\t39\tnmod\t_\t_\n",
            "42\tBuí\tbuí\tADJ\tAdj\tCase=Nom|Gender=Masc|Number=Sing\t41\tamod\t_\t_\n",
            "43\tdeich\tdeich\tNUM\tNum\tNumType=Card\t44\tnummod\t_\t_\n",
            "44\tmbliana\tbliain\tNOUN\tNoun\tCase=Nom|Form=Ecl|Gender=Fem|Number=Plur\t29\tobl:tmod\t_\t_\n",
            "45\tagus\tagus\tCCONJ\tCoord\t_\t46\tcc\t_\t_\n",
            "46\tdaichead\tdaichead\tNUM\tNum\tNumType=Card\t44\tconj\t_\t_\n",
            "47\troimis\troimh\tADP\tPrep\tGender=Masc|Number=Sing|Person=3\t48\tcase\t_\t_\n",
            "48\tsin\tsin\tPRON\tDem\tPronType=Dem\t46\tnmod\t_\tSpaceAfter=No\n",
            "49\t.\t.\tPUNCT\t.\t_\t2\tpunct\t_\t_\n",
            "\n"
          ]
        }
      ]
    }
  ]
}