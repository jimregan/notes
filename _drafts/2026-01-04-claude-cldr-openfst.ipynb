{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04314e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import pynini\n",
    "from pynini import Fst\n",
    "from pynini.lib import utf8\n",
    "\n",
    "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
    "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
    "_ESCAPED = re.compile(r\"\\\\([][\\\\/\\-^(){}_.*+?|])\")\n",
    "\n",
    "def _decode_escapes(s: str) -> str:\n",
    "    \"\"\"Decodes CLDR-style escapes.\"\"\"\n",
    "    def rpl4(m): return chr(int(m.group(1), 16))\n",
    "    def rpl8(m): return chr(int(m.group(1), 16))\n",
    "    s = _U_HEX.sub(rpl4, s)\n",
    "    s = _U_HEX_LONG.sub(rpl8, s)\n",
    "    s = _ESCAPED.sub(lambda m: m.group(1), s)\n",
    "    return s\n",
    "\n",
    "def _char_range(a: str, b: str) -> List[str]:\n",
    "    return [chr(cp) for cp in range(ord(a), ord(b) + 1)]\n",
    "\n",
    "def _parse_unicode_set(text: str) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"Simple parser for CLDR-style character sets like [a-z] or [^abc].\"\"\"\n",
    "    assert text.startswith(\"[\") and text.endswith(\"]\"), \"not a set\"\n",
    "    inner = text[1:-1]\n",
    "    neg = inner.startswith(\"^\")\n",
    "    if neg: inner = inner[1:]\n",
    "    items: List[str] = []\n",
    "    i = 0\n",
    "    def read_char(ix: int) -> Tuple[str, int]:\n",
    "        if ix < len(inner) and inner[ix] == \"\\\\\":\n",
    "            m4 = _U_HEX.match(inner, ix)\n",
    "            if m4: return (chr(int(m4.group(1), 16)), m4.end())\n",
    "            m8 = _U_HEX_LONG.match(inner, ix)\n",
    "            if m8: return (chr(int(m8.group(1), 16)), m8.end())\n",
    "            if ix + 1 < len(inner): return (inner[ix + 1], ix + 2)\n",
    "            return (\"\\\\\", ix + 1)\n",
    "        return (inner[ix], ix + 1)\n",
    "    while i < len(inner):\n",
    "        c1, j = read_char(i)\n",
    "        if j < len(inner) - 1 and inner[j] == \"-\" and j + 1 < len(inner):\n",
    "            c2, k = read_char(j + 1)\n",
    "            items.extend(_char_range(c1, c2))\n",
    "            i = k\n",
    "        else:\n",
    "            items.append(c1)\n",
    "            i = j\n",
    "    return (neg, items)\n",
    "\n",
    "def _acceptor(s: str) -> Fst:\n",
    "    return pynini.accep(s, token_type=\"utf8\")\n",
    "\n",
    "def _transducer(inp: str, out: str) -> Fst:\n",
    "    return pynini.cross(pynini.accep(inp, token_type=\"utf8\"), \n",
    "                        pynini.accep(out, token_type=\"utf8\"))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Rule:\n",
    "    lhs: str\n",
    "    rhs: str\n",
    "    op: str\n",
    "    left: Optional[str] = None\n",
    "    right: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Directive:\n",
    "    kind: str\n",
    "    payload: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class VarAssign:\n",
    "    name: str\n",
    "    expr: str\n",
    "\n",
    "Line = Tuple[str, object]\n",
    "\n",
    "_RULE_CTX_RE = re.compile(r\"\"\"\n",
    "    ^\\s*\n",
    "    (?P<lhs>.+?)\n",
    "    \\s*\n",
    "    (?P<op><>|>|<)\n",
    "    \\s*\n",
    "    (?P<rhs>.+?)\n",
    "    (?:\n",
    "        /\n",
    "        \\s*\n",
    "        (?P<L>.*?)\n",
    "        \\s*\n",
    "        _\n",
    "        \\s*\n",
    "        (?P<R>.*?)\n",
    "    )?\n",
    "    \\s*;\n",
    "    \\s*$\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "_VAR_RE = re.compile(r\"\"\"\n",
    "    ^\\s*\n",
    "    (?P<name>\\$[A-Za-z0-9_]+)\n",
    "    \\s*=\\s*\n",
    "    (?P<expr>.+?)\n",
    "    \\s*;?\\s*$\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "def _parse_line(line: str) -> Optional[Line]:\n",
    "    s = line.strip()\n",
    "    if not s or s.startswith(\"#\"):\n",
    "        return None\n",
    "    \n",
    "    mvar = _VAR_RE.match(s)\n",
    "    if mvar and mvar.group(\"expr\").strip():\n",
    "        name = mvar.group(\"name\")\n",
    "        expr = mvar.group(\"expr\").strip().rstrip(';')\n",
    "        return (\"var\", VarAssign(name=name, expr=expr))\n",
    "    \n",
    "    if s.startswith(\"::\"):\n",
    "        body = s[2:].strip()\n",
    "        if body.lower() == \"null;\": return (\"dir\", Directive(kind=\"null\"))\n",
    "        if body.lower() == \"nfd;\": return (\"dir\", Directive(kind=\"nfd\"))\n",
    "        if body.lower() == \"nfc;\": return (\"dir\", Directive(kind=\"nfc\"))\n",
    "        if body.startswith(\"[\") and body.endswith(\";\"):\n",
    "            payload = body[:-1].strip()\n",
    "            return (\"dir\", Directive(kind=\"filter\", payload=payload))\n",
    "        return (\"dir\", Directive(kind=\"unknown\", payload=body))\n",
    "    \n",
    "    m = _RULE_CTX_RE.match(s)\n",
    "    if m:\n",
    "        L_ctx = m.group(\"L\")\n",
    "        R_ctx = m.group(\"R\")\n",
    "        return (\"rule\", Rule(\n",
    "            lhs=m.group(\"lhs\").strip(),\n",
    "            rhs=m.group(\"rhs\").strip(),\n",
    "            op=m.group(\"op\"),\n",
    "            left=(L_ctx.strip() if L_ctx else None),\n",
    "            right=(R_ctx.strip() if R_ctx else None),\n",
    "        ))\n",
    "    return None\n",
    "\n",
    "def parse_cldr(text: str) -> List[Line]:\n",
    "    out: List[Line] = []\n",
    "    for raw in text.splitlines():\n",
    "        p = _parse_line(raw)\n",
    "        if p: out.append(p)\n",
    "    return out\n",
    "\n",
    "class Env:\n",
    "    def __init__(self) -> None:\n",
    "        self.vars: Dict[str, Fst] = {}\n",
    "        # Acceptor for all valid UTF-8 characters\n",
    "        self.sigma: Fst = utf8.VALID_UTF8_CHAR.optimize()\n",
    "        # Acceptor closure\n",
    "        self.sigma_star: Fst = self.sigma.closure().optimize()\n",
    "        \n",
    "        # Identity Transducer for all valid UTF-8 characters\n",
    "        I_char = pynini.cross(self.sigma, self.sigma)\n",
    "        self.I_sigma_star: Fst = I_char.closure().optimize()\n",
    "\n",
    "    def get(self, name: str) -> Fst:\n",
    "        if name not in self.vars:\n",
    "            raise KeyError(f\"Undefined variable {name}\")\n",
    "        return self.vars[name]\n",
    "\n",
    "def _compile_atom(expr: str, env: Env) -> Fst:\n",
    "    expr = expr.strip()\n",
    "    if expr.startswith(\"$\"):\n",
    "        return env.get(expr)\n",
    "    if expr.startswith(\"[\") and expr.endswith(\"]\"):\n",
    "        neg, items = _parse_unicode_set(expr)\n",
    "        if not items: \n",
    "            return env.sigma if neg else pynini.Fst()\n",
    "        u = pynini.union(*(_acceptor(ch) for ch in items)).optimize()\n",
    "        if neg:\n",
    "            return (env.sigma - u).optimize()\n",
    "        return u\n",
    "\n",
    "    return _acceptor(_decode_escapes(expr))\n",
    "\n",
    "def _compile_seq(expr: Optional[str], env: Env) -> Fst:\n",
    "    \"\"\"Compiles a sequence of atoms (literals, sets, variables) into an Fst acceptor.\"\"\"\n",
    "    if not expr: return _acceptor(\"\")\n",
    "    s = expr\n",
    "    parts: List[str] = []\n",
    "    i = 0\n",
    "    cur = []\n",
    "    depth = 0\n",
    "    \n",
    "    # Simple tokenizer logic\n",
    "    while i < len(s):\n",
    "        ch = s[i]\n",
    "        is_escape_char = (ch == '\\\\') and (i + 1 < len(s) and s[i+1].isalpha())\n",
    "        \n",
    "        if ch == \"[\" and not is_escape_char: depth += 1\n",
    "        elif ch == \"]\" and depth > 0 and not is_escape_char: depth -= 1\n",
    "        elif ch.isspace() and depth == 0:\n",
    "            if cur: parts.append(\"\".join(cur)); cur = []\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        cur.append(ch)\n",
    "        i += 1\n",
    "        if is_escape_char and i < len(s):\n",
    "            pass \n",
    "            \n",
    "    if cur: parts.append(\"\".join(cur))\n",
    "    if not parts: return _acceptor(\"\")\n",
    "\n",
    "    # Composition of FST atoms\n",
    "    fst = _compile_atom(parts[0], env)\n",
    "    for p in parts[1:]:\n",
    "        fst = fst + _compile_atom(p, env)\n",
    "    return fst.optimize()\n",
    "\n",
    "def compile_lines(lines: List[Line]) -> Fst:\n",
    "    env = Env()\n",
    "    \n",
    "    vars_to_compile: List[VarAssign] = []\n",
    "    for kind, payload in lines:\n",
    "        if kind == \"var\":\n",
    "            vars_to_compile.append(payload) # type: ignore[arg-type]\n",
    "\n",
    "    for va in vars_to_compile:\n",
    "        env.vars[va.name] = _compile_seq(va.expr, env).optimize()\n",
    "    \n",
    "    all_rules: List[Rule] = []\n",
    "    for kind, payload in lines:\n",
    "        if kind == \"rule\":\n",
    "            all_rules.append(payload) # type: ignore[arg-type]\n",
    "\n",
    "    # Start with identity transducer\n",
    "    cascade = env.I_sigma_star.copy()\n",
    "    \n",
    "    for r in all_rules:\n",
    "        lhs = _compile_seq(r.lhs, env)\n",
    "        rhs = _compile_seq(r.rhs, env)\n",
    "        \n",
    "        Lctx = _compile_seq(r.left, env) if r.left else pynini.accep(\"\", token_type=\"utf8\")\n",
    "        Rctx = _compile_seq(r.right, env) if r.right else pynini.accep(\"\", token_type=\"utf8\")\n",
    "        \n",
    "        if lhs.num_states() == 0: \n",
    "            print(f\"Warning: Empty LHS FST for rule {r.lhs} > {r.rhs}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract strings and rebuild with proper token types\n",
    "        try:\n",
    "            lhs_str = lhs.string(token_type=\"utf8\")\n",
    "            if isinstance(lhs_str, bytes):\n",
    "                lhs_str = lhs_str.decode('utf-8')\n",
    "        except:\n",
    "            print(f\"Warning: Could not extract LHS string for rule {r.lhs}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            rhs_str = rhs.string(token_type=\"utf8\")\n",
    "            if isinstance(rhs_str, bytes):\n",
    "                rhs_str = rhs_str.decode('utf-8')\n",
    "        except:\n",
    "            print(f\"Warning: Could not extract RHS string for rule {r.rhs}\")\n",
    "            continue\n",
    "        \n",
    "        # Create fresh acceptors with UTF-8 token type\n",
    "        lhs_new = pynini.accep(lhs_str, token_type=\"utf8\")\n",
    "        rhs_new = pynini.accep(rhs_str, token_type=\"utf8\")\n",
    "        \n",
    "        # Build tau with cross product\n",
    "        tau = pynini.cross(lhs_new, rhs_new)\n",
    "        \n",
    "        if r.op == \">\":\n",
    "            rewrite = pynini.cdrewrite(tau, Lctx, Rctx, env.sigma_star)\n",
    "            cascade = pynini.compose(cascade, rewrite)\n",
    "        elif r.op == \"<\":\n",
    "            rewrite = pynini.cdrewrite(tau.invert(), Rctx, Lctx, env.sigma_star)\n",
    "            cascade = pynini.compose(cascade, rewrite)\n",
    "        elif r.op == \"<>\":\n",
    "            rewrite1 = pynini.cdrewrite(tau, Lctx, Rctx, env.sigma_star)\n",
    "            cascade = pynini.compose(cascade, rewrite1)\n",
    "            rewrite2 = pynini.cdrewrite(tau.invert(), Rctx, Lctx, env.sigma_star)\n",
    "            cascade = pynini.compose(cascade, rewrite2)\n",
    "\n",
    "    # Optimize only once at the very end, more carefully\n",
    "    cascade.rmepsilon()\n",
    "    return cascade\n",
    "\n",
    "def prefix_language_token(lang_token: str) -> Fst:\n",
    "    # Maps <lang_token> (input) to \"\" (output), deleting the prefix.\n",
    "    return pynini.cross(_acceptor(lang_token), _acceptor(\"\")) \n",
    "\n",
    "def build_multilingual_transducer(lang_to_rules_text: Dict[str, str], token_fmt: str = \"<{lang}>\") -> Fst:\n",
    "    unified: Optional[Fst] = None\n",
    "    for lang, text in lang_to_rules_text.items():\n",
    "        lines = parse_cldr(text)\n",
    "        t = compile_lines(lines)\n",
    "        # Sequence: [ <lang> : \"\" ] . [ rules_for_lang ]\n",
    "        lt = pynini.concat(prefix_language_token(token_fmt.format(lang=lang)), t)\n",
    "        lt = lt.optimize()\n",
    "        unified = lt if unified is None else pynini.union(unified, lt)\n",
    "    if unified is None:\n",
    "        unified = pynini.transducer(\"\", \"\")\n",
    "    unified = unified.optimize()\n",
    "    return unified\n",
    "\n",
    "def save_fst(fst: Fst, path: str) -> None:\n",
    "    fst.write(path)\n",
    "\n",
    "def demo():\n",
    "    cldr_en = r\"\"\"\n",
    "        $V = [aeiou] ; \n",
    "        th > θ ;\n",
    "        sh > ʃ ;\n",
    "        ng > ŋ ;\n",
    "        ch > t͡ʃ ;\n",
    "        c > s / _ [ei] ; \n",
    "        $V > a ;\n",
    "    \"\"\"\n",
    "\n",
    "    cldr_es = r\"\"\"\n",
    "        $V = [aeiou] ;\n",
    "        ll > ʎ ;\n",
    "        ñ > ɲ ;\n",
    "        qu > k ;\n",
    "        c > s / _ [ei] ; \n",
    "        c > k ;\n",
    "        z > s ;\n",
    "        $V > a ;\n",
    "    \"\"\"\n",
    "\n",
    "    lang_rules = {\"eng\": cldr_en, \"spa\": cldr_es}\n",
    "\n",
    "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
    "    save_fst(fst, \"multilang.fst\")\n",
    "\n",
    "    tests = [\n",
    "        \"<eng>thing\", \n",
    "        \"<eng>mashing\", \n",
    "        \"<spa>llama\", \n",
    "        \"<spa>quiza\", \n",
    "        \"<spa>cita\", \n",
    "        \"<spa>cuna\"\n",
    "    ]\n",
    "\n",
    "    def apply(s: str) -> str:\n",
    "        input_fst = _acceptor(s)\n",
    "        lat = pynini.compose(input_fst, fst)\n",
    "        \n",
    "        if lat.start() == pynini.NO_STATE_ID:\n",
    "            return \"<no-path>\"\n",
    "\n",
    "        # Get shortest path\n",
    "        shortest = pynini.shortestpath(lat)\n",
    "        \n",
    "        # Project to output and extract\n",
    "        output_only = shortest.project(\"output\")\n",
    "        output_only.rmepsilon()\n",
    "        \n",
    "        # Try the simple string extraction\n",
    "        try:\n",
    "            result = output_only.string(token_type=\"utf8\")\n",
    "            if isinstance(result, bytes):\n",
    "                return result.decode('utf-8')\n",
    "            return str(result)\n",
    "        except:\n",
    "            # Fallback to stringify\n",
    "            full_str = pynini.stringify(shortest, token_type=\"utf8\")\n",
    "            if \" : \" in full_str:\n",
    "                return full_str.split(\" : \", 1)[1]\n",
    "            return full_str\n",
    "\n",
    "    for t in tests:\n",
    "        print(f\"{t} -> {apply(t)}\")\n",
    "        \n",
    "    print(\"\\nWrote multilang.fst\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
