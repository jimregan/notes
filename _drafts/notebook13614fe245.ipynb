{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":270049783,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle-ready script: obvious misclassifications in large lexicon files\n# ---------------------------------------------------------------\n# Focus: skip catch-all buckets, analyze files with >= MIN_ENTRIES (default: 1000)\n# Output:\n#   - braxen_obvious_misclassifications_filtered.csv          (detailed flags)\n#   - braxen_obvious_misclassifications_per_file_summary.csv  (per-file counts)\n#   - braxen_obvious_misclassifications_samples.csv           (top N examples / file)\n#\n# Assumptions:\n#   - You uploaded `braxen.zip` in the notebook working directory\n#   - Files inside the zip are UTF-8 (we read with errors=\"ignore\" to be robust)\n\nimport os, zipfile, re, unicodedata\nfrom collections import defaultdict, Counter\nimport pandas as pd\n\n# -------------------------\n# CONFIG\n# -------------------------\nZIP_PATH = \"braxen.zip\"   # change if needed\nEXTRACT_DIR = \"braxen_extracted\"\nMIN_ENTRIES = 1000\nSKIP_CODES = {\n    # Skip \"catch-all\" / broad buckets\n    \"afr\",\"asi\",\"aus\",\"sla\",\"mix\",\"rom\",  # adjust if \"rom\" is *specific* in your data\n    # Add any others you want to ignore:\n    # \"fisa\", ...\n}\n# How many example rows per file to keep in the samples CSV\nSAMPLES_PER_FILE = 50\n\n# -------------------------\n# Helpers: script checks (no external 'regex' dependency)\n# -------------------------\ndef has_cyrillic(s: str) -> bool:\n    for ch in s:\n        o = ord(ch)\n        # Cyrillic blocks: 0400–04FF, 0500–052F, 2DE0–2DFF (Cyrillic Extended-A), A640–A69F (Ext-B)\n        if (0x0400 <= o <= 0x04FF) or (0x0500 <= o <= 0x052F) or (0x2DE0 <= o <= 0x2DFF) or (0xA640 <= o <= 0xA69F):\n            return True\n    return False\n\ndef has_greek(s: str) -> bool:\n    for ch in s:\n        o = ord(ch)\n        # Greek: 0370–03FF; Greek Extended: 1F00–1FFF\n        if (0x0370 <= o <= 0x03FF) or (0x1F00 <= o <= 0x1FFF):\n            return True\n    return False\n\ndef has_han(s: str) -> bool:\n    for ch in s:\n        o = ord(ch)\n        # CJK Unified Ideographs (basic range). (Extensions omitted intentionally)\n        if 0x4E00 <= o <= 0x9FFF:\n            return True\n    return False\n\n# -------------------------\n# Diacritic inventories (distinctive)\n# -------------------------\nDIACRITICS = {\n    \"pol\": set(\"ąćęłńóśźżĄĆĘŁŃÓŚŹŻ\"),\n    \"cze\": set(\"áéíóúýčďěňřšťůžÁÉÍÓÚÝČĎĚŇŘŠŤŮŽ\"),\n    \"slk\": set(\"áäčďéíľĺňóôŕšťúýžÁÄČĎÉÍĽĹŇÓÔŔŠŤÚÝŽ\"),\n    \"slv\": set(\"čšžČŠŽ\"),\n    \"hrv\": set(\"čćđšžČĆĐŠŽ\"),\n    \"srp\": set(\"čćđšžČĆĐŠŽ\"),  # Latin Serbian\n    \"rom\": set(\"ăâîșţșțĂÂÎȘŢȚ\"),  # include both ş/ţ and ș/ț usage\n    \"hun\": set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\"),\n    \"tur\": set(\"çğıöşüÇĞİÖŞÜ\"),\n    \"lit\": set(\"ąčęėįšųūžĄČĘĖĮŠŲŪŽ\"),\n    \"lav\": set(\"āčēģīķļņšūžĀČĒĢĪĶĻŅŠŪŽ\"),\n    \"deu\": set(\"äöüßÄÖÜ\"),\n    \"fra\": set(\"àâæçéèêëîïôœùûüÿÀÂÆÇÉÈÊËÎÏÔŒÙÛÜŸ\"),\n    \"spa\": set(\"áéíñóúüÁÉÍÑÓÚÜ\"),\n    \"por\": set(\"áâãàçéêíóôõúÁÂÃÀÇÉÊÍÓÔÕÚ\"),\n    \"isl\": set(\"áéíóúýðþæöÁÉÍÓÚÝÐÞÆÖ\"),\n}\n\ndef file_code_from_name(fname: str) -> str:\n    # e.g., \"braxen-cze.txt\" -> \"cze\"\n    base = os.path.basename(fname)\n    if base.startswith(\"braxen-\") and base.endswith(\".txt\"):\n        return base[len(\"braxen-\"):-len(\".txt\")]\n    return base\n\ndef read_text_safely(path):\n    with open(path, \"rb\") as f:\n        data = f.read()\n    # Try UTF-8; fall back to \"ignore\" to keep going\n    try:\n        return data.decode(\"utf-8\")\n    except UnicodeDecodeError:\n        return data.decode(\"utf-8\", errors=\"ignore\")\n\nWORD_RE = re.compile(r\"[^\\W\\d_][\\w’'--]*\", flags=re.UNICODE)\n\ndef tokenize_words(text: str):\n    # Words starting with a letter; keeps diacritics and connector punctuation\n    return WORD_RE.findall(text)\n\ndef detect_diacritic_langs(word: str):\n    hits = []\n    for code, chars in DIACRITICS.items():\n        if any(ch in word for ch in chars):\n            hits.append(code)\n    return hits\n\n# Special-case checks (lightweight and obvious)\ndef is_bulgarian_like_cyrillic(word: str) -> bool:\n    w = word.lower()\n    # Bulgarian hallmark: 'ъ' vowel, and definite article suffixes\n    if \"ъ\" in w:\n        return True\n    if (w.endswith((\"ът\",\"та\",\"то\",\"те\")) and (\"ь\" not in w)) and (\"ы\" not in w and \"ё\" not in w):\n        return True\n    return False\n\n# Czech-ish diacritics present?\nCZE_DIACS = DIACRITICS[\"cze\"]\n\n# -------------------------\n# 1) Extract the archive\n# -------------------------\nif not os.path.exists(EXTRACT_DIR):\n    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n        z.extractall(EXTRACT_DIR)\n\nfiles = [os.path.join(EXTRACT_DIR, f) for f in os.listdir(EXTRACT_DIR)\n         if f.startswith(\"braxen-\") and f.endswith(\".txt\")]\n\n# -------------------------\n# 2) Count entries per file (unique tokens)\n# -------------------------\nfile_word_sets = {}\nfile_sizes = {}\nfor path in files:\n    code = file_code_from_name(path)\n    text = read_text_safely(path)\n    words = tokenize_words(text)\n    uniq = set(words)\n    file_word_sets[path] = uniq\n    file_sizes[path] = len(uniq)\n\n# Filter to large, non-catch-all files\ncandidate_paths = []\nfor p, n in file_sizes.items():\n    code = file_code_from_name(p)\n    if n >= MIN_ENTRIES and code not in SKIP_CODES:\n        candidate_paths.append(p)\n\nprint(f\"Found {len(candidate_paths)} large non-catch-all files (>= {MIN_ENTRIES} entries).\")\n\n# -------------------------\n# 3) Scan candidates for obvious mismatches\n# -------------------------\nrecords = []\nper_file_flags = defaultdict(lambda: defaultdict(int))\n\ndef add_record(word, current_code, suggested, reason):\n    records.append((word, current_code, suggested, reason))\n    per_file_flags[current_code][reason] += 1\n\nfor path in candidate_paths:\n    code = file_code_from_name(path)\n    uniq_words = file_word_sets[path]\n\n    # a) Script mismatches: Greek/Cyrillic where they don't belong\n    for w in uniq_words:\n        if has_greek(w) and code != \"gre\":\n            add_record(w, code, \"gre\", \"Greek script in non-Greek file\")\n        elif has_cyrillic(w) and code not in {\"rus\",\"ukr\",\"bul\",\"mkd\",\"srp\"}:\n            add_record(w, code, \"cyrillic?\", \"Cyrillic characters in non-Cyrillic file\")\n\n    # b) Diacritic-driven mismatches (high-confidence when unambiguous)\n    for w in uniq_words:\n        # Only consider words containing non-ASCII chars (skip plain ASCII names)\n        if all(ord(ch) < 128 for ch in w):\n            continue\n        langs = detect_diacritic_langs(w)\n        if langs and code not in langs:\n            suggested = langs[0] if len(langs) == 1 else \"ambiguous(\" + \",\".join(sorted(langs)) + \")\"\n            add_record(w, code, suggested, f\"Contains diacritics typical of {','.join(sorted(langs))}\")\n\n    # c) Special fixes (based on your notes)\n\n    #   - Lithuanian/Latvian that landed in LAT\n    if code == \"lat\":\n        for w in uniq_words:\n            if any(ch in DIACRITICS[\"lit\"] for ch in w):\n                add_record(w, code, \"lit\", \"Lithuanian diacritics in LAT\")\n            elif any(ch in DIACRITICS[\"lav\"] for ch in w):\n                add_record(w, code, \"lav\", \"Latvian diacritics in LAT\")\n\n    #   - Bulgarian words that landed in RUS\n    if code == \"rus\":\n        for w in uniq_words:\n            if has_cyrillic(w) and is_bulgarian_like_cyrillic(w):\n                add_record(w, code, \"bul\", \"Bulgarian hard vowel/definite article in RUS\")\n\n    #   - Czech diacritics that landed in POL\n    if code == \"pol\":\n        for w in uniq_words:\n            if any(ch in CZE_DIACS for ch in w):\n                add_record(w, code, \"cze\", \"Czech diacritics in POL\")\n\n    #   - Czech diacritics that landed in CHI (Chinese)\n    if code == \"chi\":\n        for w in uniq_words:\n            if any(ch in CZE_DIACS for ch in w) and w not in {\"Zhéng\"}:  # leave Zhéng (pinyin tone) alone\n                add_record(w, code, \"cze\", \"Czech diacritics in CHI\")\n\n# -------------------------\n# 4) Save outputs\n# -------------------------\ndetailed_df = pd.DataFrame(records, columns=[\"word\", \"current_code\", \"suggested_code\", \"reason\"]).drop_duplicates()\ndetailed_df.to_csv(\"braxen_obvious_misclassifications_filtered.csv\", index=False, encoding=\"utf-8\")\n\nsummary_rows = []\nfor code, counters in per_file_flags.items():\n    total = sum(counters.values())\n    row = {\"file_code\": code, \"total_flags\": total}\n    row.update(counters)\n    summary_rows.append(row)\nsummary_df = pd.DataFrame(summary_rows).sort_values(\"total_flags\", ascending=False)\nsummary_df.to_csv(\"braxen_obvious_misclassifications_per_file_summary.csv\", index=False, encoding=\"utf-8\")\n\n# Optional: provide compact samples per file for quick eyeballing\nsample_rows = []\nif not detailed_df.empty:\n    for code, sub in detailed_df.groupby(\"current_code\"):\n        sample = sub.head(SAMPLES_PER_FILE)\n        sample_rows.append(sample)\n    samples_df = pd.concat(sample_rows, ignore_index=True)\n    samples_df.to_csv(\"braxen_obvious_misclassifications_samples.csv\", index=False, encoding=\"utf-8\")\nelse:\n    # Create an empty file so it's obvious no flags were found\n    pd.DataFrame(columns=[\"word\",\"current_code\",\"suggested_code\",\"reason\"]).to_csv(\n        \"braxen_obvious_misclassifications_samples.csv\", index=False, encoding=\"utf-8\"\n    )\n\nprint(\"Wrote:\")\nprint(\" - braxen_obvious_misclassifications_filtered.csv\")\nprint(\" - braxen_obvious_misclassifications_per_file_summary.csv\")\nprint(\" - braxen_obvious_misclassifications_samples.csv\")\n\n# Show quick summary preview\ndisplay(summary_df.head(20))\ndisplay(detailed_df.head(50))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}