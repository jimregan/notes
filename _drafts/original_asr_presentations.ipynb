{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9d09aa-5bed-4c13-9917-e214581a26a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from notebook.services.config import ConfigManager\n",
    "# cm = ConfigManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a68036-8314-4073-a129-94015670f091",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from manim import *\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b8362",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DT2112\n",
    "\n",
    "# Automatic Speech Recognition\n",
    "\n",
    "Jim O'Regan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71adcb20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Motivation for Automatic Speech Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1c6d5-f0de-48fd-a9cf-53dd18771ede",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Motivation\n",
    "\n",
    "- Natural communication\n",
    "- Hands-free\n",
    "- Fast(er than typing)\n",
    "- Natural input method for phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d162e41-a286-4d5b-95a0-6048a7125818",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Motivation\n",
    "\n",
    "- Search\n",
    "- Language learning (pronunciation training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5102b55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ASR in a Broader Context\n",
    "\n",
    "Dialogue Manager\n",
    "\n",
    "Automatic Speech Recognition\n",
    "\n",
    "Spoken Language Understanding\n",
    "\n",
    "Text to Speech\n",
    "\n",
    "5 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd809f3",
   "metadata": {},
   "source": [
    "# The ASR Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7a713",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The ASR Scope\n",
    "\n",
    "Convert speech into text\n",
    "\n",
    "\n",
    "\n",
    "Automatic Speech Recognition\n",
    "\n",
    "“My name is . . . ” [confidence score]\n",
    "\n",
    "Not considered here: I non-verbal signals I prosody\n",
    "I multi-modal interaction\n",
    "\n",
    "6 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6848c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# A very long endeavour\n",
    "1952, Bell laboratories, isolated digit recognition, single speaker, hardware based [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82a359",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "A very long endeavour\n",
    "1952, Bell laboratories, isolated digit recognition, single speaker, hardware based [2]\n",
    "\n",
    "[2] K. H. Davis, R. Biddulph, and S. Balashek. “Automatic Recognition of Spoken Digits”. In: 24.6 (1952), pp. 637–642\n",
    "\n",
    "7 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca761bd",
   "metadata": {},
   "source": [
    "# 8 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482b0ef",
   "metadata": {},
   "source": [
    "8 / 112\n",
    "\n",
    "Historical Perspective\n",
    "\n",
    "I 1950’s Bell lab: 10 digits, 1 speaker\n",
    "I 1960’s IBM: 16 words\n",
    "I 1970’s Large investments from DARPA\n",
    "I CMU Harpy: 1011 words (beam search) I Threshold Technology: first ASR company I Bell labs: multiple voices\n",
    "I 1980’s\n",
    "I from template matching to probabilistic models (Hidden Markov Models)\n",
    "I from hundreds to thousands of words\n",
    "I 1990’s Dragon Dictate and later Dragon NaturallySpeaking\n",
    "I 2000’s no big improvements\n",
    "I 2010’s Google, Apple, Microsoft, Amazon get heavily involved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec6c2",
   "metadata": {},
   "source": [
    "# http://www.itl.nist.gov/iad/mig/publications/ASRhistory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89209b7",
   "metadata": {},
   "source": [
    "http://www.itl.nist.gov/iad/mig/publications/ASRhistory/\n",
    "\n",
    "9 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddefc49",
   "metadata": {},
   "source": [
    "# Loud and clear\n",
    "Speech-recognition word-error rate, selected benchmarks, %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307aa1c",
   "metadata": {},
   "source": [
    "Loud and clear\n",
    "Speech-recognition word-error rate, selected benchmarks, %\n",
    "\n",
    "Switchboard\n",
    "\n",
    "Broadcast\n",
    "speech\n",
    "\n",
    "Switchboard cellular\n",
    "\n",
    "The Switchboard corpus is a collection of recorded te\\ephone comers ations wide\\y used to train and test speech-recognition systems\n",
    "\n",
    "Meeting speech\n",
    "\n",
    "IBM, Switchboard\n",
    "\n",
    "02\n",
    "\n",
    "04\n",
    "\n",
    "06\n",
    "\n",
    "08\n",
    "\n",
    "10\n",
    "\n",
    "12\n",
    "\n",
    "14\n",
    "\n",
    "16\n",
    "\n",
    "Log scoJe\n",
    "]00\n",
    "\n",
    "10\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510016a4",
   "metadata": {},
   "source": [
    "# 10 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3265ad",
   "metadata": {},
   "source": [
    "10 / 112\n",
    "\n",
    "Main variables in ASR\n",
    "\n",
    "Speaking mode isolated words vs continuous speech Speaking style read speech vs spontaneous speech\n",
    "Speakers speaker dependent vs speaker independent Vocabulary small (<20 words) vs large (>50 000 words) Robustness against background noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15538b0c",
   "metadata": {},
   "source": [
    "# 11 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8723338",
   "metadata": {},
   "source": [
    "11 / 112\n",
    "\n",
    "Challenges — Variability\n",
    "\n",
    "Between speakers\n",
    "I Age\n",
    "I Gender I Anatomy I Dialect\n",
    "Within speaker\n",
    "I Stress\n",
    "I Emotion\n",
    "I Health condition\n",
    "I Read vs Spontaneous\n",
    "I Adaptation to environment (Lombard effect)\n",
    "I Adaptation to listener\n",
    "\n",
    "Environment\n",
    "I Noise\n",
    "I Room acoustics\n",
    "I Microphone distance I Microphone, telephone I Bandwidth\n",
    "Listener\n",
    "I Age\n",
    "I Mother tongue\n",
    "I Hearing loss\n",
    "I Known / unknown\n",
    "I Human / Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fb969",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Components of ASR System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6c059",
   "metadata": {},
   "source": [
    "Components of ASR System\n",
    "\n",
    "Speech Signal\n",
    "\n",
    "Spectral Analysis\n",
    "\n",
    "Feature Extraction\n",
    "\n",
    "Search and Match\n",
    "\n",
    "Recognised Words\n",
    "\n",
    "Acoustic Models\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "Language Models\n",
    "\n",
    "\n",
    "\n",
    "Representation\n",
    "\n",
    "Constraints - Knowledge\n",
    "\n",
    "\n",
    "\n",
    "Decoder\n",
    "\n",
    "14 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72de8ae",
   "metadata": {},
   "source": [
    "# Speech Signal Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1bcc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Speech Signal Representations\n",
    "\n",
    "Speech Signal\n",
    "\n",
    "Spectral Analysis\n",
    "\n",
    "Feature Extraction\n",
    "\n",
    "Representation\n",
    "\n",
    "16 / 112\n",
    "\n",
    "Goals:\n",
    "\n",
    "I disregard irrelevant information\n",
    "I optimise relevant information for modelling Means:\n",
    "I try to model essential aspects of speech production\n",
    "I imitate auditory processes\n",
    "I consider properties of statistical modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb609854",
   "metadata": {},
   "source": [
    "# Feature Extraction and Speech Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3fdd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Feature Extraction and Speech Production\n",
    "\n",
    "Soft palate\n",
    "\n",
    "(velum)\n",
    "\n",
    "Hard palate\n",
    "\n",
    "Lung\n",
    "\n",
    "Nasal cavity Nostril\n",
    "Lip Tongue\n",
    "Teeth\n",
    "Oral cavity Jaw Trachea\n",
    "\n",
    "Diaphragm\n",
    "\n",
    "Pharyngeal\n",
    "cavity\n",
    "Larynx\n",
    "Oesophagus\n",
    "\n",
    "Trachea\n",
    "\n",
    "Muscle Force and Relaxation\n",
    "\n",
    "Lungs\n",
    "\n",
    "Vocal Folds\n",
    "\n",
    "Glottis\n",
    "\n",
    "Pharyngeal Cavity\n",
    "\n",
    "18 / 112\n",
    "\n",
    "Oral Cavity\n",
    "\n",
    "Nasal Cavity\n",
    "\n",
    "Velum\n",
    "\n",
    "Nose Output\n",
    "\n",
    "Mouth Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb883a",
   "metadata": {},
   "source": [
    "# Source/Filter Model, General Case\n",
    "Vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d140350",
   "metadata": {},
   "source": [
    "Source/Filter Model, General Case\n",
    "Vowels\n",
    "\n",
    "Soft palate\n",
    "\n",
    "(velum)\n",
    "\n",
    "Hard palate\n",
    "\n",
    "Lung\n",
    "\n",
    "Nasal cavity Nostril\n",
    "Lip Tongue\n",
    "Teeth\n",
    "Oral cavity Jaw Trachea\n",
    "\n",
    "Pharyngeal\n",
    "cavity\n",
    "Larynx\n",
    "Oesophagus\n",
    "\n",
    "⇤ Source (periodic)\n",
    "⇤ Front Cavity\n",
    "⇤ Back Cavity\n",
    "\n",
    "⇤ Back Cavity (2nd approx.)\n",
    "\n",
    "Diaphragm\n",
    "\n",
    "19 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec8407",
   "metadata": {},
   "source": [
    "# Source/Filter Model, General Case\n",
    "Fricatives (e.g. sh) or Plosive (e.g. k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de6c87",
   "metadata": {},
   "source": [
    "Source/Filter Model, General Case\n",
    "Fricatives (e.g. sh) or Plosive (e.g. k)\n",
    "\n",
    "Soft palate\n",
    "\n",
    "(velum)\n",
    "\n",
    "Hard palate\n",
    "\n",
    "Lung\n",
    "\n",
    "Nasal cavity Nostril\n",
    "Lip Tongue\n",
    "Teeth\n",
    "Oral cavity Jaw Trachea\n",
    "\n",
    "Pharyngeal\n",
    "cavity\n",
    "Larynx\n",
    "Oesophagus\n",
    "\n",
    "⇤ Source (noise or impulsive)\n",
    "⇤ Front Cavity\n",
    "⇤ Back Cavity\n",
    "\n",
    "⇤ Back Cavity (2nd approx.)\n",
    "\n",
    "Diaphragm\n",
    "\n",
    "19 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8636cbf",
   "metadata": {},
   "source": [
    "# Source/Filter Model, General Case\n",
    "Fricatives (e.g. s) or Plosive (e.g. t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4ef93",
   "metadata": {},
   "source": [
    "Source/Filter Model, General Case\n",
    "Fricatives (e.g. s) or Plosive (e.g. t)\n",
    "\n",
    "Soft palate\n",
    "\n",
    "(velum)\n",
    "\n",
    "Hard palate\n",
    "\n",
    "Lung\n",
    "\n",
    "Nasal cavity Nostril\n",
    "Lip Tongue\n",
    "Teeth\n",
    "Oral cavity Jaw Trachea\n",
    "\n",
    "Pharyngeal\n",
    "cavity\n",
    "Larynx\n",
    "Oesophagus\n",
    "\n",
    "⇤ Source (noise or impulsive)\n",
    "⇤ Front Cavity\n",
    "⇤ Back Cavity\n",
    "\n",
    "⇤ Back Cavity (2nd approx.)\n",
    "\n",
    "Diaphragm\n",
    "\n",
    "19 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82c823",
   "metadata": {},
   "source": [
    "# Source/Filter Model, General Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd88f1",
   "metadata": {},
   "source": [
    "Source/Filter Model, General Case\n",
    "\n",
    "Nasalised Vowels\n",
    "\n",
    "Soft palate\n",
    "\n",
    "(velum)\n",
    "\n",
    "Hard palate\n",
    "\n",
    "Lung\n",
    "\n",
    "Nasal cavity Nostril\n",
    "Lip Tongue\n",
    "Teeth\n",
    "Oral cavity Jaw Trachea\n",
    "\n",
    "Diaphragm\n",
    "\n",
    "Pharyngeal\n",
    "cavity\n",
    "Larynx\n",
    "Oesophagus\n",
    "\n",
    "⇤ Source (periodic)\n",
    "⇤ Front Cavity\n",
    "⇤ Back Cavity\n",
    "⇤ Back Cavity (2nd approx.)\n",
    "\n",
    "19 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57ac1",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2480a",
   "metadata": {},
   "source": [
    "Examples\n",
    "\n",
    "0\n",
    "\n",
    "1\n",
    "\n",
    "2\n",
    "\n",
    "3\n",
    "\n",
    "4\n",
    "frequency (kHz)\n",
    "\n",
    "5\n",
    "\n",
    "6\n",
    "\n",
    "7\n",
    "\n",
    "8\n",
    "\n",
    "dB\n",
    "\n",
    "vocalic sound\n",
    "\n",
    "0\n",
    "\n",
    "1\n",
    "\n",
    "2\n",
    "\n",
    "3\n",
    "\n",
    "4\n",
    "frequency (kHz)\n",
    "\n",
    "5\n",
    "\n",
    "6\n",
    "\n",
    "7\n",
    "\n",
    "8\n",
    "\n",
    "dB\n",
    "\n",
    "fricative sound\n",
    "\n",
    "20 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b0098",
   "metadata": {},
   "source": [
    "# Relevant vs Irrelevant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3d266",
   "metadata": {},
   "source": [
    "Relevant vs Irrelevant Information\n",
    "\n",
    "For the purpose of transcribing words:\n",
    "Relevant: vocal tract shape → spectral envelope\n",
    "Irrelevant: vocal fold vibration frequency (f0) → spectral details\n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "\n",
    "2\n",
    "\n",
    "\n",
    "\n",
    "5\n",
    "\n",
    "\n",
    "\n",
    "6\n",
    "\n",
    "\n",
    "\n",
    "7\n",
    "\n",
    "\n",
    "\n",
    "8\n",
    "\n",
    "3\t4\n",
    "frequency (kHz)\n",
    "\n",
    "21 / 112\n",
    "\n",
    "dB\n",
    "\n",
    "vocalic sound\n",
    "\n",
    "Exceptions:\n",
    "I tonal languages (Chinese)\n",
    "I pitch and prosody convey meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bad27",
   "metadata": {},
   "source": [
    "# Linear Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb304ef",
   "metadata": {},
   "source": [
    "Linear Prediction Analysis\n",
    "\n",
    "Attempt to model the vocal tract filter\n",
    "\n",
    "\n",
    "better match at spectral peaks than valleys\n",
    "\n",
    "22 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc428d6",
   "metadata": {},
   "source": [
    "# 23 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334373d6",
   "metadata": {},
   "source": [
    "23 / 112\n",
    "\n",
    "Mel Frequency Cepstrum Coefficients\n",
    "\n",
    "I imitate aspects of auditory processing\n",
    "I de facto standard in ASR\n",
    "I does not assume all-pole model of the spectrum\n",
    "I uncorrelated: easier to model statistically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241523b",
   "metadata": {},
   "source": [
    "# MFCCs Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7ca63",
   "metadata": {},
   "source": [
    "MFCCs Calculation\n",
    "\n",
    "pre-emph\n",
    "\n",
    "windowing\n",
    "\n",
    "FFT\n",
    "\n",
    "|.|2\n",
    "\n",
    "Mel Filterbank\n",
    "\n",
    "log()\n",
    "\n",
    "0\n",
    "\n",
    "15\n",
    "\n",
    "5\t10\n",
    "filterbank channels\n",
    "\n",
    "40\n",
    "\n",
    "60\n",
    "\n",
    "100\n",
    "\n",
    "80\n",
    "\n",
    "spectrum of /a:/\n",
    "\n",
    "Cosine Transform\n",
    "\n",
    "C1\n",
    "\n",
    "\n",
    "\n",
    "C2\n",
    "\n",
    "\n",
    "\n",
    "C3\n",
    "\n",
    "\n",
    "\n",
    "C4\n",
    "\n",
    "\n",
    "\n",
    "−20\n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "\n",
    "20\n",
    "\n",
    "\n",
    "\n",
    "40\n",
    "\n",
    "cepstrum of /a:/\n",
    "\n",
    "24 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce572c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Cosine Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58644af3-e2c2-48b5-baba-b792e359ba1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.19.0</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m19.0\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/CosineTransformIllustration@2025-01-29@20-17-25.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim -qm -v WARNING CosineTransformIllustration\n",
    "\n",
    "class CosineTransformIllustration(Scene):\n",
    "    def construct(self):\n",
    "        # Phase 1: Display the formula and input signal spectrum\n",
    "        title = Text(\"Cosine Transform\", font_size=48).to_edge(UP)\n",
    "        self.play(Write(title))\n",
    "\n",
    "        formula = MathTex(\n",
    "            r\"C_j = \\sqrt{\\frac{2}{N}} \\sum_{i=1}^{N} A_i \\cos\\left(\\frac{j\\pi (i - 0.5)}{N}\\right)\",\n",
    "            font_size=36\n",
    "        ).next_to(title, DOWN, buff=0.5)\n",
    "        self.play(Write(formula))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Input signal spectrum visualization\n",
    "        ai_graph_label = Tex(r\"$A_i$: Spectrum of input signal\").to_edge(LEFT).shift(UP)\n",
    "        ai_graph = Axes(\n",
    "            x_range=[0, 16, 1], y_range=[0, 100, 20],\n",
    "            axis_config={\"include_numbers\": True},\n",
    "            x_length=5, y_length=3\n",
    "        ).shift(LEFT * 2 + DOWN * 1.5)\n",
    "        spectrum_curve = ai_graph.plot(\n",
    "            lambda x: 80 - 4 * x + (x % 3) * 10, x_range=[0, 15], color=BLUE\n",
    "        )\n",
    "\n",
    "        self.play(Write(ai_graph_label), Create(ai_graph), Create(spectrum_curve))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Remove title and input signal before transitioning\n",
    "        self.play(FadeOut(title, ai_graph_label, ai_graph, spectrum_curve, formula))\n",
    "        self.wait(1)\n",
    "\n",
    "        # Phase 2: Render cosine weights together\n",
    "        weights_label = Tex(r\"Cosine weights: $w_1, w_2, \\ldots$\", font_size=30).to_edge(UP)\n",
    "        self.play(Write(weights_label))\n",
    "\n",
    "        # Generate axes and curves for weights\n",
    "        weight_graphs = VGroup()\n",
    "        for i in range(4):  # Render all cosine weights together\n",
    "            # Create individual axes with smaller size\n",
    "            graph = Axes(\n",
    "                x_range=[0, 16, 1], y_range=[-1, 1, 0.5],\n",
    "                axis_config={\"include_numbers\": False},\n",
    "                x_length=2.5,  # Reduced length\n",
    "                y_length=1.5   # Reduced height\n",
    "            ).shift(DOWN * (i - 1.5))  # Stack vertically\n",
    "            \n",
    "            # Create corresponding curve\n",
    "            curve = graph.plot(\n",
    "                lambda x: np.cos((i + 1) * np.pi * (x - 0.5) / 16),\n",
    "                x_range=[0, 15], color=BLUE\n",
    "            )\n",
    "            graph.add(curve)\n",
    "            weight_graphs.add(graph)\n",
    "\n",
    "        # Center the group of graphs on the left side\n",
    "        weight_graphs.arrange(DOWN, buff=0.5).to_edge(LEFT, buff=1.5)\n",
    "        self.play(Create(weight_graphs))\n",
    "        self.wait(2)\n",
    "\n",
    "        # Phase 3: Render cepstrum outputs on the right\n",
    "        cj_label = Tex(r\"$C_j$: Cepstrum Coefficients\", font_size=30).next_to(weights_label, RIGHT, buff=1.5)\n",
    "        self.play(Write(cj_label))\n",
    "\n",
    "        cj_bars = BarChart(\n",
    "            values=[40, -20, 10, -5], bar_colors=[BLUE, RED, GREEN, ORANGE],\n",
    "            y_range=[-60, 60, 20],\n",
    "            x_length=4, y_length=3\n",
    "        ).next_to(weight_graphs, RIGHT, buff=2)\n",
    "\n",
    "        self.play(Create(cj_bars))\n",
    "        self.wait(3)\n",
    "\n",
    "        # Clean up\n",
    "        self.play(FadeOut(weights_label, weight_graphs, cj_label, cj_bars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a4940",
   "metadata": {},
   "source": [
    "# 26 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c5be5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "26 / 112\n",
    "\n",
    "MFCCs: typical values\n",
    "\n",
    "I 12 Coefficients C1–C12\n",
    "I Energy (could be C0)\n",
    "I Delta coefficients (derivatives in time)\n",
    "I Delta-delta (second order derivatives)\n",
    "I total: 39 coefficients per frame (analysis window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faffd93",
   "metadata": {},
   "source": [
    "# A time varying signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9214c",
   "metadata": {},
   "source": [
    "A time varying signal\n",
    "\n",
    "I speech is time varying\n",
    "I short segment are quasi-stationary\n",
    "I use short time analysis\n",
    "\n",
    "27 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899e84f",
   "metadata": {},
   "source": [
    "# Short-Time Fourier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30881f2",
   "metadata": {},
   "source": [
    "Short-Time Fourier Analysis\n",
    "\n",
    "7500\n",
    "\n",
    "8000\n",
    "\n",
    "9000\n",
    "\n",
    "8500\n",
    "samples\n",
    "\n",
    "9500\n",
    "\n",
    "10000\n",
    "\n",
    "0.1\n",
    "\n",
    "0\n",
    "\n",
    "-0.1\n",
    "\n",
    "amplitude\n",
    "\n",
    "0\n",
    "\n",
    "-5\n",
    "\n",
    "-10\n",
    "\n",
    "-15\n",
    "\n",
    "-20\n",
    "\n",
    "0\n",
    "\n",
    "1000\n",
    "\n",
    "2000\n",
    "\n",
    "3000\n",
    "\n",
    "4000\n",
    "\n",
    "5000\n",
    "\n",
    "6000\n",
    "\n",
    "7000\n",
    "\n",
    "8000\n",
    "\n",
    "frequency (Hz)\n",
    "\n",
    "28 / 112\n",
    "\n",
    "5\n",
    "\n",
    "power spectrum (dB)\n",
    "\n",
    "FFT, window length: 512, window kind: hamming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ce387",
   "metadata": {},
   "source": [
    "# Short-Time Fourier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e03352",
   "metadata": {},
   "source": [
    "Short-Time Fourier Analysis\n",
    "\n",
    "7500\n",
    "\n",
    "8000\n",
    "\n",
    "9000\n",
    "\n",
    "8500\n",
    "samples\n",
    "\n",
    "9500\n",
    "\n",
    "10000\n",
    "\n",
    "0.1\n",
    "\n",
    "0\n",
    "\n",
    "-0.1\n",
    "\n",
    "amplitude\n",
    "\n",
    "-5\n",
    "\n",
    "-10\n",
    "\n",
    "-15\n",
    "\n",
    "-20\n",
    "\n",
    "0\n",
    "\n",
    "1000\n",
    "\n",
    "2000\n",
    "\n",
    "3000\n",
    "\n",
    "4000\n",
    "\n",
    "5000\n",
    "\n",
    "6000\n",
    "\n",
    "7000\n",
    "\n",
    "8000\n",
    "\n",
    "frequency (Hz)\n",
    "\n",
    "28 / 112\n",
    "\n",
    "0\n",
    "\n",
    "power spectrum (dB)\n",
    "\n",
    "FFT, window length: 512, window kind: hamming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c3ed3",
   "metadata": {},
   "source": [
    "# 29 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02682d4",
   "metadata": {},
   "source": [
    "29 / 112\n",
    "\n",
    "Windowing, typical values\n",
    "\n",
    "I signal sampling frequency: 8–20kHz\n",
    "I analysis window: 10–50ms\n",
    "I frame interval: 10–25ms (100–40Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a445e4",
   "metadata": {},
   "source": [
    "# Frame-Based Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8f668",
   "metadata": {},
   "source": [
    "Frame-Based Processing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "05\n",
    "\n",
    "0.10\n",
    "\n",
    "\n",
    "\n",
    "0.15\n",
    "\n",
    "\n",
    "\n",
    "0.20\n",
    "\n",
    "\n",
    "\n",
    "0.25\n",
    "\n",
    "\n",
    "\n",
    "0.30\n",
    "\n",
    "\n",
    "\n",
    "0.35\n",
    "\n",
    "\n",
    "\n",
    "0.40\n",
    "\n",
    "\n",
    "\n",
    "0.45\n",
    "\n",
    "\n",
    "\n",
    "0.50\n",
    "\n",
    "\n",
    "\n",
    "0.55\n",
    "\n",
    "\n",
    "\n",
    "0.60\n",
    "\n",
    "\n",
    "\n",
    "0.65\n",
    "\n",
    "\n",
    "\n",
    "0.70\n",
    "\n",
    "\n",
    "\n",
    "0.75\n",
    "\n",
    "\n",
    "\n",
    "0.80\n",
    "\n",
    "\n",
    "\n",
    "0.85\n",
    "\n",
    "\n",
    "\n",
    "0.90\n",
    "\n",
    "File: sx352.WAV Page: 1 of 1 Printed: Mon Dec 05 09:01:39\n",
    "\n",
    "kHz\n",
    "7\n",
    "\n",
    "6\n",
    "\n",
    "5\n",
    "\n",
    "4\n",
    "\n",
    "3\n",
    "\n",
    "2\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "time\n",
    "\n",
    "\n",
    "\n",
    "t\n",
    "\n",
    "ix n tcl\n",
    "\n",
    "ay\n",
    "\n",
    "m\n",
    "\n",
    "ax\n",
    "\n",
    "t\n",
    "\n",
    "n  tcl\n",
    "\n",
    "r dx  iy\n",
    "\n",
    "ao\n",
    "\n",
    "k\n",
    "\n",
    "ix\tkcl\n",
    "\n",
    "h#\n",
    "\n",
    "my\n",
    "\n",
    "\n",
    "\n",
    "to\n",
    "\n",
    "according\n",
    "\n",
    "30 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b712e",
   "metadata": {},
   "source": [
    "# How Many Frames?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffc090",
   "metadata": {},
   "source": [
    "How Many Frames?\n",
    "\n",
    "Suppose that:\n",
    "I the audio recording is 3 seconds long\n",
    "I the sampling rate is 16 KHz\n",
    "I the window length is 20 milliseconds\n",
    "I the window step is 10 milliseconds Answer the following:\n",
    "I how many speech samples correspond to each window\n",
    "I how many frames correspond to the audio recording\n",
    "\n",
    "\n",
    "\n",
    "7500\n",
    "\n",
    "8000\n",
    "\n",
    "9000\n",
    "\n",
    "8500\n",
    "samples\n",
    "\n",
    "9500\n",
    "\n",
    "10000\n",
    "\n",
    "-0.1\n",
    "\n",
    "0.1\n",
    "\n",
    "0\n",
    "\n",
    "amplitude\n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "1000  2000  3000  4000  5000  6000  7000  8000\n",
    "frequency (Hz)\n",
    "\n",
    "-20\n",
    "\n",
    "-15\n",
    "\n",
    "-10\n",
    "\n",
    "-5\n",
    "\n",
    "0\n",
    "\n",
    "5\n",
    "\n",
    "power spectrum (dB)\n",
    "\n",
    "FFT, window length: 512, window kind: hamming\n",
    "\n",
    "File: sx352.WAV Page: 1 of 1 Printed: Mon Dec 05 09:01:39\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "\n",
    "2\n",
    "\n",
    "\n",
    "\n",
    "3\n",
    "\n",
    "\n",
    "\n",
    "4\n",
    "\n",
    "\n",
    "\n",
    "5\n",
    "\n",
    "\n",
    "\n",
    "7\n",
    "\n",
    "\n",
    "\n",
    "kHz\n",
    "\n",
    "\n",
    "\n",
    "6\n",
    "\n",
    "time\n",
    "\n",
    "05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90\n",
    "h#\tix kcl  k  ao  r dx iy n tcl t ax m  ay ix n tcl t\n",
    "according\tto\tmy\n",
    "\n",
    "31 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06032627",
   "metadata": {},
   "source": [
    "# Comparing frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b06fc7",
   "metadata": {},
   "source": [
    "Comparing frames\n",
    "\n",
    "05\n",
    "\n",
    "0.10\n",
    "\n",
    "\n",
    "\n",
    "0.15\n",
    "\n",
    "\n",
    "\n",
    "0.20\n",
    "\n",
    "\n",
    "\n",
    "0.25\n",
    "\n",
    "\n",
    "\n",
    "0.30\n",
    "\n",
    "\n",
    "\n",
    "0.35\n",
    "\n",
    "\n",
    "\n",
    "0.40\t0.45\t0.50\n",
    "\n",
    "\n",
    "\n",
    "0.55\t0.60\n",
    "\n",
    "\n",
    "\n",
    "0.65\n",
    "\n",
    "\n",
    "\n",
    "0.70\t0.75\n",
    "\n",
    "\n",
    "\n",
    "0.90\n",
    "\n",
    "File: sx352.WAV Page: 1 of 1 Printed: Mon Dec 05 09:01:39\n",
    "kHz\n",
    "7\n",
    "\n",
    "6\n",
    "\n",
    "5\n",
    "\n",
    "4\n",
    "\n",
    "3\n",
    "\n",
    "2\n",
    "\n",
    "1\n",
    "\n",
    "time\n",
    "\n",
    "\n",
    "\n",
    "t\n",
    "\n",
    "0.80\t0.85\n",
    "ix n tcl\n",
    "\n",
    "ay\n",
    "\n",
    "m\n",
    "\n",
    "ax\n",
    "\n",
    "t\n",
    "\n",
    "n tcl\n",
    "\n",
    "r dx iy\n",
    "\n",
    "ao\n",
    "\n",
    "k\n",
    "\n",
    "ix  kcl\n",
    "\n",
    "h#\n",
    "\n",
    "my\n",
    "\n",
    "\n",
    "\n",
    "to\n",
    "\n",
    "according\n",
    "\n",
    "I fixed-length representation\n",
    "I any metric can be used (Euclidean distance for example)\n",
    "I any standard machine learning method (nearest neighbour, Gaussian distributions, neural networks, . . . )\n",
    "\n",
    "32 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b705e",
   "metadata": {},
   "source": [
    "# Fixed Length Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61783e",
   "metadata": {},
   "source": [
    "Fixed Length Representation\n",
    "\n",
    "MNIST: written digit recognition\n",
    "\n",
    "33 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5770b5",
   "metadata": {},
   "source": [
    "# Comparing Utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47097731",
   "metadata": {},
   "source": [
    "Comparing Utterances\n",
    "\n",
    "In order to recognise speech we have to be able to compare different utterances\n",
    "\n",
    "Va jobbaru me\n",
    "\n",
    "34 / 112\n",
    "\n",
    "Vad jobbar du med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a074b1",
   "metadata": {},
   "source": [
    "# 35 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfe399",
   "metadata": {},
   "source": [
    "35 / 112\n",
    "\n",
    "Combining frame-wise scores into utterance scores\n",
    "\n",
    "Template Matching\n",
    "I oldest technique\n",
    "I simple comparison of template patterns\n",
    "I compensate for varying speech rate (Dynamic Programming) Hidden Markov Models (HMMs)\n",
    "I most used technique\n",
    "I models of segmental structure of speech\n",
    "I recognition by Viterbi search (Dynamic Programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3144a",
   "metadata": {},
   "source": [
    "# Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bc493",
   "metadata": {},
   "source": [
    "Template Matching\n",
    "\n",
    "37 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe85ef",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55babc80",
   "metadata": {},
   "source": [
    "Dynamic Programming\n",
    "\n",
    "I compare any possible alignment\n",
    "I problem: exponential with H and K!\n",
    "\n",
    "1\n",
    "\n",
    "H\n",
    "\n",
    "1\n",
    "\n",
    "K\n",
    "\n",
    "unknown utterance\n",
    "\n",
    "reference utterance\n",
    "\n",
    "38 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72841da",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e6a56",
   "metadata": {},
   "source": [
    "Dynamic Programming\n",
    "\n",
    "Dynamic Time Warping (DTW) algorithm 1: for h =1 to H do\n",
    "2:\tfor k =1 to K do\n",
    "3:\tAccD[h, k]= LocD[h, k]+ min(AccD[h — 1, k],\n",
    "AccD[h — 1, k — 1], AccD[h, k — 1])\n",
    "\n",
    "1\n",
    "\n",
    "H\n",
    "\n",
    "1\n",
    "\n",
    "K\n",
    "\n",
    "unknown utterance\n",
    "\n",
    "reference utterance\n",
    "\n",
    "38 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23c8d5",
   "metadata": {},
   "source": [
    "# 39 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01088ba",
   "metadata": {},
   "source": [
    "39 / 112\n",
    "\n",
    "DP Example: Spelling\n",
    "\n",
    "I observations are letters\n",
    "I local distance: 0 (same letter), 1 (different letter)\n",
    "I Unknown utterance: ALLDRIG\n",
    "I Reference1: ALDRIG\n",
    "I Reference2: ALLTID\n",
    "I Problem: find closest match\n",
    "Distance char-by-char:\n",
    "I ALLDRIG–ALDRIG = 5\n",
    "I ALLDRIG–ALLTID = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965da139",
   "metadata": {},
   "source": [
    "# 40 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193e5fb",
   "metadata": {},
   "source": [
    "40 / 112\n",
    "\n",
    "DP Example: ALLDRIG vs ALDRIG\n",
    "\n",
    "LocD[h,k]=\n",
    "\n",
    "AccD[h,k]=\n",
    "\n",
    "Distance ALLDRIG–ALDRIG: AccD[H,K] = 0\n",
    "Distance ALLDRIG–ALLTID? (5min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6bb69",
   "metadata": {},
   "source": [
    "# 42 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c0634",
   "metadata": {},
   "source": [
    "42 / 112\n",
    "\n",
    "Best path: Backtracking\n",
    "\n",
    "Sometimes we want to know the path\n",
    "at each point [h,k] remember the minimum distance predecessor (back \tpointer)\n",
    "at the end point [H,K] follow the back pointers until the start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8145e8b",
   "metadata": {},
   "source": [
    "# 43 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75433155",
   "metadata": {},
   "source": [
    "43 / 112\n",
    "\n",
    "Properties of Template Matching\n",
    "\n",
    "Pros:\n",
    "+ No need for phonetic transcriptions\n",
    "+ within-word co-articulation for free\n",
    "+ high time resolution Cons:\n",
    "cross-word co-articulation not modelled\n",
    "requires recordings of every word\n",
    "not easy to model variation\n",
    "does not scale up with vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7cef3",
   "metadata": {},
   "source": [
    "# Components of ASR System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22184d8c",
   "metadata": {},
   "source": [
    "Components of ASR System\n",
    "\n",
    "Speech Signal\n",
    "\n",
    "Spectral Analysis\n",
    "\n",
    "Feature Extraction\n",
    "\n",
    "Search and Match\n",
    "\n",
    "Recognised Words\n",
    "\n",
    "Acoustic Models\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "Language Models\n",
    "\n",
    "\n",
    "\n",
    "Representation\n",
    "\n",
    "Constraints - Knowledge\n",
    "\n",
    "\n",
    "\n",
    "Decoder\n",
    "\n",
    "45 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3edbc7",
   "metadata": {},
   "source": [
    "# A probabilistic perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96653fec",
   "metadata": {},
   "source": [
    "A probabilistic perspective\n",
    "\n",
    "46 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5555bff",
   "metadata": {},
   "source": [
    "# A probabilistic perspective: Bayes’ rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d782c",
   "metadata": {},
   "source": [
    "A probabilistic perspective: Bayes’ rule\n",
    "\n",
    "P(words|sounds) =\n",
    "\n",
    "P(sounds|words)P(words)\n",
    "\n",
    "\n",
    "\n",
    "P(sounds)\n",
    "\n",
    "47 / 112\n",
    "\n",
    "I P(sounds|words) can be estimated from training data and transcriptions\n",
    "I P(words): a priori probability of the words (Language Model)\n",
    "I P(sounds): a priori probability of the sounds (constant, can be ignored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c649364",
   "metadata": {},
   "source": [
    "# Components of ASR System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62ee25",
   "metadata": {},
   "source": [
    "Components of ASR System\n",
    "\n",
    "Speech Signal\n",
    "\n",
    "Spectral Analysis\n",
    "\n",
    "Feature Extraction\n",
    "\n",
    "Search and Match\n",
    "\n",
    "Recognised Words\n",
    "\n",
    "Acoustic Models\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "Language Models\n",
    "\n",
    "\n",
    "\n",
    "Representation\n",
    "\n",
    "Constraints - Knowledge\n",
    "\n",
    "\n",
    "\n",
    "Decoder\n",
    "\n",
    "Acoustic Models\n",
    "\n",
    "48 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e411f1",
   "metadata": {},
   "source": [
    "# Probabilistic Modelling\n",
    "Problem: How do we model P(sounds|words)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654c0e9",
   "metadata": {},
   "source": [
    "Probabilistic Modelling\n",
    "Problem: How do we model P(sounds|words)?\n",
    "\n",
    "05\n",
    "\n",
    "0.10\n",
    "\n",
    "\n",
    "\n",
    "0.15\n",
    "\n",
    "\n",
    "\n",
    "0.20\n",
    "\n",
    "\n",
    "\n",
    "0.25\n",
    "\n",
    "\n",
    "\n",
    "0.30\t0.35\n",
    "\n",
    "\n",
    "\n",
    "0.40\t0.45\t0.50\n",
    "\n",
    "\n",
    "\n",
    "0.55\t0.60\n",
    "\n",
    "\n",
    "\n",
    "0.65\n",
    "\n",
    "\n",
    "\n",
    "0.70\t0.75\n",
    "\n",
    "\n",
    "\n",
    "0.90\n",
    "\n",
    "File: sx352.WAV Page: 1 of 1 Printed: Mon Dec 05 09:01:39\n",
    "\n",
    "kHz\n",
    "7\n",
    "\n",
    "6\n",
    "\n",
    "5\n",
    "\n",
    "4\n",
    "\n",
    "3\n",
    "\n",
    "2\n",
    "\n",
    "1\n",
    "\n",
    "\n",
    "time\n",
    "\n",
    "\n",
    "\n",
    "t\n",
    "\n",
    "0.80\t0.85\n",
    "ix n tcl\n",
    "\n",
    "ay\n",
    "\n",
    "m\n",
    "\n",
    "ax\n",
    "\n",
    "t\n",
    "\n",
    "n tcl\n",
    "\n",
    "r dx iy\n",
    "\n",
    "ao\n",
    "\n",
    "k\n",
    "\n",
    "ix  kcl\n",
    "\n",
    "h#\n",
    "\n",
    "my\n",
    "\n",
    "\n",
    "\n",
    "to\n",
    "\n",
    "according\n",
    "\n",
    "Every feature vector (observation at time t) is a continuous stochastic variable (e.g. MFCC)\n",
    "\n",
    "49 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a1678",
   "metadata": {},
   "source": [
    "# Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4924c",
   "metadata": {},
   "source": [
    "Stationarity\n",
    "\n",
    "Problem: speech is not stationary\n",
    "I we need to model short segments independently\n",
    "I the fundamental unit can not be the word, but must be shorter\n",
    "I usually we model three segments for each phoneme\n",
    "\n",
    "Acoustic Models\n",
    "P(sounds|segment)\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "50 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64dd541",
   "metadata": {},
   "source": [
    "# Local probabilities (frame-wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84599347",
   "metadata": {},
   "source": [
    "Local probabilities (frame-wise)\n",
    "\n",
    "If segment sufficiently short\n",
    "P(sounds|segment)\n",
    "can be modelled with standard probability distributions Usually Gaussian or Gaussian Mixture\n",
    "\n",
    "51 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a0b4c",
   "metadata": {},
   "source": [
    "# Global Probabilities (utterance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e887b",
   "metadata": {},
   "source": [
    "Global Probabilities (utterance)\n",
    "\n",
    "Problem: How do we combine the different P(sounds|segment) to form\n",
    "P(sounds|words)?\n",
    "Answer: Hidden Markov Model (HMM)\n",
    "\n",
    "\n",
    "\n",
    "w1\n",
    "\n",
    "w2\n",
    "\n",
    "w3\n",
    "\n",
    "w\n",
    "\n",
    "ao1\n",
    "\n",
    "ao2\n",
    "\n",
    "ao3\n",
    "\n",
    "ao\n",
    "\n",
    "sh1\n",
    "\n",
    "sh2\n",
    "\n",
    "sh3\n",
    "\n",
    "sh\n",
    "\n",
    "wash\n",
    "\n",
    "52 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c545ce2",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5761b",
   "metadata": {},
   "source": [
    "Hidden Markov Models (HMMs)\n",
    "\n",
    "s1\n",
    "\n",
    "s2\n",
    "\n",
    "s3\n",
    "\n",
    "Elements:\n",
    "set of states:\n",
    "transition probabilities:\n",
    "prior probabilities:\n",
    "state to observation probabilities:\n",
    "\n",
    "53 / 112\n",
    "\n",
    "S = {s1, s2, s3}\n",
    "T (sa, sb )= P(sb , t|sa, t — 1)\n",
    "π(sa)= P(sa, t0)\n",
    "B(o, sa)= P(o|sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eaee91",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e0531",
   "metadata": {},
   "source": [
    "Hidden Markov Models (HMMs)\n",
    "\n",
    "1\n",
    "\n",
    "H\n",
    "\n",
    "unknown utterance\n",
    "\n",
    "54 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29774f",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26f361",
   "metadata": {},
   "source": [
    "Hidden Markov Models (HMMs)\n",
    "\n",
    "1\n",
    "\n",
    "H\n",
    "\n",
    "unknown utterance\n",
    "\n",
    "54 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556bce7f",
   "metadata": {},
   "source": [
    "# HMM Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162ef96",
   "metadata": {},
   "source": [
    "HMM Questions\n",
    "\n",
    "what is the probability that the model has generated the sequence of \tobservations? (isolated word recognition) forward algorithm\n",
    "what is the most likely state sequence given the observation sequence? \t(continuous speech recognition) Viterbi algorithm [5]\n",
    "how can the model parameters be estimated from examples? (training) \tBaum-Welch[1]\n",
    "\n",
    "\n",
    "\n",
    "[5] A. J. Viterbi. “Error Bounds for Convolutional Codes and an Asymtotically optimum decoding algorithm”. In: IEEE Trans. Inform. Theory IT-13 (1967), pp. 260–269\n",
    "[1] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. “A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains”. In: Ann. Math. Statist. 41.1 (1970), pp. 164–171\n",
    "\n",
    "55 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a1e95",
   "metadata": {},
   "source": [
    "# Isolated Words Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857228d1",
   "metadata": {},
   "source": [
    "Isolated Words Recognition\n",
    "\n",
    "word model 1\n",
    "\n",
    "word model 2\n",
    "\n",
    "word model 3\n",
    "\n",
    "... \n",
    "\n",
    "word model K\n",
    "\n",
    "Compare Likelihoods (forward-backward)\n",
    "\n",
    "56 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f69016",
   "metadata": {},
   "source": [
    "# Continuous Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b57031b",
   "metadata": {},
   "source": [
    "Continuous Speech Recognition\n",
    "\n",
    "word model 1\n",
    "\n",
    "word model 2\n",
    "\n",
    "word model 3\n",
    "\n",
    "... \n",
    "\n",
    "word model K\n",
    "\n",
    "Viterbi algorithm\n",
    "\n",
    "57 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a013689",
   "metadata": {},
   "source": [
    "# Modelling Coarticulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0ab72",
   "metadata": {},
   "source": [
    "Modelling Coarticulation\n",
    "\n",
    "Example peat /pi:t/ vs wheel /wi:l/\n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "\n",
    "0. 1\n",
    "\n",
    "\n",
    "\n",
    "0. 2\n",
    "\n",
    "\n",
    "\n",
    "0. 3\n",
    "\n",
    "-2 0 0 0\n",
    "\n",
    "-1 0 0 0\n",
    "\n",
    "0\n",
    "\n",
    "10 00 \n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "\n",
    "0. 1\n",
    "\n",
    "\n",
    "\n",
    "0. 2\n",
    "\n",
    "\n",
    "\n",
    "0. 3\n",
    "\n",
    "-2 0 0 0\n",
    "\n",
    "0\n",
    "-1 0 0 0\n",
    "\n",
    "10 0 0\n",
    "\n",
    "20 0 0\n",
    "\n",
    "Frequency (Hz)\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "\n",
    "0. 1\t0. 2\n",
    "Time ( seconds)\n",
    "\n",
    "\n",
    "\n",
    "0. 3\n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "10 00 \n",
    "\n",
    "20 00 \n",
    "\n",
    "30 00 \n",
    "\n",
    "40 00 \n",
    "\n",
    "Frequency (Hz)\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "\n",
    "0. 1\t0. 2\n",
    "Time ( seconds)\n",
    "\n",
    "\n",
    "\n",
    "0. 3\n",
    "\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "58 / 112\n",
    "\n",
    "10 0 0\n",
    "\n",
    "20 0 0\n",
    "\n",
    "30 0 0\n",
    "\n",
    "40 0 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4865e",
   "metadata": {},
   "source": [
    "# 59 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2d25e",
   "metadata": {},
   "source": [
    "59 / 112\n",
    "\n",
    "Modelling Coarticulation\n",
    "\n",
    "Context dependent models (CD-HMMs)\n",
    "I Duplicate each phoneme model depending on left and right context:\n",
    "I from “a” monophone model\n",
    "I to “d—a+f”, “d—a+g”, “l—a+s”. . . triphone models\n",
    "I If there are N = 50 phonemes in the language, there are N3 = 125000 potential triphones\n",
    "I many of them are not exploited by the language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a1c22",
   "metadata": {},
   "source": [
    "# 60 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f824678",
   "metadata": {},
   "source": [
    "60 / 112\n",
    "\n",
    "Amount of parameters\n",
    "\n",
    "Example:\n",
    "I a large vocabulary recogniser may have 60000 triphone models\n",
    "I each model has 3 states\n",
    "I each state may have 32 mixture components with 1 + 39 × 2 parameters each (weight, means, variances): 39 × 32 × 2+ 32 = 2528\n",
    "Totally it is 60000 × 3 × 2528 = 455 million parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9d4fb",
   "metadata": {},
   "source": [
    "# Similar Coarticulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f413d8",
   "metadata": {},
   "source": [
    "Similar Coarticulation\n",
    "\n",
    "/ri:/ vs /wi:/\n",
    "\n",
    "0\t0.1  0.2  0.3  0.4\n",
    "\n",
    "400\n",
    "200\n",
    "0\n",
    "-200\n",
    "-400\n",
    "-600\n",
    "\n",
    "0\n",
    "0\t0.1  0.2  0.3  0.4\n",
    "Time (seconds)\n",
    "\n",
    "1000\n",
    "\n",
    "2000\n",
    "\n",
    "3000\n",
    "\n",
    "4000\n",
    "\n",
    "0\t0.1  0.2\t0.3\t0.4\n",
    "\n",
    "400\n",
    "200\n",
    "0\n",
    "-200\n",
    "-400\n",
    "-600\n",
    "\n",
    "0\n",
    "0\t0.1  0.2\t0.3\t0.4\n",
    "Time (seconds)\n",
    "\n",
    "1000\n",
    "\n",
    "61 / 112\n",
    "\n",
    "2000\n",
    "\n",
    "3000\n",
    "\n",
    "4000\n",
    "\n",
    "Frequency (Hz)\n",
    "\n",
    "Frequency (Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c3d98",
   "metadata": {},
   "source": [
    "# 62 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971bb5c",
   "metadata": {},
   "source": [
    "62 / 112\n",
    "\n",
    "Tying to reduce complexity\n",
    "\n",
    "Example: similar triphones d—a+m and t—a+m\n",
    "I same right context, similar left context I 3rd state is expected to be very similar I 2nd state may also be similar\n",
    "States (and their parameters) can be shared between models\n",
    "+ reduce complexity\n",
    "+ more data to estimate each parameter – fine detail may be lost\n",
    "done with CART tree methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1947c44",
   "metadata": {},
   "source": [
    "# Components of ASR System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bd4c0",
   "metadata": {},
   "source": [
    "Components of ASR System\n",
    "\n",
    "Speech Signal\n",
    "\n",
    "Spectral Analysis\n",
    "\n",
    "Feature Extraction\n",
    "\n",
    "Search and Match\n",
    "\n",
    "Recognised Words\n",
    "\n",
    "Acoustic Models\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "Language Models\n",
    "\n",
    "\n",
    "\n",
    "Representation\n",
    "\n",
    "Constraints - Knowledge\n",
    "\n",
    "\n",
    "\n",
    "Decoder\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "63 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78feccd",
   "metadata": {},
   "source": [
    "# 64 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87bdf6",
   "metadata": {},
   "source": [
    "64 / 112\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "I in general specify sequence of phoneme for each word\n",
    "I example:\n",
    "\n",
    "“dictionary”\n",
    "UK: USA:\n",
    "I expensive resources\n",
    "\n",
    "IPA\n",
    "/d ɪ k ʃ ə n (ə) ɹ i/\n",
    "/d ɪ k ʃ ə n ɛ ɹ i/\n",
    "\n",
    "X-SAMPA\n",
    "/d I k S @ n (@) r i/\n",
    "/d I k S @ n E r i/\n",
    "\n",
    "I include multiple pronunciations\n",
    "I phonological rules (assimilation, deletion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e899e",
   "metadata": {},
   "source": [
    "# Pronunciation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21232211",
   "metadata": {},
   "source": [
    "Pronunciation Network\n",
    "\n",
    "Example: tomato\n",
    "\n",
    "\n",
    "\n",
    "/t/\n",
    "\n",
    "\n",
    "\n",
    "/ə/\n",
    "\n",
    "\n",
    "\n",
    "/oʊ/\n",
    "\n",
    "\n",
    "\n",
    "/m/\n",
    "\n",
    "\n",
    "\n",
    "/ɑ/\n",
    "\n",
    "\n",
    "\n",
    "/eı/\n",
    "\n",
    "\n",
    "\n",
    "/ɾ/\n",
    "\n",
    "\n",
    "\n",
    "/t/\n",
    "\n",
    "\n",
    "\n",
    "/oʊ/\n",
    "\n",
    "0.5\n",
    "\n",
    "0.3\n",
    "\n",
    "0.2\n",
    "\n",
    "0.7\n",
    "\n",
    "0.3\n",
    "\n",
    "0.2\n",
    "\n",
    "0.8\n",
    "\n",
    "0.8\n",
    "\n",
    "0.2\n",
    "\n",
    "65 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6741ca",
   "metadata": {},
   "source": [
    "# 66 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbb8f1",
   "metadata": {},
   "source": [
    "66 / 112\n",
    "\n",
    "Assimilation\n",
    "\n",
    "did you set you last year\n",
    "because you’ve\n",
    "\n",
    "/d ɪ dʒ ə/\n",
    "/s ɛ tʃ ə/\n",
    "/l æ s tʃ iː ɹ/\n",
    "/b iː k ə ʒ uː v/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23971693",
   "metadata": {},
   "source": [
    "# 67 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87abc767",
   "metadata": {},
   "source": [
    "67 / 112\n",
    "\n",
    "Deletion\n",
    "\n",
    "find him around this let me\n",
    "\n",
    "/f aɪ n ɪ m/\n",
    "/ə ɹ aʊ n ɪ s/\n",
    "/l ɛ m iː/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b4d1c",
   "metadata": {},
   "source": [
    "# 68 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de4c48",
   "metadata": {},
   "source": [
    "68 / 112\n",
    "\n",
    "Out of Vocabulary Words\n",
    "\n",
    "I Proper names often not in lexicon\n",
    "I derive pronunciation automatically\n",
    "I English has very complex grapheme-to-phoneme rules\n",
    "I attempts to derive pronunciation from speech recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc532b",
   "metadata": {},
   "source": [
    "# Computer Readable Phonetic Symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246dee3",
   "metadata": {},
   "source": [
    "Computer Readable Phonetic Symbols\n",
    "\n",
    "I IPA symbols are hard to handle in computer programs\n",
    "I many alternative codes\n",
    "I SAMPA https://www.phon.ucl.ac.uk/home/sampa/\n",
    "I only uses ASCII codes\n",
    "\n",
    "69 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf046bd",
   "metadata": {},
   "source": [
    "# Components of ASR System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c0e68",
   "metadata": {},
   "source": [
    "Components of ASR System\n",
    "\n",
    "Speech Signal\n",
    "\n",
    "Spectral Analysis\n",
    "\n",
    "Feature Extraction\n",
    "\n",
    "Search and Match\n",
    "\n",
    "Recognised Words\n",
    "\n",
    "Acoustic Models\n",
    "\n",
    "Lexical Models\n",
    "\n",
    "Language Models\n",
    "\n",
    "\n",
    "\n",
    "Representation\n",
    "\n",
    "Constraints - Knowledge\n",
    "\n",
    "\n",
    "\n",
    "Decoder\n",
    "\n",
    "Language Models\n",
    "\n",
    "71 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571af6fc",
   "metadata": {},
   "source": [
    "# Why do we need language models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156da88b",
   "metadata": {},
   "source": [
    "Why do we need language models?\n",
    "\n",
    "Bayes’ rule:\n",
    "\n",
    "P(words|sounds) =\n",
    "\n",
    "P(sounds|words)P(words)\n",
    "\n",
    "\n",
    "\n",
    "P(sounds)\n",
    "\n",
    "72 / 112\n",
    "\n",
    "where\n",
    "P(words): a priori probability of the words (Language Model)\n",
    "\n",
    "We could use non informative priors (P(words) = 1/N), but. . ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b7ac9",
   "metadata": {},
   "source": [
    "# Branching Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe028cf",
   "metadata": {},
   "source": [
    "Branching Factor\n",
    "\n",
    "I if we have N words in the dictionary\n",
    "I at every word boundary we have to consider N equally likely alternatives\n",
    "I N can be in the order of millions\n",
    "word1 word2\n",
    "word\n",
    "... \n",
    "\n",
    "wordN\n",
    "\n",
    "73 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0304a2",
   "metadata": {},
   "source": [
    "# 74 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5307658",
   "metadata": {},
   "source": [
    "74 / 112\n",
    "\n",
    "Ambiguity\n",
    "\n",
    "“ice cream” vs “I scream”\n",
    "/aɪ s k ɹ iː m/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25c843",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991bafb",
   "metadata": {},
   "source": [
    "Language Models\n",
    "\n",
    "P(words|sounds) =\n",
    "\n",
    "P(sounds|words)P(words)\n",
    "\n",
    "\n",
    "\n",
    "P(sounds)\n",
    "\n",
    "75 / 112\n",
    "\n",
    "Finite state networks (hand-made, see lab)\n",
    "I formal language, e.g. traffic control Statistical Models (N-grams)\n",
    "I unigrams: P(wi )\n",
    "I bigrams: P(wi |wi—1)\n",
    "I trigrams: P(wi |wi—1, wi—2)\n",
    "I ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8ee9b",
   "metadata": {},
   "source": [
    "# 78 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469928d5",
   "metadata": {},
   "source": [
    "78 / 112\n",
    "\n",
    "Formal Language Models\n",
    "\n",
    "I only used for simple tasks\n",
    "I hard to code by hand\n",
    "I people do not speak following formal grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855cb43",
   "metadata": {},
   "source": [
    "# 79 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176a174",
   "metadata": {},
   "source": [
    "79 / 112\n",
    "\n",
    "Example: HTK grammar definition\n",
    "\n",
    "I Terminals: strings (up, down, left, . . . )\n",
    "I Non-terminals: dollar sign ($command, $nounPhrase)\n",
    "I [.] optional (zero or one)\n",
    "I {.} zero or more\n",
    "I (.) block\n",
    "I <.> loop\n",
    "I .|. alternative\n",
    "I S: (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd373f",
   "metadata": {},
   "source": [
    "# Example: HTK grammar definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff104ab4",
   "metadata": {},
   "source": [
    "Example: HTK grammar definition\n",
    "\n",
    "I Terminals: strings (up, down, left,\n",
    "... )\n",
    "I Non-terminals: dollar sign ($command, $nounPhrase)\n",
    "I [.] optional (zero or one)\n",
    "I {.} zero or more\n",
    "I (.) block\n",
    "I <.> loop\n",
    "I .|. alternative\n",
    "I S: (...)\n",
    "\n",
    "bottom\n",
    "\n",
    "top\n",
    "\n",
    "move\n",
    "\n",
    "down\n",
    "\n",
    "left\n",
    "\n",
    "up\n",
    "\n",
    "right\n",
    "\n",
    "\n",
    "\n",
    "80 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490fef9",
   "metadata": {},
   "source": [
    "# 82 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a3302",
   "metadata": {},
   "source": [
    "82 / 112\n",
    "\n",
    "Statistical Grammar Models (N-grams)\n",
    "\n",
    "Simply count co-occurrence of words in large text data sets\n",
    "I unigrams: P(wi )\n",
    "I bigrams: P(wi |wi—1)\n",
    "I trigrams: P(wi |wi—1, wi—2)\n",
    "I ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb6b8d",
   "metadata": {},
   "source": [
    "# Language Models: complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e2909",
   "metadata": {},
   "source": [
    "Language Models: complexity\n",
    "\n",
    "Increasing N in N-grams leads to:\n",
    "1. more complex decoders\n",
    "\n",
    "2. difficulties in training the LM parameters\n",
    "\n",
    "83 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d409b",
   "metadata": {},
   "source": [
    "# 84 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc0894",
   "metadata": {},
   "source": [
    "84 / 112\n",
    "\n",
    "Knowledge Models in ASR\n",
    "\n",
    "Acoustic Models trained on hours of annotated speech recordings (specially developed speech databases)\n",
    "Lexical Model usually produced by hand by experts (or generated by rules) Language Models trained on millions of words of text (often from newspapers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa4f0a",
   "metadata": {},
   "source": [
    "# Word Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d27f5",
   "metadata": {},
   "source": [
    "Word Accuracy\n",
    "\n",
    "A = 100\n",
    "\n",
    "N — S — D — I\n",
    "\n",
    "\n",
    "\n",
    "N\n",
    "\n",
    "86 / 112\n",
    "\n",
    "Where\n",
    "I N: total number of reference words\n",
    "I S : substitutions\n",
    "I D: deletions\n",
    "I I : insertions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c076cd3",
   "metadata": {},
   "source": [
    "# Word Accuracy: example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a3ff6",
   "metadata": {},
   "source": [
    "Word Accuracy: example\n",
    "\n",
    "6 words, 1 substitution, 1 insertion, 1 deletion\n",
    "\n",
    "A = 100\n",
    "\n",
    "6 — 1 — 1 — 1\n",
    "\n",
    "\n",
    "\n",
    "6\n",
    "\n",
    "87 / 112\n",
    "\n",
    "= 50%\n",
    "\n",
    "requires dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d7869",
   "metadata": {},
   "source": [
    "# 88 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef052884",
   "metadata": {},
   "source": [
    "88 / 112\n",
    "\n",
    "Measure Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ca844",
   "metadata": {},
   "source": [
    "# Effect of adding features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858e2e5",
   "metadata": {},
   "source": [
    "Effect of adding features\n",
    "\n",
    "89 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af9274",
   "metadata": {},
   "source": [
    "# Effect of adding training data\n",
    "Swichboard data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ede7c",
   "metadata": {},
   "source": [
    "Effect of adding training data\n",
    "Swichboard data\n",
    "\n",
    "90 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cd6c6",
   "metadata": {},
   "source": [
    "# Effect of adding Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a2b1b",
   "metadata": {},
   "source": [
    "Effect of adding Gaussians\n",
    "\n",
    "91 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d26b0",
   "metadata": {},
   "source": [
    "# Effect of adding data for language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d74390",
   "metadata": {},
   "source": [
    "Effect of adding data for language models\n",
    "\n",
    "Million sentences\n",
    "\n",
    "92 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376b6c4",
   "metadata": {},
   "source": [
    "# 93 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c46bb",
   "metadata": {},
   "source": [
    "93 / 112\n",
    "\n",
    "Some dictation systems\n",
    "\n",
    "I vocabulary over 100 000 words\n",
    "I many languages\n",
    "I systems: Nuance NatuallySpeaking, Microsoft, (IBM ViaVoice), (Dragon Dictate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0af186",
   "metadata": {},
   "source": [
    "# 94 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6b0f4",
   "metadata": {},
   "source": [
    "94 / 112\n",
    "\n",
    "New applications\n",
    "\n",
    "I Indexing of TV and radio programs (offline), Google\n",
    "I real-time subtitling of TV programs (re-speaker that summarises)\n",
    "I voice search (Google)\n",
    "I language learning\n",
    "I smart phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4455fe",
   "metadata": {},
   "source": [
    "# 96 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03864f10",
   "metadata": {},
   "source": [
    "96 / 112\n",
    "\n",
    "Main variables in ASR\n",
    "\n",
    "Speaking mode isolated words vs continuous speech Speaking style read speech vs spontaneous speech\n",
    "Speakers speaker dependent vs speaker independent Vocabulary small (<20 words) vs large (>50 000 words) Robustness against background noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4155d1",
   "metadata": {},
   "source": [
    "# http://www.itl.nist.gov/iad/mig/publications/ASRhistory/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d414b10",
   "metadata": {},
   "source": [
    "http://www.itl.nist.gov/iad/mig/publications/ASRhistory/\n",
    "\n",
    "97 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec936e79",
   "metadata": {},
   "source": [
    "# Why is it so hard?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fcda3c",
   "metadata": {},
   "source": [
    "Why is it so hard?\n",
    "\n",
    "Language\n",
    "\n",
    "database\n",
    "\n",
    "98 / 112\n",
    "\n",
    "application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73772d",
   "metadata": {},
   "source": [
    "# 99 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22236cea",
   "metadata": {},
   "source": [
    "99 / 112\n",
    "\n",
    "Challenges — Variability\n",
    "\n",
    "Between speakers\n",
    "I Age\n",
    "I Gender I Anatomy I Dialect\n",
    "Within speaker\n",
    "I Stress\n",
    "I Emotion\n",
    "I Health condition\n",
    "I Read vs Spontaneous\n",
    "I Adaptation to environment (Lombard effect)\n",
    "I Adaptation to listener\n",
    "\n",
    "Environment\n",
    "I Noise\n",
    "I Room acoustics\n",
    "I Microphone distance I Microphone, telephone I Bandwidth\n",
    "Listener\n",
    "I Age\n",
    "I Mother tongue\n",
    "I Hearing loss\n",
    "I Known / unknown\n",
    "I Human / Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eac42f",
   "metadata": {},
   "source": [
    "# Example: spontaneous vs hyper-articulated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823a9f1",
   "metadata": {},
   "source": [
    "Example: spontaneous vs hyper-articulated\n",
    "\n",
    "Va jobbaru me\n",
    "\n",
    "101 / 112\n",
    "\n",
    "Vad jobbar du med\n",
    "\n",
    "“What is your occupation” (“What work you with”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a06df8",
   "metadata": {},
   "source": [
    "# 102 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ce09e",
   "metadata": {},
   "source": [
    "102 / 112\n",
    "\n",
    "Examples of reduced pronunciation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54075ba",
   "metadata": {},
   "source": [
    "# Microphone distance\n",
    "Headset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320ed2d",
   "metadata": {},
   "source": [
    "Microphone distance\n",
    "Headset\n",
    "\n",
    "2 m distance\n",
    "\n",
    "103 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dabcc7",
   "metadata": {},
   "source": [
    "# How do we cope with variability?\n",
    "Ideally: models that generalise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4d6b5",
   "metadata": {},
   "source": [
    "How do we cope with variability?\n",
    "Ideally: models that generalise\n",
    "\n",
    "database\n",
    "\n",
    "application\n",
    "\n",
    "Language\n",
    "\n",
    "104 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce2654",
   "metadata": {},
   "source": [
    "# How do we cope with variability?\n",
    "Large companies use insane quantities of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01833cf5",
   "metadata": {},
   "source": [
    "How do we cope with variability?\n",
    "Large companies use insane quantities of data\n",
    "\n",
    "application\n",
    "database\n",
    "\n",
    "Language\n",
    "\n",
    "105 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c102e",
   "metadata": {},
   "source": [
    "# How do we cope with variability?\n",
    "Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7720ac6",
   "metadata": {},
   "source": [
    "How do we cope with variability?\n",
    "Adaptation\n",
    "\n",
    "database\n",
    "\n",
    "application\n",
    "\n",
    "Language\n",
    "\n",
    "106 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3dcda",
   "metadata": {},
   "source": [
    "# 107 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e27a39d",
   "metadata": {},
   "source": [
    "107 / 112\n",
    "\n",
    "Adaptation: Example\n",
    "\n",
    "Enrolment in Dictation Systems\n",
    "I let the user read a small text before using the system\n",
    "\n",
    "Beta version of smartphone applications\n",
    "I the company has all the rights on data generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b241f",
   "metadata": {},
   "source": [
    "# 108 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0990c7e",
   "metadata": {},
   "source": [
    "108 / 112\n",
    "\n",
    "Limitations\n",
    "\n",
    "I lack of context\n",
    "I require huge amounts of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a54dd7",
   "metadata": {},
   "source": [
    "# Lack of Generalisation[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec242072",
   "metadata": {},
   "source": [
    "Lack of Generalisation[4]\n",
    "\n",
    "15\n",
    "\n",
    "\n",
    "10\n",
    "\n",
    "\n",
    "5\n",
    "\n",
    "\n",
    "0\n",
    "\n",
    "20\n",
    "\n",
    "30\n",
    "\n",
    "\n",
    "25\n",
    "\n",
    "1\n",
    "\n",
    "Word Error Rate (%)\n",
    "\n",
    "10\t100\t1,000\t10,000\t100,000\t1,000,000 10,000,000\n",
    "hours of training data\n",
    "Less supervised\tMore supervised\n",
    "\n",
    "Broadcast news transcription\n",
    "\n",
    "linear extrapolation\n",
    "\n",
    "\n",
    "\n",
    "In order to reach 10-years-old’s performance, ASR needs 4 to 70 human lifetimes exposure to speech!!\n",
    "\n",
    "\n",
    "\n",
    "[4] R. Moore. “A Comparison of the Data Requirements of Automatic Speech Recognition Systems and Human Listeners”. In: Geneva, Switzerland,\n",
    "\n",
    "2003, pp. 2582–2584\n",
    "\n",
    "110 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95aa9a4",
   "metadata": {},
   "source": [
    "# More information in DT2119 starting in period 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee878b",
   "metadata": {},
   "source": [
    "More information in DT2119 starting in period 4:\n",
    "\n",
    "I theoretical foundations\n",
    "I python implementation of basic ASR algorithms\n",
    "I Pytorch implementation of Deep Learning for ASR\n",
    "I freely chosen research project\n",
    "\n",
    "112 / 112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b27ad4",
   "metadata": {},
   "source": [
    "# Recent developments in ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1a6ac",
   "metadata": {},
   "source": [
    "Recent developments in ASR\n",
    "\n",
    "Gustav Eje Henter\n",
    "for DT2112\n",
    "2023-02-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e4b25",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f041e5",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "Deep learning + ASR\n",
    "Different sequence-to-sequence models\n",
    "Performance developments\n",
    "Where are we now?\n",
    "Comparison to humans\n",
    "Should we be end-to-end or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64385b3c",
   "metadata": {},
   "source": [
    "# Putting deep learning into ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51961c54",
   "metadata": {},
   "source": [
    "Putting deep learning into ASR\n",
    "\n",
    "Sequence-to-sequence models\n",
    "From a sequence of feature vectors to a sequence of phonemes/letters\n",
    "Connectionist temporal classification (CTC)\n",
    "Attention models\n",
    "Transducer models\n",
    "Layers\n",
    "Earlier: CNNs and RNNs\n",
    "Nowadays: Transformers (self-attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8a243",
   "metadata": {},
   "source": [
    "# Connectionist temporal classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7629c",
   "metadata": {},
   "source": [
    "Connectionist temporal classification\n",
    "\n",
    "Very simple\n",
    "Input acoustics, output senones (triphones) or “blank” symbol\n",
    "Issues\n",
    "Output cannot be longer than input\n",
    "Outputs are assumed to be independent \n",
    "“I eight food” instead of “I ate food”\n",
    "Requires a language model to fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7364235",
   "metadata": {},
   "source": [
    "# Attention-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ab899",
   "metadata": {},
   "source": [
    "Attention-based models\n",
    "\n",
    "Neural cross-attention\n",
    "Taken from NLP \n",
    "Issues\n",
    "Slow if inputs and outputs are long sequences, O(TU)\n",
    "Cannot be run online/streaming\n",
    "The entire input sequence needs to be available for the attention\n",
    "That input and output words come in the same order (“monotonicity”) has to be learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8d976",
   "metadata": {},
   "source": [
    "# Monotonicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c5e6c",
   "metadata": {},
   "source": [
    "Monotonicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a95dde",
   "metadata": {},
   "source": [
    "# Transducer models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c1435",
   "metadata": {},
   "source": [
    "Transducer models\n",
    "\n",
    "From 2012, but recently popular\n",
    "Can handle longer outputs\n",
    "Takes into account dependencies between words\n",
    "Can be run online/streaming\n",
    "Predictor only needs text data\n",
    "Requires more memory to train\n",
    "But little memory to run\n",
    "Popular with big companies!\n",
    "HMMs are a type of transducer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790e468",
   "metadata": {},
   "source": [
    "# DNN-HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c01f8",
   "metadata": {},
   "source": [
    "DNN-HMMs\n",
    "\n",
    "You can put deep neural networks into HMM-based ASR!\n",
    "Including discriminative neural network classifiers\n",
    "\n",
    "\n",
    "\n",
    "Great, because neural nets make great classifiers\n",
    "Such DNN-HMMs gave leading ASR WER for many years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674a333",
   "metadata": {},
   "source": [
    "# ASR keeps improving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a091",
   "metadata": {},
   "source": [
    "ASR keeps improving\n",
    "\n",
    "Performance on public benchmarks keeps improving\n",
    "The best “leaderboard” I know is “WER are we?”\n",
    "https://github.com/syhw/wer_are_we"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5877a",
   "metadata": {},
   "source": [
    "# Comparing humans and ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef1740",
   "metadata": {},
   "source": [
    "Comparing humans and ASR\n",
    "\n",
    "A. Stolcke, J. Droppo, “Comparing human and machine errors in conversational speech transcription”, in Proc. Interspeech, 2017.\n",
    "Some speakers are hard to understand\n",
    "Very high human WER\n",
    "How can these people communicate? XD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e1143",
   "metadata": {},
   "source": [
    "# Comparing humans and ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564b8df",
   "metadata": {},
   "source": [
    "Comparing humans and ASR\n",
    "\n",
    "Most common ASR errors are similar to most common human “errors”\n",
    "Confusing short words\n",
    "Exception is filled pauses (“uh”) versus backchannels (“uhuh”)\n",
    "These fill different roles in speaking, so humans are less likely to confuse them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee42e7",
   "metadata": {},
   "source": [
    "# What does super-human performance mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477094b",
   "metadata": {},
   "source": [
    "What does super-human performance mean?\n",
    "\n",
    "Human performance depends on audio context, transcription guidelines, amount of time invested, etc.\n",
    "Machines have been better than humans for a long time in constrained vocabularies and high noise\n",
    "“Matrix sentences” or tank driving commands (Alec Seward, KTH)\n",
    "Here, we know what the person wanted to say\n",
    "But if the “ground truth” is from humans transcribing to the audio…\n",
    "Human error rate is annotator agreement – how often do humans agree?\n",
    "How can a machine agree more with what humans hear, than humans agree with each other on what they hear?\n",
    "“Average transcription”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c3baa",
   "metadata": {},
   "source": [
    "# Whisper – a recent ASR system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e5984",
   "metadata": {},
   "source": [
    "Whisper – a recent ASR system\n",
    "\n",
    "Whisper ASR from OpenAI\n",
    "https://arxiv.org/abs/2212.04356\n",
    "https://github.com/openai/whisper\n",
    "Free and easy to use\n",
    "680,000 hours(!) of training data\n",
    "Sourced online\n",
    "1/3 not in English\n",
    "Low-quality text removed (“transcriptese”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f82c1",
   "metadata": {},
   "source": [
    "# Whisper architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b46a4",
   "metadata": {},
   "source": [
    "Whisper architecture\n",
    "\n",
    "“End-to-end Transformer architecture”\n",
    "Acoustics to letters\n",
    "Attention-based\n",
    "Largest model is 32 layers, width 1280, 20 attention heads, 1.5 billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bc994",
   "metadata": {},
   "source": [
    "# Whisper performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b4926",
   "metadata": {},
   "source": [
    "Whisper performance\n",
    "\n",
    "English: 4.2%\u000b",
    "\n",
    "on Fleurs benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49547e9",
   "metadata": {},
   "source": [
    "# Whisper performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2d40e",
   "metadata": {},
   "source": [
    "Whisper performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8b390",
   "metadata": {},
   "source": [
    "# Whisper behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809ba80",
   "metadata": {},
   "source": [
    "Whisper behaviour\n",
    "\n",
    "Can recreate dialogue in transcripts\n",
    "Can be “prompted” to transcribe in a particular way\n",
    "For instance to keep filled pauses\n",
    "Tends to hallucinate reasonable words not spoken\n",
    "This problem is new to ASR\n",
    "Seems to happen more with their larger model\n",
    "\n",
    "“He married a lady also belonging to the Society of Friends, who brought him a large fortune which, and his own money, he put into a city firm, and he was a very rich man.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cb7e2",
   "metadata": {},
   "source": [
    "# Should we be end-to-end or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f25c546",
   "metadata": {},
   "source": [
    "Should we be end-to-end or not?\n",
    "\n",
    "“End-to-end” – i.e., learning to do everything in one system at the same time – can optimise the entire pipeline jointly for best performance\n",
    "But usually not all the way from the waveform in practice\n",
    "Whisper is one example\n",
    "And not all the way to the written form, at least not in all languages/writing systems\n",
    "\n",
    "Modular systems can use unimodal (non-parallel) data to train\n",
    "There’s more unimodal than multimodal data\n",
    "Pre-trained neural language models, e.g., GPT\n",
    "Self-supervised acoustic feature representations, e.g., wav2vec, HuBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c23b3",
   "metadata": {},
   "source": [
    "# Should we be end-to-end or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcf12a",
   "metadata": {},
   "source": [
    "Should we be end-to-end or not?\n",
    "\n",
    "Hybrid approaches\n",
    "Fine-tuning modular systems\n",
    "Data augmentation\n",
    "Using signal processing\n",
    "Using text-to-speech\n",
    "It is not clear which approach is better\n",
    "Neither today, nor in the long term\n",
    "It might depend on the data, the problem, and on whom you ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b64c1e",
   "metadata": {},
   "source": [
    "# Still not perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c84fd8",
   "metadata": {},
   "source": [
    "Still not perfect\n",
    "\n",
    "YouTube’s current (late 2022) video transcription system really likes the word “foreign”…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b6982",
   "metadata": {},
   "source": [
    "# Thank you for listening!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d1757",
   "metadata": {},
   "source": [
    "Thank you for listening!\n",
    "\n",
    "Want to know more?\u000b",
    "\n",
    "Take DT2119, “Speech and Speaker Recognition”\u000b",
    "\n",
    "7.5 credits, starts in period 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
