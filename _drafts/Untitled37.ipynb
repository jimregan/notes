{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ka-wUraJWk",
        "outputId": "8fb3e51e-57dc-4241-c545-ddba7945a03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 7336, done.\u001b[K\n",
            "remote: Total 7336 (delta 0), reused 0 (delta 0), pack-reused 7336\u001b[K\n",
            "Receiving objects: 100% (7336/7336), 10.87 MiB | 20.88 MiB/s, done.\n",
            "Resolving deltas: 100% (4753/4753), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/whisper.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd whisper.cpp\n",
        "!make -j 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbQi0j3gaiZd",
        "outputId": "2261573c-eef9-4130-aa00-0887f8ad6273"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/whisper.cpp\n",
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml.c -o ggml.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-backend.c -o ggml-backend.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 -c whisper.cpp -o whisper.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/main/main.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o main \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/bench/bench.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o bench \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/quantize/quantize.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o quantize \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/server/server.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o server  \n",
            "./main -h\n",
            "\n",
            "usage: ./main [options] file0.wav file1.wav ...\n",
            "\n",
            "options:\n",
            "  -h,        --help              [default] show this help message and exit\n",
            "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
            "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
            "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
            "  -on N,     --offset-n N        [0      ] segment index offset\n",
            "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
            "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
            "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
            "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
            "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
            "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
            "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
            "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
            "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
            "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
            "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
            "  -tr,       --translate         [false  ] translate from source language to english\n",
            "  -di,       --diarize           [false  ] stereo audio diarization\n",
            "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
            "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
            "  -otxt,     --output-txt        [false  ] output result in a text file\n",
            "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
            "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
            "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
            "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
            "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
            "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
            "  -oj,       --output-json       [false  ] output result in a JSON file\n",
            "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
            "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
            "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
            "  -ps,       --print-special     [false  ] print special tokens\n",
            "  -pc,       --print-colors      [false  ] print colors\n",
            "  -pp,       --print-progress    [false  ] print progress\n",
            "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
            "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
            "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
            "             --prompt PROMPT     [       ] initial prompt\n",
            "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
            "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
            "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
            "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
            "  -ng,       --no-gpu            [false  ] disable GPU\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g0pwpVgdG25",
        "outputId": "d340d15a-c7a0-4866-83f3-2d639d48399b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://huggingface.co/NbAiLab/whisper-large-sme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyuTzLClaUyj",
        "outputId": "d1a4fa5a-6adf-4f72-cd82-8ee99221646a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'whisper-large-sme'...\n",
            "remote: Enumerating objects: 444, done.\u001b[K\n",
            "remote: Total 444 (delta 0), reused 0 (delta 0), pack-reused 444\u001b[K\n",
            "Receiving objects: 100% (444/444), 642.41 KiB | 9.73 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/openai/whisper"
      ],
      "metadata": {
        "id": "1Dl3eCSGf5sM",
        "outputId": "f7ab6e09-daf0-488b-de9a-448b636455ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'whisper'...\n",
            "remote: Enumerating objects: 712, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 712 (delta 1), reused 3 (delta 0), pack-reused 702\u001b[K\n",
            "Receiving objects: 100% (712/712), 12.43 MiB | 20.53 MiB/s, done.\n",
            "Resolving deltas: 100% (419/419), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir whisper-ggml-sme\n",
        "!python whisper.cpp/models/convert-h5-to-ggml.py ./whisper-large-sme/ ./whisper ./whisper-ggml-sme/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOuUcH67aXNC",
        "outputId": "b5604369-7eff-49a5-dce8-2acf4287e974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘whisper-ggml-sme’: File exists\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://media.globalrecordings.net/GOKit_MP3s_named/Saami%20North%20-%20The%20Two%20Roads.mp3 -O sample.mp3\n",
        "!ffmpeg -i sample.mp3 -acodec pcm_s16le -ac 1 -ar 16000 sample.wav"
      ],
      "metadata": {
        "id": "rhZPHQ91aZB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./whisper.cpp/main  -m whisper-ggml-sme/ggml-model.bin -f sample.wav"
      ],
      "metadata": {
        "id": "Ya-oxJc8eA02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}