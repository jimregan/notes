{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ka-wUraJWk",
        "outputId": "8fb3e51e-57dc-4241-c545-ddba7945a03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'whisper.cpp'...\n",
            "remote: Enumerating objects: 7336, done.\u001b[K\n",
            "remote: Total 7336 (delta 0), reused 0 (delta 0), pack-reused 7336\u001b[K\n",
            "Receiving objects: 100% (7336/7336), 10.87 MiB | 20.88 MiB/s, done.\n",
            "Resolving deltas: 100% (4753/4753), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/whisper.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd whisper.cpp\n",
        "!make -j 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbQi0j3gaiZd",
        "outputId": "2261573c-eef9-4130-aa00-0887f8ad6273"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/whisper.cpp\n",
            "I whisper.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:      g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml.c -o ggml.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-backend.c -o ggml-backend.o\n",
            "cc  -I.              -O3 -DNDEBUG -std=c11   -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3   -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 -c whisper.cpp -o whisper.o\n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/main/main.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o main \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/bench/bench.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o bench \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/quantize/quantize.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o quantize \n",
            "g++ -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -pthread -mavx -mavx2 -mfma -mf16c -msse3 -mssse3 examples/server/server.cpp examples/common.cpp examples/common-ggml.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o whisper.o -o server  \n",
            "./main -h\n",
            "\n",
            "usage: ./main [options] file0.wav file1.wav ...\n",
            "\n",
            "options:\n",
            "  -h,        --help              [default] show this help message and exit\n",
            "  -t N,      --threads N         [2      ] number of threads to use during computation\n",
            "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
            "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
            "  -on N,     --offset-n N        [0      ] segment index offset\n",
            "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
            "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
            "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
            "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
            "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
            "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
            "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
            "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
            "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
            "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
            "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
            "  -tr,       --translate         [false  ] translate from source language to english\n",
            "  -di,       --diarize           [false  ] stereo audio diarization\n",
            "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
            "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
            "  -otxt,     --output-txt        [false  ] output result in a text file\n",
            "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
            "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
            "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
            "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
            "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
            "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
            "  -oj,       --output-json       [false  ] output result in a JSON file\n",
            "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
            "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
            "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
            "  -ps,       --print-special     [false  ] print special tokens\n",
            "  -pc,       --print-colors      [false  ] print colors\n",
            "  -pp,       --print-progress    [false  ] print progress\n",
            "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
            "  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n",
            "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
            "             --prompt PROMPT     [       ] initial prompt\n",
            "  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n",
            "  -f FNAME,  --file FNAME        [       ] input WAV file path\n",
            "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
            "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
            "  -ng,       --no-gpu            [false  ] disable GPU\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g0pwpVgdG25",
        "outputId": "d340d15a-c7a0-4866-83f3-2d639d48399b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://huggingface.co/NbAiLab/whisper-large-sme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyuTzLClaUyj",
        "outputId": "d1a4fa5a-6adf-4f72-cd82-8ee99221646a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'whisper-large-sme'...\n",
            "remote: Enumerating objects: 444, done.\u001b[K\n",
            "remote: Total 444 (delta 0), reused 0 (delta 0), pack-reused 444\u001b[K\n",
            "Receiving objects: 100% (444/444), 642.41 KiB | 9.73 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/openai/whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dl3eCSGf5sM",
        "outputId": "f7ab6e09-daf0-488b-de9a-448b636455ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'whisper'...\n",
            "remote: Enumerating objects: 712, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 712 (delta 1), reused 3 (delta 0), pack-reused 702\u001b[K\n",
            "Receiving objects: 100% (712/712), 12.43 MiB | 20.53 MiB/s, done.\n",
            "Resolving deltas: 100% (419/419), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir whisper-ggml-sme\n",
        "!python whisper.cpp/models/convert-h5-to-ggml.py ./whisper-large-sme/ ./whisper ./whisper-ggml-sme/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOuUcH67aXNC",
        "outputId": "b5604369-7eff-49a5-dce8-2acf4287e974"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘whisper-ggml-sme’: File exists\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "model.encoder.conv1.weight  ->  encoder.conv1.weight\n",
            "encoder.conv1.weight 3 (1280, 80, 3)\n",
            "model.encoder.conv1.bias  ->  encoder.conv1.bias\n",
            "  Reshaped variable:  encoder.conv1.bias  to shape:  (1280, 1)\n",
            "encoder.conv1.bias 2 (1280, 1)\n",
            "  Converting to float32\n",
            "model.encoder.conv2.weight  ->  encoder.conv2.weight\n",
            "encoder.conv2.weight 3 (1280, 1280, 3)\n",
            "model.encoder.conv2.bias  ->  encoder.conv2.bias\n",
            "  Reshaped variable:  encoder.conv2.bias  to shape:  (1280, 1)\n",
            "encoder.conv2.bias 2 (1280, 1)\n",
            "  Converting to float32\n",
            "model.encoder.embed_positions.weight  ->  encoder.positional_embedding\n",
            "encoder.positional_embedding 2 (1500, 1280)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn.k_proj.weight  ->  encoder.blocks.0.attn.key.weight\n",
            "encoder.blocks.0.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.0.self_attn.v_proj.weight  ->  encoder.blocks.0.attn.value.weight\n",
            "encoder.blocks.0.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.0.self_attn.v_proj.bias  ->  encoder.blocks.0.attn.value.bias\n",
            "encoder.blocks.0.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn.q_proj.weight  ->  encoder.blocks.0.attn.query.weight\n",
            "encoder.blocks.0.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.0.self_attn.q_proj.bias  ->  encoder.blocks.0.attn.query.bias\n",
            "encoder.blocks.0.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn.out_proj.weight  ->  encoder.blocks.0.attn.out.weight\n",
            "encoder.blocks.0.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.0.self_attn.out_proj.bias  ->  encoder.blocks.0.attn.out.bias\n",
            "encoder.blocks.0.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn_layer_norm.weight  ->  encoder.blocks.0.attn_ln.weight\n",
            "encoder.blocks.0.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn_layer_norm.bias  ->  encoder.blocks.0.attn_ln.bias\n",
            "encoder.blocks.0.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.fc1.weight  ->  encoder.blocks.0.mlp.0.weight\n",
            "encoder.blocks.0.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.0.fc1.bias  ->  encoder.blocks.0.mlp.0.bias\n",
            "encoder.blocks.0.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.fc2.weight  ->  encoder.blocks.0.mlp.2.weight\n",
            "encoder.blocks.0.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.0.fc2.bias  ->  encoder.blocks.0.mlp.2.bias\n",
            "encoder.blocks.0.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.final_layer_norm.weight  ->  encoder.blocks.0.mlp_ln.weight\n",
            "encoder.blocks.0.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.final_layer_norm.bias  ->  encoder.blocks.0.mlp_ln.bias\n",
            "encoder.blocks.0.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn.k_proj.weight  ->  encoder.blocks.1.attn.key.weight\n",
            "encoder.blocks.1.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.1.self_attn.v_proj.weight  ->  encoder.blocks.1.attn.value.weight\n",
            "encoder.blocks.1.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.1.self_attn.v_proj.bias  ->  encoder.blocks.1.attn.value.bias\n",
            "encoder.blocks.1.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn.q_proj.weight  ->  encoder.blocks.1.attn.query.weight\n",
            "encoder.blocks.1.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.1.self_attn.q_proj.bias  ->  encoder.blocks.1.attn.query.bias\n",
            "encoder.blocks.1.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn.out_proj.weight  ->  encoder.blocks.1.attn.out.weight\n",
            "encoder.blocks.1.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.1.self_attn.out_proj.bias  ->  encoder.blocks.1.attn.out.bias\n",
            "encoder.blocks.1.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn_layer_norm.weight  ->  encoder.blocks.1.attn_ln.weight\n",
            "encoder.blocks.1.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn_layer_norm.bias  ->  encoder.blocks.1.attn_ln.bias\n",
            "encoder.blocks.1.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.fc1.weight  ->  encoder.blocks.1.mlp.0.weight\n",
            "encoder.blocks.1.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.1.fc1.bias  ->  encoder.blocks.1.mlp.0.bias\n",
            "encoder.blocks.1.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.fc2.weight  ->  encoder.blocks.1.mlp.2.weight\n",
            "encoder.blocks.1.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.1.fc2.bias  ->  encoder.blocks.1.mlp.2.bias\n",
            "encoder.blocks.1.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.final_layer_norm.weight  ->  encoder.blocks.1.mlp_ln.weight\n",
            "encoder.blocks.1.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.final_layer_norm.bias  ->  encoder.blocks.1.mlp_ln.bias\n",
            "encoder.blocks.1.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn.k_proj.weight  ->  encoder.blocks.2.attn.key.weight\n",
            "encoder.blocks.2.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.2.self_attn.v_proj.weight  ->  encoder.blocks.2.attn.value.weight\n",
            "encoder.blocks.2.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.2.self_attn.v_proj.bias  ->  encoder.blocks.2.attn.value.bias\n",
            "encoder.blocks.2.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn.q_proj.weight  ->  encoder.blocks.2.attn.query.weight\n",
            "encoder.blocks.2.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.2.self_attn.q_proj.bias  ->  encoder.blocks.2.attn.query.bias\n",
            "encoder.blocks.2.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn.out_proj.weight  ->  encoder.blocks.2.attn.out.weight\n",
            "encoder.blocks.2.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.2.self_attn.out_proj.bias  ->  encoder.blocks.2.attn.out.bias\n",
            "encoder.blocks.2.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn_layer_norm.weight  ->  encoder.blocks.2.attn_ln.weight\n",
            "encoder.blocks.2.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn_layer_norm.bias  ->  encoder.blocks.2.attn_ln.bias\n",
            "encoder.blocks.2.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.fc1.weight  ->  encoder.blocks.2.mlp.0.weight\n",
            "encoder.blocks.2.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.2.fc1.bias  ->  encoder.blocks.2.mlp.0.bias\n",
            "encoder.blocks.2.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.fc2.weight  ->  encoder.blocks.2.mlp.2.weight\n",
            "encoder.blocks.2.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.2.fc2.bias  ->  encoder.blocks.2.mlp.2.bias\n",
            "encoder.blocks.2.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.final_layer_norm.weight  ->  encoder.blocks.2.mlp_ln.weight\n",
            "encoder.blocks.2.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.final_layer_norm.bias  ->  encoder.blocks.2.mlp_ln.bias\n",
            "encoder.blocks.2.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn.k_proj.weight  ->  encoder.blocks.3.attn.key.weight\n",
            "encoder.blocks.3.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.3.self_attn.v_proj.weight  ->  encoder.blocks.3.attn.value.weight\n",
            "encoder.blocks.3.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.3.self_attn.v_proj.bias  ->  encoder.blocks.3.attn.value.bias\n",
            "encoder.blocks.3.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn.q_proj.weight  ->  encoder.blocks.3.attn.query.weight\n",
            "encoder.blocks.3.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.3.self_attn.q_proj.bias  ->  encoder.blocks.3.attn.query.bias\n",
            "encoder.blocks.3.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn.out_proj.weight  ->  encoder.blocks.3.attn.out.weight\n",
            "encoder.blocks.3.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.3.self_attn.out_proj.bias  ->  encoder.blocks.3.attn.out.bias\n",
            "encoder.blocks.3.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn_layer_norm.weight  ->  encoder.blocks.3.attn_ln.weight\n",
            "encoder.blocks.3.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn_layer_norm.bias  ->  encoder.blocks.3.attn_ln.bias\n",
            "encoder.blocks.3.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.fc1.weight  ->  encoder.blocks.3.mlp.0.weight\n",
            "encoder.blocks.3.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.3.fc1.bias  ->  encoder.blocks.3.mlp.0.bias\n",
            "encoder.blocks.3.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.fc2.weight  ->  encoder.blocks.3.mlp.2.weight\n",
            "encoder.blocks.3.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.3.fc2.bias  ->  encoder.blocks.3.mlp.2.bias\n",
            "encoder.blocks.3.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.final_layer_norm.weight  ->  encoder.blocks.3.mlp_ln.weight\n",
            "encoder.blocks.3.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.final_layer_norm.bias  ->  encoder.blocks.3.mlp_ln.bias\n",
            "encoder.blocks.3.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.self_attn.k_proj.weight  ->  encoder.blocks.4.attn.key.weight\n",
            "encoder.blocks.4.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.4.self_attn.v_proj.weight  ->  encoder.blocks.4.attn.value.weight\n",
            "encoder.blocks.4.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.4.self_attn.v_proj.bias  ->  encoder.blocks.4.attn.value.bias\n",
            "encoder.blocks.4.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.self_attn.q_proj.weight  ->  encoder.blocks.4.attn.query.weight\n",
            "encoder.blocks.4.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.4.self_attn.q_proj.bias  ->  encoder.blocks.4.attn.query.bias\n",
            "encoder.blocks.4.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.self_attn.out_proj.weight  ->  encoder.blocks.4.attn.out.weight\n",
            "encoder.blocks.4.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.4.self_attn.out_proj.bias  ->  encoder.blocks.4.attn.out.bias\n",
            "encoder.blocks.4.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.self_attn_layer_norm.weight  ->  encoder.blocks.4.attn_ln.weight\n",
            "encoder.blocks.4.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.self_attn_layer_norm.bias  ->  encoder.blocks.4.attn_ln.bias\n",
            "encoder.blocks.4.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.fc1.weight  ->  encoder.blocks.4.mlp.0.weight\n",
            "encoder.blocks.4.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.4.fc1.bias  ->  encoder.blocks.4.mlp.0.bias\n",
            "encoder.blocks.4.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.fc2.weight  ->  encoder.blocks.4.mlp.2.weight\n",
            "encoder.blocks.4.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.4.fc2.bias  ->  encoder.blocks.4.mlp.2.bias\n",
            "encoder.blocks.4.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.final_layer_norm.weight  ->  encoder.blocks.4.mlp_ln.weight\n",
            "encoder.blocks.4.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.4.final_layer_norm.bias  ->  encoder.blocks.4.mlp_ln.bias\n",
            "encoder.blocks.4.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.self_attn.k_proj.weight  ->  encoder.blocks.5.attn.key.weight\n",
            "encoder.blocks.5.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.5.self_attn.v_proj.weight  ->  encoder.blocks.5.attn.value.weight\n",
            "encoder.blocks.5.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.5.self_attn.v_proj.bias  ->  encoder.blocks.5.attn.value.bias\n",
            "encoder.blocks.5.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.self_attn.q_proj.weight  ->  encoder.blocks.5.attn.query.weight\n",
            "encoder.blocks.5.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.5.self_attn.q_proj.bias  ->  encoder.blocks.5.attn.query.bias\n",
            "encoder.blocks.5.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.self_attn.out_proj.weight  ->  encoder.blocks.5.attn.out.weight\n",
            "encoder.blocks.5.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.5.self_attn.out_proj.bias  ->  encoder.blocks.5.attn.out.bias\n",
            "encoder.blocks.5.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.self_attn_layer_norm.weight  ->  encoder.blocks.5.attn_ln.weight\n",
            "encoder.blocks.5.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.self_attn_layer_norm.bias  ->  encoder.blocks.5.attn_ln.bias\n",
            "encoder.blocks.5.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.fc1.weight  ->  encoder.blocks.5.mlp.0.weight\n",
            "encoder.blocks.5.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.5.fc1.bias  ->  encoder.blocks.5.mlp.0.bias\n",
            "encoder.blocks.5.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.fc2.weight  ->  encoder.blocks.5.mlp.2.weight\n",
            "encoder.blocks.5.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.5.fc2.bias  ->  encoder.blocks.5.mlp.2.bias\n",
            "encoder.blocks.5.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.final_layer_norm.weight  ->  encoder.blocks.5.mlp_ln.weight\n",
            "encoder.blocks.5.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.5.final_layer_norm.bias  ->  encoder.blocks.5.mlp_ln.bias\n",
            "encoder.blocks.5.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.self_attn.k_proj.weight  ->  encoder.blocks.6.attn.key.weight\n",
            "encoder.blocks.6.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.6.self_attn.v_proj.weight  ->  encoder.blocks.6.attn.value.weight\n",
            "encoder.blocks.6.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.6.self_attn.v_proj.bias  ->  encoder.blocks.6.attn.value.bias\n",
            "encoder.blocks.6.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.self_attn.q_proj.weight  ->  encoder.blocks.6.attn.query.weight\n",
            "encoder.blocks.6.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.6.self_attn.q_proj.bias  ->  encoder.blocks.6.attn.query.bias\n",
            "encoder.blocks.6.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.self_attn.out_proj.weight  ->  encoder.blocks.6.attn.out.weight\n",
            "encoder.blocks.6.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.6.self_attn.out_proj.bias  ->  encoder.blocks.6.attn.out.bias\n",
            "encoder.blocks.6.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.self_attn_layer_norm.weight  ->  encoder.blocks.6.attn_ln.weight\n",
            "encoder.blocks.6.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.self_attn_layer_norm.bias  ->  encoder.blocks.6.attn_ln.bias\n",
            "encoder.blocks.6.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.fc1.weight  ->  encoder.blocks.6.mlp.0.weight\n",
            "encoder.blocks.6.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.6.fc1.bias  ->  encoder.blocks.6.mlp.0.bias\n",
            "encoder.blocks.6.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.fc2.weight  ->  encoder.blocks.6.mlp.2.weight\n",
            "encoder.blocks.6.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.6.fc2.bias  ->  encoder.blocks.6.mlp.2.bias\n",
            "encoder.blocks.6.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.final_layer_norm.weight  ->  encoder.blocks.6.mlp_ln.weight\n",
            "encoder.blocks.6.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.6.final_layer_norm.bias  ->  encoder.blocks.6.mlp_ln.bias\n",
            "encoder.blocks.6.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.self_attn.k_proj.weight  ->  encoder.blocks.7.attn.key.weight\n",
            "encoder.blocks.7.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.7.self_attn.v_proj.weight  ->  encoder.blocks.7.attn.value.weight\n",
            "encoder.blocks.7.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.7.self_attn.v_proj.bias  ->  encoder.blocks.7.attn.value.bias\n",
            "encoder.blocks.7.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.self_attn.q_proj.weight  ->  encoder.blocks.7.attn.query.weight\n",
            "encoder.blocks.7.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.7.self_attn.q_proj.bias  ->  encoder.blocks.7.attn.query.bias\n",
            "encoder.blocks.7.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.self_attn.out_proj.weight  ->  encoder.blocks.7.attn.out.weight\n",
            "encoder.blocks.7.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.7.self_attn.out_proj.bias  ->  encoder.blocks.7.attn.out.bias\n",
            "encoder.blocks.7.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.self_attn_layer_norm.weight  ->  encoder.blocks.7.attn_ln.weight\n",
            "encoder.blocks.7.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.self_attn_layer_norm.bias  ->  encoder.blocks.7.attn_ln.bias\n",
            "encoder.blocks.7.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.fc1.weight  ->  encoder.blocks.7.mlp.0.weight\n",
            "encoder.blocks.7.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.7.fc1.bias  ->  encoder.blocks.7.mlp.0.bias\n",
            "encoder.blocks.7.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.fc2.weight  ->  encoder.blocks.7.mlp.2.weight\n",
            "encoder.blocks.7.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.7.fc2.bias  ->  encoder.blocks.7.mlp.2.bias\n",
            "encoder.blocks.7.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.final_layer_norm.weight  ->  encoder.blocks.7.mlp_ln.weight\n",
            "encoder.blocks.7.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.7.final_layer_norm.bias  ->  encoder.blocks.7.mlp_ln.bias\n",
            "encoder.blocks.7.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.self_attn.k_proj.weight  ->  encoder.blocks.8.attn.key.weight\n",
            "encoder.blocks.8.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.8.self_attn.v_proj.weight  ->  encoder.blocks.8.attn.value.weight\n",
            "encoder.blocks.8.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.8.self_attn.v_proj.bias  ->  encoder.blocks.8.attn.value.bias\n",
            "encoder.blocks.8.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.self_attn.q_proj.weight  ->  encoder.blocks.8.attn.query.weight\n",
            "encoder.blocks.8.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.8.self_attn.q_proj.bias  ->  encoder.blocks.8.attn.query.bias\n",
            "encoder.blocks.8.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.self_attn.out_proj.weight  ->  encoder.blocks.8.attn.out.weight\n",
            "encoder.blocks.8.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.8.self_attn.out_proj.bias  ->  encoder.blocks.8.attn.out.bias\n",
            "encoder.blocks.8.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.self_attn_layer_norm.weight  ->  encoder.blocks.8.attn_ln.weight\n",
            "encoder.blocks.8.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.self_attn_layer_norm.bias  ->  encoder.blocks.8.attn_ln.bias\n",
            "encoder.blocks.8.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.fc1.weight  ->  encoder.blocks.8.mlp.0.weight\n",
            "encoder.blocks.8.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.8.fc1.bias  ->  encoder.blocks.8.mlp.0.bias\n",
            "encoder.blocks.8.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.fc2.weight  ->  encoder.blocks.8.mlp.2.weight\n",
            "encoder.blocks.8.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.8.fc2.bias  ->  encoder.blocks.8.mlp.2.bias\n",
            "encoder.blocks.8.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.final_layer_norm.weight  ->  encoder.blocks.8.mlp_ln.weight\n",
            "encoder.blocks.8.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.8.final_layer_norm.bias  ->  encoder.blocks.8.mlp_ln.bias\n",
            "encoder.blocks.8.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.self_attn.k_proj.weight  ->  encoder.blocks.9.attn.key.weight\n",
            "encoder.blocks.9.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.9.self_attn.v_proj.weight  ->  encoder.blocks.9.attn.value.weight\n",
            "encoder.blocks.9.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.9.self_attn.v_proj.bias  ->  encoder.blocks.9.attn.value.bias\n",
            "encoder.blocks.9.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.self_attn.q_proj.weight  ->  encoder.blocks.9.attn.query.weight\n",
            "encoder.blocks.9.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.9.self_attn.q_proj.bias  ->  encoder.blocks.9.attn.query.bias\n",
            "encoder.blocks.9.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.self_attn.out_proj.weight  ->  encoder.blocks.9.attn.out.weight\n",
            "encoder.blocks.9.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.9.self_attn.out_proj.bias  ->  encoder.blocks.9.attn.out.bias\n",
            "encoder.blocks.9.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.self_attn_layer_norm.weight  ->  encoder.blocks.9.attn_ln.weight\n",
            "encoder.blocks.9.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.self_attn_layer_norm.bias  ->  encoder.blocks.9.attn_ln.bias\n",
            "encoder.blocks.9.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.fc1.weight  ->  encoder.blocks.9.mlp.0.weight\n",
            "encoder.blocks.9.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.9.fc1.bias  ->  encoder.blocks.9.mlp.0.bias\n",
            "encoder.blocks.9.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.fc2.weight  ->  encoder.blocks.9.mlp.2.weight\n",
            "encoder.blocks.9.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.9.fc2.bias  ->  encoder.blocks.9.mlp.2.bias\n",
            "encoder.blocks.9.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.final_layer_norm.weight  ->  encoder.blocks.9.mlp_ln.weight\n",
            "encoder.blocks.9.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.9.final_layer_norm.bias  ->  encoder.blocks.9.mlp_ln.bias\n",
            "encoder.blocks.9.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.self_attn.k_proj.weight  ->  encoder.blocks.10.attn.key.weight\n",
            "encoder.blocks.10.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.10.self_attn.v_proj.weight  ->  encoder.blocks.10.attn.value.weight\n",
            "encoder.blocks.10.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.10.self_attn.v_proj.bias  ->  encoder.blocks.10.attn.value.bias\n",
            "encoder.blocks.10.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.self_attn.q_proj.weight  ->  encoder.blocks.10.attn.query.weight\n",
            "encoder.blocks.10.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.10.self_attn.q_proj.bias  ->  encoder.blocks.10.attn.query.bias\n",
            "encoder.blocks.10.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.self_attn.out_proj.weight  ->  encoder.blocks.10.attn.out.weight\n",
            "encoder.blocks.10.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.10.self_attn.out_proj.bias  ->  encoder.blocks.10.attn.out.bias\n",
            "encoder.blocks.10.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.self_attn_layer_norm.weight  ->  encoder.blocks.10.attn_ln.weight\n",
            "encoder.blocks.10.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.self_attn_layer_norm.bias  ->  encoder.blocks.10.attn_ln.bias\n",
            "encoder.blocks.10.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.fc1.weight  ->  encoder.blocks.10.mlp.0.weight\n",
            "encoder.blocks.10.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.10.fc1.bias  ->  encoder.blocks.10.mlp.0.bias\n",
            "encoder.blocks.10.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.fc2.weight  ->  encoder.blocks.10.mlp.2.weight\n",
            "encoder.blocks.10.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.10.fc2.bias  ->  encoder.blocks.10.mlp.2.bias\n",
            "encoder.blocks.10.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.final_layer_norm.weight  ->  encoder.blocks.10.mlp_ln.weight\n",
            "encoder.blocks.10.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.10.final_layer_norm.bias  ->  encoder.blocks.10.mlp_ln.bias\n",
            "encoder.blocks.10.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.self_attn.k_proj.weight  ->  encoder.blocks.11.attn.key.weight\n",
            "encoder.blocks.11.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.11.self_attn.v_proj.weight  ->  encoder.blocks.11.attn.value.weight\n",
            "encoder.blocks.11.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.11.self_attn.v_proj.bias  ->  encoder.blocks.11.attn.value.bias\n",
            "encoder.blocks.11.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.self_attn.q_proj.weight  ->  encoder.blocks.11.attn.query.weight\n",
            "encoder.blocks.11.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.11.self_attn.q_proj.bias  ->  encoder.blocks.11.attn.query.bias\n",
            "encoder.blocks.11.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.self_attn.out_proj.weight  ->  encoder.blocks.11.attn.out.weight\n",
            "encoder.blocks.11.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.11.self_attn.out_proj.bias  ->  encoder.blocks.11.attn.out.bias\n",
            "encoder.blocks.11.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.self_attn_layer_norm.weight  ->  encoder.blocks.11.attn_ln.weight\n",
            "encoder.blocks.11.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.self_attn_layer_norm.bias  ->  encoder.blocks.11.attn_ln.bias\n",
            "encoder.blocks.11.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.fc1.weight  ->  encoder.blocks.11.mlp.0.weight\n",
            "encoder.blocks.11.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.11.fc1.bias  ->  encoder.blocks.11.mlp.0.bias\n",
            "encoder.blocks.11.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.fc2.weight  ->  encoder.blocks.11.mlp.2.weight\n",
            "encoder.blocks.11.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.11.fc2.bias  ->  encoder.blocks.11.mlp.2.bias\n",
            "encoder.blocks.11.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.final_layer_norm.weight  ->  encoder.blocks.11.mlp_ln.weight\n",
            "encoder.blocks.11.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.11.final_layer_norm.bias  ->  encoder.blocks.11.mlp_ln.bias\n",
            "encoder.blocks.11.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.self_attn.k_proj.weight  ->  encoder.blocks.12.attn.key.weight\n",
            "encoder.blocks.12.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.12.self_attn.v_proj.weight  ->  encoder.blocks.12.attn.value.weight\n",
            "encoder.blocks.12.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.12.self_attn.v_proj.bias  ->  encoder.blocks.12.attn.value.bias\n",
            "encoder.blocks.12.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.self_attn.q_proj.weight  ->  encoder.blocks.12.attn.query.weight\n",
            "encoder.blocks.12.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.12.self_attn.q_proj.bias  ->  encoder.blocks.12.attn.query.bias\n",
            "encoder.blocks.12.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.self_attn.out_proj.weight  ->  encoder.blocks.12.attn.out.weight\n",
            "encoder.blocks.12.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.12.self_attn.out_proj.bias  ->  encoder.blocks.12.attn.out.bias\n",
            "encoder.blocks.12.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.self_attn_layer_norm.weight  ->  encoder.blocks.12.attn_ln.weight\n",
            "encoder.blocks.12.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.self_attn_layer_norm.bias  ->  encoder.blocks.12.attn_ln.bias\n",
            "encoder.blocks.12.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.fc1.weight  ->  encoder.blocks.12.mlp.0.weight\n",
            "encoder.blocks.12.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.12.fc1.bias  ->  encoder.blocks.12.mlp.0.bias\n",
            "encoder.blocks.12.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.fc2.weight  ->  encoder.blocks.12.mlp.2.weight\n",
            "encoder.blocks.12.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.12.fc2.bias  ->  encoder.blocks.12.mlp.2.bias\n",
            "encoder.blocks.12.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.final_layer_norm.weight  ->  encoder.blocks.12.mlp_ln.weight\n",
            "encoder.blocks.12.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.12.final_layer_norm.bias  ->  encoder.blocks.12.mlp_ln.bias\n",
            "encoder.blocks.12.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.self_attn.k_proj.weight  ->  encoder.blocks.13.attn.key.weight\n",
            "encoder.blocks.13.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.13.self_attn.v_proj.weight  ->  encoder.blocks.13.attn.value.weight\n",
            "encoder.blocks.13.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.13.self_attn.v_proj.bias  ->  encoder.blocks.13.attn.value.bias\n",
            "encoder.blocks.13.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.self_attn.q_proj.weight  ->  encoder.blocks.13.attn.query.weight\n",
            "encoder.blocks.13.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.13.self_attn.q_proj.bias  ->  encoder.blocks.13.attn.query.bias\n",
            "encoder.blocks.13.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.self_attn.out_proj.weight  ->  encoder.blocks.13.attn.out.weight\n",
            "encoder.blocks.13.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.13.self_attn.out_proj.bias  ->  encoder.blocks.13.attn.out.bias\n",
            "encoder.blocks.13.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.self_attn_layer_norm.weight  ->  encoder.blocks.13.attn_ln.weight\n",
            "encoder.blocks.13.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.self_attn_layer_norm.bias  ->  encoder.blocks.13.attn_ln.bias\n",
            "encoder.blocks.13.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.fc1.weight  ->  encoder.blocks.13.mlp.0.weight\n",
            "encoder.blocks.13.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.13.fc1.bias  ->  encoder.blocks.13.mlp.0.bias\n",
            "encoder.blocks.13.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.fc2.weight  ->  encoder.blocks.13.mlp.2.weight\n",
            "encoder.blocks.13.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.13.fc2.bias  ->  encoder.blocks.13.mlp.2.bias\n",
            "encoder.blocks.13.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.final_layer_norm.weight  ->  encoder.blocks.13.mlp_ln.weight\n",
            "encoder.blocks.13.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.13.final_layer_norm.bias  ->  encoder.blocks.13.mlp_ln.bias\n",
            "encoder.blocks.13.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.self_attn.k_proj.weight  ->  encoder.blocks.14.attn.key.weight\n",
            "encoder.blocks.14.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.14.self_attn.v_proj.weight  ->  encoder.blocks.14.attn.value.weight\n",
            "encoder.blocks.14.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.14.self_attn.v_proj.bias  ->  encoder.blocks.14.attn.value.bias\n",
            "encoder.blocks.14.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.self_attn.q_proj.weight  ->  encoder.blocks.14.attn.query.weight\n",
            "encoder.blocks.14.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.14.self_attn.q_proj.bias  ->  encoder.blocks.14.attn.query.bias\n",
            "encoder.blocks.14.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.self_attn.out_proj.weight  ->  encoder.blocks.14.attn.out.weight\n",
            "encoder.blocks.14.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.14.self_attn.out_proj.bias  ->  encoder.blocks.14.attn.out.bias\n",
            "encoder.blocks.14.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.self_attn_layer_norm.weight  ->  encoder.blocks.14.attn_ln.weight\n",
            "encoder.blocks.14.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.self_attn_layer_norm.bias  ->  encoder.blocks.14.attn_ln.bias\n",
            "encoder.blocks.14.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.fc1.weight  ->  encoder.blocks.14.mlp.0.weight\n",
            "encoder.blocks.14.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.14.fc1.bias  ->  encoder.blocks.14.mlp.0.bias\n",
            "encoder.blocks.14.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.fc2.weight  ->  encoder.blocks.14.mlp.2.weight\n",
            "encoder.blocks.14.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.14.fc2.bias  ->  encoder.blocks.14.mlp.2.bias\n",
            "encoder.blocks.14.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.final_layer_norm.weight  ->  encoder.blocks.14.mlp_ln.weight\n",
            "encoder.blocks.14.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.14.final_layer_norm.bias  ->  encoder.blocks.14.mlp_ln.bias\n",
            "encoder.blocks.14.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.self_attn.k_proj.weight  ->  encoder.blocks.15.attn.key.weight\n",
            "encoder.blocks.15.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.15.self_attn.v_proj.weight  ->  encoder.blocks.15.attn.value.weight\n",
            "encoder.blocks.15.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.15.self_attn.v_proj.bias  ->  encoder.blocks.15.attn.value.bias\n",
            "encoder.blocks.15.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.self_attn.q_proj.weight  ->  encoder.blocks.15.attn.query.weight\n",
            "encoder.blocks.15.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.15.self_attn.q_proj.bias  ->  encoder.blocks.15.attn.query.bias\n",
            "encoder.blocks.15.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.self_attn.out_proj.weight  ->  encoder.blocks.15.attn.out.weight\n",
            "encoder.blocks.15.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.15.self_attn.out_proj.bias  ->  encoder.blocks.15.attn.out.bias\n",
            "encoder.blocks.15.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.self_attn_layer_norm.weight  ->  encoder.blocks.15.attn_ln.weight\n",
            "encoder.blocks.15.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.self_attn_layer_norm.bias  ->  encoder.blocks.15.attn_ln.bias\n",
            "encoder.blocks.15.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.fc1.weight  ->  encoder.blocks.15.mlp.0.weight\n",
            "encoder.blocks.15.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.15.fc1.bias  ->  encoder.blocks.15.mlp.0.bias\n",
            "encoder.blocks.15.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.fc2.weight  ->  encoder.blocks.15.mlp.2.weight\n",
            "encoder.blocks.15.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.15.fc2.bias  ->  encoder.blocks.15.mlp.2.bias\n",
            "encoder.blocks.15.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.final_layer_norm.weight  ->  encoder.blocks.15.mlp_ln.weight\n",
            "encoder.blocks.15.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.15.final_layer_norm.bias  ->  encoder.blocks.15.mlp_ln.bias\n",
            "encoder.blocks.15.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.self_attn.k_proj.weight  ->  encoder.blocks.16.attn.key.weight\n",
            "encoder.blocks.16.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.16.self_attn.v_proj.weight  ->  encoder.blocks.16.attn.value.weight\n",
            "encoder.blocks.16.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.16.self_attn.v_proj.bias  ->  encoder.blocks.16.attn.value.bias\n",
            "encoder.blocks.16.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.self_attn.q_proj.weight  ->  encoder.blocks.16.attn.query.weight\n",
            "encoder.blocks.16.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.16.self_attn.q_proj.bias  ->  encoder.blocks.16.attn.query.bias\n",
            "encoder.blocks.16.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.self_attn.out_proj.weight  ->  encoder.blocks.16.attn.out.weight\n",
            "encoder.blocks.16.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.16.self_attn.out_proj.bias  ->  encoder.blocks.16.attn.out.bias\n",
            "encoder.blocks.16.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.self_attn_layer_norm.weight  ->  encoder.blocks.16.attn_ln.weight\n",
            "encoder.blocks.16.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.self_attn_layer_norm.bias  ->  encoder.blocks.16.attn_ln.bias\n",
            "encoder.blocks.16.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.fc1.weight  ->  encoder.blocks.16.mlp.0.weight\n",
            "encoder.blocks.16.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.16.fc1.bias  ->  encoder.blocks.16.mlp.0.bias\n",
            "encoder.blocks.16.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.fc2.weight  ->  encoder.blocks.16.mlp.2.weight\n",
            "encoder.blocks.16.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.16.fc2.bias  ->  encoder.blocks.16.mlp.2.bias\n",
            "encoder.blocks.16.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.final_layer_norm.weight  ->  encoder.blocks.16.mlp_ln.weight\n",
            "encoder.blocks.16.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.16.final_layer_norm.bias  ->  encoder.blocks.16.mlp_ln.bias\n",
            "encoder.blocks.16.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.self_attn.k_proj.weight  ->  encoder.blocks.17.attn.key.weight\n",
            "encoder.blocks.17.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.17.self_attn.v_proj.weight  ->  encoder.blocks.17.attn.value.weight\n",
            "encoder.blocks.17.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.17.self_attn.v_proj.bias  ->  encoder.blocks.17.attn.value.bias\n",
            "encoder.blocks.17.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.self_attn.q_proj.weight  ->  encoder.blocks.17.attn.query.weight\n",
            "encoder.blocks.17.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.17.self_attn.q_proj.bias  ->  encoder.blocks.17.attn.query.bias\n",
            "encoder.blocks.17.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.self_attn.out_proj.weight  ->  encoder.blocks.17.attn.out.weight\n",
            "encoder.blocks.17.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.17.self_attn.out_proj.bias  ->  encoder.blocks.17.attn.out.bias\n",
            "encoder.blocks.17.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.self_attn_layer_norm.weight  ->  encoder.blocks.17.attn_ln.weight\n",
            "encoder.blocks.17.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.self_attn_layer_norm.bias  ->  encoder.blocks.17.attn_ln.bias\n",
            "encoder.blocks.17.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.fc1.weight  ->  encoder.blocks.17.mlp.0.weight\n",
            "encoder.blocks.17.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.17.fc1.bias  ->  encoder.blocks.17.mlp.0.bias\n",
            "encoder.blocks.17.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.fc2.weight  ->  encoder.blocks.17.mlp.2.weight\n",
            "encoder.blocks.17.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.17.fc2.bias  ->  encoder.blocks.17.mlp.2.bias\n",
            "encoder.blocks.17.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.final_layer_norm.weight  ->  encoder.blocks.17.mlp_ln.weight\n",
            "encoder.blocks.17.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.17.final_layer_norm.bias  ->  encoder.blocks.17.mlp_ln.bias\n",
            "encoder.blocks.17.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.self_attn.k_proj.weight  ->  encoder.blocks.18.attn.key.weight\n",
            "encoder.blocks.18.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.18.self_attn.v_proj.weight  ->  encoder.blocks.18.attn.value.weight\n",
            "encoder.blocks.18.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.18.self_attn.v_proj.bias  ->  encoder.blocks.18.attn.value.bias\n",
            "encoder.blocks.18.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.self_attn.q_proj.weight  ->  encoder.blocks.18.attn.query.weight\n",
            "encoder.blocks.18.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.18.self_attn.q_proj.bias  ->  encoder.blocks.18.attn.query.bias\n",
            "encoder.blocks.18.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.self_attn.out_proj.weight  ->  encoder.blocks.18.attn.out.weight\n",
            "encoder.blocks.18.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.18.self_attn.out_proj.bias  ->  encoder.blocks.18.attn.out.bias\n",
            "encoder.blocks.18.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.self_attn_layer_norm.weight  ->  encoder.blocks.18.attn_ln.weight\n",
            "encoder.blocks.18.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.self_attn_layer_norm.bias  ->  encoder.blocks.18.attn_ln.bias\n",
            "encoder.blocks.18.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.fc1.weight  ->  encoder.blocks.18.mlp.0.weight\n",
            "encoder.blocks.18.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.18.fc1.bias  ->  encoder.blocks.18.mlp.0.bias\n",
            "encoder.blocks.18.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.fc2.weight  ->  encoder.blocks.18.mlp.2.weight\n",
            "encoder.blocks.18.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.18.fc2.bias  ->  encoder.blocks.18.mlp.2.bias\n",
            "encoder.blocks.18.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.final_layer_norm.weight  ->  encoder.blocks.18.mlp_ln.weight\n",
            "encoder.blocks.18.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.18.final_layer_norm.bias  ->  encoder.blocks.18.mlp_ln.bias\n",
            "encoder.blocks.18.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.self_attn.k_proj.weight  ->  encoder.blocks.19.attn.key.weight\n",
            "encoder.blocks.19.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.19.self_attn.v_proj.weight  ->  encoder.blocks.19.attn.value.weight\n",
            "encoder.blocks.19.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.19.self_attn.v_proj.bias  ->  encoder.blocks.19.attn.value.bias\n",
            "encoder.blocks.19.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.self_attn.q_proj.weight  ->  encoder.blocks.19.attn.query.weight\n",
            "encoder.blocks.19.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.19.self_attn.q_proj.bias  ->  encoder.blocks.19.attn.query.bias\n",
            "encoder.blocks.19.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.self_attn.out_proj.weight  ->  encoder.blocks.19.attn.out.weight\n",
            "encoder.blocks.19.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.19.self_attn.out_proj.bias  ->  encoder.blocks.19.attn.out.bias\n",
            "encoder.blocks.19.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.self_attn_layer_norm.weight  ->  encoder.blocks.19.attn_ln.weight\n",
            "encoder.blocks.19.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.self_attn_layer_norm.bias  ->  encoder.blocks.19.attn_ln.bias\n",
            "encoder.blocks.19.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.fc1.weight  ->  encoder.blocks.19.mlp.0.weight\n",
            "encoder.blocks.19.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.19.fc1.bias  ->  encoder.blocks.19.mlp.0.bias\n",
            "encoder.blocks.19.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.fc2.weight  ->  encoder.blocks.19.mlp.2.weight\n",
            "encoder.blocks.19.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.19.fc2.bias  ->  encoder.blocks.19.mlp.2.bias\n",
            "encoder.blocks.19.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.final_layer_norm.weight  ->  encoder.blocks.19.mlp_ln.weight\n",
            "encoder.blocks.19.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.19.final_layer_norm.bias  ->  encoder.blocks.19.mlp_ln.bias\n",
            "encoder.blocks.19.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.self_attn.k_proj.weight  ->  encoder.blocks.20.attn.key.weight\n",
            "encoder.blocks.20.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.20.self_attn.v_proj.weight  ->  encoder.blocks.20.attn.value.weight\n",
            "encoder.blocks.20.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.20.self_attn.v_proj.bias  ->  encoder.blocks.20.attn.value.bias\n",
            "encoder.blocks.20.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.self_attn.q_proj.weight  ->  encoder.blocks.20.attn.query.weight\n",
            "encoder.blocks.20.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.20.self_attn.q_proj.bias  ->  encoder.blocks.20.attn.query.bias\n",
            "encoder.blocks.20.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.self_attn.out_proj.weight  ->  encoder.blocks.20.attn.out.weight\n",
            "encoder.blocks.20.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.20.self_attn.out_proj.bias  ->  encoder.blocks.20.attn.out.bias\n",
            "encoder.blocks.20.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.self_attn_layer_norm.weight  ->  encoder.blocks.20.attn_ln.weight\n",
            "encoder.blocks.20.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.self_attn_layer_norm.bias  ->  encoder.blocks.20.attn_ln.bias\n",
            "encoder.blocks.20.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.fc1.weight  ->  encoder.blocks.20.mlp.0.weight\n",
            "encoder.blocks.20.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.20.fc1.bias  ->  encoder.blocks.20.mlp.0.bias\n",
            "encoder.blocks.20.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.fc2.weight  ->  encoder.blocks.20.mlp.2.weight\n",
            "encoder.blocks.20.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.20.fc2.bias  ->  encoder.blocks.20.mlp.2.bias\n",
            "encoder.blocks.20.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.final_layer_norm.weight  ->  encoder.blocks.20.mlp_ln.weight\n",
            "encoder.blocks.20.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.20.final_layer_norm.bias  ->  encoder.blocks.20.mlp_ln.bias\n",
            "encoder.blocks.20.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.self_attn.k_proj.weight  ->  encoder.blocks.21.attn.key.weight\n",
            "encoder.blocks.21.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.21.self_attn.v_proj.weight  ->  encoder.blocks.21.attn.value.weight\n",
            "encoder.blocks.21.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.21.self_attn.v_proj.bias  ->  encoder.blocks.21.attn.value.bias\n",
            "encoder.blocks.21.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.self_attn.q_proj.weight  ->  encoder.blocks.21.attn.query.weight\n",
            "encoder.blocks.21.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.21.self_attn.q_proj.bias  ->  encoder.blocks.21.attn.query.bias\n",
            "encoder.blocks.21.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.self_attn.out_proj.weight  ->  encoder.blocks.21.attn.out.weight\n",
            "encoder.blocks.21.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.21.self_attn.out_proj.bias  ->  encoder.blocks.21.attn.out.bias\n",
            "encoder.blocks.21.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.self_attn_layer_norm.weight  ->  encoder.blocks.21.attn_ln.weight\n",
            "encoder.blocks.21.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.self_attn_layer_norm.bias  ->  encoder.blocks.21.attn_ln.bias\n",
            "encoder.blocks.21.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.fc1.weight  ->  encoder.blocks.21.mlp.0.weight\n",
            "encoder.blocks.21.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.21.fc1.bias  ->  encoder.blocks.21.mlp.0.bias\n",
            "encoder.blocks.21.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.fc2.weight  ->  encoder.blocks.21.mlp.2.weight\n",
            "encoder.blocks.21.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.21.fc2.bias  ->  encoder.blocks.21.mlp.2.bias\n",
            "encoder.blocks.21.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.final_layer_norm.weight  ->  encoder.blocks.21.mlp_ln.weight\n",
            "encoder.blocks.21.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.21.final_layer_norm.bias  ->  encoder.blocks.21.mlp_ln.bias\n",
            "encoder.blocks.21.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.self_attn.k_proj.weight  ->  encoder.blocks.22.attn.key.weight\n",
            "encoder.blocks.22.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.22.self_attn.v_proj.weight  ->  encoder.blocks.22.attn.value.weight\n",
            "encoder.blocks.22.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.22.self_attn.v_proj.bias  ->  encoder.blocks.22.attn.value.bias\n",
            "encoder.blocks.22.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.self_attn.q_proj.weight  ->  encoder.blocks.22.attn.query.weight\n",
            "encoder.blocks.22.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.22.self_attn.q_proj.bias  ->  encoder.blocks.22.attn.query.bias\n",
            "encoder.blocks.22.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.self_attn.out_proj.weight  ->  encoder.blocks.22.attn.out.weight\n",
            "encoder.blocks.22.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.22.self_attn.out_proj.bias  ->  encoder.blocks.22.attn.out.bias\n",
            "encoder.blocks.22.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.self_attn_layer_norm.weight  ->  encoder.blocks.22.attn_ln.weight\n",
            "encoder.blocks.22.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.self_attn_layer_norm.bias  ->  encoder.blocks.22.attn_ln.bias\n",
            "encoder.blocks.22.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.fc1.weight  ->  encoder.blocks.22.mlp.0.weight\n",
            "encoder.blocks.22.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.22.fc1.bias  ->  encoder.blocks.22.mlp.0.bias\n",
            "encoder.blocks.22.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.fc2.weight  ->  encoder.blocks.22.mlp.2.weight\n",
            "encoder.blocks.22.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.22.fc2.bias  ->  encoder.blocks.22.mlp.2.bias\n",
            "encoder.blocks.22.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.final_layer_norm.weight  ->  encoder.blocks.22.mlp_ln.weight\n",
            "encoder.blocks.22.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.22.final_layer_norm.bias  ->  encoder.blocks.22.mlp_ln.bias\n",
            "encoder.blocks.22.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.self_attn.k_proj.weight  ->  encoder.blocks.23.attn.key.weight\n",
            "encoder.blocks.23.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.23.self_attn.v_proj.weight  ->  encoder.blocks.23.attn.value.weight\n",
            "encoder.blocks.23.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.23.self_attn.v_proj.bias  ->  encoder.blocks.23.attn.value.bias\n",
            "encoder.blocks.23.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.self_attn.q_proj.weight  ->  encoder.blocks.23.attn.query.weight\n",
            "encoder.blocks.23.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.23.self_attn.q_proj.bias  ->  encoder.blocks.23.attn.query.bias\n",
            "encoder.blocks.23.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.self_attn.out_proj.weight  ->  encoder.blocks.23.attn.out.weight\n",
            "encoder.blocks.23.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.23.self_attn.out_proj.bias  ->  encoder.blocks.23.attn.out.bias\n",
            "encoder.blocks.23.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.self_attn_layer_norm.weight  ->  encoder.blocks.23.attn_ln.weight\n",
            "encoder.blocks.23.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.self_attn_layer_norm.bias  ->  encoder.blocks.23.attn_ln.bias\n",
            "encoder.blocks.23.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.fc1.weight  ->  encoder.blocks.23.mlp.0.weight\n",
            "encoder.blocks.23.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.23.fc1.bias  ->  encoder.blocks.23.mlp.0.bias\n",
            "encoder.blocks.23.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.fc2.weight  ->  encoder.blocks.23.mlp.2.weight\n",
            "encoder.blocks.23.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.23.fc2.bias  ->  encoder.blocks.23.mlp.2.bias\n",
            "encoder.blocks.23.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.final_layer_norm.weight  ->  encoder.blocks.23.mlp_ln.weight\n",
            "encoder.blocks.23.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.23.final_layer_norm.bias  ->  encoder.blocks.23.mlp_ln.bias\n",
            "encoder.blocks.23.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.self_attn.k_proj.weight  ->  encoder.blocks.24.attn.key.weight\n",
            "encoder.blocks.24.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.24.self_attn.v_proj.weight  ->  encoder.blocks.24.attn.value.weight\n",
            "encoder.blocks.24.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.24.self_attn.v_proj.bias  ->  encoder.blocks.24.attn.value.bias\n",
            "encoder.blocks.24.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.self_attn.q_proj.weight  ->  encoder.blocks.24.attn.query.weight\n",
            "encoder.blocks.24.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.24.self_attn.q_proj.bias  ->  encoder.blocks.24.attn.query.bias\n",
            "encoder.blocks.24.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.self_attn.out_proj.weight  ->  encoder.blocks.24.attn.out.weight\n",
            "encoder.blocks.24.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.24.self_attn.out_proj.bias  ->  encoder.blocks.24.attn.out.bias\n",
            "encoder.blocks.24.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.self_attn_layer_norm.weight  ->  encoder.blocks.24.attn_ln.weight\n",
            "encoder.blocks.24.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.self_attn_layer_norm.bias  ->  encoder.blocks.24.attn_ln.bias\n",
            "encoder.blocks.24.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.fc1.weight  ->  encoder.blocks.24.mlp.0.weight\n",
            "encoder.blocks.24.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.24.fc1.bias  ->  encoder.blocks.24.mlp.0.bias\n",
            "encoder.blocks.24.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.fc2.weight  ->  encoder.blocks.24.mlp.2.weight\n",
            "encoder.blocks.24.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.24.fc2.bias  ->  encoder.blocks.24.mlp.2.bias\n",
            "encoder.blocks.24.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.final_layer_norm.weight  ->  encoder.blocks.24.mlp_ln.weight\n",
            "encoder.blocks.24.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.24.final_layer_norm.bias  ->  encoder.blocks.24.mlp_ln.bias\n",
            "encoder.blocks.24.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.self_attn.k_proj.weight  ->  encoder.blocks.25.attn.key.weight\n",
            "encoder.blocks.25.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.25.self_attn.v_proj.weight  ->  encoder.blocks.25.attn.value.weight\n",
            "encoder.blocks.25.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.25.self_attn.v_proj.bias  ->  encoder.blocks.25.attn.value.bias\n",
            "encoder.blocks.25.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.self_attn.q_proj.weight  ->  encoder.blocks.25.attn.query.weight\n",
            "encoder.blocks.25.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.25.self_attn.q_proj.bias  ->  encoder.blocks.25.attn.query.bias\n",
            "encoder.blocks.25.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.self_attn.out_proj.weight  ->  encoder.blocks.25.attn.out.weight\n",
            "encoder.blocks.25.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.25.self_attn.out_proj.bias  ->  encoder.blocks.25.attn.out.bias\n",
            "encoder.blocks.25.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.self_attn_layer_norm.weight  ->  encoder.blocks.25.attn_ln.weight\n",
            "encoder.blocks.25.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.self_attn_layer_norm.bias  ->  encoder.blocks.25.attn_ln.bias\n",
            "encoder.blocks.25.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.fc1.weight  ->  encoder.blocks.25.mlp.0.weight\n",
            "encoder.blocks.25.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.25.fc1.bias  ->  encoder.blocks.25.mlp.0.bias\n",
            "encoder.blocks.25.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.fc2.weight  ->  encoder.blocks.25.mlp.2.weight\n",
            "encoder.blocks.25.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.25.fc2.bias  ->  encoder.blocks.25.mlp.2.bias\n",
            "encoder.blocks.25.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.final_layer_norm.weight  ->  encoder.blocks.25.mlp_ln.weight\n",
            "encoder.blocks.25.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.25.final_layer_norm.bias  ->  encoder.blocks.25.mlp_ln.bias\n",
            "encoder.blocks.25.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.self_attn.k_proj.weight  ->  encoder.blocks.26.attn.key.weight\n",
            "encoder.blocks.26.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.26.self_attn.v_proj.weight  ->  encoder.blocks.26.attn.value.weight\n",
            "encoder.blocks.26.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.26.self_attn.v_proj.bias  ->  encoder.blocks.26.attn.value.bias\n",
            "encoder.blocks.26.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.self_attn.q_proj.weight  ->  encoder.blocks.26.attn.query.weight\n",
            "encoder.blocks.26.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.26.self_attn.q_proj.bias  ->  encoder.blocks.26.attn.query.bias\n",
            "encoder.blocks.26.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.self_attn.out_proj.weight  ->  encoder.blocks.26.attn.out.weight\n",
            "encoder.blocks.26.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.26.self_attn.out_proj.bias  ->  encoder.blocks.26.attn.out.bias\n",
            "encoder.blocks.26.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.self_attn_layer_norm.weight  ->  encoder.blocks.26.attn_ln.weight\n",
            "encoder.blocks.26.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.self_attn_layer_norm.bias  ->  encoder.blocks.26.attn_ln.bias\n",
            "encoder.blocks.26.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.fc1.weight  ->  encoder.blocks.26.mlp.0.weight\n",
            "encoder.blocks.26.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.26.fc1.bias  ->  encoder.blocks.26.mlp.0.bias\n",
            "encoder.blocks.26.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.fc2.weight  ->  encoder.blocks.26.mlp.2.weight\n",
            "encoder.blocks.26.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.26.fc2.bias  ->  encoder.blocks.26.mlp.2.bias\n",
            "encoder.blocks.26.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.final_layer_norm.weight  ->  encoder.blocks.26.mlp_ln.weight\n",
            "encoder.blocks.26.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.26.final_layer_norm.bias  ->  encoder.blocks.26.mlp_ln.bias\n",
            "encoder.blocks.26.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.self_attn.k_proj.weight  ->  encoder.blocks.27.attn.key.weight\n",
            "encoder.blocks.27.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.27.self_attn.v_proj.weight  ->  encoder.blocks.27.attn.value.weight\n",
            "encoder.blocks.27.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.27.self_attn.v_proj.bias  ->  encoder.blocks.27.attn.value.bias\n",
            "encoder.blocks.27.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.self_attn.q_proj.weight  ->  encoder.blocks.27.attn.query.weight\n",
            "encoder.blocks.27.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.27.self_attn.q_proj.bias  ->  encoder.blocks.27.attn.query.bias\n",
            "encoder.blocks.27.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.self_attn.out_proj.weight  ->  encoder.blocks.27.attn.out.weight\n",
            "encoder.blocks.27.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.27.self_attn.out_proj.bias  ->  encoder.blocks.27.attn.out.bias\n",
            "encoder.blocks.27.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.self_attn_layer_norm.weight  ->  encoder.blocks.27.attn_ln.weight\n",
            "encoder.blocks.27.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.self_attn_layer_norm.bias  ->  encoder.blocks.27.attn_ln.bias\n",
            "encoder.blocks.27.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.fc1.weight  ->  encoder.blocks.27.mlp.0.weight\n",
            "encoder.blocks.27.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.27.fc1.bias  ->  encoder.blocks.27.mlp.0.bias\n",
            "encoder.blocks.27.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.fc2.weight  ->  encoder.blocks.27.mlp.2.weight\n",
            "encoder.blocks.27.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.27.fc2.bias  ->  encoder.blocks.27.mlp.2.bias\n",
            "encoder.blocks.27.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.final_layer_norm.weight  ->  encoder.blocks.27.mlp_ln.weight\n",
            "encoder.blocks.27.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.27.final_layer_norm.bias  ->  encoder.blocks.27.mlp_ln.bias\n",
            "encoder.blocks.27.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.self_attn.k_proj.weight  ->  encoder.blocks.28.attn.key.weight\n",
            "encoder.blocks.28.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.28.self_attn.v_proj.weight  ->  encoder.blocks.28.attn.value.weight\n",
            "encoder.blocks.28.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.28.self_attn.v_proj.bias  ->  encoder.blocks.28.attn.value.bias\n",
            "encoder.blocks.28.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.self_attn.q_proj.weight  ->  encoder.blocks.28.attn.query.weight\n",
            "encoder.blocks.28.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.28.self_attn.q_proj.bias  ->  encoder.blocks.28.attn.query.bias\n",
            "encoder.blocks.28.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.self_attn.out_proj.weight  ->  encoder.blocks.28.attn.out.weight\n",
            "encoder.blocks.28.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.28.self_attn.out_proj.bias  ->  encoder.blocks.28.attn.out.bias\n",
            "encoder.blocks.28.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.self_attn_layer_norm.weight  ->  encoder.blocks.28.attn_ln.weight\n",
            "encoder.blocks.28.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.self_attn_layer_norm.bias  ->  encoder.blocks.28.attn_ln.bias\n",
            "encoder.blocks.28.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.fc1.weight  ->  encoder.blocks.28.mlp.0.weight\n",
            "encoder.blocks.28.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.28.fc1.bias  ->  encoder.blocks.28.mlp.0.bias\n",
            "encoder.blocks.28.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.fc2.weight  ->  encoder.blocks.28.mlp.2.weight\n",
            "encoder.blocks.28.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.28.fc2.bias  ->  encoder.blocks.28.mlp.2.bias\n",
            "encoder.blocks.28.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.final_layer_norm.weight  ->  encoder.blocks.28.mlp_ln.weight\n",
            "encoder.blocks.28.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.28.final_layer_norm.bias  ->  encoder.blocks.28.mlp_ln.bias\n",
            "encoder.blocks.28.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.self_attn.k_proj.weight  ->  encoder.blocks.29.attn.key.weight\n",
            "encoder.blocks.29.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.29.self_attn.v_proj.weight  ->  encoder.blocks.29.attn.value.weight\n",
            "encoder.blocks.29.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.29.self_attn.v_proj.bias  ->  encoder.blocks.29.attn.value.bias\n",
            "encoder.blocks.29.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.self_attn.q_proj.weight  ->  encoder.blocks.29.attn.query.weight\n",
            "encoder.blocks.29.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.29.self_attn.q_proj.bias  ->  encoder.blocks.29.attn.query.bias\n",
            "encoder.blocks.29.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.self_attn.out_proj.weight  ->  encoder.blocks.29.attn.out.weight\n",
            "encoder.blocks.29.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.29.self_attn.out_proj.bias  ->  encoder.blocks.29.attn.out.bias\n",
            "encoder.blocks.29.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.self_attn_layer_norm.weight  ->  encoder.blocks.29.attn_ln.weight\n",
            "encoder.blocks.29.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.self_attn_layer_norm.bias  ->  encoder.blocks.29.attn_ln.bias\n",
            "encoder.blocks.29.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.fc1.weight  ->  encoder.blocks.29.mlp.0.weight\n",
            "encoder.blocks.29.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.29.fc1.bias  ->  encoder.blocks.29.mlp.0.bias\n",
            "encoder.blocks.29.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.fc2.weight  ->  encoder.blocks.29.mlp.2.weight\n",
            "encoder.blocks.29.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.29.fc2.bias  ->  encoder.blocks.29.mlp.2.bias\n",
            "encoder.blocks.29.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.final_layer_norm.weight  ->  encoder.blocks.29.mlp_ln.weight\n",
            "encoder.blocks.29.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.29.final_layer_norm.bias  ->  encoder.blocks.29.mlp_ln.bias\n",
            "encoder.blocks.29.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.self_attn.k_proj.weight  ->  encoder.blocks.30.attn.key.weight\n",
            "encoder.blocks.30.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.30.self_attn.v_proj.weight  ->  encoder.blocks.30.attn.value.weight\n",
            "encoder.blocks.30.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.30.self_attn.v_proj.bias  ->  encoder.blocks.30.attn.value.bias\n",
            "encoder.blocks.30.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.self_attn.q_proj.weight  ->  encoder.blocks.30.attn.query.weight\n",
            "encoder.blocks.30.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.30.self_attn.q_proj.bias  ->  encoder.blocks.30.attn.query.bias\n",
            "encoder.blocks.30.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.self_attn.out_proj.weight  ->  encoder.blocks.30.attn.out.weight\n",
            "encoder.blocks.30.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.30.self_attn.out_proj.bias  ->  encoder.blocks.30.attn.out.bias\n",
            "encoder.blocks.30.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.self_attn_layer_norm.weight  ->  encoder.blocks.30.attn_ln.weight\n",
            "encoder.blocks.30.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.self_attn_layer_norm.bias  ->  encoder.blocks.30.attn_ln.bias\n",
            "encoder.blocks.30.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.fc1.weight  ->  encoder.blocks.30.mlp.0.weight\n",
            "encoder.blocks.30.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.30.fc1.bias  ->  encoder.blocks.30.mlp.0.bias\n",
            "encoder.blocks.30.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.fc2.weight  ->  encoder.blocks.30.mlp.2.weight\n",
            "encoder.blocks.30.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.30.fc2.bias  ->  encoder.blocks.30.mlp.2.bias\n",
            "encoder.blocks.30.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.final_layer_norm.weight  ->  encoder.blocks.30.mlp_ln.weight\n",
            "encoder.blocks.30.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.30.final_layer_norm.bias  ->  encoder.blocks.30.mlp_ln.bias\n",
            "encoder.blocks.30.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.self_attn.k_proj.weight  ->  encoder.blocks.31.attn.key.weight\n",
            "encoder.blocks.31.attn.key.weight 2 (1280, 1280)\n",
            "model.encoder.layers.31.self_attn.v_proj.weight  ->  encoder.blocks.31.attn.value.weight\n",
            "encoder.blocks.31.attn.value.weight 2 (1280, 1280)\n",
            "model.encoder.layers.31.self_attn.v_proj.bias  ->  encoder.blocks.31.attn.value.bias\n",
            "encoder.blocks.31.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.self_attn.q_proj.weight  ->  encoder.blocks.31.attn.query.weight\n",
            "encoder.blocks.31.attn.query.weight 2 (1280, 1280)\n",
            "model.encoder.layers.31.self_attn.q_proj.bias  ->  encoder.blocks.31.attn.query.bias\n",
            "encoder.blocks.31.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.self_attn.out_proj.weight  ->  encoder.blocks.31.attn.out.weight\n",
            "encoder.blocks.31.attn.out.weight 2 (1280, 1280)\n",
            "model.encoder.layers.31.self_attn.out_proj.bias  ->  encoder.blocks.31.attn.out.bias\n",
            "encoder.blocks.31.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.self_attn_layer_norm.weight  ->  encoder.blocks.31.attn_ln.weight\n",
            "encoder.blocks.31.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.self_attn_layer_norm.bias  ->  encoder.blocks.31.attn_ln.bias\n",
            "encoder.blocks.31.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.fc1.weight  ->  encoder.blocks.31.mlp.0.weight\n",
            "encoder.blocks.31.mlp.0.weight 2 (5120, 1280)\n",
            "model.encoder.layers.31.fc1.bias  ->  encoder.blocks.31.mlp.0.bias\n",
            "encoder.blocks.31.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.fc2.weight  ->  encoder.blocks.31.mlp.2.weight\n",
            "encoder.blocks.31.mlp.2.weight 2 (1280, 5120)\n",
            "model.encoder.layers.31.fc2.bias  ->  encoder.blocks.31.mlp.2.bias\n",
            "encoder.blocks.31.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.final_layer_norm.weight  ->  encoder.blocks.31.mlp_ln.weight\n",
            "encoder.blocks.31.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.31.final_layer_norm.bias  ->  encoder.blocks.31.mlp_ln.bias\n",
            "encoder.blocks.31.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layer_norm.weight  ->  encoder.ln_post.weight\n",
            "encoder.ln_post.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.encoder.layer_norm.bias  ->  encoder.ln_post.bias\n",
            "encoder.ln_post.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.embed_tokens.weight  ->  decoder.token_embedding.weight\n",
            "decoder.token_embedding.weight 2 (51865, 1280)\n",
            "model.decoder.embed_positions.weight  ->  decoder.positional_embedding\n",
            "decoder.positional_embedding 2 (448, 1280)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn.k_proj.weight  ->  decoder.blocks.0.attn.key.weight\n",
            "decoder.blocks.0.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.self_attn.v_proj.weight  ->  decoder.blocks.0.attn.value.weight\n",
            "decoder.blocks.0.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.self_attn.v_proj.bias  ->  decoder.blocks.0.attn.value.bias\n",
            "decoder.blocks.0.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn.q_proj.weight  ->  decoder.blocks.0.attn.query.weight\n",
            "decoder.blocks.0.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.self_attn.q_proj.bias  ->  decoder.blocks.0.attn.query.bias\n",
            "decoder.blocks.0.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn.out_proj.weight  ->  decoder.blocks.0.attn.out.weight\n",
            "decoder.blocks.0.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.self_attn.out_proj.bias  ->  decoder.blocks.0.attn.out.bias\n",
            "decoder.blocks.0.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn_layer_norm.weight  ->  decoder.blocks.0.attn_ln.weight\n",
            "decoder.blocks.0.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn_layer_norm.bias  ->  decoder.blocks.0.attn_ln.bias\n",
            "decoder.blocks.0.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn.k_proj.weight  ->  decoder.blocks.0.cross_attn.key.weight\n",
            "decoder.blocks.0.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.encoder_attn.v_proj.weight  ->  decoder.blocks.0.cross_attn.value.weight\n",
            "decoder.blocks.0.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.encoder_attn.v_proj.bias  ->  decoder.blocks.0.cross_attn.value.bias\n",
            "decoder.blocks.0.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn.q_proj.weight  ->  decoder.blocks.0.cross_attn.query.weight\n",
            "decoder.blocks.0.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.encoder_attn.q_proj.bias  ->  decoder.blocks.0.cross_attn.query.bias\n",
            "decoder.blocks.0.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn.out_proj.weight  ->  decoder.blocks.0.cross_attn.out.weight\n",
            "decoder.blocks.0.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.0.encoder_attn.out_proj.bias  ->  decoder.blocks.0.cross_attn.out.bias\n",
            "decoder.blocks.0.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn_layer_norm.weight  ->  decoder.blocks.0.cross_attn_ln.weight\n",
            "decoder.blocks.0.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn_layer_norm.bias  ->  decoder.blocks.0.cross_attn_ln.bias\n",
            "decoder.blocks.0.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.fc1.weight  ->  decoder.blocks.0.mlp.0.weight\n",
            "decoder.blocks.0.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.0.fc1.bias  ->  decoder.blocks.0.mlp.0.bias\n",
            "decoder.blocks.0.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.fc2.weight  ->  decoder.blocks.0.mlp.2.weight\n",
            "decoder.blocks.0.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.0.fc2.bias  ->  decoder.blocks.0.mlp.2.bias\n",
            "decoder.blocks.0.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.final_layer_norm.weight  ->  decoder.blocks.0.mlp_ln.weight\n",
            "decoder.blocks.0.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.final_layer_norm.bias  ->  decoder.blocks.0.mlp_ln.bias\n",
            "decoder.blocks.0.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn.k_proj.weight  ->  decoder.blocks.1.attn.key.weight\n",
            "decoder.blocks.1.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.self_attn.v_proj.weight  ->  decoder.blocks.1.attn.value.weight\n",
            "decoder.blocks.1.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.self_attn.v_proj.bias  ->  decoder.blocks.1.attn.value.bias\n",
            "decoder.blocks.1.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn.q_proj.weight  ->  decoder.blocks.1.attn.query.weight\n",
            "decoder.blocks.1.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.self_attn.q_proj.bias  ->  decoder.blocks.1.attn.query.bias\n",
            "decoder.blocks.1.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn.out_proj.weight  ->  decoder.blocks.1.attn.out.weight\n",
            "decoder.blocks.1.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.self_attn.out_proj.bias  ->  decoder.blocks.1.attn.out.bias\n",
            "decoder.blocks.1.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn_layer_norm.weight  ->  decoder.blocks.1.attn_ln.weight\n",
            "decoder.blocks.1.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn_layer_norm.bias  ->  decoder.blocks.1.attn_ln.bias\n",
            "decoder.blocks.1.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn.k_proj.weight  ->  decoder.blocks.1.cross_attn.key.weight\n",
            "decoder.blocks.1.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.encoder_attn.v_proj.weight  ->  decoder.blocks.1.cross_attn.value.weight\n",
            "decoder.blocks.1.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.encoder_attn.v_proj.bias  ->  decoder.blocks.1.cross_attn.value.bias\n",
            "decoder.blocks.1.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn.q_proj.weight  ->  decoder.blocks.1.cross_attn.query.weight\n",
            "decoder.blocks.1.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.encoder_attn.q_proj.bias  ->  decoder.blocks.1.cross_attn.query.bias\n",
            "decoder.blocks.1.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn.out_proj.weight  ->  decoder.blocks.1.cross_attn.out.weight\n",
            "decoder.blocks.1.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.1.encoder_attn.out_proj.bias  ->  decoder.blocks.1.cross_attn.out.bias\n",
            "decoder.blocks.1.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn_layer_norm.weight  ->  decoder.blocks.1.cross_attn_ln.weight\n",
            "decoder.blocks.1.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn_layer_norm.bias  ->  decoder.blocks.1.cross_attn_ln.bias\n",
            "decoder.blocks.1.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.fc1.weight  ->  decoder.blocks.1.mlp.0.weight\n",
            "decoder.blocks.1.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.1.fc1.bias  ->  decoder.blocks.1.mlp.0.bias\n",
            "decoder.blocks.1.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.fc2.weight  ->  decoder.blocks.1.mlp.2.weight\n",
            "decoder.blocks.1.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.1.fc2.bias  ->  decoder.blocks.1.mlp.2.bias\n",
            "decoder.blocks.1.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.final_layer_norm.weight  ->  decoder.blocks.1.mlp_ln.weight\n",
            "decoder.blocks.1.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.final_layer_norm.bias  ->  decoder.blocks.1.mlp_ln.bias\n",
            "decoder.blocks.1.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn.k_proj.weight  ->  decoder.blocks.2.attn.key.weight\n",
            "decoder.blocks.2.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.self_attn.v_proj.weight  ->  decoder.blocks.2.attn.value.weight\n",
            "decoder.blocks.2.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.self_attn.v_proj.bias  ->  decoder.blocks.2.attn.value.bias\n",
            "decoder.blocks.2.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn.q_proj.weight  ->  decoder.blocks.2.attn.query.weight\n",
            "decoder.blocks.2.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.self_attn.q_proj.bias  ->  decoder.blocks.2.attn.query.bias\n",
            "decoder.blocks.2.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn.out_proj.weight  ->  decoder.blocks.2.attn.out.weight\n",
            "decoder.blocks.2.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.self_attn.out_proj.bias  ->  decoder.blocks.2.attn.out.bias\n",
            "decoder.blocks.2.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn_layer_norm.weight  ->  decoder.blocks.2.attn_ln.weight\n",
            "decoder.blocks.2.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn_layer_norm.bias  ->  decoder.blocks.2.attn_ln.bias\n",
            "decoder.blocks.2.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn.k_proj.weight  ->  decoder.blocks.2.cross_attn.key.weight\n",
            "decoder.blocks.2.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.encoder_attn.v_proj.weight  ->  decoder.blocks.2.cross_attn.value.weight\n",
            "decoder.blocks.2.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.encoder_attn.v_proj.bias  ->  decoder.blocks.2.cross_attn.value.bias\n",
            "decoder.blocks.2.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn.q_proj.weight  ->  decoder.blocks.2.cross_attn.query.weight\n",
            "decoder.blocks.2.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.encoder_attn.q_proj.bias  ->  decoder.blocks.2.cross_attn.query.bias\n",
            "decoder.blocks.2.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn.out_proj.weight  ->  decoder.blocks.2.cross_attn.out.weight\n",
            "decoder.blocks.2.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.2.encoder_attn.out_proj.bias  ->  decoder.blocks.2.cross_attn.out.bias\n",
            "decoder.blocks.2.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn_layer_norm.weight  ->  decoder.blocks.2.cross_attn_ln.weight\n",
            "decoder.blocks.2.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn_layer_norm.bias  ->  decoder.blocks.2.cross_attn_ln.bias\n",
            "decoder.blocks.2.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.fc1.weight  ->  decoder.blocks.2.mlp.0.weight\n",
            "decoder.blocks.2.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.2.fc1.bias  ->  decoder.blocks.2.mlp.0.bias\n",
            "decoder.blocks.2.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.fc2.weight  ->  decoder.blocks.2.mlp.2.weight\n",
            "decoder.blocks.2.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.2.fc2.bias  ->  decoder.blocks.2.mlp.2.bias\n",
            "decoder.blocks.2.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.final_layer_norm.weight  ->  decoder.blocks.2.mlp_ln.weight\n",
            "decoder.blocks.2.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.final_layer_norm.bias  ->  decoder.blocks.2.mlp_ln.bias\n",
            "decoder.blocks.2.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn.k_proj.weight  ->  decoder.blocks.3.attn.key.weight\n",
            "decoder.blocks.3.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.self_attn.v_proj.weight  ->  decoder.blocks.3.attn.value.weight\n",
            "decoder.blocks.3.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.self_attn.v_proj.bias  ->  decoder.blocks.3.attn.value.bias\n",
            "decoder.blocks.3.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn.q_proj.weight  ->  decoder.blocks.3.attn.query.weight\n",
            "decoder.blocks.3.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.self_attn.q_proj.bias  ->  decoder.blocks.3.attn.query.bias\n",
            "decoder.blocks.3.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn.out_proj.weight  ->  decoder.blocks.3.attn.out.weight\n",
            "decoder.blocks.3.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.self_attn.out_proj.bias  ->  decoder.blocks.3.attn.out.bias\n",
            "decoder.blocks.3.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn_layer_norm.weight  ->  decoder.blocks.3.attn_ln.weight\n",
            "decoder.blocks.3.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn_layer_norm.bias  ->  decoder.blocks.3.attn_ln.bias\n",
            "decoder.blocks.3.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn.k_proj.weight  ->  decoder.blocks.3.cross_attn.key.weight\n",
            "decoder.blocks.3.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.encoder_attn.v_proj.weight  ->  decoder.blocks.3.cross_attn.value.weight\n",
            "decoder.blocks.3.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.encoder_attn.v_proj.bias  ->  decoder.blocks.3.cross_attn.value.bias\n",
            "decoder.blocks.3.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn.q_proj.weight  ->  decoder.blocks.3.cross_attn.query.weight\n",
            "decoder.blocks.3.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.encoder_attn.q_proj.bias  ->  decoder.blocks.3.cross_attn.query.bias\n",
            "decoder.blocks.3.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn.out_proj.weight  ->  decoder.blocks.3.cross_attn.out.weight\n",
            "decoder.blocks.3.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.3.encoder_attn.out_proj.bias  ->  decoder.blocks.3.cross_attn.out.bias\n",
            "decoder.blocks.3.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn_layer_norm.weight  ->  decoder.blocks.3.cross_attn_ln.weight\n",
            "decoder.blocks.3.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn_layer_norm.bias  ->  decoder.blocks.3.cross_attn_ln.bias\n",
            "decoder.blocks.3.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.fc1.weight  ->  decoder.blocks.3.mlp.0.weight\n",
            "decoder.blocks.3.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.3.fc1.bias  ->  decoder.blocks.3.mlp.0.bias\n",
            "decoder.blocks.3.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.fc2.weight  ->  decoder.blocks.3.mlp.2.weight\n",
            "decoder.blocks.3.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.3.fc2.bias  ->  decoder.blocks.3.mlp.2.bias\n",
            "decoder.blocks.3.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.final_layer_norm.weight  ->  decoder.blocks.3.mlp_ln.weight\n",
            "decoder.blocks.3.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.final_layer_norm.bias  ->  decoder.blocks.3.mlp_ln.bias\n",
            "decoder.blocks.3.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.self_attn.k_proj.weight  ->  decoder.blocks.4.attn.key.weight\n",
            "decoder.blocks.4.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.self_attn.v_proj.weight  ->  decoder.blocks.4.attn.value.weight\n",
            "decoder.blocks.4.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.self_attn.v_proj.bias  ->  decoder.blocks.4.attn.value.bias\n",
            "decoder.blocks.4.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.self_attn.q_proj.weight  ->  decoder.blocks.4.attn.query.weight\n",
            "decoder.blocks.4.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.self_attn.q_proj.bias  ->  decoder.blocks.4.attn.query.bias\n",
            "decoder.blocks.4.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.self_attn.out_proj.weight  ->  decoder.blocks.4.attn.out.weight\n",
            "decoder.blocks.4.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.self_attn.out_proj.bias  ->  decoder.blocks.4.attn.out.bias\n",
            "decoder.blocks.4.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.self_attn_layer_norm.weight  ->  decoder.blocks.4.attn_ln.weight\n",
            "decoder.blocks.4.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.self_attn_layer_norm.bias  ->  decoder.blocks.4.attn_ln.bias\n",
            "decoder.blocks.4.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.encoder_attn.k_proj.weight  ->  decoder.blocks.4.cross_attn.key.weight\n",
            "decoder.blocks.4.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.encoder_attn.v_proj.weight  ->  decoder.blocks.4.cross_attn.value.weight\n",
            "decoder.blocks.4.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.encoder_attn.v_proj.bias  ->  decoder.blocks.4.cross_attn.value.bias\n",
            "decoder.blocks.4.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.encoder_attn.q_proj.weight  ->  decoder.blocks.4.cross_attn.query.weight\n",
            "decoder.blocks.4.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.encoder_attn.q_proj.bias  ->  decoder.blocks.4.cross_attn.query.bias\n",
            "decoder.blocks.4.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.encoder_attn.out_proj.weight  ->  decoder.blocks.4.cross_attn.out.weight\n",
            "decoder.blocks.4.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.4.encoder_attn.out_proj.bias  ->  decoder.blocks.4.cross_attn.out.bias\n",
            "decoder.blocks.4.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.encoder_attn_layer_norm.weight  ->  decoder.blocks.4.cross_attn_ln.weight\n",
            "decoder.blocks.4.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.encoder_attn_layer_norm.bias  ->  decoder.blocks.4.cross_attn_ln.bias\n",
            "decoder.blocks.4.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.fc1.weight  ->  decoder.blocks.4.mlp.0.weight\n",
            "decoder.blocks.4.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.4.fc1.bias  ->  decoder.blocks.4.mlp.0.bias\n",
            "decoder.blocks.4.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.fc2.weight  ->  decoder.blocks.4.mlp.2.weight\n",
            "decoder.blocks.4.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.4.fc2.bias  ->  decoder.blocks.4.mlp.2.bias\n",
            "decoder.blocks.4.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.final_layer_norm.weight  ->  decoder.blocks.4.mlp_ln.weight\n",
            "decoder.blocks.4.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.4.final_layer_norm.bias  ->  decoder.blocks.4.mlp_ln.bias\n",
            "decoder.blocks.4.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.self_attn.k_proj.weight  ->  decoder.blocks.5.attn.key.weight\n",
            "decoder.blocks.5.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.self_attn.v_proj.weight  ->  decoder.blocks.5.attn.value.weight\n",
            "decoder.blocks.5.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.self_attn.v_proj.bias  ->  decoder.blocks.5.attn.value.bias\n",
            "decoder.blocks.5.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.self_attn.q_proj.weight  ->  decoder.blocks.5.attn.query.weight\n",
            "decoder.blocks.5.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.self_attn.q_proj.bias  ->  decoder.blocks.5.attn.query.bias\n",
            "decoder.blocks.5.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.self_attn.out_proj.weight  ->  decoder.blocks.5.attn.out.weight\n",
            "decoder.blocks.5.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.self_attn.out_proj.bias  ->  decoder.blocks.5.attn.out.bias\n",
            "decoder.blocks.5.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.self_attn_layer_norm.weight  ->  decoder.blocks.5.attn_ln.weight\n",
            "decoder.blocks.5.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.self_attn_layer_norm.bias  ->  decoder.blocks.5.attn_ln.bias\n",
            "decoder.blocks.5.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.encoder_attn.k_proj.weight  ->  decoder.blocks.5.cross_attn.key.weight\n",
            "decoder.blocks.5.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.encoder_attn.v_proj.weight  ->  decoder.blocks.5.cross_attn.value.weight\n",
            "decoder.blocks.5.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.encoder_attn.v_proj.bias  ->  decoder.blocks.5.cross_attn.value.bias\n",
            "decoder.blocks.5.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.encoder_attn.q_proj.weight  ->  decoder.blocks.5.cross_attn.query.weight\n",
            "decoder.blocks.5.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.encoder_attn.q_proj.bias  ->  decoder.blocks.5.cross_attn.query.bias\n",
            "decoder.blocks.5.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.encoder_attn.out_proj.weight  ->  decoder.blocks.5.cross_attn.out.weight\n",
            "decoder.blocks.5.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.5.encoder_attn.out_proj.bias  ->  decoder.blocks.5.cross_attn.out.bias\n",
            "decoder.blocks.5.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.encoder_attn_layer_norm.weight  ->  decoder.blocks.5.cross_attn_ln.weight\n",
            "decoder.blocks.5.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.encoder_attn_layer_norm.bias  ->  decoder.blocks.5.cross_attn_ln.bias\n",
            "decoder.blocks.5.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.fc1.weight  ->  decoder.blocks.5.mlp.0.weight\n",
            "decoder.blocks.5.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.5.fc1.bias  ->  decoder.blocks.5.mlp.0.bias\n",
            "decoder.blocks.5.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.fc2.weight  ->  decoder.blocks.5.mlp.2.weight\n",
            "decoder.blocks.5.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.5.fc2.bias  ->  decoder.blocks.5.mlp.2.bias\n",
            "decoder.blocks.5.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.final_layer_norm.weight  ->  decoder.blocks.5.mlp_ln.weight\n",
            "decoder.blocks.5.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.5.final_layer_norm.bias  ->  decoder.blocks.5.mlp_ln.bias\n",
            "decoder.blocks.5.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.self_attn.k_proj.weight  ->  decoder.blocks.6.attn.key.weight\n",
            "decoder.blocks.6.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.self_attn.v_proj.weight  ->  decoder.blocks.6.attn.value.weight\n",
            "decoder.blocks.6.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.self_attn.v_proj.bias  ->  decoder.blocks.6.attn.value.bias\n",
            "decoder.blocks.6.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.self_attn.q_proj.weight  ->  decoder.blocks.6.attn.query.weight\n",
            "decoder.blocks.6.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.self_attn.q_proj.bias  ->  decoder.blocks.6.attn.query.bias\n",
            "decoder.blocks.6.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.self_attn.out_proj.weight  ->  decoder.blocks.6.attn.out.weight\n",
            "decoder.blocks.6.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.self_attn.out_proj.bias  ->  decoder.blocks.6.attn.out.bias\n",
            "decoder.blocks.6.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.self_attn_layer_norm.weight  ->  decoder.blocks.6.attn_ln.weight\n",
            "decoder.blocks.6.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.self_attn_layer_norm.bias  ->  decoder.blocks.6.attn_ln.bias\n",
            "decoder.blocks.6.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.encoder_attn.k_proj.weight  ->  decoder.blocks.6.cross_attn.key.weight\n",
            "decoder.blocks.6.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.encoder_attn.v_proj.weight  ->  decoder.blocks.6.cross_attn.value.weight\n",
            "decoder.blocks.6.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.encoder_attn.v_proj.bias  ->  decoder.blocks.6.cross_attn.value.bias\n",
            "decoder.blocks.6.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.encoder_attn.q_proj.weight  ->  decoder.blocks.6.cross_attn.query.weight\n",
            "decoder.blocks.6.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.encoder_attn.q_proj.bias  ->  decoder.blocks.6.cross_attn.query.bias\n",
            "decoder.blocks.6.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.encoder_attn.out_proj.weight  ->  decoder.blocks.6.cross_attn.out.weight\n",
            "decoder.blocks.6.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.6.encoder_attn.out_proj.bias  ->  decoder.blocks.6.cross_attn.out.bias\n",
            "decoder.blocks.6.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.encoder_attn_layer_norm.weight  ->  decoder.blocks.6.cross_attn_ln.weight\n",
            "decoder.blocks.6.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.encoder_attn_layer_norm.bias  ->  decoder.blocks.6.cross_attn_ln.bias\n",
            "decoder.blocks.6.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.fc1.weight  ->  decoder.blocks.6.mlp.0.weight\n",
            "decoder.blocks.6.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.6.fc1.bias  ->  decoder.blocks.6.mlp.0.bias\n",
            "decoder.blocks.6.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.fc2.weight  ->  decoder.blocks.6.mlp.2.weight\n",
            "decoder.blocks.6.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.6.fc2.bias  ->  decoder.blocks.6.mlp.2.bias\n",
            "decoder.blocks.6.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.final_layer_norm.weight  ->  decoder.blocks.6.mlp_ln.weight\n",
            "decoder.blocks.6.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.6.final_layer_norm.bias  ->  decoder.blocks.6.mlp_ln.bias\n",
            "decoder.blocks.6.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.self_attn.k_proj.weight  ->  decoder.blocks.7.attn.key.weight\n",
            "decoder.blocks.7.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.self_attn.v_proj.weight  ->  decoder.blocks.7.attn.value.weight\n",
            "decoder.blocks.7.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.self_attn.v_proj.bias  ->  decoder.blocks.7.attn.value.bias\n",
            "decoder.blocks.7.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.self_attn.q_proj.weight  ->  decoder.blocks.7.attn.query.weight\n",
            "decoder.blocks.7.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.self_attn.q_proj.bias  ->  decoder.blocks.7.attn.query.bias\n",
            "decoder.blocks.7.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.self_attn.out_proj.weight  ->  decoder.blocks.7.attn.out.weight\n",
            "decoder.blocks.7.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.self_attn.out_proj.bias  ->  decoder.blocks.7.attn.out.bias\n",
            "decoder.blocks.7.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.self_attn_layer_norm.weight  ->  decoder.blocks.7.attn_ln.weight\n",
            "decoder.blocks.7.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.self_attn_layer_norm.bias  ->  decoder.blocks.7.attn_ln.bias\n",
            "decoder.blocks.7.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.encoder_attn.k_proj.weight  ->  decoder.blocks.7.cross_attn.key.weight\n",
            "decoder.blocks.7.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.encoder_attn.v_proj.weight  ->  decoder.blocks.7.cross_attn.value.weight\n",
            "decoder.blocks.7.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.encoder_attn.v_proj.bias  ->  decoder.blocks.7.cross_attn.value.bias\n",
            "decoder.blocks.7.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.encoder_attn.q_proj.weight  ->  decoder.blocks.7.cross_attn.query.weight\n",
            "decoder.blocks.7.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.encoder_attn.q_proj.bias  ->  decoder.blocks.7.cross_attn.query.bias\n",
            "decoder.blocks.7.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.encoder_attn.out_proj.weight  ->  decoder.blocks.7.cross_attn.out.weight\n",
            "decoder.blocks.7.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.7.encoder_attn.out_proj.bias  ->  decoder.blocks.7.cross_attn.out.bias\n",
            "decoder.blocks.7.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.encoder_attn_layer_norm.weight  ->  decoder.blocks.7.cross_attn_ln.weight\n",
            "decoder.blocks.7.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.encoder_attn_layer_norm.bias  ->  decoder.blocks.7.cross_attn_ln.bias\n",
            "decoder.blocks.7.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.fc1.weight  ->  decoder.blocks.7.mlp.0.weight\n",
            "decoder.blocks.7.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.7.fc1.bias  ->  decoder.blocks.7.mlp.0.bias\n",
            "decoder.blocks.7.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.fc2.weight  ->  decoder.blocks.7.mlp.2.weight\n",
            "decoder.blocks.7.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.7.fc2.bias  ->  decoder.blocks.7.mlp.2.bias\n",
            "decoder.blocks.7.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.final_layer_norm.weight  ->  decoder.blocks.7.mlp_ln.weight\n",
            "decoder.blocks.7.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.7.final_layer_norm.bias  ->  decoder.blocks.7.mlp_ln.bias\n",
            "decoder.blocks.7.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.self_attn.k_proj.weight  ->  decoder.blocks.8.attn.key.weight\n",
            "decoder.blocks.8.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.self_attn.v_proj.weight  ->  decoder.blocks.8.attn.value.weight\n",
            "decoder.blocks.8.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.self_attn.v_proj.bias  ->  decoder.blocks.8.attn.value.bias\n",
            "decoder.blocks.8.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.self_attn.q_proj.weight  ->  decoder.blocks.8.attn.query.weight\n",
            "decoder.blocks.8.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.self_attn.q_proj.bias  ->  decoder.blocks.8.attn.query.bias\n",
            "decoder.blocks.8.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.self_attn.out_proj.weight  ->  decoder.blocks.8.attn.out.weight\n",
            "decoder.blocks.8.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.self_attn.out_proj.bias  ->  decoder.blocks.8.attn.out.bias\n",
            "decoder.blocks.8.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.self_attn_layer_norm.weight  ->  decoder.blocks.8.attn_ln.weight\n",
            "decoder.blocks.8.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.self_attn_layer_norm.bias  ->  decoder.blocks.8.attn_ln.bias\n",
            "decoder.blocks.8.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.encoder_attn.k_proj.weight  ->  decoder.blocks.8.cross_attn.key.weight\n",
            "decoder.blocks.8.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.encoder_attn.v_proj.weight  ->  decoder.blocks.8.cross_attn.value.weight\n",
            "decoder.blocks.8.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.encoder_attn.v_proj.bias  ->  decoder.blocks.8.cross_attn.value.bias\n",
            "decoder.blocks.8.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.encoder_attn.q_proj.weight  ->  decoder.blocks.8.cross_attn.query.weight\n",
            "decoder.blocks.8.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.encoder_attn.q_proj.bias  ->  decoder.blocks.8.cross_attn.query.bias\n",
            "decoder.blocks.8.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.encoder_attn.out_proj.weight  ->  decoder.blocks.8.cross_attn.out.weight\n",
            "decoder.blocks.8.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.8.encoder_attn.out_proj.bias  ->  decoder.blocks.8.cross_attn.out.bias\n",
            "decoder.blocks.8.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.encoder_attn_layer_norm.weight  ->  decoder.blocks.8.cross_attn_ln.weight\n",
            "decoder.blocks.8.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.encoder_attn_layer_norm.bias  ->  decoder.blocks.8.cross_attn_ln.bias\n",
            "decoder.blocks.8.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.fc1.weight  ->  decoder.blocks.8.mlp.0.weight\n",
            "decoder.blocks.8.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.8.fc1.bias  ->  decoder.blocks.8.mlp.0.bias\n",
            "decoder.blocks.8.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.fc2.weight  ->  decoder.blocks.8.mlp.2.weight\n",
            "decoder.blocks.8.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.8.fc2.bias  ->  decoder.blocks.8.mlp.2.bias\n",
            "decoder.blocks.8.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.final_layer_norm.weight  ->  decoder.blocks.8.mlp_ln.weight\n",
            "decoder.blocks.8.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.8.final_layer_norm.bias  ->  decoder.blocks.8.mlp_ln.bias\n",
            "decoder.blocks.8.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.self_attn.k_proj.weight  ->  decoder.blocks.9.attn.key.weight\n",
            "decoder.blocks.9.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.self_attn.v_proj.weight  ->  decoder.blocks.9.attn.value.weight\n",
            "decoder.blocks.9.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.self_attn.v_proj.bias  ->  decoder.blocks.9.attn.value.bias\n",
            "decoder.blocks.9.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.self_attn.q_proj.weight  ->  decoder.blocks.9.attn.query.weight\n",
            "decoder.blocks.9.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.self_attn.q_proj.bias  ->  decoder.blocks.9.attn.query.bias\n",
            "decoder.blocks.9.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.self_attn.out_proj.weight  ->  decoder.blocks.9.attn.out.weight\n",
            "decoder.blocks.9.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.self_attn.out_proj.bias  ->  decoder.blocks.9.attn.out.bias\n",
            "decoder.blocks.9.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.self_attn_layer_norm.weight  ->  decoder.blocks.9.attn_ln.weight\n",
            "decoder.blocks.9.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.self_attn_layer_norm.bias  ->  decoder.blocks.9.attn_ln.bias\n",
            "decoder.blocks.9.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.encoder_attn.k_proj.weight  ->  decoder.blocks.9.cross_attn.key.weight\n",
            "decoder.blocks.9.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.encoder_attn.v_proj.weight  ->  decoder.blocks.9.cross_attn.value.weight\n",
            "decoder.blocks.9.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.encoder_attn.v_proj.bias  ->  decoder.blocks.9.cross_attn.value.bias\n",
            "decoder.blocks.9.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.encoder_attn.q_proj.weight  ->  decoder.blocks.9.cross_attn.query.weight\n",
            "decoder.blocks.9.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.encoder_attn.q_proj.bias  ->  decoder.blocks.9.cross_attn.query.bias\n",
            "decoder.blocks.9.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.encoder_attn.out_proj.weight  ->  decoder.blocks.9.cross_attn.out.weight\n",
            "decoder.blocks.9.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.9.encoder_attn.out_proj.bias  ->  decoder.blocks.9.cross_attn.out.bias\n",
            "decoder.blocks.9.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.encoder_attn_layer_norm.weight  ->  decoder.blocks.9.cross_attn_ln.weight\n",
            "decoder.blocks.9.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.encoder_attn_layer_norm.bias  ->  decoder.blocks.9.cross_attn_ln.bias\n",
            "decoder.blocks.9.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.fc1.weight  ->  decoder.blocks.9.mlp.0.weight\n",
            "decoder.blocks.9.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.9.fc1.bias  ->  decoder.blocks.9.mlp.0.bias\n",
            "decoder.blocks.9.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.fc2.weight  ->  decoder.blocks.9.mlp.2.weight\n",
            "decoder.blocks.9.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.9.fc2.bias  ->  decoder.blocks.9.mlp.2.bias\n",
            "decoder.blocks.9.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.final_layer_norm.weight  ->  decoder.blocks.9.mlp_ln.weight\n",
            "decoder.blocks.9.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.9.final_layer_norm.bias  ->  decoder.blocks.9.mlp_ln.bias\n",
            "decoder.blocks.9.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.self_attn.k_proj.weight  ->  decoder.blocks.10.attn.key.weight\n",
            "decoder.blocks.10.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.self_attn.v_proj.weight  ->  decoder.blocks.10.attn.value.weight\n",
            "decoder.blocks.10.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.self_attn.v_proj.bias  ->  decoder.blocks.10.attn.value.bias\n",
            "decoder.blocks.10.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.self_attn.q_proj.weight  ->  decoder.blocks.10.attn.query.weight\n",
            "decoder.blocks.10.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.self_attn.q_proj.bias  ->  decoder.blocks.10.attn.query.bias\n",
            "decoder.blocks.10.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.self_attn.out_proj.weight  ->  decoder.blocks.10.attn.out.weight\n",
            "decoder.blocks.10.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.self_attn.out_proj.bias  ->  decoder.blocks.10.attn.out.bias\n",
            "decoder.blocks.10.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.self_attn_layer_norm.weight  ->  decoder.blocks.10.attn_ln.weight\n",
            "decoder.blocks.10.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.self_attn_layer_norm.bias  ->  decoder.blocks.10.attn_ln.bias\n",
            "decoder.blocks.10.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.encoder_attn.k_proj.weight  ->  decoder.blocks.10.cross_attn.key.weight\n",
            "decoder.blocks.10.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.encoder_attn.v_proj.weight  ->  decoder.blocks.10.cross_attn.value.weight\n",
            "decoder.blocks.10.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.encoder_attn.v_proj.bias  ->  decoder.blocks.10.cross_attn.value.bias\n",
            "decoder.blocks.10.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.encoder_attn.q_proj.weight  ->  decoder.blocks.10.cross_attn.query.weight\n",
            "decoder.blocks.10.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.encoder_attn.q_proj.bias  ->  decoder.blocks.10.cross_attn.query.bias\n",
            "decoder.blocks.10.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.encoder_attn.out_proj.weight  ->  decoder.blocks.10.cross_attn.out.weight\n",
            "decoder.blocks.10.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.10.encoder_attn.out_proj.bias  ->  decoder.blocks.10.cross_attn.out.bias\n",
            "decoder.blocks.10.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.encoder_attn_layer_norm.weight  ->  decoder.blocks.10.cross_attn_ln.weight\n",
            "decoder.blocks.10.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.encoder_attn_layer_norm.bias  ->  decoder.blocks.10.cross_attn_ln.bias\n",
            "decoder.blocks.10.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.fc1.weight  ->  decoder.blocks.10.mlp.0.weight\n",
            "decoder.blocks.10.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.10.fc1.bias  ->  decoder.blocks.10.mlp.0.bias\n",
            "decoder.blocks.10.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.fc2.weight  ->  decoder.blocks.10.mlp.2.weight\n",
            "decoder.blocks.10.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.10.fc2.bias  ->  decoder.blocks.10.mlp.2.bias\n",
            "decoder.blocks.10.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.final_layer_norm.weight  ->  decoder.blocks.10.mlp_ln.weight\n",
            "decoder.blocks.10.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.10.final_layer_norm.bias  ->  decoder.blocks.10.mlp_ln.bias\n",
            "decoder.blocks.10.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.self_attn.k_proj.weight  ->  decoder.blocks.11.attn.key.weight\n",
            "decoder.blocks.11.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.self_attn.v_proj.weight  ->  decoder.blocks.11.attn.value.weight\n",
            "decoder.blocks.11.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.self_attn.v_proj.bias  ->  decoder.blocks.11.attn.value.bias\n",
            "decoder.blocks.11.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.self_attn.q_proj.weight  ->  decoder.blocks.11.attn.query.weight\n",
            "decoder.blocks.11.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.self_attn.q_proj.bias  ->  decoder.blocks.11.attn.query.bias\n",
            "decoder.blocks.11.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.self_attn.out_proj.weight  ->  decoder.blocks.11.attn.out.weight\n",
            "decoder.blocks.11.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.self_attn.out_proj.bias  ->  decoder.blocks.11.attn.out.bias\n",
            "decoder.blocks.11.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.self_attn_layer_norm.weight  ->  decoder.blocks.11.attn_ln.weight\n",
            "decoder.blocks.11.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.self_attn_layer_norm.bias  ->  decoder.blocks.11.attn_ln.bias\n",
            "decoder.blocks.11.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.encoder_attn.k_proj.weight  ->  decoder.blocks.11.cross_attn.key.weight\n",
            "decoder.blocks.11.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.encoder_attn.v_proj.weight  ->  decoder.blocks.11.cross_attn.value.weight\n",
            "decoder.blocks.11.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.encoder_attn.v_proj.bias  ->  decoder.blocks.11.cross_attn.value.bias\n",
            "decoder.blocks.11.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.encoder_attn.q_proj.weight  ->  decoder.blocks.11.cross_attn.query.weight\n",
            "decoder.blocks.11.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.encoder_attn.q_proj.bias  ->  decoder.blocks.11.cross_attn.query.bias\n",
            "decoder.blocks.11.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.encoder_attn.out_proj.weight  ->  decoder.blocks.11.cross_attn.out.weight\n",
            "decoder.blocks.11.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.11.encoder_attn.out_proj.bias  ->  decoder.blocks.11.cross_attn.out.bias\n",
            "decoder.blocks.11.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.encoder_attn_layer_norm.weight  ->  decoder.blocks.11.cross_attn_ln.weight\n",
            "decoder.blocks.11.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.encoder_attn_layer_norm.bias  ->  decoder.blocks.11.cross_attn_ln.bias\n",
            "decoder.blocks.11.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.fc1.weight  ->  decoder.blocks.11.mlp.0.weight\n",
            "decoder.blocks.11.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.11.fc1.bias  ->  decoder.blocks.11.mlp.0.bias\n",
            "decoder.blocks.11.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.fc2.weight  ->  decoder.blocks.11.mlp.2.weight\n",
            "decoder.blocks.11.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.11.fc2.bias  ->  decoder.blocks.11.mlp.2.bias\n",
            "decoder.blocks.11.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.final_layer_norm.weight  ->  decoder.blocks.11.mlp_ln.weight\n",
            "decoder.blocks.11.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.11.final_layer_norm.bias  ->  decoder.blocks.11.mlp_ln.bias\n",
            "decoder.blocks.11.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.self_attn.k_proj.weight  ->  decoder.blocks.12.attn.key.weight\n",
            "decoder.blocks.12.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.self_attn.v_proj.weight  ->  decoder.blocks.12.attn.value.weight\n",
            "decoder.blocks.12.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.self_attn.v_proj.bias  ->  decoder.blocks.12.attn.value.bias\n",
            "decoder.blocks.12.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.self_attn.q_proj.weight  ->  decoder.blocks.12.attn.query.weight\n",
            "decoder.blocks.12.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.self_attn.q_proj.bias  ->  decoder.blocks.12.attn.query.bias\n",
            "decoder.blocks.12.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.self_attn.out_proj.weight  ->  decoder.blocks.12.attn.out.weight\n",
            "decoder.blocks.12.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.self_attn.out_proj.bias  ->  decoder.blocks.12.attn.out.bias\n",
            "decoder.blocks.12.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.self_attn_layer_norm.weight  ->  decoder.blocks.12.attn_ln.weight\n",
            "decoder.blocks.12.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.self_attn_layer_norm.bias  ->  decoder.blocks.12.attn_ln.bias\n",
            "decoder.blocks.12.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.encoder_attn.k_proj.weight  ->  decoder.blocks.12.cross_attn.key.weight\n",
            "decoder.blocks.12.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.encoder_attn.v_proj.weight  ->  decoder.blocks.12.cross_attn.value.weight\n",
            "decoder.blocks.12.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.encoder_attn.v_proj.bias  ->  decoder.blocks.12.cross_attn.value.bias\n",
            "decoder.blocks.12.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.encoder_attn.q_proj.weight  ->  decoder.blocks.12.cross_attn.query.weight\n",
            "decoder.blocks.12.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.encoder_attn.q_proj.bias  ->  decoder.blocks.12.cross_attn.query.bias\n",
            "decoder.blocks.12.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.encoder_attn.out_proj.weight  ->  decoder.blocks.12.cross_attn.out.weight\n",
            "decoder.blocks.12.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.12.encoder_attn.out_proj.bias  ->  decoder.blocks.12.cross_attn.out.bias\n",
            "decoder.blocks.12.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.encoder_attn_layer_norm.weight  ->  decoder.blocks.12.cross_attn_ln.weight\n",
            "decoder.blocks.12.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.encoder_attn_layer_norm.bias  ->  decoder.blocks.12.cross_attn_ln.bias\n",
            "decoder.blocks.12.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.fc1.weight  ->  decoder.blocks.12.mlp.0.weight\n",
            "decoder.blocks.12.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.12.fc1.bias  ->  decoder.blocks.12.mlp.0.bias\n",
            "decoder.blocks.12.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.fc2.weight  ->  decoder.blocks.12.mlp.2.weight\n",
            "decoder.blocks.12.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.12.fc2.bias  ->  decoder.blocks.12.mlp.2.bias\n",
            "decoder.blocks.12.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.final_layer_norm.weight  ->  decoder.blocks.12.mlp_ln.weight\n",
            "decoder.blocks.12.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.12.final_layer_norm.bias  ->  decoder.blocks.12.mlp_ln.bias\n",
            "decoder.blocks.12.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.self_attn.k_proj.weight  ->  decoder.blocks.13.attn.key.weight\n",
            "decoder.blocks.13.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.self_attn.v_proj.weight  ->  decoder.blocks.13.attn.value.weight\n",
            "decoder.blocks.13.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.self_attn.v_proj.bias  ->  decoder.blocks.13.attn.value.bias\n",
            "decoder.blocks.13.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.self_attn.q_proj.weight  ->  decoder.blocks.13.attn.query.weight\n",
            "decoder.blocks.13.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.self_attn.q_proj.bias  ->  decoder.blocks.13.attn.query.bias\n",
            "decoder.blocks.13.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.self_attn.out_proj.weight  ->  decoder.blocks.13.attn.out.weight\n",
            "decoder.blocks.13.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.self_attn.out_proj.bias  ->  decoder.blocks.13.attn.out.bias\n",
            "decoder.blocks.13.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.self_attn_layer_norm.weight  ->  decoder.blocks.13.attn_ln.weight\n",
            "decoder.blocks.13.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.self_attn_layer_norm.bias  ->  decoder.blocks.13.attn_ln.bias\n",
            "decoder.blocks.13.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.encoder_attn.k_proj.weight  ->  decoder.blocks.13.cross_attn.key.weight\n",
            "decoder.blocks.13.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.encoder_attn.v_proj.weight  ->  decoder.blocks.13.cross_attn.value.weight\n",
            "decoder.blocks.13.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.encoder_attn.v_proj.bias  ->  decoder.blocks.13.cross_attn.value.bias\n",
            "decoder.blocks.13.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.encoder_attn.q_proj.weight  ->  decoder.blocks.13.cross_attn.query.weight\n",
            "decoder.blocks.13.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.encoder_attn.q_proj.bias  ->  decoder.blocks.13.cross_attn.query.bias\n",
            "decoder.blocks.13.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.encoder_attn.out_proj.weight  ->  decoder.blocks.13.cross_attn.out.weight\n",
            "decoder.blocks.13.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.13.encoder_attn.out_proj.bias  ->  decoder.blocks.13.cross_attn.out.bias\n",
            "decoder.blocks.13.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.encoder_attn_layer_norm.weight  ->  decoder.blocks.13.cross_attn_ln.weight\n",
            "decoder.blocks.13.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.encoder_attn_layer_norm.bias  ->  decoder.blocks.13.cross_attn_ln.bias\n",
            "decoder.blocks.13.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.fc1.weight  ->  decoder.blocks.13.mlp.0.weight\n",
            "decoder.blocks.13.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.13.fc1.bias  ->  decoder.blocks.13.mlp.0.bias\n",
            "decoder.blocks.13.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.fc2.weight  ->  decoder.blocks.13.mlp.2.weight\n",
            "decoder.blocks.13.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.13.fc2.bias  ->  decoder.blocks.13.mlp.2.bias\n",
            "decoder.blocks.13.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.final_layer_norm.weight  ->  decoder.blocks.13.mlp_ln.weight\n",
            "decoder.blocks.13.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.13.final_layer_norm.bias  ->  decoder.blocks.13.mlp_ln.bias\n",
            "decoder.blocks.13.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.self_attn.k_proj.weight  ->  decoder.blocks.14.attn.key.weight\n",
            "decoder.blocks.14.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.self_attn.v_proj.weight  ->  decoder.blocks.14.attn.value.weight\n",
            "decoder.blocks.14.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.self_attn.v_proj.bias  ->  decoder.blocks.14.attn.value.bias\n",
            "decoder.blocks.14.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.self_attn.q_proj.weight  ->  decoder.blocks.14.attn.query.weight\n",
            "decoder.blocks.14.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.self_attn.q_proj.bias  ->  decoder.blocks.14.attn.query.bias\n",
            "decoder.blocks.14.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.self_attn.out_proj.weight  ->  decoder.blocks.14.attn.out.weight\n",
            "decoder.blocks.14.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.self_attn.out_proj.bias  ->  decoder.blocks.14.attn.out.bias\n",
            "decoder.blocks.14.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.self_attn_layer_norm.weight  ->  decoder.blocks.14.attn_ln.weight\n",
            "decoder.blocks.14.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.self_attn_layer_norm.bias  ->  decoder.blocks.14.attn_ln.bias\n",
            "decoder.blocks.14.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.encoder_attn.k_proj.weight  ->  decoder.blocks.14.cross_attn.key.weight\n",
            "decoder.blocks.14.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.encoder_attn.v_proj.weight  ->  decoder.blocks.14.cross_attn.value.weight\n",
            "decoder.blocks.14.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.encoder_attn.v_proj.bias  ->  decoder.blocks.14.cross_attn.value.bias\n",
            "decoder.blocks.14.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.encoder_attn.q_proj.weight  ->  decoder.blocks.14.cross_attn.query.weight\n",
            "decoder.blocks.14.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.encoder_attn.q_proj.bias  ->  decoder.blocks.14.cross_attn.query.bias\n",
            "decoder.blocks.14.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.encoder_attn.out_proj.weight  ->  decoder.blocks.14.cross_attn.out.weight\n",
            "decoder.blocks.14.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.14.encoder_attn.out_proj.bias  ->  decoder.blocks.14.cross_attn.out.bias\n",
            "decoder.blocks.14.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.encoder_attn_layer_norm.weight  ->  decoder.blocks.14.cross_attn_ln.weight\n",
            "decoder.blocks.14.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.encoder_attn_layer_norm.bias  ->  decoder.blocks.14.cross_attn_ln.bias\n",
            "decoder.blocks.14.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.fc1.weight  ->  decoder.blocks.14.mlp.0.weight\n",
            "decoder.blocks.14.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.14.fc1.bias  ->  decoder.blocks.14.mlp.0.bias\n",
            "decoder.blocks.14.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.fc2.weight  ->  decoder.blocks.14.mlp.2.weight\n",
            "decoder.blocks.14.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.14.fc2.bias  ->  decoder.blocks.14.mlp.2.bias\n",
            "decoder.blocks.14.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.final_layer_norm.weight  ->  decoder.blocks.14.mlp_ln.weight\n",
            "decoder.blocks.14.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.14.final_layer_norm.bias  ->  decoder.blocks.14.mlp_ln.bias\n",
            "decoder.blocks.14.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.self_attn.k_proj.weight  ->  decoder.blocks.15.attn.key.weight\n",
            "decoder.blocks.15.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.self_attn.v_proj.weight  ->  decoder.blocks.15.attn.value.weight\n",
            "decoder.blocks.15.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.self_attn.v_proj.bias  ->  decoder.blocks.15.attn.value.bias\n",
            "decoder.blocks.15.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.self_attn.q_proj.weight  ->  decoder.blocks.15.attn.query.weight\n",
            "decoder.blocks.15.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.self_attn.q_proj.bias  ->  decoder.blocks.15.attn.query.bias\n",
            "decoder.blocks.15.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.self_attn.out_proj.weight  ->  decoder.blocks.15.attn.out.weight\n",
            "decoder.blocks.15.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.self_attn.out_proj.bias  ->  decoder.blocks.15.attn.out.bias\n",
            "decoder.blocks.15.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.self_attn_layer_norm.weight  ->  decoder.blocks.15.attn_ln.weight\n",
            "decoder.blocks.15.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.self_attn_layer_norm.bias  ->  decoder.blocks.15.attn_ln.bias\n",
            "decoder.blocks.15.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.encoder_attn.k_proj.weight  ->  decoder.blocks.15.cross_attn.key.weight\n",
            "decoder.blocks.15.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.encoder_attn.v_proj.weight  ->  decoder.blocks.15.cross_attn.value.weight\n",
            "decoder.blocks.15.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.encoder_attn.v_proj.bias  ->  decoder.blocks.15.cross_attn.value.bias\n",
            "decoder.blocks.15.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.encoder_attn.q_proj.weight  ->  decoder.blocks.15.cross_attn.query.weight\n",
            "decoder.blocks.15.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.encoder_attn.q_proj.bias  ->  decoder.blocks.15.cross_attn.query.bias\n",
            "decoder.blocks.15.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.encoder_attn.out_proj.weight  ->  decoder.blocks.15.cross_attn.out.weight\n",
            "decoder.blocks.15.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.15.encoder_attn.out_proj.bias  ->  decoder.blocks.15.cross_attn.out.bias\n",
            "decoder.blocks.15.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.encoder_attn_layer_norm.weight  ->  decoder.blocks.15.cross_attn_ln.weight\n",
            "decoder.blocks.15.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.encoder_attn_layer_norm.bias  ->  decoder.blocks.15.cross_attn_ln.bias\n",
            "decoder.blocks.15.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.fc1.weight  ->  decoder.blocks.15.mlp.0.weight\n",
            "decoder.blocks.15.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.15.fc1.bias  ->  decoder.blocks.15.mlp.0.bias\n",
            "decoder.blocks.15.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.fc2.weight  ->  decoder.blocks.15.mlp.2.weight\n",
            "decoder.blocks.15.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.15.fc2.bias  ->  decoder.blocks.15.mlp.2.bias\n",
            "decoder.blocks.15.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.final_layer_norm.weight  ->  decoder.blocks.15.mlp_ln.weight\n",
            "decoder.blocks.15.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.15.final_layer_norm.bias  ->  decoder.blocks.15.mlp_ln.bias\n",
            "decoder.blocks.15.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.self_attn.k_proj.weight  ->  decoder.blocks.16.attn.key.weight\n",
            "decoder.blocks.16.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.self_attn.v_proj.weight  ->  decoder.blocks.16.attn.value.weight\n",
            "decoder.blocks.16.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.self_attn.v_proj.bias  ->  decoder.blocks.16.attn.value.bias\n",
            "decoder.blocks.16.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.self_attn.q_proj.weight  ->  decoder.blocks.16.attn.query.weight\n",
            "decoder.blocks.16.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.self_attn.q_proj.bias  ->  decoder.blocks.16.attn.query.bias\n",
            "decoder.blocks.16.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.self_attn.out_proj.weight  ->  decoder.blocks.16.attn.out.weight\n",
            "decoder.blocks.16.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.self_attn.out_proj.bias  ->  decoder.blocks.16.attn.out.bias\n",
            "decoder.blocks.16.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.self_attn_layer_norm.weight  ->  decoder.blocks.16.attn_ln.weight\n",
            "decoder.blocks.16.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.self_attn_layer_norm.bias  ->  decoder.blocks.16.attn_ln.bias\n",
            "decoder.blocks.16.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.encoder_attn.k_proj.weight  ->  decoder.blocks.16.cross_attn.key.weight\n",
            "decoder.blocks.16.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.encoder_attn.v_proj.weight  ->  decoder.blocks.16.cross_attn.value.weight\n",
            "decoder.blocks.16.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.encoder_attn.v_proj.bias  ->  decoder.blocks.16.cross_attn.value.bias\n",
            "decoder.blocks.16.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.encoder_attn.q_proj.weight  ->  decoder.blocks.16.cross_attn.query.weight\n",
            "decoder.blocks.16.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.encoder_attn.q_proj.bias  ->  decoder.blocks.16.cross_attn.query.bias\n",
            "decoder.blocks.16.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.encoder_attn.out_proj.weight  ->  decoder.blocks.16.cross_attn.out.weight\n",
            "decoder.blocks.16.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.16.encoder_attn.out_proj.bias  ->  decoder.blocks.16.cross_attn.out.bias\n",
            "decoder.blocks.16.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.encoder_attn_layer_norm.weight  ->  decoder.blocks.16.cross_attn_ln.weight\n",
            "decoder.blocks.16.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.encoder_attn_layer_norm.bias  ->  decoder.blocks.16.cross_attn_ln.bias\n",
            "decoder.blocks.16.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.fc1.weight  ->  decoder.blocks.16.mlp.0.weight\n",
            "decoder.blocks.16.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.16.fc1.bias  ->  decoder.blocks.16.mlp.0.bias\n",
            "decoder.blocks.16.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.fc2.weight  ->  decoder.blocks.16.mlp.2.weight\n",
            "decoder.blocks.16.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.16.fc2.bias  ->  decoder.blocks.16.mlp.2.bias\n",
            "decoder.blocks.16.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.final_layer_norm.weight  ->  decoder.blocks.16.mlp_ln.weight\n",
            "decoder.blocks.16.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.16.final_layer_norm.bias  ->  decoder.blocks.16.mlp_ln.bias\n",
            "decoder.blocks.16.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.self_attn.k_proj.weight  ->  decoder.blocks.17.attn.key.weight\n",
            "decoder.blocks.17.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.self_attn.v_proj.weight  ->  decoder.blocks.17.attn.value.weight\n",
            "decoder.blocks.17.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.self_attn.v_proj.bias  ->  decoder.blocks.17.attn.value.bias\n",
            "decoder.blocks.17.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.self_attn.q_proj.weight  ->  decoder.blocks.17.attn.query.weight\n",
            "decoder.blocks.17.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.self_attn.q_proj.bias  ->  decoder.blocks.17.attn.query.bias\n",
            "decoder.blocks.17.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.self_attn.out_proj.weight  ->  decoder.blocks.17.attn.out.weight\n",
            "decoder.blocks.17.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.self_attn.out_proj.bias  ->  decoder.blocks.17.attn.out.bias\n",
            "decoder.blocks.17.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.self_attn_layer_norm.weight  ->  decoder.blocks.17.attn_ln.weight\n",
            "decoder.blocks.17.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.self_attn_layer_norm.bias  ->  decoder.blocks.17.attn_ln.bias\n",
            "decoder.blocks.17.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.encoder_attn.k_proj.weight  ->  decoder.blocks.17.cross_attn.key.weight\n",
            "decoder.blocks.17.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.encoder_attn.v_proj.weight  ->  decoder.blocks.17.cross_attn.value.weight\n",
            "decoder.blocks.17.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.encoder_attn.v_proj.bias  ->  decoder.blocks.17.cross_attn.value.bias\n",
            "decoder.blocks.17.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.encoder_attn.q_proj.weight  ->  decoder.blocks.17.cross_attn.query.weight\n",
            "decoder.blocks.17.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.encoder_attn.q_proj.bias  ->  decoder.blocks.17.cross_attn.query.bias\n",
            "decoder.blocks.17.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.encoder_attn.out_proj.weight  ->  decoder.blocks.17.cross_attn.out.weight\n",
            "decoder.blocks.17.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.17.encoder_attn.out_proj.bias  ->  decoder.blocks.17.cross_attn.out.bias\n",
            "decoder.blocks.17.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.encoder_attn_layer_norm.weight  ->  decoder.blocks.17.cross_attn_ln.weight\n",
            "decoder.blocks.17.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.encoder_attn_layer_norm.bias  ->  decoder.blocks.17.cross_attn_ln.bias\n",
            "decoder.blocks.17.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.fc1.weight  ->  decoder.blocks.17.mlp.0.weight\n",
            "decoder.blocks.17.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.17.fc1.bias  ->  decoder.blocks.17.mlp.0.bias\n",
            "decoder.blocks.17.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.fc2.weight  ->  decoder.blocks.17.mlp.2.weight\n",
            "decoder.blocks.17.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.17.fc2.bias  ->  decoder.blocks.17.mlp.2.bias\n",
            "decoder.blocks.17.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.final_layer_norm.weight  ->  decoder.blocks.17.mlp_ln.weight\n",
            "decoder.blocks.17.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.17.final_layer_norm.bias  ->  decoder.blocks.17.mlp_ln.bias\n",
            "decoder.blocks.17.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.self_attn.k_proj.weight  ->  decoder.blocks.18.attn.key.weight\n",
            "decoder.blocks.18.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.self_attn.v_proj.weight  ->  decoder.blocks.18.attn.value.weight\n",
            "decoder.blocks.18.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.self_attn.v_proj.bias  ->  decoder.blocks.18.attn.value.bias\n",
            "decoder.blocks.18.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.self_attn.q_proj.weight  ->  decoder.blocks.18.attn.query.weight\n",
            "decoder.blocks.18.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.self_attn.q_proj.bias  ->  decoder.blocks.18.attn.query.bias\n",
            "decoder.blocks.18.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.self_attn.out_proj.weight  ->  decoder.blocks.18.attn.out.weight\n",
            "decoder.blocks.18.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.self_attn.out_proj.bias  ->  decoder.blocks.18.attn.out.bias\n",
            "decoder.blocks.18.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.self_attn_layer_norm.weight  ->  decoder.blocks.18.attn_ln.weight\n",
            "decoder.blocks.18.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.self_attn_layer_norm.bias  ->  decoder.blocks.18.attn_ln.bias\n",
            "decoder.blocks.18.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.encoder_attn.k_proj.weight  ->  decoder.blocks.18.cross_attn.key.weight\n",
            "decoder.blocks.18.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.encoder_attn.v_proj.weight  ->  decoder.blocks.18.cross_attn.value.weight\n",
            "decoder.blocks.18.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.encoder_attn.v_proj.bias  ->  decoder.blocks.18.cross_attn.value.bias\n",
            "decoder.blocks.18.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.encoder_attn.q_proj.weight  ->  decoder.blocks.18.cross_attn.query.weight\n",
            "decoder.blocks.18.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.encoder_attn.q_proj.bias  ->  decoder.blocks.18.cross_attn.query.bias\n",
            "decoder.blocks.18.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.encoder_attn.out_proj.weight  ->  decoder.blocks.18.cross_attn.out.weight\n",
            "decoder.blocks.18.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.18.encoder_attn.out_proj.bias  ->  decoder.blocks.18.cross_attn.out.bias\n",
            "decoder.blocks.18.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.encoder_attn_layer_norm.weight  ->  decoder.blocks.18.cross_attn_ln.weight\n",
            "decoder.blocks.18.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.encoder_attn_layer_norm.bias  ->  decoder.blocks.18.cross_attn_ln.bias\n",
            "decoder.blocks.18.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.fc1.weight  ->  decoder.blocks.18.mlp.0.weight\n",
            "decoder.blocks.18.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.18.fc1.bias  ->  decoder.blocks.18.mlp.0.bias\n",
            "decoder.blocks.18.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.fc2.weight  ->  decoder.blocks.18.mlp.2.weight\n",
            "decoder.blocks.18.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.18.fc2.bias  ->  decoder.blocks.18.mlp.2.bias\n",
            "decoder.blocks.18.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.final_layer_norm.weight  ->  decoder.blocks.18.mlp_ln.weight\n",
            "decoder.blocks.18.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.18.final_layer_norm.bias  ->  decoder.blocks.18.mlp_ln.bias\n",
            "decoder.blocks.18.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.self_attn.k_proj.weight  ->  decoder.blocks.19.attn.key.weight\n",
            "decoder.blocks.19.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.self_attn.v_proj.weight  ->  decoder.blocks.19.attn.value.weight\n",
            "decoder.blocks.19.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.self_attn.v_proj.bias  ->  decoder.blocks.19.attn.value.bias\n",
            "decoder.blocks.19.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.self_attn.q_proj.weight  ->  decoder.blocks.19.attn.query.weight\n",
            "decoder.blocks.19.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.self_attn.q_proj.bias  ->  decoder.blocks.19.attn.query.bias\n",
            "decoder.blocks.19.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.self_attn.out_proj.weight  ->  decoder.blocks.19.attn.out.weight\n",
            "decoder.blocks.19.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.self_attn.out_proj.bias  ->  decoder.blocks.19.attn.out.bias\n",
            "decoder.blocks.19.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.self_attn_layer_norm.weight  ->  decoder.blocks.19.attn_ln.weight\n",
            "decoder.blocks.19.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.self_attn_layer_norm.bias  ->  decoder.blocks.19.attn_ln.bias\n",
            "decoder.blocks.19.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.encoder_attn.k_proj.weight  ->  decoder.blocks.19.cross_attn.key.weight\n",
            "decoder.blocks.19.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.encoder_attn.v_proj.weight  ->  decoder.blocks.19.cross_attn.value.weight\n",
            "decoder.blocks.19.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.encoder_attn.v_proj.bias  ->  decoder.blocks.19.cross_attn.value.bias\n",
            "decoder.blocks.19.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.encoder_attn.q_proj.weight  ->  decoder.blocks.19.cross_attn.query.weight\n",
            "decoder.blocks.19.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.encoder_attn.q_proj.bias  ->  decoder.blocks.19.cross_attn.query.bias\n",
            "decoder.blocks.19.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.encoder_attn.out_proj.weight  ->  decoder.blocks.19.cross_attn.out.weight\n",
            "decoder.blocks.19.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.19.encoder_attn.out_proj.bias  ->  decoder.blocks.19.cross_attn.out.bias\n",
            "decoder.blocks.19.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.encoder_attn_layer_norm.weight  ->  decoder.blocks.19.cross_attn_ln.weight\n",
            "decoder.blocks.19.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.encoder_attn_layer_norm.bias  ->  decoder.blocks.19.cross_attn_ln.bias\n",
            "decoder.blocks.19.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.fc1.weight  ->  decoder.blocks.19.mlp.0.weight\n",
            "decoder.blocks.19.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.19.fc1.bias  ->  decoder.blocks.19.mlp.0.bias\n",
            "decoder.blocks.19.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.fc2.weight  ->  decoder.blocks.19.mlp.2.weight\n",
            "decoder.blocks.19.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.19.fc2.bias  ->  decoder.blocks.19.mlp.2.bias\n",
            "decoder.blocks.19.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.final_layer_norm.weight  ->  decoder.blocks.19.mlp_ln.weight\n",
            "decoder.blocks.19.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.19.final_layer_norm.bias  ->  decoder.blocks.19.mlp_ln.bias\n",
            "decoder.blocks.19.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.self_attn.k_proj.weight  ->  decoder.blocks.20.attn.key.weight\n",
            "decoder.blocks.20.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.self_attn.v_proj.weight  ->  decoder.blocks.20.attn.value.weight\n",
            "decoder.blocks.20.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.self_attn.v_proj.bias  ->  decoder.blocks.20.attn.value.bias\n",
            "decoder.blocks.20.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.self_attn.q_proj.weight  ->  decoder.blocks.20.attn.query.weight\n",
            "decoder.blocks.20.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.self_attn.q_proj.bias  ->  decoder.blocks.20.attn.query.bias\n",
            "decoder.blocks.20.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.self_attn.out_proj.weight  ->  decoder.blocks.20.attn.out.weight\n",
            "decoder.blocks.20.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.self_attn.out_proj.bias  ->  decoder.blocks.20.attn.out.bias\n",
            "decoder.blocks.20.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.self_attn_layer_norm.weight  ->  decoder.blocks.20.attn_ln.weight\n",
            "decoder.blocks.20.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.self_attn_layer_norm.bias  ->  decoder.blocks.20.attn_ln.bias\n",
            "decoder.blocks.20.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.encoder_attn.k_proj.weight  ->  decoder.blocks.20.cross_attn.key.weight\n",
            "decoder.blocks.20.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.encoder_attn.v_proj.weight  ->  decoder.blocks.20.cross_attn.value.weight\n",
            "decoder.blocks.20.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.encoder_attn.v_proj.bias  ->  decoder.blocks.20.cross_attn.value.bias\n",
            "decoder.blocks.20.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.encoder_attn.q_proj.weight  ->  decoder.blocks.20.cross_attn.query.weight\n",
            "decoder.blocks.20.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.encoder_attn.q_proj.bias  ->  decoder.blocks.20.cross_attn.query.bias\n",
            "decoder.blocks.20.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.encoder_attn.out_proj.weight  ->  decoder.blocks.20.cross_attn.out.weight\n",
            "decoder.blocks.20.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.20.encoder_attn.out_proj.bias  ->  decoder.blocks.20.cross_attn.out.bias\n",
            "decoder.blocks.20.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.encoder_attn_layer_norm.weight  ->  decoder.blocks.20.cross_attn_ln.weight\n",
            "decoder.blocks.20.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.encoder_attn_layer_norm.bias  ->  decoder.blocks.20.cross_attn_ln.bias\n",
            "decoder.blocks.20.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.fc1.weight  ->  decoder.blocks.20.mlp.0.weight\n",
            "decoder.blocks.20.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.20.fc1.bias  ->  decoder.blocks.20.mlp.0.bias\n",
            "decoder.blocks.20.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.fc2.weight  ->  decoder.blocks.20.mlp.2.weight\n",
            "decoder.blocks.20.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.20.fc2.bias  ->  decoder.blocks.20.mlp.2.bias\n",
            "decoder.blocks.20.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.final_layer_norm.weight  ->  decoder.blocks.20.mlp_ln.weight\n",
            "decoder.blocks.20.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.20.final_layer_norm.bias  ->  decoder.blocks.20.mlp_ln.bias\n",
            "decoder.blocks.20.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.self_attn.k_proj.weight  ->  decoder.blocks.21.attn.key.weight\n",
            "decoder.blocks.21.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.self_attn.v_proj.weight  ->  decoder.blocks.21.attn.value.weight\n",
            "decoder.blocks.21.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.self_attn.v_proj.bias  ->  decoder.blocks.21.attn.value.bias\n",
            "decoder.blocks.21.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.self_attn.q_proj.weight  ->  decoder.blocks.21.attn.query.weight\n",
            "decoder.blocks.21.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.self_attn.q_proj.bias  ->  decoder.blocks.21.attn.query.bias\n",
            "decoder.blocks.21.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.self_attn.out_proj.weight  ->  decoder.blocks.21.attn.out.weight\n",
            "decoder.blocks.21.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.self_attn.out_proj.bias  ->  decoder.blocks.21.attn.out.bias\n",
            "decoder.blocks.21.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.self_attn_layer_norm.weight  ->  decoder.blocks.21.attn_ln.weight\n",
            "decoder.blocks.21.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.self_attn_layer_norm.bias  ->  decoder.blocks.21.attn_ln.bias\n",
            "decoder.blocks.21.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.encoder_attn.k_proj.weight  ->  decoder.blocks.21.cross_attn.key.weight\n",
            "decoder.blocks.21.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.encoder_attn.v_proj.weight  ->  decoder.blocks.21.cross_attn.value.weight\n",
            "decoder.blocks.21.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.encoder_attn.v_proj.bias  ->  decoder.blocks.21.cross_attn.value.bias\n",
            "decoder.blocks.21.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.encoder_attn.q_proj.weight  ->  decoder.blocks.21.cross_attn.query.weight\n",
            "decoder.blocks.21.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.encoder_attn.q_proj.bias  ->  decoder.blocks.21.cross_attn.query.bias\n",
            "decoder.blocks.21.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.encoder_attn.out_proj.weight  ->  decoder.blocks.21.cross_attn.out.weight\n",
            "decoder.blocks.21.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.21.encoder_attn.out_proj.bias  ->  decoder.blocks.21.cross_attn.out.bias\n",
            "decoder.blocks.21.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.encoder_attn_layer_norm.weight  ->  decoder.blocks.21.cross_attn_ln.weight\n",
            "decoder.blocks.21.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.encoder_attn_layer_norm.bias  ->  decoder.blocks.21.cross_attn_ln.bias\n",
            "decoder.blocks.21.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.fc1.weight  ->  decoder.blocks.21.mlp.0.weight\n",
            "decoder.blocks.21.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.21.fc1.bias  ->  decoder.blocks.21.mlp.0.bias\n",
            "decoder.blocks.21.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.fc2.weight  ->  decoder.blocks.21.mlp.2.weight\n",
            "decoder.blocks.21.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.21.fc2.bias  ->  decoder.blocks.21.mlp.2.bias\n",
            "decoder.blocks.21.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.final_layer_norm.weight  ->  decoder.blocks.21.mlp_ln.weight\n",
            "decoder.blocks.21.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.21.final_layer_norm.bias  ->  decoder.blocks.21.mlp_ln.bias\n",
            "decoder.blocks.21.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.self_attn.k_proj.weight  ->  decoder.blocks.22.attn.key.weight\n",
            "decoder.blocks.22.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.self_attn.v_proj.weight  ->  decoder.blocks.22.attn.value.weight\n",
            "decoder.blocks.22.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.self_attn.v_proj.bias  ->  decoder.blocks.22.attn.value.bias\n",
            "decoder.blocks.22.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.self_attn.q_proj.weight  ->  decoder.blocks.22.attn.query.weight\n",
            "decoder.blocks.22.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.self_attn.q_proj.bias  ->  decoder.blocks.22.attn.query.bias\n",
            "decoder.blocks.22.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.self_attn.out_proj.weight  ->  decoder.blocks.22.attn.out.weight\n",
            "decoder.blocks.22.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.self_attn.out_proj.bias  ->  decoder.blocks.22.attn.out.bias\n",
            "decoder.blocks.22.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.self_attn_layer_norm.weight  ->  decoder.blocks.22.attn_ln.weight\n",
            "decoder.blocks.22.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.self_attn_layer_norm.bias  ->  decoder.blocks.22.attn_ln.bias\n",
            "decoder.blocks.22.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.encoder_attn.k_proj.weight  ->  decoder.blocks.22.cross_attn.key.weight\n",
            "decoder.blocks.22.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.encoder_attn.v_proj.weight  ->  decoder.blocks.22.cross_attn.value.weight\n",
            "decoder.blocks.22.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.encoder_attn.v_proj.bias  ->  decoder.blocks.22.cross_attn.value.bias\n",
            "decoder.blocks.22.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.encoder_attn.q_proj.weight  ->  decoder.blocks.22.cross_attn.query.weight\n",
            "decoder.blocks.22.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.encoder_attn.q_proj.bias  ->  decoder.blocks.22.cross_attn.query.bias\n",
            "decoder.blocks.22.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.encoder_attn.out_proj.weight  ->  decoder.blocks.22.cross_attn.out.weight\n",
            "decoder.blocks.22.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.22.encoder_attn.out_proj.bias  ->  decoder.blocks.22.cross_attn.out.bias\n",
            "decoder.blocks.22.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.encoder_attn_layer_norm.weight  ->  decoder.blocks.22.cross_attn_ln.weight\n",
            "decoder.blocks.22.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.encoder_attn_layer_norm.bias  ->  decoder.blocks.22.cross_attn_ln.bias\n",
            "decoder.blocks.22.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.fc1.weight  ->  decoder.blocks.22.mlp.0.weight\n",
            "decoder.blocks.22.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.22.fc1.bias  ->  decoder.blocks.22.mlp.0.bias\n",
            "decoder.blocks.22.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.fc2.weight  ->  decoder.blocks.22.mlp.2.weight\n",
            "decoder.blocks.22.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.22.fc2.bias  ->  decoder.blocks.22.mlp.2.bias\n",
            "decoder.blocks.22.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.final_layer_norm.weight  ->  decoder.blocks.22.mlp_ln.weight\n",
            "decoder.blocks.22.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.22.final_layer_norm.bias  ->  decoder.blocks.22.mlp_ln.bias\n",
            "decoder.blocks.22.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.self_attn.k_proj.weight  ->  decoder.blocks.23.attn.key.weight\n",
            "decoder.blocks.23.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.self_attn.v_proj.weight  ->  decoder.blocks.23.attn.value.weight\n",
            "decoder.blocks.23.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.self_attn.v_proj.bias  ->  decoder.blocks.23.attn.value.bias\n",
            "decoder.blocks.23.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.self_attn.q_proj.weight  ->  decoder.blocks.23.attn.query.weight\n",
            "decoder.blocks.23.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.self_attn.q_proj.bias  ->  decoder.blocks.23.attn.query.bias\n",
            "decoder.blocks.23.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.self_attn.out_proj.weight  ->  decoder.blocks.23.attn.out.weight\n",
            "decoder.blocks.23.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.self_attn.out_proj.bias  ->  decoder.blocks.23.attn.out.bias\n",
            "decoder.blocks.23.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.self_attn_layer_norm.weight  ->  decoder.blocks.23.attn_ln.weight\n",
            "decoder.blocks.23.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.self_attn_layer_norm.bias  ->  decoder.blocks.23.attn_ln.bias\n",
            "decoder.blocks.23.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.encoder_attn.k_proj.weight  ->  decoder.blocks.23.cross_attn.key.weight\n",
            "decoder.blocks.23.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.encoder_attn.v_proj.weight  ->  decoder.blocks.23.cross_attn.value.weight\n",
            "decoder.blocks.23.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.encoder_attn.v_proj.bias  ->  decoder.blocks.23.cross_attn.value.bias\n",
            "decoder.blocks.23.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.encoder_attn.q_proj.weight  ->  decoder.blocks.23.cross_attn.query.weight\n",
            "decoder.blocks.23.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.encoder_attn.q_proj.bias  ->  decoder.blocks.23.cross_attn.query.bias\n",
            "decoder.blocks.23.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.encoder_attn.out_proj.weight  ->  decoder.blocks.23.cross_attn.out.weight\n",
            "decoder.blocks.23.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.23.encoder_attn.out_proj.bias  ->  decoder.blocks.23.cross_attn.out.bias\n",
            "decoder.blocks.23.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.encoder_attn_layer_norm.weight  ->  decoder.blocks.23.cross_attn_ln.weight\n",
            "decoder.blocks.23.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.encoder_attn_layer_norm.bias  ->  decoder.blocks.23.cross_attn_ln.bias\n",
            "decoder.blocks.23.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.fc1.weight  ->  decoder.blocks.23.mlp.0.weight\n",
            "decoder.blocks.23.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.23.fc1.bias  ->  decoder.blocks.23.mlp.0.bias\n",
            "decoder.blocks.23.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.fc2.weight  ->  decoder.blocks.23.mlp.2.weight\n",
            "decoder.blocks.23.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.23.fc2.bias  ->  decoder.blocks.23.mlp.2.bias\n",
            "decoder.blocks.23.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.final_layer_norm.weight  ->  decoder.blocks.23.mlp_ln.weight\n",
            "decoder.blocks.23.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.23.final_layer_norm.bias  ->  decoder.blocks.23.mlp_ln.bias\n",
            "decoder.blocks.23.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.self_attn.k_proj.weight  ->  decoder.blocks.24.attn.key.weight\n",
            "decoder.blocks.24.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.self_attn.v_proj.weight  ->  decoder.blocks.24.attn.value.weight\n",
            "decoder.blocks.24.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.self_attn.v_proj.bias  ->  decoder.blocks.24.attn.value.bias\n",
            "decoder.blocks.24.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.self_attn.q_proj.weight  ->  decoder.blocks.24.attn.query.weight\n",
            "decoder.blocks.24.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.self_attn.q_proj.bias  ->  decoder.blocks.24.attn.query.bias\n",
            "decoder.blocks.24.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.self_attn.out_proj.weight  ->  decoder.blocks.24.attn.out.weight\n",
            "decoder.blocks.24.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.self_attn.out_proj.bias  ->  decoder.blocks.24.attn.out.bias\n",
            "decoder.blocks.24.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.self_attn_layer_norm.weight  ->  decoder.blocks.24.attn_ln.weight\n",
            "decoder.blocks.24.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.self_attn_layer_norm.bias  ->  decoder.blocks.24.attn_ln.bias\n",
            "decoder.blocks.24.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.encoder_attn.k_proj.weight  ->  decoder.blocks.24.cross_attn.key.weight\n",
            "decoder.blocks.24.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.encoder_attn.v_proj.weight  ->  decoder.blocks.24.cross_attn.value.weight\n",
            "decoder.blocks.24.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.encoder_attn.v_proj.bias  ->  decoder.blocks.24.cross_attn.value.bias\n",
            "decoder.blocks.24.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.encoder_attn.q_proj.weight  ->  decoder.blocks.24.cross_attn.query.weight\n",
            "decoder.blocks.24.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.encoder_attn.q_proj.bias  ->  decoder.blocks.24.cross_attn.query.bias\n",
            "decoder.blocks.24.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.encoder_attn.out_proj.weight  ->  decoder.blocks.24.cross_attn.out.weight\n",
            "decoder.blocks.24.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.24.encoder_attn.out_proj.bias  ->  decoder.blocks.24.cross_attn.out.bias\n",
            "decoder.blocks.24.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.encoder_attn_layer_norm.weight  ->  decoder.blocks.24.cross_attn_ln.weight\n",
            "decoder.blocks.24.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.encoder_attn_layer_norm.bias  ->  decoder.blocks.24.cross_attn_ln.bias\n",
            "decoder.blocks.24.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.fc1.weight  ->  decoder.blocks.24.mlp.0.weight\n",
            "decoder.blocks.24.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.24.fc1.bias  ->  decoder.blocks.24.mlp.0.bias\n",
            "decoder.blocks.24.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.fc2.weight  ->  decoder.blocks.24.mlp.2.weight\n",
            "decoder.blocks.24.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.24.fc2.bias  ->  decoder.blocks.24.mlp.2.bias\n",
            "decoder.blocks.24.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.final_layer_norm.weight  ->  decoder.blocks.24.mlp_ln.weight\n",
            "decoder.blocks.24.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.24.final_layer_norm.bias  ->  decoder.blocks.24.mlp_ln.bias\n",
            "decoder.blocks.24.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.self_attn.k_proj.weight  ->  decoder.blocks.25.attn.key.weight\n",
            "decoder.blocks.25.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.self_attn.v_proj.weight  ->  decoder.blocks.25.attn.value.weight\n",
            "decoder.blocks.25.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.self_attn.v_proj.bias  ->  decoder.blocks.25.attn.value.bias\n",
            "decoder.blocks.25.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.self_attn.q_proj.weight  ->  decoder.blocks.25.attn.query.weight\n",
            "decoder.blocks.25.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.self_attn.q_proj.bias  ->  decoder.blocks.25.attn.query.bias\n",
            "decoder.blocks.25.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.self_attn.out_proj.weight  ->  decoder.blocks.25.attn.out.weight\n",
            "decoder.blocks.25.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.self_attn.out_proj.bias  ->  decoder.blocks.25.attn.out.bias\n",
            "decoder.blocks.25.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.self_attn_layer_norm.weight  ->  decoder.blocks.25.attn_ln.weight\n",
            "decoder.blocks.25.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.self_attn_layer_norm.bias  ->  decoder.blocks.25.attn_ln.bias\n",
            "decoder.blocks.25.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.encoder_attn.k_proj.weight  ->  decoder.blocks.25.cross_attn.key.weight\n",
            "decoder.blocks.25.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.encoder_attn.v_proj.weight  ->  decoder.blocks.25.cross_attn.value.weight\n",
            "decoder.blocks.25.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.encoder_attn.v_proj.bias  ->  decoder.blocks.25.cross_attn.value.bias\n",
            "decoder.blocks.25.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.encoder_attn.q_proj.weight  ->  decoder.blocks.25.cross_attn.query.weight\n",
            "decoder.blocks.25.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.encoder_attn.q_proj.bias  ->  decoder.blocks.25.cross_attn.query.bias\n",
            "decoder.blocks.25.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.encoder_attn.out_proj.weight  ->  decoder.blocks.25.cross_attn.out.weight\n",
            "decoder.blocks.25.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.25.encoder_attn.out_proj.bias  ->  decoder.blocks.25.cross_attn.out.bias\n",
            "decoder.blocks.25.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.encoder_attn_layer_norm.weight  ->  decoder.blocks.25.cross_attn_ln.weight\n",
            "decoder.blocks.25.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.encoder_attn_layer_norm.bias  ->  decoder.blocks.25.cross_attn_ln.bias\n",
            "decoder.blocks.25.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.fc1.weight  ->  decoder.blocks.25.mlp.0.weight\n",
            "decoder.blocks.25.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.25.fc1.bias  ->  decoder.blocks.25.mlp.0.bias\n",
            "decoder.blocks.25.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.fc2.weight  ->  decoder.blocks.25.mlp.2.weight\n",
            "decoder.blocks.25.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.25.fc2.bias  ->  decoder.blocks.25.mlp.2.bias\n",
            "decoder.blocks.25.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.final_layer_norm.weight  ->  decoder.blocks.25.mlp_ln.weight\n",
            "decoder.blocks.25.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.25.final_layer_norm.bias  ->  decoder.blocks.25.mlp_ln.bias\n",
            "decoder.blocks.25.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.self_attn.k_proj.weight  ->  decoder.blocks.26.attn.key.weight\n",
            "decoder.blocks.26.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.self_attn.v_proj.weight  ->  decoder.blocks.26.attn.value.weight\n",
            "decoder.blocks.26.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.self_attn.v_proj.bias  ->  decoder.blocks.26.attn.value.bias\n",
            "decoder.blocks.26.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.self_attn.q_proj.weight  ->  decoder.blocks.26.attn.query.weight\n",
            "decoder.blocks.26.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.self_attn.q_proj.bias  ->  decoder.blocks.26.attn.query.bias\n",
            "decoder.blocks.26.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.self_attn.out_proj.weight  ->  decoder.blocks.26.attn.out.weight\n",
            "decoder.blocks.26.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.self_attn.out_proj.bias  ->  decoder.blocks.26.attn.out.bias\n",
            "decoder.blocks.26.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.self_attn_layer_norm.weight  ->  decoder.blocks.26.attn_ln.weight\n",
            "decoder.blocks.26.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.self_attn_layer_norm.bias  ->  decoder.blocks.26.attn_ln.bias\n",
            "decoder.blocks.26.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.encoder_attn.k_proj.weight  ->  decoder.blocks.26.cross_attn.key.weight\n",
            "decoder.blocks.26.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.encoder_attn.v_proj.weight  ->  decoder.blocks.26.cross_attn.value.weight\n",
            "decoder.blocks.26.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.encoder_attn.v_proj.bias  ->  decoder.blocks.26.cross_attn.value.bias\n",
            "decoder.blocks.26.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.encoder_attn.q_proj.weight  ->  decoder.blocks.26.cross_attn.query.weight\n",
            "decoder.blocks.26.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.encoder_attn.q_proj.bias  ->  decoder.blocks.26.cross_attn.query.bias\n",
            "decoder.blocks.26.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.encoder_attn.out_proj.weight  ->  decoder.blocks.26.cross_attn.out.weight\n",
            "decoder.blocks.26.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.26.encoder_attn.out_proj.bias  ->  decoder.blocks.26.cross_attn.out.bias\n",
            "decoder.blocks.26.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.encoder_attn_layer_norm.weight  ->  decoder.blocks.26.cross_attn_ln.weight\n",
            "decoder.blocks.26.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.encoder_attn_layer_norm.bias  ->  decoder.blocks.26.cross_attn_ln.bias\n",
            "decoder.blocks.26.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.fc1.weight  ->  decoder.blocks.26.mlp.0.weight\n",
            "decoder.blocks.26.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.26.fc1.bias  ->  decoder.blocks.26.mlp.0.bias\n",
            "decoder.blocks.26.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.fc2.weight  ->  decoder.blocks.26.mlp.2.weight\n",
            "decoder.blocks.26.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.26.fc2.bias  ->  decoder.blocks.26.mlp.2.bias\n",
            "decoder.blocks.26.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.final_layer_norm.weight  ->  decoder.blocks.26.mlp_ln.weight\n",
            "decoder.blocks.26.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.26.final_layer_norm.bias  ->  decoder.blocks.26.mlp_ln.bias\n",
            "decoder.blocks.26.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.self_attn.k_proj.weight  ->  decoder.blocks.27.attn.key.weight\n",
            "decoder.blocks.27.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.self_attn.v_proj.weight  ->  decoder.blocks.27.attn.value.weight\n",
            "decoder.blocks.27.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.self_attn.v_proj.bias  ->  decoder.blocks.27.attn.value.bias\n",
            "decoder.blocks.27.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.self_attn.q_proj.weight  ->  decoder.blocks.27.attn.query.weight\n",
            "decoder.blocks.27.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.self_attn.q_proj.bias  ->  decoder.blocks.27.attn.query.bias\n",
            "decoder.blocks.27.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.self_attn.out_proj.weight  ->  decoder.blocks.27.attn.out.weight\n",
            "decoder.blocks.27.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.self_attn.out_proj.bias  ->  decoder.blocks.27.attn.out.bias\n",
            "decoder.blocks.27.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.self_attn_layer_norm.weight  ->  decoder.blocks.27.attn_ln.weight\n",
            "decoder.blocks.27.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.self_attn_layer_norm.bias  ->  decoder.blocks.27.attn_ln.bias\n",
            "decoder.blocks.27.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.encoder_attn.k_proj.weight  ->  decoder.blocks.27.cross_attn.key.weight\n",
            "decoder.blocks.27.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.encoder_attn.v_proj.weight  ->  decoder.blocks.27.cross_attn.value.weight\n",
            "decoder.blocks.27.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.encoder_attn.v_proj.bias  ->  decoder.blocks.27.cross_attn.value.bias\n",
            "decoder.blocks.27.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.encoder_attn.q_proj.weight  ->  decoder.blocks.27.cross_attn.query.weight\n",
            "decoder.blocks.27.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.encoder_attn.q_proj.bias  ->  decoder.blocks.27.cross_attn.query.bias\n",
            "decoder.blocks.27.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.encoder_attn.out_proj.weight  ->  decoder.blocks.27.cross_attn.out.weight\n",
            "decoder.blocks.27.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.27.encoder_attn.out_proj.bias  ->  decoder.blocks.27.cross_attn.out.bias\n",
            "decoder.blocks.27.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.encoder_attn_layer_norm.weight  ->  decoder.blocks.27.cross_attn_ln.weight\n",
            "decoder.blocks.27.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.encoder_attn_layer_norm.bias  ->  decoder.blocks.27.cross_attn_ln.bias\n",
            "decoder.blocks.27.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.fc1.weight  ->  decoder.blocks.27.mlp.0.weight\n",
            "decoder.blocks.27.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.27.fc1.bias  ->  decoder.blocks.27.mlp.0.bias\n",
            "decoder.blocks.27.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.fc2.weight  ->  decoder.blocks.27.mlp.2.weight\n",
            "decoder.blocks.27.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.27.fc2.bias  ->  decoder.blocks.27.mlp.2.bias\n",
            "decoder.blocks.27.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.final_layer_norm.weight  ->  decoder.blocks.27.mlp_ln.weight\n",
            "decoder.blocks.27.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.27.final_layer_norm.bias  ->  decoder.blocks.27.mlp_ln.bias\n",
            "decoder.blocks.27.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.self_attn.k_proj.weight  ->  decoder.blocks.28.attn.key.weight\n",
            "decoder.blocks.28.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.self_attn.v_proj.weight  ->  decoder.blocks.28.attn.value.weight\n",
            "decoder.blocks.28.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.self_attn.v_proj.bias  ->  decoder.blocks.28.attn.value.bias\n",
            "decoder.blocks.28.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.self_attn.q_proj.weight  ->  decoder.blocks.28.attn.query.weight\n",
            "decoder.blocks.28.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.self_attn.q_proj.bias  ->  decoder.blocks.28.attn.query.bias\n",
            "decoder.blocks.28.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.self_attn.out_proj.weight  ->  decoder.blocks.28.attn.out.weight\n",
            "decoder.blocks.28.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.self_attn.out_proj.bias  ->  decoder.blocks.28.attn.out.bias\n",
            "decoder.blocks.28.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.self_attn_layer_norm.weight  ->  decoder.blocks.28.attn_ln.weight\n",
            "decoder.blocks.28.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.self_attn_layer_norm.bias  ->  decoder.blocks.28.attn_ln.bias\n",
            "decoder.blocks.28.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.encoder_attn.k_proj.weight  ->  decoder.blocks.28.cross_attn.key.weight\n",
            "decoder.blocks.28.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.encoder_attn.v_proj.weight  ->  decoder.blocks.28.cross_attn.value.weight\n",
            "decoder.blocks.28.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.encoder_attn.v_proj.bias  ->  decoder.blocks.28.cross_attn.value.bias\n",
            "decoder.blocks.28.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.encoder_attn.q_proj.weight  ->  decoder.blocks.28.cross_attn.query.weight\n",
            "decoder.blocks.28.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.encoder_attn.q_proj.bias  ->  decoder.blocks.28.cross_attn.query.bias\n",
            "decoder.blocks.28.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.encoder_attn.out_proj.weight  ->  decoder.blocks.28.cross_attn.out.weight\n",
            "decoder.blocks.28.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.28.encoder_attn.out_proj.bias  ->  decoder.blocks.28.cross_attn.out.bias\n",
            "decoder.blocks.28.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.encoder_attn_layer_norm.weight  ->  decoder.blocks.28.cross_attn_ln.weight\n",
            "decoder.blocks.28.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.encoder_attn_layer_norm.bias  ->  decoder.blocks.28.cross_attn_ln.bias\n",
            "decoder.blocks.28.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.fc1.weight  ->  decoder.blocks.28.mlp.0.weight\n",
            "decoder.blocks.28.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.28.fc1.bias  ->  decoder.blocks.28.mlp.0.bias\n",
            "decoder.blocks.28.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.fc2.weight  ->  decoder.blocks.28.mlp.2.weight\n",
            "decoder.blocks.28.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.28.fc2.bias  ->  decoder.blocks.28.mlp.2.bias\n",
            "decoder.blocks.28.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.final_layer_norm.weight  ->  decoder.blocks.28.mlp_ln.weight\n",
            "decoder.blocks.28.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.28.final_layer_norm.bias  ->  decoder.blocks.28.mlp_ln.bias\n",
            "decoder.blocks.28.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.self_attn.k_proj.weight  ->  decoder.blocks.29.attn.key.weight\n",
            "decoder.blocks.29.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.self_attn.v_proj.weight  ->  decoder.blocks.29.attn.value.weight\n",
            "decoder.blocks.29.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.self_attn.v_proj.bias  ->  decoder.blocks.29.attn.value.bias\n",
            "decoder.blocks.29.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.self_attn.q_proj.weight  ->  decoder.blocks.29.attn.query.weight\n",
            "decoder.blocks.29.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.self_attn.q_proj.bias  ->  decoder.blocks.29.attn.query.bias\n",
            "decoder.blocks.29.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.self_attn.out_proj.weight  ->  decoder.blocks.29.attn.out.weight\n",
            "decoder.blocks.29.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.self_attn.out_proj.bias  ->  decoder.blocks.29.attn.out.bias\n",
            "decoder.blocks.29.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.self_attn_layer_norm.weight  ->  decoder.blocks.29.attn_ln.weight\n",
            "decoder.blocks.29.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.self_attn_layer_norm.bias  ->  decoder.blocks.29.attn_ln.bias\n",
            "decoder.blocks.29.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.encoder_attn.k_proj.weight  ->  decoder.blocks.29.cross_attn.key.weight\n",
            "decoder.blocks.29.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.encoder_attn.v_proj.weight  ->  decoder.blocks.29.cross_attn.value.weight\n",
            "decoder.blocks.29.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.encoder_attn.v_proj.bias  ->  decoder.blocks.29.cross_attn.value.bias\n",
            "decoder.blocks.29.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.encoder_attn.q_proj.weight  ->  decoder.blocks.29.cross_attn.query.weight\n",
            "decoder.blocks.29.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.encoder_attn.q_proj.bias  ->  decoder.blocks.29.cross_attn.query.bias\n",
            "decoder.blocks.29.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.encoder_attn.out_proj.weight  ->  decoder.blocks.29.cross_attn.out.weight\n",
            "decoder.blocks.29.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.29.encoder_attn.out_proj.bias  ->  decoder.blocks.29.cross_attn.out.bias\n",
            "decoder.blocks.29.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.encoder_attn_layer_norm.weight  ->  decoder.blocks.29.cross_attn_ln.weight\n",
            "decoder.blocks.29.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.encoder_attn_layer_norm.bias  ->  decoder.blocks.29.cross_attn_ln.bias\n",
            "decoder.blocks.29.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.fc1.weight  ->  decoder.blocks.29.mlp.0.weight\n",
            "decoder.blocks.29.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.29.fc1.bias  ->  decoder.blocks.29.mlp.0.bias\n",
            "decoder.blocks.29.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.fc2.weight  ->  decoder.blocks.29.mlp.2.weight\n",
            "decoder.blocks.29.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.29.fc2.bias  ->  decoder.blocks.29.mlp.2.bias\n",
            "decoder.blocks.29.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.final_layer_norm.weight  ->  decoder.blocks.29.mlp_ln.weight\n",
            "decoder.blocks.29.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.29.final_layer_norm.bias  ->  decoder.blocks.29.mlp_ln.bias\n",
            "decoder.blocks.29.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.self_attn.k_proj.weight  ->  decoder.blocks.30.attn.key.weight\n",
            "decoder.blocks.30.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.self_attn.v_proj.weight  ->  decoder.blocks.30.attn.value.weight\n",
            "decoder.blocks.30.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.self_attn.v_proj.bias  ->  decoder.blocks.30.attn.value.bias\n",
            "decoder.blocks.30.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.self_attn.q_proj.weight  ->  decoder.blocks.30.attn.query.weight\n",
            "decoder.blocks.30.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.self_attn.q_proj.bias  ->  decoder.blocks.30.attn.query.bias\n",
            "decoder.blocks.30.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.self_attn.out_proj.weight  ->  decoder.blocks.30.attn.out.weight\n",
            "decoder.blocks.30.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.self_attn.out_proj.bias  ->  decoder.blocks.30.attn.out.bias\n",
            "decoder.blocks.30.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.self_attn_layer_norm.weight  ->  decoder.blocks.30.attn_ln.weight\n",
            "decoder.blocks.30.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.self_attn_layer_norm.bias  ->  decoder.blocks.30.attn_ln.bias\n",
            "decoder.blocks.30.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.encoder_attn.k_proj.weight  ->  decoder.blocks.30.cross_attn.key.weight\n",
            "decoder.blocks.30.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.encoder_attn.v_proj.weight  ->  decoder.blocks.30.cross_attn.value.weight\n",
            "decoder.blocks.30.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.encoder_attn.v_proj.bias  ->  decoder.blocks.30.cross_attn.value.bias\n",
            "decoder.blocks.30.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.encoder_attn.q_proj.weight  ->  decoder.blocks.30.cross_attn.query.weight\n",
            "decoder.blocks.30.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.encoder_attn.q_proj.bias  ->  decoder.blocks.30.cross_attn.query.bias\n",
            "decoder.blocks.30.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.encoder_attn.out_proj.weight  ->  decoder.blocks.30.cross_attn.out.weight\n",
            "decoder.blocks.30.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.30.encoder_attn.out_proj.bias  ->  decoder.blocks.30.cross_attn.out.bias\n",
            "decoder.blocks.30.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.encoder_attn_layer_norm.weight  ->  decoder.blocks.30.cross_attn_ln.weight\n",
            "decoder.blocks.30.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.encoder_attn_layer_norm.bias  ->  decoder.blocks.30.cross_attn_ln.bias\n",
            "decoder.blocks.30.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.fc1.weight  ->  decoder.blocks.30.mlp.0.weight\n",
            "decoder.blocks.30.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.30.fc1.bias  ->  decoder.blocks.30.mlp.0.bias\n",
            "decoder.blocks.30.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.fc2.weight  ->  decoder.blocks.30.mlp.2.weight\n",
            "decoder.blocks.30.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.30.fc2.bias  ->  decoder.blocks.30.mlp.2.bias\n",
            "decoder.blocks.30.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.final_layer_norm.weight  ->  decoder.blocks.30.mlp_ln.weight\n",
            "decoder.blocks.30.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.30.final_layer_norm.bias  ->  decoder.blocks.30.mlp_ln.bias\n",
            "decoder.blocks.30.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.self_attn.k_proj.weight  ->  decoder.blocks.31.attn.key.weight\n",
            "decoder.blocks.31.attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.self_attn.v_proj.weight  ->  decoder.blocks.31.attn.value.weight\n",
            "decoder.blocks.31.attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.self_attn.v_proj.bias  ->  decoder.blocks.31.attn.value.bias\n",
            "decoder.blocks.31.attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.self_attn.q_proj.weight  ->  decoder.blocks.31.attn.query.weight\n",
            "decoder.blocks.31.attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.self_attn.q_proj.bias  ->  decoder.blocks.31.attn.query.bias\n",
            "decoder.blocks.31.attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.self_attn.out_proj.weight  ->  decoder.blocks.31.attn.out.weight\n",
            "decoder.blocks.31.attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.self_attn.out_proj.bias  ->  decoder.blocks.31.attn.out.bias\n",
            "decoder.blocks.31.attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.self_attn_layer_norm.weight  ->  decoder.blocks.31.attn_ln.weight\n",
            "decoder.blocks.31.attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.self_attn_layer_norm.bias  ->  decoder.blocks.31.attn_ln.bias\n",
            "decoder.blocks.31.attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.encoder_attn.k_proj.weight  ->  decoder.blocks.31.cross_attn.key.weight\n",
            "decoder.blocks.31.cross_attn.key.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.encoder_attn.v_proj.weight  ->  decoder.blocks.31.cross_attn.value.weight\n",
            "decoder.blocks.31.cross_attn.value.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.encoder_attn.v_proj.bias  ->  decoder.blocks.31.cross_attn.value.bias\n",
            "decoder.blocks.31.cross_attn.value.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.encoder_attn.q_proj.weight  ->  decoder.blocks.31.cross_attn.query.weight\n",
            "decoder.blocks.31.cross_attn.query.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.encoder_attn.q_proj.bias  ->  decoder.blocks.31.cross_attn.query.bias\n",
            "decoder.blocks.31.cross_attn.query.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.encoder_attn.out_proj.weight  ->  decoder.blocks.31.cross_attn.out.weight\n",
            "decoder.blocks.31.cross_attn.out.weight 2 (1280, 1280)\n",
            "model.decoder.layers.31.encoder_attn.out_proj.bias  ->  decoder.blocks.31.cross_attn.out.bias\n",
            "decoder.blocks.31.cross_attn.out.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.encoder_attn_layer_norm.weight  ->  decoder.blocks.31.cross_attn_ln.weight\n",
            "decoder.blocks.31.cross_attn_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.encoder_attn_layer_norm.bias  ->  decoder.blocks.31.cross_attn_ln.bias\n",
            "decoder.blocks.31.cross_attn_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.fc1.weight  ->  decoder.blocks.31.mlp.0.weight\n",
            "decoder.blocks.31.mlp.0.weight 2 (5120, 1280)\n",
            "model.decoder.layers.31.fc1.bias  ->  decoder.blocks.31.mlp.0.bias\n",
            "decoder.blocks.31.mlp.0.bias 1 (5120,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.fc2.weight  ->  decoder.blocks.31.mlp.2.weight\n",
            "decoder.blocks.31.mlp.2.weight 2 (1280, 5120)\n",
            "model.decoder.layers.31.fc2.bias  ->  decoder.blocks.31.mlp.2.bias\n",
            "decoder.blocks.31.mlp.2.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.final_layer_norm.weight  ->  decoder.blocks.31.mlp_ln.weight\n",
            "decoder.blocks.31.mlp_ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.31.final_layer_norm.bias  ->  decoder.blocks.31.mlp_ln.bias\n",
            "decoder.blocks.31.mlp_ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layer_norm.weight  ->  decoder.ln.weight\n",
            "decoder.ln.weight 1 (1280,)\n",
            "  Converting to float32\n",
            "model.decoder.layer_norm.bias  ->  decoder.ln.bias\n",
            "decoder.ln.bias 1 (1280,)\n",
            "  Converting to float32\n",
            "Skipping proj_out.weight\n",
            "Done. Output file:  whisper-ggml-sme/ggml-model.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://media.globalrecordings.net/GOKit_MP3s_named/Saami%20North%20-%20The%20Two%20Roads.mp3 -O sample.mp3\n",
        "!ffmpeg -i sample.mp3 -acodec pcm_s16le -ac 1 -ar 16000 sample.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhZPHQ91aZB0",
        "outputId": "866a22cf-94b2-46b7-e802-ad948bc4e471"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-03 18:09:42--  https://media.globalrecordings.net/GOKit_MP3s_named/Saami%20North%20-%20The%20Two%20Roads.mp3\n",
            "Resolving media.globalrecordings.net (media.globalrecordings.net)... 35.208.248.145\n",
            "Connecting to media.globalrecordings.net (media.globalrecordings.net)|35.208.248.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 493511 (482K) [audio/mpeg]\n",
            "Saving to: ‘sample.mp3’\n",
            "\n",
            "sample.mp3          100%[===================>] 481.94K  3.00MB/s    in 0.2s    \n",
            "\n",
            "2024-03-03 18:09:42 (3.00 MB/s) - ‘sample.mp3’ saved [493511/493511]\n",
            "\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mp3, from 'sample.mp3':\n",
            "  Metadata:\n",
            "    title           : Saami, North Picture 22: The Two Roads The Two Roads\n",
            "    artist          : GRN Language Samples\n",
            "    album           : Saami, North SME\n",
            "    composer        : Saami, North\n",
            "    genre           : GRN Language Sample\n",
            "    encoder         : Lavf58.76.100\n",
            "    track           : 1\n",
            "    copyright       : 2010 GRN\n",
            "    comment         : https://globalrecordings.net/en/language/3475\n",
            "    grnprog         : A63258\n",
            "    grnlang         : 3475\n",
            "    grnprepared     : 20240113\n",
            "    date            : 2010\n",
            "  Duration: 00:01:44.67, start: 0.050111, bitrate: 37 kb/s\n",
            "  Stream #0:0: Audio: mp3, 22050 Hz, mono, fltp, 32 kb/s\n",
            "  Stream #0:1: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 300x300 [SAR 300:300 DAR 1:1], 90k tbr, 90k tbn, 90k tbc (attached pic)\n",
            "    Metadata:\n",
            "      title           : gn-22\n",
            "      comment         : Cover (front)\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'sample.wav':\n",
            "  Metadata:\n",
            "    INAM            : Saami, North Picture 22: The Two Roads The Two Roads\n",
            "    IART            : GRN Language Samples\n",
            "    IPRD            : Saami, North SME\n",
            "    composer        : Saami, North\n",
            "    IGNR            : GRN Language Sample\n",
            "    ICRD            : 2010\n",
            "    IPRT            : 1\n",
            "    ICOP            : 2010 GRN\n",
            "    ICMT            : https://globalrecordings.net/en/language/3475\n",
            "    grnprog         : A63258\n",
            "    grnlang         : 3475\n",
            "    grnprepared     : 20240113\n",
            "    ISFT            : Lavf58.76.100\n",
            "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 pcm_s16le\n",
            "size=    3269kB time=00:01:44.61 bitrate= 256.0kbits/s speed= 722x    \n",
            "video:0kB audio:3269kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.009559%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./whisper.cpp/main  -m whisper-ggml-sme/ggml-model.bin -f sample.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya-oxJc8eA02",
        "outputId": "0ebc27fb-4a27-4d8f-8c9c-7ad021a12cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisper_init_from_file_with_params_no_state: loading model from 'whisper-ggml-sme/ggml-model.bin'\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51865\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 1280\n",
            "whisper_model_load: n_audio_head  = 20\n",
            "whisper_model_load: n_audio_layer = 32\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 1280\n",
            "whisper_model_load: n_text_head   = 20\n",
            "whisper_model_load: n_text_layer  = 32\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 5 (large)\n",
            "whisper_model_load: adding 1608 extra tokens\n",
            "whisper_model_load: n_langs       = 99\n",
            "whisper_model_load:      CPU total size =  3093.99 MB\n",
            "whisper_model_load: model size    = 3093.99 MB\n",
            "whisper_init_state: kv self size  =  220.20 MB\n",
            "whisper_init_state: kv cross size =  245.76 MB\n",
            "whisper_init_state: compute buffer (conv)   =   34.82 MB\n",
            "whisper_init_state: compute buffer (encode) =  926.66 MB\n",
            "whisper_init_state: compute buffer (cross)  =    9.38 MB\n",
            "whisper_init_state: compute buffer (decode) =  209.26 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0 | \n",
            "\n",
            "main: processing 'sample.wav' (1673814 samples, 104.6 sec), 2 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}