{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train spacy IDT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyZmjveMocr"
      },
      "source": [
        "# \"Training spaCy 3 on IDT\"\n",
        "> \"Not going well so far\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- comments: true\n",
        "- categories: [spacy, idt]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9blVYqNcObhq"
      },
      "source": [
        "%%capture\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install thinc --pre\n",
        "!pip install -U spacy spacy-lookups-data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH27c_5MCuri",
        "outputId": "7b5604d5-1199-4601-ed42-4e7bb64a2090"
      },
      "source": [
        "!python -m spacy project clone pipelines/tagger_parser_ud"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'pipelines/tagger_parser_ud' from explosion/projects\u001b[0m\n",
            "/content/tagger_parser_ud\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/tagger_parser_ud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ryAtsK5DZeP",
        "outputId": "5e39f98d-b2bb-4150-970b-a37987a9f42f"
      },
      "source": [
        "%%writefile tagger_parser_ud/project.yml\n",
        "title: \"Part-of-speech Tagging & Dependency Parsing (Universal Dependencies)\"\n",
        "description: \"This project template lets you train a part-of-speech tagger, morphologizer and dependency parser from a [Universal Dependencies](https://universaldependencies.org/) corpus. It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `lang` and treebank settings in the variables below. Use `xx` for multi-language if no language-specific tokenizer is available in spaCy. Note that multi-word tokens will be merged together when the corpus is converted since spaCy does not support multi-word token expansion.\"\n",
        "\n",
        "# Variables can be referenced across the project.yml using ${vars.var_name}\n",
        "vars:\n",
        "  config: \"default\"\n",
        "  lang: \"ga\"\n",
        "  treebank: \"UD_Irish-IDT\"\n",
        "  train_name: \"ga_idt-ud-train\"\n",
        "  dev_name: \"ga_idt-ud-dev\"\n",
        "  test_name: \"ga_idt-ud-test\"\n",
        "  package_name: \"ud_ga_idt\"\n",
        "  package_version: \"0.0.0\"\n",
        "  gpu: -1\n",
        "\n",
        "# These are the directories that the project needs. The project CLI will make\n",
        "# sure that they always exist.\n",
        "directories: [\"assets\", \"corpus\", \"training\", \"metrics\", \"configs\", \"packages\"]\n",
        "\n",
        "assets:\n",
        "  - url: \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ga.300.vec.gz\"\n",
        "    dest: \"assets/vectors.vec.gz\"\n",
        "    description: \"FastText vectors\"\n",
        "  - dest: \"assets/${vars.treebank}\"\n",
        "    git:\n",
        "      repo: \"https://github.com/UniversalDependencies/${vars.treebank}\"\n",
        "      branch: \"master\"\n",
        "      path: \"\"\n",
        "\n",
        "workflows:\n",
        "  all:\n",
        "    - preprocess\n",
        "    - vectors\n",
        "    - train\n",
        "    - evaluate\n",
        "    - package\n",
        "\n",
        "commands:\n",
        "  - name: preprocess\n",
        "    help: \"Convert the data to spaCy's format\"\n",
        "    script:\n",
        "      - \"mkdir -p corpus/${vars.treebank}\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.train_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.dev_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.test_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.train_name}.spacy corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.dev_name}.spacy corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.test_name}.spacy corpus/${vars.treebank}/test.spacy\"\n",
        "    deps:\n",
        "      - \"assets/${vars.treebank}/${vars.train_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.dev_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.test_name}.conllu\"\n",
        "    outputs:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "\n",
        "  - name: vectors\n",
        "    help: \"Convert, truncate and prune the vectors.\"\n",
        "    script:\n",
        "      - \"python -m spacy init vectors ga assets/vectors.vec.gz corpus/ga_vectors -n ga_fasttext_cc_vectors_md\"\n",
        "    deps:\n",
        "      - \"assets/vectors.vec.gz\"\n",
        "    outputs:\n",
        "      - \"corpus/ga_vectors\"\n",
        "\n",
        "  - name: train\n",
        "    help: \"Train ${vars.treebank}\"\n",
        "    script:\n",
        "      - \"python -m spacy train configs/${vars.config}.cfg --output training/${vars.treebank} --gpu-id ${vars.gpu} --paths.train corpus/${vars.treebank}/train.spacy --paths.dev corpus/${vars.treebank}/dev.spacy --paths.vectors corpus/ga_vectors --nlp.lang=${vars.lang}\"\n",
        "    deps:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"configs/${vars.config}.cfg\"\n",
        "      - \"corpus/ga_vectors\"\n",
        "    outputs:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "\n",
        "  - name: evaluate\n",
        "    help: \"Evaluate on the test data and save the metrics\"\n",
        "    script:\n",
        "      - \"python -m spacy evaluate ./training/${vars.treebank}/model-best ./corpus/${vars.treebank}/test.spacy --output ./metrics/${vars.treebank}.json --gpu-id ${vars.gpu}\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "    outputs:\n",
        "      - \"metrics/${vars.treebank}.json\"\n",
        "\n",
        "  - name: package\n",
        "    help: \"Package the trained model so it can be installed\"\n",
        "    script:\n",
        "      - \"python -m spacy package training/${vars.treebank}/model-best packages --name ${vars.package_name} --version ${vars.package_version} --force\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "    outputs_no_cache:\n",
        "      - \"packages/${vars.lang}_${vars.package_name}-${vars.package_version}/dist/en_${vars.package_name}-${vars.package_version}.tar.gz\"\n",
        "\n",
        "  - name: clean\n",
        "    help: \"Remove intermediate files\"\n",
        "    script:\n",
        "      - \"rm -rf training/*\"\n",
        "      - \"rm -rf metrics/*\"\n",
        "      - \"rm -rf corpus/*\""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tagger_parser_ud/project.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP3p5fS3D1qF",
        "outputId": "7c818c83-24a1-46af-d113-102f14270774"
      },
      "source": [
        "!python -m spacy project assets /content/tagger_parser_ud"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Fetching 2 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/vectors.vec.gz\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/UD_Irish-IDT\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFJmpaRU6E8",
        "outputId": "1abf2585-d697-4c73-e423-878461468b06"
      },
      "source": [
        "!python -m spacy project run vectors /content/tagger_parser_ud"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== vectors ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy init vectors ga assets/vectors.vec.gz corpus/ga_vectors -n ga_fasttext_cc_vectors_md\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'ga'\u001b[0m\n",
            "[2021-11-30 10:39:18,055] [INFO] Reading vectors from assets/vectors.vec.gz\n",
            "316836it [00:28, 11251.98it/s]\n",
            "[2021-11-30 10:39:46,415] [INFO] Loaded vectors from assets/vectors.vec.gz\n",
            "\u001b[38;5;2m✔ Successfully converted 316836 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/tagger_parser_ud/corpus/ga_vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65yySCQfFVsx",
        "outputId": "7051435b-4edd-4f00-b28e-9b2cfa6b3e3e"
      },
      "source": [
        "%cd /content\n",
        "!python -m spacy project run preprocess tagger_parser_ud"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: mkdir -p corpus/UD_Irish-IDT\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-train.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (401 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-train.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-dev.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-test.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-test.spacy\u001b[0m\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-train.spacy corpus/UD_Irish-IDT/train.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy corpus/UD_Irish-IDT/dev.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-test.spacy corpus/UD_Irish-IDT/test.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi1Ny0PWWlWE",
        "outputId": "1a61acf2-1432-4124-90aa-a75b3584f388"
      },
      "source": [
        "%%writefile /content/tagger_parser_ud/configs/base_default.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = null\n",
        "init_tok2vec = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"ga\"\n",
        "pipeline = [\"tok2vec\",\"tagger\",\"morphologizer\",\"parser\"]\n",
        "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
        "disabled = []\n",
        "before_creation = null\n",
        "after_creation = null\n",
        "after_pipeline_creation = null\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.morphologizer]\n",
        "factory = \"morphologizer\"\n",
        "\n",
        "[components.morphologizer.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.morphologizer.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.parser]\n",
        "factory = \"parser\"\n",
        "learn_tokens = false\n",
        "min_action_freq = 30\n",
        "moves = null\n",
        "update_with_oracle_cut_size = 100\n",
        "\n",
        "[components.parser.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"parser\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 128\n",
        "maxout_pieces = 3\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.parser.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.tagger]\n",
        "factory = \"tagger\"\n",
        "\n",
        "[components.tagger.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.tagger.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v1\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"LOWER\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
        "rows = [5000,2500,2500,2500]\n",
        "include_static_vectors = false\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v1\"\n",
        "width = 96\n",
        "depth = 4\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 2000\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "dropout = 0.1\n",
        "accumulate_gradient = 1\n",
        "patience = 1600\n",
        "max_epochs = 0\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "get_length = null\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "t = 0.0\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = true\n",
        "L2 = 0.01\n",
        "grad_clip = 1.0\n",
        "use_averages = false\n",
        "eps = 0.00000001\n",
        "learn_rate = 0.001\n",
        "\n",
        "[training.score_weights]\n",
        "morph_per_feat = null\n",
        "dep_las_per_type = null\n",
        "sents_p = null\n",
        "sents_r = null\n",
        "tag_acc = 0.33\n",
        "pos_acc = 0.17\n",
        "morph_acc = 0.17\n",
        "dep_uas = 0.17\n",
        "dep_las = 0.17\n",
        "sents_f = 0.0\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}\n",
        "init_tok2vec = ${paths.init_tok2vec}\n",
        "vocab_data = null\n",
        "lookups = null\n",
        "before_init = null\n",
        "after_init = null\n",
        "\n",
        "[initialize.components]\n",
        "\n",
        "[initialize.tokenizer]\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tagger_parser_ud/configs/base_default.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTfokWSkZuOY",
        "outputId": "3a4f288f-443b-4b76-8d7a-801f881cf334"
      },
      "source": [
        "!rm /content/tagger_parser_ud/configs/default.cfg\n",
        "!python -m spacy init fill-config /content/tagger_parser_ud/configs/base_default.cfg /content/tagger_parser_ud/configs/default.cfg"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/tagger_parser_ud/configs/default.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train default.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q8R-yL3GiO5",
        "outputId": "45209ffd-58e6-4836-f080-9a40ef4a92d4"
      },
      "source": [
        "!python -m spacy project run train tagger_parser_ud"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/default.cfg --output training/UD_Irish-IDT --gpu-id -1 --paths.train corpus/UD_Irish-IDT/train.spacy --paths.dev corpus/UD_Irish-IDT/dev.spacy --paths.vectors corpus/ga_vectors --nlp.lang=ga\n",
            "\u001b[38;5;2m✔ Created output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-11-30 10:41:10,449] [INFO] Set up nlp object from config\n",
            "[2021-11-30 10:41:10,463] [INFO] Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "[2021-11-30 10:41:10,469] [INFO] Created vocabulary\n",
            "[2021-11-30 10:41:11,485] [INFO] Added vectors: corpus/ga_vectors\n",
            "[2021-11-30 10:41:13,027] [INFO] Finished initializing nlp object\n",
            "[2021-11-30 10:41:25,163] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS MORPH...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE \n",
            "---  ------  ------------  -----------  -------------  -----------  -------  -------  ---------  -------  -------  -------  ------\n",
            "  0       0          0.00       223.78         223.72       475.65    18.71    36.23      19.74    12.33     6.54     0.00    0.19\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "KeyError('')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1152, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 202, in spacy.pipeline.tagger.Tagger.update\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 263, in spacy.pipeline.tagger.Tagger.get_loss\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 193, in __call__\n",
            "    grads = self.get_grad(guesses, truths)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 206, in get_grad\n",
            "    d_yh = self.cc.get_grad(yh, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 120, in get_grad\n",
            "    target, mask = self.convert_truths(truths, guesses)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 93, in convert_truths\n",
            "    neg_index = self._name_to_i[truths[i]]\n",
            "KeyError: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wig6uL9M0oov",
        "outputId": "fd3dbf6e-1add-4ea2-fb4b-c395d7edf071"
      },
      "source": [
        "!apt install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (1,931 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMvOJDOY2CvP"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoEbpnA-61DJ"
      },
      "source": [
        "!transformers-cli login\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}