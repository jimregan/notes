{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train spacy IDT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyZmjveMocr"
      },
      "source": [
        "# \"Training spaCy 3 on IDT\"\n",
        "> \"Not going well so far\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- comments: true\n",
        "- categories: [spacy, idt]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9blVYqNcObhq"
      },
      "source": [
        "%%capture\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install thinc --pre\n",
        "!!pip install -U spacy spacy-lookups-data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH27c_5MCuri",
        "outputId": "bcf69f59-f3dc-4c74-e67a-54f33902b042"
      },
      "source": [
        "!python -m spacy project clone pipelines/tagger_parser_ud"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'pipelines/tagger_parser_ud' from explosion/projects\u001b[0m\n",
            "/content/tagger_parser_ud\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/tagger_parser_ud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ryAtsK5DZeP",
        "outputId": "096131aa-f7d0-4592-a09b-744ae639ad32"
      },
      "source": [
        "%%writefile tagger_parser_ud/project.yml\n",
        "title: \"Part-of-speech Tagging & Dependency Parsing (Universal Dependencies)\"\n",
        "description: \"This project template lets you train a part-of-speech tagger, morphologizer and dependency parser from a [Universal Dependencies](https://universaldependencies.org/) corpus. It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `lang` and treebank settings in the variables below. Use `xx` for multi-language if no language-specific tokenizer is available in spaCy. Note that multi-word tokens will be merged together when the corpus is converted since spaCy does not support multi-word token expansion.\"\n",
        "\n",
        "# Variables can be referenced across the project.yml using ${vars.var_name}\n",
        "vars:\n",
        "  config: \"default\"\n",
        "  lang: \"ga\"\n",
        "  treebank: \"UD_Irish-IDT\"\n",
        "  train_name: \"ga_idt-ud-train\"\n",
        "  dev_name: \"ga_idt-ud-dev\"\n",
        "  test_name: \"ga_idt-ud-test\"\n",
        "  package_name: \"ud_ga_idt\"\n",
        "  package_version: \"0.0.0\"\n",
        "  gpu: 0\n",
        "\n",
        "# These are the directories that the project needs. The project CLI will make\n",
        "# sure that they always exist.\n",
        "directories: [\"assets\", \"corpus\", \"training\", \"metrics\", \"configs\", \"packages\"]\n",
        "\n",
        "assets:\n",
        "  - dest: \"assets/${vars.treebank}\"\n",
        "    git:\n",
        "      repo: \"https://github.com/UniversalDependencies/${vars.treebank}\"\n",
        "      branch: \"dev\"\n",
        "      path: \"\"\n",
        "\n",
        "workflows:\n",
        "  all:\n",
        "    - preprocess\n",
        "    - train\n",
        "    - evaluate\n",
        "    - package\n",
        "\n",
        "commands:\n",
        "  - name: preprocess\n",
        "    help: \"Convert the data to spaCy's format\"\n",
        "    script:\n",
        "      - \"mkdir -p corpus/${vars.treebank}\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.train_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.dev_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.test_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.train_name}.spacy corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.dev_name}.spacy corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.test_name}.spacy corpus/${vars.treebank}/test.spacy\"\n",
        "    deps:\n",
        "      - \"assets/${vars.treebank}/${vars.train_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.dev_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.test_name}.conllu\"\n",
        "    outputs:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "\n",
        "  - name: train\n",
        "    help: \"Train ${vars.treebank}\"\n",
        "    script:\n",
        "      - \"python -m spacy train configs/${vars.config}.cfg --output training/${vars.treebank} --gpu-id ${vars.gpu} --paths.train corpus/${vars.treebank}/train.spacy --paths.dev corpus/${vars.treebank}/dev.spacy --nlp.lang=${vars.lang}\"\n",
        "    deps:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"configs/${vars.config}.cfg\"\n",
        "    outputs:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "\n",
        "  - name: evaluate\n",
        "    help: \"Evaluate on the test data and save the metrics\"\n",
        "    script:\n",
        "      - \"python -m spacy evaluate ./training/${vars.treebank}/model-best ./corpus/${vars.treebank}/test.spacy --output ./metrics/${vars.treebank}.json --gpu-id ${vars.gpu}\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "    outputs:\n",
        "      - \"metrics/${vars.treebank}.json\"\n",
        "\n",
        "  - name: package\n",
        "    help: \"Package the trained model so it can be installed\"\n",
        "    script:\n",
        "      - \"python -m spacy package training/${vars.treebank}/model-best packages --name ${vars.package_name} --version ${vars.package_version} --force\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "    outputs_no_cache:\n",
        "      - \"packages/${vars.lang}_${vars.package_name}-${vars.package_version}/dist/en_${vars.package_name}-${vars.package_version}.tar.gz\"\n",
        "\n",
        "  - name: clean\n",
        "    help: \"Remove intermediate files\"\n",
        "    script:\n",
        "      - \"rm -rf training/*\"\n",
        "      - \"rm -rf metrics/*\"\n",
        "      - \"rm -rf corpus/*\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tagger_parser_ud/project.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP3p5fS3D1qF",
        "outputId": "ab7cac77-609e-47db-afc9-8d2b9d788e83"
      },
      "source": [
        "!python -m spacy project assets /content/tagger_parser_ud"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Fetching 1 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/UD_Irish-IDT\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65yySCQfFVsx",
        "outputId": "ea840706-76d8-4e0f-cd2a-c8f95f2ecb0d"
      },
      "source": [
        "%cd /content\n",
        "!python -m spacy project run preprocess tagger_parser_ud"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: mkdir -p corpus/UD_Irish-IDT\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-train.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (401 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-train.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-dev.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-test.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-test.spacy\u001b[0m\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-train.spacy corpus/UD_Irish-IDT/train.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy corpus/UD_Irish-IDT/dev.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-test.spacy corpus/UD_Irish-IDT/test.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q8R-yL3GiO5",
        "outputId": "cfe74d07-56a8-494c-832f-dd39b06b87e8"
      },
      "source": [
        "!python -m spacy project run train tagger_parser_ud"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/default.cfg --output training/UD_Irish-IDT --gpu-id 0 --paths.train corpus/UD_Irish-IDT/train.spacy --paths.dev corpus/UD_Irish-IDT/dev.spacy --nlp.lang=ga\n",
            "\u001b[38;5;2m✔ Created output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-11-17 23:48:13,392] [INFO] Set up nlp object from config\n",
            "[2021-11-17 23:48:13,407] [INFO] Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "[2021-11-17 23:48:13,414] [INFO] Created vocabulary\n",
            "[2021-11-17 23:48:13,416] [INFO] Finished initializing nlp object\n",
            "[2021-11-17 23:49:40,722] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS MORPH...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE \n",
            "---  ------  ------------  -----------  -------------  -----------  -------  -------  ---------  -------  -------  -------  ------\n",
            "  0       0          0.00       223.78         223.72       475.65    19.11    34.99      19.14    12.38     6.44     0.03    0.19\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "KeyError('')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1152, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 202, in spacy.pipeline.tagger.Tagger.update\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 263, in spacy.pipeline.tagger.Tagger.get_loss\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 193, in __call__\n",
            "    grads = self.get_grad(guesses, truths)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 206, in get_grad\n",
            "    d_yh = self.cc.get_grad(yh, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 120, in get_grad\n",
            "    target, mask = self.convert_truths(truths, guesses)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 93, in convert_truths\n",
            "    neg_index = self._name_to_i[truths[i]]\n",
            "KeyError: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wig6uL9M0oov",
        "outputId": "fd3dbf6e-1add-4ea2-fb4b-c395d7edf071"
      },
      "source": [
        "!apt install git-lfs"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (1,931 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMvOJDOY2CvP"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoEbpnA-61DJ"
      },
      "source": [
        "!transformers-cli login\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}