{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train spacy IDT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyZmjveMocr"
      },
      "source": [
        "# \"Training spaCy 3 on IDT\"\n",
        "> \"Not going well so far\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- comments: true\n",
        "- categories: [spacy, idt]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9blVYqNcObhq"
      },
      "source": [
        "%%capture\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install thinc --pre\n",
        "#!pip install -U spacy spacy-lookups-data\n",
        "!pip install -U git+https://github.com/jimregan/spaCy@patch-2\n",
        "!pip install -U spacy-lookups-data\n",
        "!pip install -U datasets floret"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVXJ4JIiU7pD",
        "outputId": "51328582-b474-4a47-a76a-4339129d5903"
      },
      "source": [
        "!pip install -U spacy spacy-lookups-data"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: spacy-lookups-data in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.4)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (59.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ47IeMfSMSo",
        "outputId": "0e9e2f90-ddaa-4e58-ef7f-eff10f591103"
      },
      "source": [
        "!pip install -U Cython numpy\n",
        "!pip install --verbose -U git+https://github.com/jimregan/spaCy@patch-2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Using pip 21.3.1 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting git+https://github.com/jimregan/spaCy@patch-2\n",
            "  Cloning https://github.com/jimregan/spaCy (to revision patch-2) to /tmp/pip-req-build-1hyhvx4v\n",
            "  Running command git version\n",
            "  git version 2.17.1\n",
            "  Running command git clone --filter=blob:none -q https://github.com/jimregan/spaCy /tmp/pip-req-build-1hyhvx4v\n",
            "  Running command git show-ref patch-2\n",
            "  c1f6144055134f520f0dd1799ac931558169cd92 refs/remotes/origin/patch-2\n",
            "  Running command git symbolic-ref -q HEAD\n",
            "  refs/heads/master\n",
            "  Running command git checkout -b patch-2 --track origin/patch-2\n",
            "  Switched to a new branch 'patch-2'\n",
            "  Branch 'patch-2' set up to track remote branch 'patch-2' from 'origin'.\n",
            "  Resolved https://github.com/jimregan/spaCy to commit c1f6144055134f520f0dd1799ac931558169cd92\n",
            "  Running command /usr/bin/python3 /tmp/pip-standalone-pip-74xqcfgi/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-ug6ddfbt/overlay --no-warn-script-location -v --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=8.0.12,<8.1.0' 'blis>=0.4.0,<0.8.0' pathy 'numpy>=1.15.0'\n",
            "  Using pip 21.3.1 from /tmp/pip-standalone-pip-74xqcfgi/__env_pip__.zip/pip (python 3.7)\n",
            "  WARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\n",
            "  Collecting setuptools\n",
            "    Using cached setuptools-59.4.0-py3-none-any.whl (952 kB)\n",
            "  Collecting cython<3.0,>=0.25\n",
            "    Using cached Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
            "  Collecting cymem<2.1.0,>=2.0.2\n",
            "    Using cached cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "  Collecting preshed<3.1.0,>=3.0.2\n",
            "    Using cached preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "  Collecting murmurhash<1.1.0,>=0.28.0\n",
            "    Using cached murmurhash-1.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "  Collecting thinc<8.1.0,>=8.0.12\n",
            "    Using cached thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "  Collecting blis<0.8.0,>=0.4.0\n",
            "    Using cached blis-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
            "  Collecting pathy\n",
            "    Using cached pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "    Link requires a different Python (3.7.12 not in: '>=3.8'): https://files.pythonhosted.org/packages/7a/7f/5384d4c85a2349bd89ff1c0253bff77c7b617e48af201f2d823fc619189a/numpy-1.22.0rc1.zip#sha256=bc991b3f8ea7c0f6703df2bc23c098cfe6f1a3a5e8a3a901eb6a5619275d53ff (from https://pypi.org/simple/numpy/) (requires-python:>=3.8)\n",
            "  Collecting numpy>=1.15.0\n",
            "    Using cached numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "  Collecting catalogue<2.1.0,>=2.0.4\n",
            "    Using cached catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "  Collecting srsly<3.0.0,>=2.4.0\n",
            "    Using cached srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "  Collecting typing-extensions<4.0.0.0,>=3.7.4.1\n",
            "    Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "  Collecting wasabi<1.1.0,>=0.8.1\n",
            "    Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
            "  Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "    Using cached pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "  Collecting typer<1.0.0,>=0.3.0\n",
            "    Using cached typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "  Collecting smart-open<6.0.0,>=5.0.0\n",
            "    Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "  Collecting zipp>=0.5\n",
            "    Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "  Collecting click<9.0.0,>=7.1.1\n",
            "    Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
            "  Collecting importlib-metadata\n",
            "    Using cached importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
            "  WARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\n",
            "  Installing collected packages: zipp, typing-extensions, importlib-metadata, numpy, murmurhash, cymem, click, catalogue, wasabi, typer, srsly, smart-open, setuptools, pydantic, preshed, blis, thinc, pathy, cython\n",
            "    Creating /tmp/pip-build-env-ug6ddfbt/overlay/bin\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/f2py to 755\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/f2py3 to 755\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/f2py3.7 to 755\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/pathy to 755\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/cygdb to 755\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/cython to 755\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/overlay/bin/cythonize to 755\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "  flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.3 which is incompatible.\n",
            "  datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "  albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\n",
            "  Successfully installed blis-0.7.5 catalogue-2.0.6 click-8.0.3 cymem-2.0.6 cython-0.29.24 importlib-metadata-4.8.2 murmurhash-1.0.6 numpy-1.21.4 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 setuptools-59.4.0 smart-open-5.2.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 typing-extensions-3.10.0.2 wasabi-0.8.2 zipp-3.6.0\n",
            "  WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp6k6lsy9u\n",
            "  Copied setup.cfg -> spacy/tests/package\n",
            "  Copied pyproject.toml -> spacy/tests/package\n",
            "  Copied requirements.txt -> spacy/tests/package\n",
            "  Copied website/meta/universe.json -> spacy/tests/universe\n",
            "  Cythonizing sources\n",
            "  Compiling spacy/training/example.pyx because it changed.\n",
            "  Compiling spacy/parts_of_speech.pyx because it changed.\n",
            "  Compiling spacy/strings.pyx because it changed.\n",
            "  Compiling spacy/lexeme.pyx because it changed.\n",
            "  Compiling spacy/vocab.pyx because it changed.\n",
            "  Compiling spacy/attrs.pyx because it changed.\n",
            "  Compiling spacy/kb.pyx because it changed.\n",
            "  Compiling spacy/ml/parser_model.pyx because it changed.\n",
            "  Compiling spacy/morphology.pyx because it changed.\n",
            "  Compiling spacy/pipeline/dep_parser.pyx because it changed.\n",
            "  Compiling spacy/pipeline/morphologizer.pyx because it changed.\n",
            "  Compiling spacy/pipeline/multitask.pyx because it changed.\n",
            "  Compiling spacy/pipeline/ner.pyx because it changed.\n",
            "  Compiling spacy/pipeline/pipe.pyx because it changed.\n",
            "  Compiling spacy/pipeline/trainable_pipe.pyx because it changed.\n",
            "  Compiling spacy/pipeline/sentencizer.pyx because it changed.\n",
            "  Compiling spacy/pipeline/senter.pyx because it changed.\n",
            "  Compiling spacy/pipeline/tagger.pyx because it changed.\n",
            "  Compiling spacy/pipeline/transition_parser.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/arc_eager.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/ner.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/nonproj.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/_state.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/stateclass.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/transition_system.pyx because it changed.\n",
            "  Compiling spacy/pipeline/_parser_internals/_beam_utils.pyx because it changed.\n",
            "  Compiling spacy/tokenizer.pyx because it changed.\n",
            "  Compiling spacy/training/align.pyx because it changed.\n",
            "  Compiling spacy/training/gold_io.pyx because it changed.\n",
            "  Compiling spacy/tokens/doc.pyx because it changed.\n",
            "  Compiling spacy/tokens/span.pyx because it changed.\n",
            "  Compiling spacy/tokens/token.pyx because it changed.\n",
            "  Compiling spacy/tokens/span_group.pyx because it changed.\n",
            "  Compiling spacy/tokens/graph.pyx because it changed.\n",
            "  Compiling spacy/tokens/morphanalysis.pyx because it changed.\n",
            "  Compiling spacy/tokens/_retokenize.pyx because it changed.\n",
            "  Compiling spacy/matcher/matcher.pyx because it changed.\n",
            "  Compiling spacy/matcher/phrasematcher.pyx because it changed.\n",
            "  Compiling spacy/matcher/dependencymatcher.pyx because it changed.\n",
            "  Compiling spacy/symbols.pyx because it changed.\n",
            "  Compiling spacy/vectors.pyx because it changed.\n",
            "  [ 1/41] Cythonizing spacy/attrs.pyx\n",
            "  [ 2/41] Cythonizing spacy/kb.pyx\n",
            "  [ 3/41] Cythonizing spacy/lexeme.pyx\n",
            "  [ 4/41] Cythonizing spacy/matcher/dependencymatcher.pyx\n",
            "  [ 5/41] Cythonizing spacy/matcher/matcher.pyx\n",
            "  [ 6/41] Cythonizing spacy/matcher/phrasematcher.pyx\n",
            "  [ 7/41] Cythonizing spacy/ml/parser_model.pyx\n",
            "  [ 8/41] Cythonizing spacy/morphology.pyx\n",
            "  [ 9/41] Cythonizing spacy/parts_of_speech.pyx\n",
            "  [10/41] Cythonizing spacy/pipeline/_parser_internals/_beam_utils.pyx\n",
            "  [11/41] Cythonizing spacy/pipeline/_parser_internals/_state.pyx\n",
            "  [12/41] Cythonizing spacy/pipeline/_parser_internals/arc_eager.pyx\n",
            "  [13/41] Cythonizing spacy/pipeline/_parser_internals/ner.pyx\n",
            "  [14/41] Cythonizing spacy/pipeline/_parser_internals/nonproj.pyx\n",
            "  [15/41] Cythonizing spacy/pipeline/_parser_internals/stateclass.pyx\n",
            "  [16/41] Cythonizing spacy/pipeline/_parser_internals/transition_system.pyx\n",
            "  [17/41] Cythonizing spacy/pipeline/dep_parser.pyx\n",
            "  [18/41] Cythonizing spacy/pipeline/morphologizer.pyx\n",
            "  [19/41] Cythonizing spacy/pipeline/multitask.pyx\n",
            "  [20/41] Cythonizing spacy/pipeline/ner.pyx\n",
            "  [21/41] Cythonizing spacy/pipeline/pipe.pyx\n",
            "  [22/41] Cythonizing spacy/pipeline/sentencizer.pyx\n",
            "  [23/41] Cythonizing spacy/pipeline/senter.pyx\n",
            "  [24/41] Cythonizing spacy/pipeline/tagger.pyx\n",
            "  [25/41] Cythonizing spacy/pipeline/trainable_pipe.pyx\n",
            "  [26/41] Cythonizing spacy/pipeline/transition_parser.pyx\n",
            "  [27/41] Cythonizing spacy/strings.pyx\n",
            "  [28/41] Cythonizing spacy/symbols.pyx\n",
            "  [29/41] Cythonizing spacy/tokenizer.pyx\n",
            "  [30/41] Cythonizing spacy/tokens/_retokenize.pyx\n",
            "  [31/41] Cythonizing spacy/tokens/doc.pyx\n",
            "  [32/41] Cythonizing spacy/tokens/graph.pyx\n",
            "  [33/41] Cythonizing spacy/tokens/morphanalysis.pyx\n",
            "  [34/41] Cythonizing spacy/tokens/span.pyx\n",
            "  [35/41] Cythonizing spacy/tokens/span_group.pyx\n",
            "  [36/41] Cythonizing spacy/tokens/token.pyx\n",
            "  [37/41] Cythonizing spacy/training/align.pyx\n",
            "  [38/41] Cythonizing spacy/training/example.pyx\n",
            "  [39/41] Cythonizing spacy/training/gold_io.pyx\n",
            "  [40/41] Cythonizing spacy/vectors.pyx\n",
            "  [41/41] Cythonizing spacy/vocab.pyx\n",
            "  warning: spacy/pipeline/_parser_internals/ner.pyx:183:29: Not all members given for struct 'Transition'\n",
            "  warning: spacy/pipeline/_parser_internals/ner.pyx:183:29: Not all members given for struct 'Transition'\n",
            "  warning: spacy/tokens/span.pyx:112:22: Not all members given for struct 'SpanC'\n",
            "  warning: spacy/tokens/span.pyx:112:22: Not all members given for struct 'SpanC'\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command /usr/bin/python3 /tmp/pip-standalone-pip-lwc0tyg_/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-ug6ddfbt/normal --no-warn-script-location -v --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- wheel\n",
            "  Using pip 21.3.1 from /tmp/pip-standalone-pip-lwc0tyg_/__env_pip__.zip/pip (python 3.7)\n",
            "  WARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\n",
            "  Collecting wheel\n",
            "    Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
            "  WARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\n",
            "  Installing collected packages: wheel\n",
            "    Creating /tmp/pip-build-env-ug6ddfbt/normal/bin\n",
            "    changing mode of /tmp/pip-build-env-ug6ddfbt/normal/bin/wheel to 755\n",
            "  Successfully installed wheel-0.37.0\n",
            "  WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/pip/_vendor/pep517/in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpe6beo_3q\n",
            "  Copied setup.cfg -> spacy/tests/package\n",
            "  Copied pyproject.toml -> spacy/tests/package\n",
            "  Copied requirements.txt -> spacy/tests/package\n",
            "  Copied website/meta/universe.json -> spacy/tests/universe\n",
            "  Cythonizing sources\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info\n",
            "  writing /tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no files found matching '*.h' under directory 'include'\n",
            "  warning: no previously-included files matching '*.json' found under directory 'spacy/lang'\n",
            "  warning: no files found matching '*.json.gz' under directory 'spacy/lang'\n",
            "  warning: no files found matching '*.json' under directory 'spacy/cli'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-ndovp9mp/spacy.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-ndovp9mp/spacy.dist-info'\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.21.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (59.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH27c_5MCuri",
        "outputId": "384c4b20-f527-4325-b429-0e27f42bc56f"
      },
      "source": [
        "!python -m spacy project clone pipelines/tagger_parser_ud"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'pipelines/tagger_parser_ud' from explosion/projects\u001b[0m\n",
            "/content/tagger_parser_ud\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/tagger_parser_ud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ryAtsK5DZeP",
        "outputId": "ad86bf26-59e1-474f-e7c0-4cbe8d955cbe"
      },
      "source": [
        "%%writefile tagger_parser_ud/project.yml\n",
        "title: \"Part-of-speech Tagging & Dependency Parsing (Universal Dependencies)\"\n",
        "description: \"This project template lets you train a part-of-speech tagger, morphologizer and dependency parser from a [Universal Dependencies](https://universaldependencies.org/) corpus. It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `lang` and treebank settings in the variables below. Use `xx` for multi-language if no language-specific tokenizer is available in spaCy. Note that multi-word tokens will be merged together when the corpus is converted since spaCy does not support multi-word token expansion.\"\n",
        "\n",
        "# Variables can be referenced across the project.yml using ${vars.var_name}\n",
        "vars:\n",
        "  config: \"default\"\n",
        "  lang: \"ga\"\n",
        "  treebank: \"UD_Irish-IDT\"\n",
        "  train_name: \"ga_idt-ud-train\"\n",
        "  dev_name: \"ga_idt-ud-dev\"\n",
        "  test_name: \"ga_idt-ud-test\"\n",
        "  package_name: \"ud_ga_idt\"\n",
        "  package_version: \"0.0.0\"\n",
        "  gpu: -1\n",
        "\n",
        "# These are the directories that the project needs. The project CLI will make\n",
        "# sure that they always exist.\n",
        "directories: [\"assets\", \"corpus\", \"training\", \"metrics\", \"configs\", \"packages\"]\n",
        "\n",
        "assets:\n",
        "  - url: \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ga.300.vec.gz\"\n",
        "    dest: \"assets/vectors.vec.gz\"\n",
        "    description: \"FastText vectors\"\n",
        "  - dest: \"assets/${vars.treebank}\"\n",
        "    git:\n",
        "      repo: \"https://github.com/UniversalDependencies/${vars.treebank}\"\n",
        "      branch: \"master\"\n",
        "      path: \"\"\n",
        "\n",
        "workflows:\n",
        "  all:\n",
        "    - preprocess\n",
        "    - vectors\n",
        "    - train\n",
        "    - evaluate\n",
        "    - package\n",
        "\n",
        "commands:\n",
        "  - name: preprocess\n",
        "    help: \"Convert the data to spaCy's format\"\n",
        "    script:\n",
        "      - \"mkdir -p corpus/${vars.treebank}\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.train_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.dev_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.test_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.train_name}.spacy corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.dev_name}.spacy corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.test_name}.spacy corpus/${vars.treebank}/test.spacy\"\n",
        "    deps:\n",
        "      - \"assets/${vars.treebank}/${vars.train_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.dev_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.test_name}.conllu\"\n",
        "    outputs:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "\n",
        "  - name: vectors\n",
        "    help: \"Convert, truncate and prune the vectors.\"\n",
        "    script:\n",
        "      - \"python -m spacy init vectors ga assets/vectors.vec.gz corpus/ga_vectors -n ga_fasttext_cc_vectors_md\"\n",
        "    deps:\n",
        "      - \"assets/vectors.vec.gz\"\n",
        "    outputs:\n",
        "      - \"corpus/ga_vectors\"\n",
        "\n",
        "  - name: train\n",
        "    help: \"Train ${vars.treebank}\"\n",
        "    script:\n",
        "      - \"python -m spacy train configs/${vars.config}.cfg --output training/${vars.treebank} --gpu-id ${vars.gpu} --paths.train corpus/${vars.treebank}/train.spacy --paths.dev corpus/${vars.treebank}/dev.spacy --paths.vectors corpus/ga_vectors --nlp.lang=${vars.lang}\"\n",
        "    deps:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"configs/${vars.config}.cfg\"\n",
        "      - \"corpus/ga_vectors\"\n",
        "    outputs:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "\n",
        "  - name: evaluate\n",
        "    help: \"Evaluate on the test data and save the metrics\"\n",
        "    script:\n",
        "      - \"python -m spacy evaluate ./training/${vars.treebank}/model-best ./corpus/${vars.treebank}/test.spacy --output ./metrics/${vars.treebank}.json --gpu-id ${vars.gpu}\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "    outputs:\n",
        "      - \"metrics/${vars.treebank}.json\"\n",
        "\n",
        "  - name: package\n",
        "    help: \"Package the trained model so it can be installed\"\n",
        "    script:\n",
        "      - \"python -m spacy package training/${vars.treebank}/model-best packages --name ${vars.package_name} --version ${vars.package_version} --force\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "    outputs_no_cache:\n",
        "      - \"packages/${vars.lang}_${vars.package_name}-${vars.package_version}/dist/en_${vars.package_name}-${vars.package_version}.tar.gz\"\n",
        "\n",
        "  - name: clean\n",
        "    help: \"Remove intermediate files\"\n",
        "    script:\n",
        "      - \"rm -rf training/*\"\n",
        "      - \"rm -rf metrics/*\"\n",
        "      - \"rm -rf corpus/*\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tagger_parser_ud/project.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP3p5fS3D1qF",
        "outputId": "be7dea09-7962-4250-b762-9629ba873124"
      },
      "source": [
        "!python -m spacy project assets /content/tagger_parser_ud"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Fetching 2 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/vectors.vec.gz\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/UD_Irish-IDT\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFJmpaRU6E8",
        "outputId": "24f626db-258b-447e-e1aa-53109cbe87d7"
      },
      "source": [
        "!python -m spacy project run vectors /content/tagger_parser_ud"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== vectors ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy init vectors ga assets/vectors.vec.gz corpus/ga_vectors -n ga_fasttext_cc_vectors_md\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'ga'\u001b[0m\n",
            "[2021-12-05 11:26:30,514] [INFO] Reading vectors from assets/vectors.vec.gz\n",
            "316836it [00:25, 12501.00it/s]\n",
            "[2021-12-05 11:26:56,041] [INFO] Loaded vectors from assets/vectors.vec.gz\n",
            "\u001b[38;5;2m✔ Successfully converted 316836 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/tagger_parser_ud/corpus/ga_vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65yySCQfFVsx",
        "outputId": "0e1ac965-0ef5-4cb2-a47a-2c6511135e73"
      },
      "source": [
        "!python -m spacy project run preprocess /content/tagger_parser_ud"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: mkdir -p corpus/UD_Irish-IDT\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-train.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (401 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-train.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-dev.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-test.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-test.spacy\u001b[0m\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-train.spacy corpus/UD_Irish-IDT/train.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy corpus/UD_Irish-IDT/dev.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-test.spacy corpus/UD_Irish-IDT/test.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi1Ny0PWWlWE",
        "outputId": "6c90727a-7c0d-41b6-fe1d-c7c966215ff8"
      },
      "source": [
        "%%writefile /content/tagger_parser_ud/configs/base_default.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = null\n",
        "init_tok2vec = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"ga\"\n",
        "pipeline = [\"tok2vec\",\"tagger\",\"morphologizer\",\"parser\"]\n",
        "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
        "disabled = []\n",
        "before_creation = null\n",
        "after_creation = null\n",
        "after_pipeline_creation = null\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.morphologizer]\n",
        "factory = \"morphologizer\"\n",
        "\n",
        "[components.morphologizer.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.morphologizer.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.parser]\n",
        "factory = \"parser\"\n",
        "learn_tokens = false\n",
        "min_action_freq = 30\n",
        "moves = null\n",
        "update_with_oracle_cut_size = 100\n",
        "\n",
        "[components.parser.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"parser\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 128\n",
        "maxout_pieces = 3\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.parser.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.tagger]\n",
        "factory = \"tagger\"\n",
        "\n",
        "[components.tagger.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.tagger.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v1\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"LOWER\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
        "rows = [5000,2500,2500,2500]\n",
        "include_static_vectors = false\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v1\"\n",
        "width = 96\n",
        "depth = 4\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 2000\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "dropout = 0.1\n",
        "accumulate_gradient = 1\n",
        "patience = 1600\n",
        "max_epochs = 0\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "get_length = null\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "t = 0.0\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = true\n",
        "L2 = 0.01\n",
        "grad_clip = 1.0\n",
        "use_averages = false\n",
        "eps = 0.00000001\n",
        "learn_rate = 0.001\n",
        "\n",
        "[training.score_weights]\n",
        "morph_per_feat = null\n",
        "dep_las_per_type = null\n",
        "sents_p = null\n",
        "sents_r = null\n",
        "tag_acc = 0.33\n",
        "pos_acc = 0.17\n",
        "morph_acc = 0.17\n",
        "dep_uas = 0.17\n",
        "dep_las = 0.17\n",
        "sents_f = 0.0\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}\n",
        "init_tok2vec = ${paths.init_tok2vec}\n",
        "vocab_data = null\n",
        "lookups = null\n",
        "before_init = null\n",
        "after_init = null\n",
        "\n",
        "[initialize.components]\n",
        "\n",
        "[initialize.tokenizer]\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tagger_parser_ud/configs/base_default.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTfokWSkZuOY",
        "outputId": "a417bf33-fed8-4740-9630-a567b0ee98f7"
      },
      "source": [
        "!rm /content/tagger_parser_ud/configs/default.cfg\n",
        "!python -m spacy init fill-config /content/tagger_parser_ud/configs/base_default.cfg /content/tagger_parser_ud/configs/default.cfg"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/tagger_parser_ud/configs/default.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train default.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q8R-yL3GiO5",
        "outputId": "3b399e29-8add-4591-bda1-666df886305f"
      },
      "source": [
        "!python -m spacy project run train tagger_parser_ud"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/default.cfg --output training/UD_Irish-IDT --gpu-id -1 --paths.train corpus/UD_Irish-IDT/train.spacy --paths.dev corpus/UD_Irish-IDT/dev.spacy --paths.vectors corpus/ga_vectors --nlp.lang=ga\n",
            "\u001b[38;5;2m✔ Created output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-12-05 11:29:28,556] [INFO] Set up nlp object from config\n",
            "[2021-12-05 11:29:28,571] [INFO] Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "[2021-12-05 11:29:28,577] [INFO] Created vocabulary\n",
            "[2021-12-05 11:29:29,510] [INFO] Added vectors: corpus/ga_vectors\n",
            "[2021-12-05 11:29:30,801] [INFO] Finished initializing nlp object\n",
            "[2021-12-05 11:29:43,414] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS MORPH...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE \n",
            "---  ------  ------------  -----------  -------------  -----------  -------  -------  ---------  -------  -------  -------  ------\n",
            "  0       0          0.00       223.78         223.72       475.65    18.71    36.23      19.74    12.33     6.54     0.00    0.19\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "KeyError('')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1152, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 202, in spacy.pipeline.tagger.Tagger.update\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 263, in spacy.pipeline.tagger.Tagger.get_loss\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 193, in __call__\n",
            "    grads = self.get_grad(guesses, truths)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 206, in get_grad\n",
            "    d_yh = self.cc.get_grad(yh, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 120, in get_grad\n",
            "    target, mask = self.convert_truths(truths, guesses)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 93, in convert_truths\n",
            "    neg_index = self._name_to_i[truths[i]]\n",
            "KeyError: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wig6uL9M0oov",
        "outputId": "4d7eb42f-e0c2-4178-a128-5030c556601c"
      },
      "source": [
        "!apt install git-lfs"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (1,760 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gulro7mTAVwC",
        "outputId": "9246b24a-ea54-496c-f647-e8d461c27762"
      },
      "source": [
        "%%writefile tmp_config.cfg\n",
        "# This is an auto-generated partial config. To use it with 'spacy train'\n",
        "# you can run spacy init fill-config to auto-fill all default settings:\n",
        "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "\n",
        "[nlp]\n",
        "lang = \"ga\"\n",
        "pipeline = [\"tok2vec\",\"tagger\",\"morphologizer\",\"parser\"]\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v2\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"ORTH\", \"SHAPE\"]\n",
        "rows = [5000, 2500]\n",
        "include_static_vectors = true\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
        "width = 256\n",
        "depth = 8\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[components.morphologizer]\n",
        "factory = \"morphologizer\"\n",
        "\n",
        "[components.morphologizer.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.morphologizer.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[components.tagger]\n",
        "factory = \"tagger\"\n",
        "\n",
        "[components.tagger.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.tagger.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[components.parser]\n",
        "factory = \"parser\"\n",
        "\n",
        "[components.parser.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"parser\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 128\n",
        "maxout_pieces = 3\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.parser.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tmp_config.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkY425VzAntO",
        "outputId": "6167f666-a761-4254-aa43-8a395a55efd9"
      },
      "source": [
        "!python -m spacy init fill-config tmp_config.cfg config.cfg"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3s4moISCiOH"
      },
      "source": [
        "#!rm -rf spacy_ud\n",
        "!mkdir spacy_ud"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irdz4cd_Br-V",
        "outputId": "3a70bbeb-e75f-4b8b-a89c-c0c12324d97a"
      },
      "source": [
        "!python -m spacy train config.cfg --paths.train /content/tagger_parser_ud/corpus/UD_Irish-IDT/train.spacy --paths.dev /content/tagger_parser_ud/corpus/UD_Irish-IDT/dev.spacy --paths.vectors /content/floret_vectors_demo/vectors/unshuffled_deduplicated_ga.1000_floret_model --output spacy_ud"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: spacy_ud\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-12-05 11:33:15,189] [INFO] Set up nlp object from config\n",
            "[2021-12-05 11:33:15,204] [INFO] Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "[2021-12-05 11:33:15,210] [INFO] Created vocabulary\n",
            "[2021-12-05 11:33:15,307] [INFO] Added vectors: /content/floret_vectors_demo/vectors/unshuffled_deduplicated_ga.1000_floret_model\n",
            "[2021-12-05 11:33:15,376] [INFO] Finished initializing nlp object\n",
            "[2021-12-05 11:33:30,875] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS MORPH...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE \n",
            "---  ------  ------------  -----------  -------------  -----------  -------  -------  ---------  -------  -------  -------  ------\n",
            "  0       0          0.00       223.78         223.72       475.65    21.82    41.18      25.53     7.83     5.01     0.02    0.21\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "KeyError('')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1152, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 202, in spacy.pipeline.tagger.Tagger.update\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 263, in spacy.pipeline.tagger.Tagger.get_loss\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 193, in __call__\n",
            "    grads = self.get_grad(guesses, truths)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 206, in get_grad\n",
            "    d_yh = self.cc.get_grad(yh, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 120, in get_grad\n",
            "    target, mask = self.convert_truths(truths, guesses)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 93, in convert_truths\n",
            "    neg_index = self._name_to_i[truths[i]]\n",
            "KeyError: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DncGLkhRD7KH",
        "outputId": "028ab0f5-4464-40ac-86a9-498826206bb7"
      },
      "source": [
        "!python -m spacy debug data /content/spacy_ud/model-best/config.cfg"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: ga\n",
            "Training pipeline: tok2vec, tagger, morphologizer, parser\n",
            "401 training docs\n",
            "46 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (401)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 95881 total word(s) in the data (14191 unique)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 600 misaligned tokens in the training data\u001b[0m\n",
            "\u001b[38;5;3m⚠ 80 misaligned tokens in the dev data\u001b[0m\n",
            "\u001b[38;5;4mℹ 20000 vectors (0 unique keys, 300 dimensions)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 0 words in training data without vectors (0%)\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 1029 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Some model labels are not present in the train data. The model\n",
            "performance may be degraded for these labels after training:\n",
            "'VT__Aspect=Imp|Form=Len|Number=Plur|Person=3|Tense=Past',\n",
            "'Cop__Form=Ecl,VF|Tense=Past|VerbForm=Cop',\n",
            "'Cop__Form=VF|Mood=Cnd|VerbForm=Cop',\n",
            "'VTI__Form=Len|Mood=Ind|Tense=Past|Typo=Yes',\n",
            "'VI__Form=Len|Mood=Cnd|Number=Plur|Person=1',\n",
            "'VERB__Form=Len|Mood=Cnd|Person=0',\n",
            "'VT__Aspect=Imp|Number=Sing|Person=1|Tense=Past',\n",
            "'VT__Aspect=Imp|Form=Len|Tense=Past',\n",
            "'VTI__Form=Len|Mood=Imp|Number=Sing|Person=2',\n",
            "'VTI__Form=Len|Mood=Cnd|Number=Plur|Person=1',\n",
            "'VTI__Form=Len|Mood=Cnd|Typo=Yes',\n",
            "'ADJ__Abbr=Yes|Case=Nom|Gender=Fem|Number=Sing',\n",
            "'Poss__Number=Sing|Person=2|Poss=Yes'.\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Morphologizer (POS+Morph) =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 795 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Some model labels are not present in the train data. The model\n",
            "performance may be degraded for these labels after training:\n",
            "'Foreign=Yes|POS=PRON', 'Form=Len|Mood=Imp|Number=Sing|POS=VERB|Person=2',\n",
            "'Form=Len|Mood=Cnd|POS=VERB|Typo=Yes',\n",
            "'Form=Ecl,VF|POS=AUX|Tense=Past|VerbForm=Cop',\n",
            "'Aspect=Imp|Number=Sing|POS=VERB|Person=1|Tense=Past',\n",
            "'Form=VF|Mood=Cnd|POS=AUX|VerbForm=Cop'.\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Dependency Parsing =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 3851 sentence(s) with an average length of 24.9 words.\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 301 nonprojective train sentence(s)\u001b[0m\n",
            "\u001b[38;5;4mℹ 42 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;4mℹ 230 label(s) in projectivized train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'orphan' (8)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'goeswith' (3)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for 181 label(s) in the projectivized\n",
            "dependency trees used for training. You may want to projectivize labels such as\n",
            "punct before training in order to improve parser performance.\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 3 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 12 warnings\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43N2OX4sJkDW",
        "outputId": "7f1f7713-84ad-432e-ad5e-de07071a4fcd"
      },
      "source": [
        "!python -m spacy project clone pipelines/floret_vectors_demo"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'pipelines/floret_vectors_demo' from explosion/projects\u001b[0m\n",
            "/content/floret_vectors_demo\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/floret_vectors_demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qh-jlrxJp7S",
        "outputId": "cac6886e-494f-41d5-d9c9-91ed9215d0b9"
      },
      "source": [
        "%%writefile /content/floret_vectors_demo/project.yml\n",
        "title: \"Demo floret vectors\"\n",
        "description: \"Train floret vectors and load them into a spaCy vectors model.\"\n",
        "spacy_version: \">=3.2.0,<4.0.0\"\n",
        "# Variables can be referenced across the project.yml using ${vars.var_name}\n",
        "vars:\n",
        "  name: \"floret_vectors\"\n",
        "  lang: \"ga\"\n",
        "  oscar_dataset: \"unshuffled_deduplicated_ga\"\n",
        "  max_texts: 1000\n",
        "  # number of processes (tokenization) and threads (floret)\n",
        "  n_process: 8\n",
        "\n",
        "# These are the directories that the project needs. The project CLI will make\n",
        "# sure that they always exist.\n",
        "directories: [\"corpus\", \"scripts\", \"vectors\"]\n",
        "\n",
        "## Workflows are sequences of commands (see below) executed in order. You can\n",
        "## run them via \"spacy project run [workflow]\". If a commands's inputs/outputs\n",
        "## haven't changed, it won't be re-run.\n",
        "workflows:\n",
        "  all:\n",
        "    - tokenize-oscar\n",
        "    - train-floret\n",
        "    - init-floret-vectors\n",
        "    - floret-nn\n",
        "\n",
        "# Project commands, specified in a style similar to CI config files (e.g. Azure\n",
        "# pipelines). The name is the command name that lets you trigger the command\n",
        "# via \"spacy project run [command] [path]\". The help message is optional and\n",
        "# shown when executing \"spacy project run [optional command] [path] --help\".\n",
        "commands:\n",
        "  - name: \"tokenize-oscar\"\n",
        "    help: \"Download, tokenize, and sentencize data\"\n",
        "    script:\n",
        "      - \"python scripts/tokenize_dataset.py ${vars.lang} ${vars.oscar_dataset} ${vars.max_texts} corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt --n-process=${vars.n_process}\"\n",
        "    deps:\n",
        "      - \"scripts/tokenize_dataset.py\"\n",
        "    outputs:\n",
        "      - \"corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt\"\n",
        "\n",
        "  - name: \"train-floret\"\n",
        "    help: \"Train floret vectors\"\n",
        "    script:\n",
        "      - \"python scripts/train_floret.py --model cbow --dim 300 --mincount 10 --minn 3 --maxn 6 --neg 10 --mode floret --hashcount 2 --bucket 20000 --thread ${vars.n_process} corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000\"\n",
        "    deps:\n",
        "      - \"scripts/train_floret.py\"\n",
        "      - \"corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt\"\n",
        "    outputs:\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\"\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.vec\"\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.bin\"\n",
        "\n",
        "  - name: \"init-floret-vectors\"\n",
        "    help: \"Create a floret vectors model\"\n",
        "    script:\n",
        "      - \"python -m spacy init vectors ${vars.lang} vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret vectors/${vars.oscar_dataset}.${vars.max_texts}_floret_model --mode floret\" \n",
        "    deps:\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\"\n",
        "    outputs:\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}_floret_model\"\n",
        "\n",
        "  - name: \"floret-nn\"\n",
        "    help: \"Demo nearest neighbors for intentional OOV misspelling 'outdooor'\"\n",
        "    script:\n",
        "      - \"python scripts/nn_floret.py vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.bin outdooor\"\n",
        "    deps:\n",
        "      - \"scripts/nn_floret.py\"\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.bin\"\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/floret_vectors_demo/project.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlO88B5GKH2p",
        "outputId": "09a600a6-a67f-4d03-e662-018f3c2928fd"
      },
      "source": [
        "!python -m spacy project run tokenize-oscar /content/floret_vectors_demo"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=============================== tokenize-oscar ===============================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/tokenize_dataset.py ga unshuffled_deduplicated_ga 1000 corpus/unshuffled_deduplicated_ga.1000.tok.txt --n-process=8\n",
            "Downloading: 14.8kB [00:00, 10.2MB/s]       \n",
            "Downloading: 3.07MB [00:00, 76.5MB/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmT9cfw2Ki_d",
        "outputId": "a27978e0-fa91-4c73-b4f5-8e97d73b846b"
      },
      "source": [
        "!tail /content/floret_vectors_demo/corpus/unshuffled_deduplicated_ga.1000.tok.txt"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tubaiste cheart a bhí i gcogadh na hIaráice .\n",
            "Maraíodh na céadta míle duine ; chuaigh an fhuath i dtaobh na Breataine sna tíortha Ioslamacha i méid ; rinneadh dochar don chomhaontas idir an Bhreatain agus an Eoraip ; Agus tarraingíodh míchliú ar oidhreacht Blair agus ar ghníomhréim a pháirtí .\n",
            "Is í Mairéad Ní Chuaig , craoltóir agus bean na haimsire ar TG4 , a bhí i mbun cainte le Sara Ní Chuirreáin an tseachtain seo … Ceist mhaith !\n",
            "Eicléicteach , banda agus boihéamach .\n",
            "Tá suim mhór agam i stíl na seascaidí .\n",
            "Bíonn an - tionchar ag ceol , scannáin agus taisteal ar mo stíl féin .\n",
            "Ní thugam suntas do na faisin is deireanaí .\n",
            "Bíonn mé ag bailiú éadaí ar fud na cruinne , agus is breá liom hataí .\n",
            "Fachtóir cosanta ar an ngrian ( Tá mé ag úsáid uachtar gréine le spf ard ó bhí mé 16 , comhairle mhaith a fuair mé ó mo mhama ! ) ,\n",
            "línitheoir súl agus maothóir .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeKxXGb0K3Ra"
      },
      "source": [
        "!cat /content/tagger_parser_ud/assets/UD_Irish-IDT/ga_idt-ud-*.conllu |grep 'text =' |sed -e 's/^# text = //' > idt-sentences.txt"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVMPoqmLRUC"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import typer\n",
        "from itertools import islice\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "lang = \"ga\"\n",
        "n_process = 8\n",
        "batch_size = 100\n",
        "nlp = spacy.blank(lang)\n",
        "input_file = \"idt-sentences.txt\"\n",
        "output_file = \"idt-split.txt\"\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "nlp.max_length = 10 ** 8\n",
        "\n",
        "idt_lines = []\n",
        "with open(input_file) as input_fileh:\n",
        "    for line in input_fileh.readlines():\n",
        "        idt_lines.append(line.strip())\n",
        "\n",
        "with open(output_file, \"w\") as output_fileh:\n",
        "    texts = (\n",
        "        re.sub(\"\\s+\", \" \", line.strip())\n",
        "        for line in idt_lines\n",
        "    )\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
        "        for sent in doc.sents:\n",
        "            output_fileh.write(\" \".join([t.text for t in sent]) + \"\\n\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWoLi3uQM_Az"
      },
      "source": [
        "!cat idt-split.txt >> /content/floret_vectors_demo/corpus/unshuffled_deduplicated_ga.1000.tok.txt"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixW07ykfNEz1",
        "outputId": "5afd632a-83d2-4dc4-ce5f-550352429a3f"
      },
      "source": [
        "!python -m spacy project run train-floret /content/floret_vectors_demo"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================ train-floret ================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/train_floret.py --model cbow --dim 300 --mincount 10 --minn 3 --maxn 6 --neg 10 --mode floret --hashcount 2 --bucket 20000 --thread 8 corpus/unshuffled_deduplicated_ga.1000.tok.txt vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000\n",
            "Read 0M words\n",
            "Number of words:  3566\n",
            "Number of labels: 0\n",
            "Progress: 100.0% words/sec/thread:    7861 lr:  0.000000 avg.loss:  3.225106 ETA:   0h 0m 0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kis5gHLNo4j",
        "outputId": "e702102f-7a13-45c0-8ad3-c0fe62683987"
      },
      "source": [
        "!python -m spacy project run init-floret-vectors /content/floret_vectors_demo"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================ init-floret-vectors ============================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy init vectors ga vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret vectors/unshuffled_deduplicated_ga.1000_floret_model --mode floret\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'ga'\u001b[0m\n",
            "[2021-12-05 11:32:36,770] [INFO] Reading vectors from vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\n",
            "20000it [00:01, 15785.12it/s]\n",
            "[2021-12-05 11:32:38,051] [INFO] Loaded vectors from vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\n",
            "\u001b[38;5;2m✔ Successfully converted 20000 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/floret_vectors_demo/vectors/unshuffled_deduplicated_ga.1000_floret_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1shhD8pVr2n",
        "outputId": "0e44d487-88d7-4a59-b67d-071565b7e754"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.blank(\"ga\")\n",
        "batch_size = 100\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "for doc in nlp.pipe([\"Thosaigh sé. Bhí mé 3.4  5% ag fanacht's 3ú t-ainm ex-théis.\"], batch_size=batch_size):\n",
        "    for sent in doc.sents:\n",
        "        print(\" \".join([t.text for t in sent]) + \"\\n\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thosaigh sé .\n",
            "\n",
            "Bhí mé 3.4   5 % ag fanacht 's 3ú t - ainm ex - théis .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "DmbWudc7WLFe",
        "outputId": "2edf2586-e8cc-425e-835f-64a155f05000"
      },
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "for doc in nlp.pipe([\"Thosaigh sé. Bhí mé $3.4 4% ag fanacht'm-ho at-ainm théis.\"], batch_size=batch_size):\n",
        "    for sent in doc.sents:\n",
        "        print(\" \".join([t.text for t in sent]) + \"\\n\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mlogger_stream_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m logger_stream_handler.setFormatter(\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%(asctime)s] [%(levelname)s] %(message)s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/en/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTOKENIZER_INFIXES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglishLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseDefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/en/lemmatizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Lemmatizer' from 'spacy.pipeline' (/usr/local/lib/python3.7/dist-packages/spacy/pipeline/__init__.py)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7fd8ce3f4ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentencizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mblank\u001b[0;34m(name, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleFrozenList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleFrozenList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleFrozenDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     74\u001b[0m logger_stream_handler.setFormatter(\n\u001b[1;32m     75\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%(asctime)s] [%(levelname)s] %(message)s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m )\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger_stream_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: [E048] Can't import language en from spacy.lang: cannot import name 'Lemmatizer' from 'spacy.pipeline' (/usr/local/lib/python3.7/dist-packages/spacy/pipeline/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBu8mT3sahAp",
        "outputId": "5b3308e2-6eb7-4cf5-ce4b-2a8371fbb536"
      },
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "for doc in nlp.pipe([\"$56.44 ex-head\"], batch_size=batch_size):\n",
        "    for sent in doc.sents:\n",
        "        print(\" \".join([t.text for t in sent]) + \"\\n\")\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$ 56.44 ex - head\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nEwuvW1Equp_",
        "outputId": "652d67b8-28b8-4773-db73-074cb9d26f57"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.2.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mlogger_stream_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m logger_stream_handler.setFormatter(\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%(asctime)s] [%(levelname)s] %(message)s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/en/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTOKENIZER_INFIXES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglishLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseDefaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/lang/en/lemmatizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Lemmatizer' from 'spacy.pipeline' (/usr/local/lib/python3.7/dist-packages/spacy/pipeline/__init__.py)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-06a13e7ed5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Apple is looking at buying U.K. startup for $1 billion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0merror\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0muser\u001b[0m \u001b[0mtries\u001b[0m \u001b[0mto\u001b[0m \u001b[0massign\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     74\u001b[0m logger_stream_handler.setFormatter(\n\u001b[1;32m     75\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%(asctime)s] [%(levelname)s] %(message)s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m )\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger_stream_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: [E048] Can't import language en from spacy.lang: cannot import name 'Lemmatizer' from 'spacy.pipeline' (/usr/local/lib/python3.7/dist-packages/spacy/pipeline/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}