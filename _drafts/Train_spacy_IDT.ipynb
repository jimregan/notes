{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train spacy IDT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXyZmjveMocr"
      },
      "source": [
        "# \"Training spaCy 3 on IDT\"\n",
        "> \"Not going well so far\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- comments: true\n",
        "- categories: [spacy, idt]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9blVYqNcObhq"
      },
      "source": [
        "%%capture\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install thinc --pre\n",
        "!pip install -U spacy spacy-lookups-data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH27c_5MCuri",
        "outputId": "d1bf5fe9-2185-4685-e093-d0de01e7fc9d"
      },
      "source": [
        "!python -m spacy project clone pipelines/tagger_parser_ud"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'pipelines/tagger_parser_ud' from explosion/projects\u001b[0m\n",
            "/content/tagger_parser_ud\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/tagger_parser_ud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ryAtsK5DZeP",
        "outputId": "86a998f7-79d8-41fe-f225-899238cfb8ad"
      },
      "source": [
        "%%writefile tagger_parser_ud/project.yml\n",
        "title: \"Part-of-speech Tagging & Dependency Parsing (Universal Dependencies)\"\n",
        "description: \"This project template lets you train a part-of-speech tagger, morphologizer and dependency parser from a [Universal Dependencies](https://universaldependencies.org/) corpus. It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model. The template uses the [`UD_English-EWT`](https://github.com/UniversalDependencies/UD_English-EWT) treebank by default, but you can swap it out for any other available treebank. Just make sure to adjust the `lang` and treebank settings in the variables below. Use `xx` for multi-language if no language-specific tokenizer is available in spaCy. Note that multi-word tokens will be merged together when the corpus is converted since spaCy does not support multi-word token expansion.\"\n",
        "\n",
        "# Variables can be referenced across the project.yml using ${vars.var_name}\n",
        "vars:\n",
        "  config: \"default\"\n",
        "  lang: \"ga\"\n",
        "  treebank: \"UD_Irish-IDT\"\n",
        "  train_name: \"ga_idt-ud-train\"\n",
        "  dev_name: \"ga_idt-ud-dev\"\n",
        "  test_name: \"ga_idt-ud-test\"\n",
        "  package_name: \"ud_ga_idt\"\n",
        "  package_version: \"0.0.0\"\n",
        "  gpu: -1\n",
        "\n",
        "# These are the directories that the project needs. The project CLI will make\n",
        "# sure that they always exist.\n",
        "directories: [\"assets\", \"corpus\", \"training\", \"metrics\", \"configs\", \"packages\"]\n",
        "\n",
        "assets:\n",
        "  - url: \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ga.300.vec.gz\"\n",
        "    dest: \"assets/vectors.vec.gz\"\n",
        "    description: \"FastText vectors\"\n",
        "  - dest: \"assets/${vars.treebank}\"\n",
        "    git:\n",
        "      repo: \"https://github.com/UniversalDependencies/${vars.treebank}\"\n",
        "      branch: \"master\"\n",
        "      path: \"\"\n",
        "\n",
        "workflows:\n",
        "  all:\n",
        "    - preprocess\n",
        "    - vectors\n",
        "    - train\n",
        "    - evaluate\n",
        "    - package\n",
        "\n",
        "commands:\n",
        "  - name: preprocess\n",
        "    help: \"Convert the data to spaCy's format\"\n",
        "    script:\n",
        "      - \"mkdir -p corpus/${vars.treebank}\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.train_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.dev_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"python -m spacy convert assets/${vars.treebank}/${vars.test_name}.conllu corpus/${vars.treebank}/ --converter conllu --n-sents 10 --merge-subtokens --morphology\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.train_name}.spacy corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.dev_name}.spacy corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"mv corpus/${vars.treebank}/${vars.test_name}.spacy corpus/${vars.treebank}/test.spacy\"\n",
        "    deps:\n",
        "      - \"assets/${vars.treebank}/${vars.train_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.dev_name}.conllu\"\n",
        "      - \"assets/${vars.treebank}/${vars.test_name}.conllu\"\n",
        "    outputs:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "\n",
        "  - name: vectors\n",
        "    help: \"Convert, truncate and prune the vectors.\"\n",
        "    script:\n",
        "      - \"python -m spacy init vectors ga assets/vectors.vec.gz corpus/ga_vectors -n ga_fasttext_cc_vectors_md\"\n",
        "    deps:\n",
        "      - \"assets/vectors.vec.gz\"\n",
        "    outputs:\n",
        "      - \"corpus/ga_vectors\"\n",
        "\n",
        "  - name: train\n",
        "    help: \"Train ${vars.treebank}\"\n",
        "    script:\n",
        "      - \"python -m spacy train configs/${vars.config}.cfg --output training/${vars.treebank} --gpu-id ${vars.gpu} --paths.train corpus/${vars.treebank}/train.spacy --paths.dev corpus/${vars.treebank}/dev.spacy --paths.vectors corpus/ga_vectors --nlp.lang=${vars.lang}\"\n",
        "    deps:\n",
        "      - \"corpus/${vars.treebank}/train.spacy\"\n",
        "      - \"corpus/${vars.treebank}/dev.spacy\"\n",
        "      - \"configs/${vars.config}.cfg\"\n",
        "      - \"corpus/ga_vectors\"\n",
        "    outputs:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "\n",
        "  - name: evaluate\n",
        "    help: \"Evaluate on the test data and save the metrics\"\n",
        "    script:\n",
        "      - \"python -m spacy evaluate ./training/${vars.treebank}/model-best ./corpus/${vars.treebank}/test.spacy --output ./metrics/${vars.treebank}.json --gpu-id ${vars.gpu}\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "      - \"corpus/${vars.treebank}/test.spacy\"\n",
        "    outputs:\n",
        "      - \"metrics/${vars.treebank}.json\"\n",
        "\n",
        "  - name: package\n",
        "    help: \"Package the trained model so it can be installed\"\n",
        "    script:\n",
        "      - \"python -m spacy package training/${vars.treebank}/model-best packages --name ${vars.package_name} --version ${vars.package_version} --force\"\n",
        "    deps:\n",
        "      - \"training/${vars.treebank}/model-best\"\n",
        "    outputs_no_cache:\n",
        "      - \"packages/${vars.lang}_${vars.package_name}-${vars.package_version}/dist/en_${vars.package_name}-${vars.package_version}.tar.gz\"\n",
        "\n",
        "  - name: clean\n",
        "    help: \"Remove intermediate files\"\n",
        "    script:\n",
        "      - \"rm -rf training/*\"\n",
        "      - \"rm -rf metrics/*\"\n",
        "      - \"rm -rf corpus/*\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tagger_parser_ud/project.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP3p5fS3D1qF",
        "outputId": "d37904c1-cd56-463d-f536-77f58aa59a60"
      },
      "source": [
        "!python -m spacy project assets /content/tagger_parser_ud"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Fetching 2 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/vectors.vec.gz\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/tagger_parser_ud/assets/UD_Irish-IDT\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meFJmpaRU6E8",
        "outputId": "7ca6f482-49c3-42f7-d439-d0f7d59acb2f"
      },
      "source": [
        "!python -m spacy project run vectors /content/tagger_parser_ud"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== vectors ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy init vectors ga assets/vectors.vec.gz corpus/ga_vectors -n ga_fasttext_cc_vectors_md\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'ga'\u001b[0m\n",
            "[2021-12-01 21:52:23,683] [INFO] Reading vectors from assets/vectors.vec.gz\n",
            "316836it [00:26, 12108.38it/s]\n",
            "[2021-12-01 21:52:49,975] [INFO] Loaded vectors from assets/vectors.vec.gz\n",
            "\u001b[38;5;2m✔ Successfully converted 316836 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/tagger_parser_ud/corpus/ga_vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65yySCQfFVsx",
        "outputId": "761e1783-afae-4885-8d39-36121e734be4"
      },
      "source": [
        "!python -m spacy project run preprocess /content/tagger_parser_ud"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================= preprocess =================================\u001b[0m\n",
            "Running command: mkdir -p corpus/UD_Irish-IDT\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-train.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (401 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-train.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-dev.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy convert assets/UD_Irish-IDT/ga_idt-ud-test.conllu corpus/UD_Irish-IDT/ --converter conllu --n-sents 10 --merge-subtokens --morphology\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (46 documents):\n",
            "corpus/UD_Irish-IDT/ga_idt-ud-test.spacy\u001b[0m\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-train.spacy corpus/UD_Irish-IDT/train.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-dev.spacy corpus/UD_Irish-IDT/dev.spacy\n",
            "Running command: mv corpus/UD_Irish-IDT/ga_idt-ud-test.spacy corpus/UD_Irish-IDT/test.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi1Ny0PWWlWE",
        "outputId": "f2c8f53e-1065-49e4-8ef7-714a0455ebf4"
      },
      "source": [
        "%%writefile /content/tagger_parser_ud/configs/base_default.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = null\n",
        "init_tok2vec = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"ga\"\n",
        "pipeline = [\"tok2vec\",\"tagger\",\"morphologizer\",\"parser\"]\n",
        "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
        "disabled = []\n",
        "before_creation = null\n",
        "after_creation = null\n",
        "after_pipeline_creation = null\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.morphologizer]\n",
        "factory = \"morphologizer\"\n",
        "\n",
        "[components.morphologizer.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.morphologizer.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.parser]\n",
        "factory = \"parser\"\n",
        "learn_tokens = false\n",
        "min_action_freq = 30\n",
        "moves = null\n",
        "update_with_oracle_cut_size = 100\n",
        "\n",
        "[components.parser.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"parser\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 128\n",
        "maxout_pieces = 3\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.parser.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.tagger]\n",
        "factory = \"tagger\"\n",
        "\n",
        "[components.tagger.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.tagger.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v1\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"LOWER\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
        "rows = [5000,2500,2500,2500]\n",
        "include_static_vectors = false\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v1\"\n",
        "width = 96\n",
        "depth = 4\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 2000\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "dropout = 0.1\n",
        "accumulate_gradient = 1\n",
        "patience = 1600\n",
        "max_epochs = 0\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "get_length = null\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "t = 0.0\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = true\n",
        "L2 = 0.01\n",
        "grad_clip = 1.0\n",
        "use_averages = false\n",
        "eps = 0.00000001\n",
        "learn_rate = 0.001\n",
        "\n",
        "[training.score_weights]\n",
        "morph_per_feat = null\n",
        "dep_las_per_type = null\n",
        "sents_p = null\n",
        "sents_r = null\n",
        "tag_acc = 0.33\n",
        "pos_acc = 0.17\n",
        "morph_acc = 0.17\n",
        "dep_uas = 0.17\n",
        "dep_las = 0.17\n",
        "sents_f = 0.0\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}\n",
        "init_tok2vec = ${paths.init_tok2vec}\n",
        "vocab_data = null\n",
        "lookups = null\n",
        "before_init = null\n",
        "after_init = null\n",
        "\n",
        "[initialize.components]\n",
        "\n",
        "[initialize.tokenizer]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/tagger_parser_ud/configs/base_default.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTfokWSkZuOY",
        "outputId": "6a4bc928-bda0-4bdf-e4b1-3cd28e3a2c9e"
      },
      "source": [
        "!rm /content/tagger_parser_ud/configs/default.cfg\n",
        "!python -m spacy init fill-config /content/tagger_parser_ud/configs/base_default.cfg /content/tagger_parser_ud/configs/default.cfg"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/tagger_parser_ud/configs/default.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train default.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q8R-yL3GiO5",
        "outputId": "43175d97-1773-4ed9-999f-ac8743d8710f"
      },
      "source": [
        "!python -m spacy project run train tagger_parser_ud"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train configs/default.cfg --output training/UD_Irish-IDT --gpu-id -1 --paths.train corpus/UD_Irish-IDT/train.spacy --paths.dev corpus/UD_Irish-IDT/dev.spacy --paths.vectors corpus/ga_vectors --nlp.lang=ga\n",
            "\u001b[38;5;2m✔ Created output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/UD_Irish-IDT\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-12-01 21:55:23,333] [INFO] Set up nlp object from config\n",
            "[2021-12-01 21:55:23,344] [INFO] Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "[2021-12-01 21:55:23,350] [INFO] Created vocabulary\n",
            "[2021-12-01 21:55:24,173] [INFO] Added vectors: corpus/ga_vectors\n",
            "[2021-12-01 21:55:25,173] [INFO] Finished initializing nlp object\n",
            "[2021-12-01 21:55:38,655] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS MORPH...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE \n",
            "---  ------  ------------  -----------  -------------  -----------  -------  -------  ---------  -------  -------  -------  ------\n",
            "  0       0          0.00       223.78         223.72       475.65    18.71    36.23      19.74    12.33     6.54     0.00    0.19\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "KeyError('')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1152, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 202, in spacy.pipeline.tagger.Tagger.update\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 263, in spacy.pipeline.tagger.Tagger.get_loss\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 193, in __call__\n",
            "    grads = self.get_grad(guesses, truths)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 206, in get_grad\n",
            "    d_yh = self.cc.get_grad(yh, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 120, in get_grad\n",
            "    target, mask = self.convert_truths(truths, guesses)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 93, in convert_truths\n",
            "    neg_index = self._name_to_i[truths[i]]\n",
            "KeyError: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wig6uL9M0oov",
        "outputId": "fd3dbf6e-1add-4ea2-fb4b-c395d7edf071"
      },
      "source": [
        "!apt install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (1,931 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMvOJDOY2CvP"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoEbpnA-61DJ"
      },
      "source": [
        "!transformers-cli login\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gulro7mTAVwC",
        "outputId": "138f53de-3aa9-4e14-c231-f248f13b4586"
      },
      "source": [
        "%%writefile tmp_config.cfg\n",
        "# This is an auto-generated partial config. To use it with 'spacy train'\n",
        "# you can run spacy init fill-config to auto-fill all default settings:\n",
        "# python -m spacy init fill-config ./base_config.cfg ./config.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "\n",
        "[nlp]\n",
        "lang = \"ga\"\n",
        "pipeline = [\"tok2vec\",\"tagger\",\"morphologizer\",\"parser\"]\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v2\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"ORTH\", \"SHAPE\"]\n",
        "rows = [5000, 2500]\n",
        "include_static_vectors = true\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
        "width = 256\n",
        "depth = 8\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[components.morphologizer]\n",
        "factory = \"morphologizer\"\n",
        "\n",
        "[components.morphologizer.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.morphologizer.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[components.tagger]\n",
        "factory = \"tagger\"\n",
        "\n",
        "[components.tagger.model]\n",
        "@architectures = \"spacy.Tagger.v1\"\n",
        "nO = null\n",
        "\n",
        "[components.tagger.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[components.parser]\n",
        "factory = \"parser\"\n",
        "\n",
        "[components.parser.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"parser\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 128\n",
        "maxout_pieces = 3\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.parser.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tmp_config.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkY425VzAntO",
        "outputId": "aad37c66-fbb5-4478-f3c8-94a4c72a7050"
      },
      "source": [
        "!python -m spacy init fill-config tmp_config.cfg config.cfg"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3s4moISCiOH"
      },
      "source": [
        "!mkdir spacy_ud"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irdz4cd_Br-V",
        "outputId": "7780b242-aa3e-43a9-c727-c962a57ac0aa"
      },
      "source": [
        "!python -m spacy train config.cfg --paths.train /content/tagger_parser_ud/corpus/UD_Irish-IDT/train.spacy --paths.dev /content/tagger_parser_ud/corpus/UD_Irish-IDT/dev.spacy --paths.vectors /content/tagger_parser_ud/corpus/ga_vectors --output spacy_ud"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Saving to output directory: spacy_ud\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2021-12-01 22:08:12,060] [INFO] Set up nlp object from config\n",
            "[2021-12-01 22:08:12,079] [INFO] Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "[2021-12-01 22:08:12,086] [INFO] Created vocabulary\n",
            "[2021-12-01 22:08:12,878] [INFO] Added vectors: /content/tagger_parser_ud/corpus/ga_vectors\n",
            "[2021-12-01 22:08:13,815] [INFO] Finished initializing nlp object\n",
            "[2021-12-01 22:08:43,407] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'morphologizer', 'parser']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'morphologizer', 'parser']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS MORPH...  LOSS PARSER  TAG_ACC  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  SCORE \n",
            "---  ------  ------------  -----------  -------------  -----------  -------  -------  ---------  -------  -------  -------  ------\n",
            "  0       0          0.00       223.78         223.72       475.65    21.33    39.57      21.57    14.44     5.29     0.00    0.21\n",
            "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
            "KeyError('')\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/_util.py\", line 71, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typer/main.py\", line 500, in wrapper\n",
            "    return callback(**use_params)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 45, in train_cli\n",
            "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/cli/train.py\", line 75, in train\n",
            "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 122, in train\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 105, in train\n",
            "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/training/loop.py\", line 209, in train_while_improving\n",
            "    annotates=annotating_components,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1152, in update\n",
            "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 202, in spacy.pipeline.tagger.Tagger.update\n",
            "  File \"spacy/pipeline/tagger.pyx\", line 263, in spacy.pipeline.tagger.Tagger.get_loss\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 193, in __call__\n",
            "    grads = self.get_grad(guesses, truths)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 206, in get_grad\n",
            "    d_yh = self.cc.get_grad(yh, y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 120, in get_grad\n",
            "    target, mask = self.convert_truths(truths, guesses)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/loss.py\", line 93, in convert_truths\n",
            "    neg_index = self._name_to_i[truths[i]]\n",
            "KeyError: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DncGLkhRD7KH",
        "outputId": "d86b9f40-b44b-4b4b-8b40-f995c5d553c8"
      },
      "source": [
        "!python -m spacy debug data /content/spacy_ud/model-best/config.cfg"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: ga\n",
            "Training pipeline: tok2vec, tagger, morphologizer, parser\n",
            "401 training docs\n",
            "46 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (401)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 95881 total word(s) in the data (14191 unique)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 600 misaligned tokens in the training data\u001b[0m\n",
            "\u001b[38;5;3m⚠ 80 misaligned tokens in the dev data\u001b[0m\n",
            "\u001b[38;5;4mℹ 316836 vectors (316836 unique keys, 300 dimensions)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 2638 words in training data without vectors (3%)\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 1029 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Some model labels are not present in the train data. The model\n",
            "performance may be degraded for these labels after training:\n",
            "'VT__Aspect=Imp|Form=Len|Tense=Past',\n",
            "'VT__Aspect=Imp|Number=Sing|Person=1|Tense=Past',\n",
            "'VTI__Form=Len|Mood=Imp|Number=Sing|Person=2',\n",
            "'VTI__Form=Len|Mood=Cnd|Number=Plur|Person=1',\n",
            "'VT__Aspect=Imp|Form=Len|Number=Plur|Person=3|Tense=Past',\n",
            "'VERB__Form=Len|Mood=Cnd|Person=0',\n",
            "'VTI__Form=Len|Mood=Ind|Tense=Past|Typo=Yes',\n",
            "'ADJ__Abbr=Yes|Case=Nom|Gender=Fem|Number=Sing',\n",
            "'Poss__Number=Sing|Person=2|Poss=Yes', 'Cop__Form=VF|Mood=Cnd|VerbForm=Cop',\n",
            "'Cop__Form=Ecl,VF|Tense=Past|VerbForm=Cop',\n",
            "'VI__Form=Len|Mood=Cnd|Number=Plur|Person=1',\n",
            "'VTI__Form=Len|Mood=Cnd|Typo=Yes'.\u001b[0m\n",
            "\u001b[1m\n",
            "========================= Morphologizer (POS+Morph) =========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 795 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Some model labels are not present in the train data. The model\n",
            "performance may be degraded for these labels after training:\n",
            "'Form=Len|Mood=Imp|Number=Sing|POS=VERB|Person=2',\n",
            "'Aspect=Imp|Number=Sing|POS=VERB|Person=1|Tense=Past',\n",
            "'Form=Ecl,VF|POS=AUX|Tense=Past|VerbForm=Cop',\n",
            "'Form=VF|Mood=Cnd|POS=AUX|VerbForm=Cop', 'Form=Len|Mood=Cnd|POS=VERB|Typo=Yes',\n",
            "'Foreign=Yes|POS=PRON'.\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Dependency Parsing =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 3851 sentence(s) with an average length of 24.9 words.\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 301 nonprojective train sentence(s)\u001b[0m\n",
            "\u001b[38;5;4mℹ 42 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;4mℹ 230 label(s) in projectivized train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'orphan' (8)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'goeswith' (3)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for 181 label(s) in the projectivized\n",
            "dependency trees used for training. You may want to projectivize labels such as\n",
            "punct before training in order to improve parser performance.\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 3 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 12 warnings\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43N2OX4sJkDW",
        "outputId": "5eb9ee5b-49e7-44ab-b7a0-27c0dee3954a"
      },
      "source": [
        "!python -m spacy project clone pipelines/floret_vectors_demo"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'pipelines/floret_vectors_demo' from explosion/projects\u001b[0m\n",
            "/content/floret_vectors_demo\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/floret_vectors_demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qh-jlrxJp7S",
        "outputId": "f870e840-dbb1-43f3-cc93-e06f081e5ba7"
      },
      "source": [
        "%%writefile /content/floret_vectors_demo/project.yml\n",
        "title: \"Demo floret vectors\"\n",
        "description: \"Train floret vectors and load them into a spaCy vectors model.\"\n",
        "spacy_version: \">=3.2.0,<4.0.0\"\n",
        "# Variables can be referenced across the project.yml using ${vars.var_name}\n",
        "vars:\n",
        "  name: \"floret_vectors\"\n",
        "  lang: \"ga\"\n",
        "  oscar_dataset: \"unshuffled_deduplicated_ga\"\n",
        "  max_texts: 1000\n",
        "  # number of processes (tokenization) and threads (floret)\n",
        "  n_process: 8\n",
        "\n",
        "# These are the directories that the project needs. The project CLI will make\n",
        "# sure that they always exist.\n",
        "directories: [\"corpus\", \"scripts\", \"vectors\"]\n",
        "\n",
        "## Workflows are sequences of commands (see below) executed in order. You can\n",
        "## run them via \"spacy project run [workflow]\". If a commands's inputs/outputs\n",
        "## haven't changed, it won't be re-run.\n",
        "workflows:\n",
        "  all:\n",
        "    - tokenize-oscar\n",
        "    - train-floret\n",
        "    - init-floret-vectors\n",
        "    - floret-nn\n",
        "\n",
        "# Project commands, specified in a style similar to CI config files (e.g. Azure\n",
        "# pipelines). The name is the command name that lets you trigger the command\n",
        "# via \"spacy project run [command] [path]\". The help message is optional and\n",
        "# shown when executing \"spacy project run [optional command] [path] --help\".\n",
        "commands:\n",
        "  - name: \"tokenize-oscar\"\n",
        "    help: \"Download, tokenize, and sentencize data\"\n",
        "    script:\n",
        "      - \"python scripts/tokenize_dataset.py ${vars.lang} ${vars.oscar_dataset} ${vars.max_texts} corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt --n-process=${vars.n_process}\"\n",
        "    deps:\n",
        "      - \"scripts/tokenize_dataset.py\"\n",
        "    outputs:\n",
        "      - \"corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt\"\n",
        "\n",
        "  - name: \"train-floret\"\n",
        "    help: \"Train floret vectors\"\n",
        "    script:\n",
        "      - \"python scripts/train_floret.py --model cbow --dim 300 --mincount 10 --minn 3 --maxn 6 --neg 10 --mode floret --hashcount 2 --bucket 20000 --thread ${vars.n_process} corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000\"\n",
        "    deps:\n",
        "      - \"scripts/train_floret.py\"\n",
        "      - \"corpus/${vars.oscar_dataset}.${vars.max_texts}.tok.txt\"\n",
        "    outputs:\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\"\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.vec\"\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.bin\"\n",
        "\n",
        "  - name: \"init-floret-vectors\"\n",
        "    help: \"Create a floret vectors model\"\n",
        "    script:\n",
        "      - \"python -m spacy init vectors ${vars.lang} vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret vectors/${vars.oscar_dataset}.${vars.max_texts}_floret_model --mode floret\" \n",
        "    deps:\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\"\n",
        "    outputs:\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}_floret_model\"\n",
        "\n",
        "  - name: \"floret-nn\"\n",
        "    help: \"Demo nearest neighbors for intentional OOV misspelling 'outdooor'\"\n",
        "    script:\n",
        "      - \"python scripts/nn_floret.py vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.bin outdooor\"\n",
        "    deps:\n",
        "      - \"scripts/nn_floret.py\"\n",
        "      - \"vectors/${vars.oscar_dataset}.${vars.max_texts}.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.bin\"\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/floret_vectors_demo/project.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39LOZu1lKVZN",
        "outputId": "50677498-d063-4a31-e2c5-163d2ba89a9e"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[?25l\r\r     |█                               | 10 kB 27.2 MB/s eta 0:00:01\r     |██▏                             | 20 kB 32.7 MB/s eta 0:00:01\r     |███▎                            | 30 kB 23.1 MB/s eta 0:00:01\r     |████▍                           | 40 kB 18.8 MB/s eta 0:00:01\r     |█████▌                          | 51 kB 11.6 MB/s eta 0:00:01\r     |██████▋                         | 61 kB 13.0 MB/s eta 0:00:01\r     |███████▊                        | 71 kB 12.7 MB/s eta 0:00:01\r     |████████▉                       | 81 kB 14.1 MB/s eta 0:00:01\r     |█████████▉                      | 92 kB 13.1 MB/s eta 0:00:01\r     |███████████                     | 102 kB 11.8 MB/s eta 0:00:01\r     |████████████                    | 112 kB 11.8 MB/s eta 0:00:01\r     |█████████████▏                  | 122 kB 11.8 MB/s eta 0:00:01\r     |██████████████▎                 | 133 kB 11.8 MB/s eta 0:00:01\r     |███████████████▍                | 143 kB 11.8 MB/s eta 0:00:01\r     |████████████████▌               | 153 kB 11.8 MB/s eta 0:00:01\r     |█████████████████▋              | 163 kB 11.8 MB/s eta 0:00:01\r     |██████████████████▊             | 174 kB 11.8 MB/s eta 0:00:01\r     |███████████████████▊            | 184 kB 11.8 MB/s eta 0:00:01\r     |████████████████████▉           | 194 kB 11.8 MB/s eta 0:00:01\r     |██████████████████████          | 204 kB 11.8 MB/s eta 0:00:01\r     |███████████████████████         | 215 kB 11.8 MB/s eta 0:00:01\r     |████████████████████████▏       | 225 kB 11.8 MB/s eta 0:00:01\r     |█████████████████████████▎      | 235 kB 11.8 MB/s eta 0:00:01\r     |██████████████████████████▍     | 245 kB 11.8 MB/s eta 0:00:01\r     |███████████████████████████▌    | 256 kB 11.8 MB/s eta 0:00:01\r     |████████████████████████████▋   | 266 kB 11.8 MB/s eta 0:00:01\r     |█████████████████████████████▋  | 276 kB 11.8 MB/s eta 0:00:01\r     |██████████████████████████████▊ | 286 kB 11.8 MB/s eta 0:00:01\r     |███████████████████████████████▉| 296 kB 11.8 MB/s eta 0:00:01\r     |████████████████████████████████| 298 kB 11.8 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 65.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "     |████████████████████████████████| 132 kB 82.3 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "     |████████████████████████████████| 243 kB 54.7 MB/s            \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.0-py3-none-any.whl (61 kB)\n",
            "     |████████████████████████████████| 61 kB 302 kB/s             \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "     |████████████████████████████████| 271 kB 68.4 MB/s            \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "     |████████████████████████████████| 160 kB 83.6 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "     |████████████████████████████████| 192 kB 68.6 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlO88B5GKH2p",
        "outputId": "afa07d1b-c897-45d0-cdce-5b8cbdb754ab"
      },
      "source": [
        "!python -m spacy project run tokenize-oscar /content/floret_vectors_demo"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=============================== tokenize-oscar ===============================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/tokenize_dataset.py ga unshuffled_deduplicated_ga 1000 corpus/unshuffled_deduplicated_ga.1000.tok.txt --n-process=8\n",
            "Downloading: 14.8kB [00:00, 10.6MB/s]       \n",
            "Downloading: 3.07MB [00:00, 134MB/s]       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmT9cfw2Ki_d",
        "outputId": "60809b10-faf9-4424-ca8b-8db693d0b2a8"
      },
      "source": [
        "!tail /content/floret_vectors_demo/corpus/unshuffled_deduplicated_ga.1000.tok.txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tubaiste cheart a bhí i gcogadh na hIaráice .\n",
            "Maraíodh na céadta míle duine ; chuaigh an fhuath i dtaobh na Breataine sna tíortha Ioslamacha i méid ; rinneadh dochar don chomhaontas idir an Bhreatain agus an Eoraip ; Agus tarraingíodh míchliú ar oidhreacht Blair agus ar ghníomhréim a pháirtí .\n",
            "Is í Mairéad Ní Chuaig , craoltóir agus bean na haimsire ar TG4 , a bhí i mbun cainte le Sara Ní Chuirreáin an tseachtain seo … Ceist mhaith !\n",
            "Eicléicteach , banda agus boihéamach .\n",
            "Tá suim mhór agam i stíl na seascaidí .\n",
            "Bíonn an - tionchar ag ceol , scannáin agus taisteal ar mo stíl féin .\n",
            "Ní thugam suntas do na faisin is deireanaí .\n",
            "Bíonn mé ag bailiú éadaí ar fud na cruinne , agus is breá liom hataí .\n",
            "Fachtóir cosanta ar an ngrian ( Tá mé ag úsáid uachtar gréine le spf ard ó bhí mé 16 , comhairle mhaith a fuair mé ó mo mhama ! ) ,\n",
            "línitheoir súl agus maothóir .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeKxXGb0K3Ra"
      },
      "source": [
        "!cat /content/tagger_parser_ud/assets/UD_Irish-IDT/ga_idt-ud-*.conllu |grep 'text =' |sed -e 's/^# text = //' > idt-sentences.txt"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVMPoqmLRUC"
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "import typer\n",
        "from itertools import islice\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "lang = \"ga\"\n",
        "n_process = 8,\n",
        "batch_size = 100,\n",
        "nlp = spacy.blank(lang)\n",
        "input_file = \"idt-sentences.txt\"\n",
        "output_file = \"idt-split.txt\"\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "nlp.max_length = 10 ** 8\n",
        "\n",
        "idt_lines = []\n",
        "with open(input_file) as input_fileh:\n",
        "    for line in input_fileh.readlines():\n",
        "        idt_lines.append(line.strip())\n",
        "\n",
        "with open(output_file, \"w\") as output_fileh:\n",
        "    texts = (\n",
        "        re.sub(\"\\s+\", \" \", line.strip())\n",
        "        for line in idt_lines\n",
        "    )\n",
        "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
        "        for sent in doc.sents:\n",
        "            output_fileh.write(\" \".join([t.text for t in sent]) + \"\\n\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWoLi3uQM_Az"
      },
      "source": [
        "!cat idt-split.txt >> /content/floret_vectors_demo/corpus/unshuffled_deduplicated_ga.1000.tok.txt"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecXVR83ZNTTI",
        "outputId": "f967669b-9835-445a-d980-28cf2b6ccf55"
      },
      "source": [
        "!pip install floret"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting floret\n",
            "  Downloading floret-0.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[?25l\r\r     |█                               | 10 kB 25.1 MB/s eta 0:00:01\r     |██                              | 20 kB 34.8 MB/s eta 0:00:01\r     |███▏                            | 30 kB 25.8 MB/s eta 0:00:01\r     |████▏                           | 40 kB 19.5 MB/s eta 0:00:01\r     |█████▎                          | 51 kB 16.7 MB/s eta 0:00:01\r     |██████▎                         | 61 kB 11.9 MB/s eta 0:00:01\r     |███████▎                        | 71 kB 13.0 MB/s eta 0:00:01\r     |████████▍                       | 81 kB 14.6 MB/s eta 0:00:01\r     |█████████▍                      | 92 kB 15.5 MB/s eta 0:00:01\r     |██████████▌                     | 102 kB 14.0 MB/s eta 0:00:01\r     |███████████▌                    | 112 kB 14.0 MB/s eta 0:00:01\r     |████████████▌                   | 122 kB 14.0 MB/s eta 0:00:01\r     |█████████████▋                  | 133 kB 14.0 MB/s eta 0:00:01\r     |██████████████▋                 | 143 kB 14.0 MB/s eta 0:00:01\r     |███████████████▊                | 153 kB 14.0 MB/s eta 0:00:01\r     |████████████████▊               | 163 kB 14.0 MB/s eta 0:00:01\r     |█████████████████▊              | 174 kB 14.0 MB/s eta 0:00:01\r     |██████████████████▉             | 184 kB 14.0 MB/s eta 0:00:01\r     |███████████████████▉            | 194 kB 14.0 MB/s eta 0:00:01\r     |█████████████████████           | 204 kB 14.0 MB/s eta 0:00:01\r     |██████████████████████          | 215 kB 14.0 MB/s eta 0:00:01\r     |███████████████████████         | 225 kB 14.0 MB/s eta 0:00:01\r     |████████████████████████        | 235 kB 14.0 MB/s eta 0:00:01\r     |█████████████████████████       | 245 kB 14.0 MB/s eta 0:00:01\r     |██████████████████████████▏     | 256 kB 14.0 MB/s eta 0:00:01\r     |███████████████████████████▏    | 266 kB 14.0 MB/s eta 0:00:01\r     |████████████████████████████▏   | 276 kB 14.0 MB/s eta 0:00:01\r     |█████████████████████████████▎  | 286 kB 14.0 MB/s eta 0:00:01\r     |██████████████████████████████▎ | 296 kB 14.0 MB/s eta 0:00:01\r     |███████████████████████████████▍| 307 kB 14.0 MB/s eta 0:00:01\r     |████████████████████████████████| 313 kB 14.0 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from floret) (1.19.5)\n",
            "Installing collected packages: floret\n",
            "Successfully installed floret-0.10.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixW07ykfNEz1",
        "outputId": "89a3a0d4-8170-4105-b4b0-1a51c4873bc0"
      },
      "source": [
        "!python -m spacy project run train-floret /content/floret_vectors_demo"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================ train-floret ================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/train_floret.py --model cbow --dim 300 --mincount 10 --minn 3 --maxn 6 --neg 10 --mode floret --hashcount 2 --bucket 20000 --thread 8 corpus/unshuffled_deduplicated_ga.1000.tok.txt vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000\n",
            "Read 0M words\n",
            "Number of words:  3566\n",
            "Number of labels: 0\n",
            "Progress: 100.0% words/sec/thread:    9731 lr:  0.000000 avg.loss:  3.231723 ETA:   0h 0m 0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kis5gHLNo4j",
        "outputId": "84c2c0df-d580-4a09-d24d-b3d8c58f8800"
      },
      "source": [
        "!python -m spacy project run init-floret-vectors /content/floret_vectors_demo"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================ init-floret-vectors ============================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy init vectors ga vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret vectors/unshuffled_deduplicated_ga.1000_floret_model --mode floret\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'ga'\u001b[0m\n",
            "[2021-12-01 22:56:39,325] [INFO] Reading vectors from vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\n",
            "20000it [00:01, 15396.03it/s]\n",
            "[2021-12-01 22:56:40,633] [INFO] Loaded vectors from vectors/unshuffled_deduplicated_ga.1000.dim300.minCount10.n3-6.neg10.modeFloret.hashCount2.bucket20000.floret\n",
            "\u001b[38;5;2m✔ Successfully converted 20000 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/floret_vectors_demo/vectors/unshuffled_deduplicated_ga.1000_floret_model\n"
          ]
        }
      ]
    }
  ]
}