kth-tmh/mm-conv-motion ¬∑ Datasets at Hugging Face
https://huggingface.co/datasets/kth-tmh/mm-conv-motion

Ashes of the Wake - Wikipedia
https://en.wikipedia.org/wiki/Ashes_of_the_Wake

New American Gospel - Wikipedia
https://en.wikipedia.org/wiki/New_American_Gospel

Is there a way to airplay from Chrome browser to AppleTV 3rd gen? : r/appletv
https://www.reddit.com/r/appletv/comments/f1cnj3/is_there_a_way_to_airplay_from_chrome_browser_to/

CBT - Reel Text Assistance
https://chatgpt.com/g/g-p-6825c4aefc748191b31f39848a1b0ef1-cbt/c/694dca12-03d4-832b-842a-e9974106392b

(3) Home / X
https://x.com/home

(3) DailyPapers on X: "Google discovers emergent temporal abstractions in autoregressive models These models learn linearly controllable action representations in their residual streams‚Äîactivating them executes long-horizon behaviors. This enables Internal RL to solve sparse-reward hierarchical tasks https://t.co/GxOObljGcB" / X
https://x.com/HuggingPapers/status/2004645512079659192

(3) Enze Xie on X: "We (@lawrence_cjs, @yuyangzhao_ , @shanasaimoe) from the SANA team just posted a blog on the core of Linear Attention: how it achieves infinite context lengths with global awareness but constant memory usage! We explore state accumulation mechanics, the evolution from Softmax to https://t.co/gY0X1nafUf" / X
https://x.com/xieenze_jr/status/1993637743960670709

Infinite Context Length with Global but Constant Attention Memory
https://hanlab.mit.edu/blog/infinite-context-length-with-global-but-constant-attention-memory

(3) Alec Helbling on X: "I'm really enjoying the diffusion model speed running literature that seems to have been spurred by REPA. The goal is to figure out how to train a reasonable quality ImageNet generator as fast as possible. It is like the nanoGPT of diffusion. https://t.co/d3K3amfHZz" / X
https://x.com/alec_helbling/status/2004554038784966837/photo/1

(3) DD Geopolitics on X: "‚ÄúIsrael is my home, Jews have been there for thousands of years.‚Äù ‚ÄúWhere were your grandparents born?‚Äù ‚ÄúPoland.‚Äù üò≠ https://t.co/TV8xvHcqNH" / X
https://x.com/DD_Geopolitics/status/2004381993749172558

Rina Luüá∑üá∫ on X: "The Soviet film explains it straight: the Zionists sacrificed their fellow believers. When asked in the 1930s, Zionist leader Weizmann said that they are the 'economic and moral dust of the old world. Only the wind will remain.' In the Warsaw Ghetto, there were Jewish policemen https://t.co/mgeTBBPEQR" / X
https://x.com/rinalu_/status/2004542023672774984

(3) Benonwine on X: "They thought they were getting a PS5 the ungrateful gits. Serves them right as well, I wouldn‚Äôt have dreamed of behaving like that as a kid. https://t.co/Y6TgTzlA1K" / X
https://x.com/benonwine/status/2004483120532451431

(3) anshuman on X: "Techniques I‚Äôd master if I wanted real speedups (not hype): Bookmark this. 1. Fused attention kernels 2. FlashAttention variants 3. Operator fusion 4. CUDA graph capture 5. Tensor parallelism 6. Pipeline parallelism 7. Decode-step kernel fusion 8. Speculative decoding internals" / X
https://x.com/athleticKoder/status/2004530262898593845

(3) Ahmad on X: "step-by-step LLM Engineering Projects each project = one concept learned the hard (i.e. real) way Tokenization &amp; Embeddings &gt; build byte-pair encoder + train your own subword vocab &gt; write a ‚Äútoken visualizer‚Äù to map words/chunks to IDs &gt; one-hot vs learned-embedding: plot" / X
https://x.com/TheAhmadOsman/status/2004467894583730648

eseckel/ai-for-grant-writing: A curated list of resources for using LLMs to develop more competitive grant applications.
https://github.com/eseckel/ai-for-grant-writing/

ekwek1/soprano: Soprano: Instant, Ultra-Realistic Text-to-Speech
https://github.com/ekwek1/soprano?tab=readme-ov-file

gemelo-ai/vocos: Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis
https://github.com/gemelo-ai/vocos

Tweet Screenshot Source Explained
https://chatgpt.com/c/694f21a2-6d00-832d-93cd-560c63893afa

sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://github.com/sihyun-yu/REPA?utm_source=chatgpt.com

d9e42b4d7163931f3689d6d6fbaa11d0-Paper-Conference.pdf
https://proceedings.iclr.cc/paper_files/paper/2025/file/d9e42b4d7163931f3689d6d6fbaa11d0-Paper-Conference.pdf?utm_source=chatgpt.com


