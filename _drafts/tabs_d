404 – Hugging Face
https://huggingface.co/datasets/kth-tmh/mm-conv-motion

CBT - Reel Text Assistance
https://chatgpt.com/g/g-p-6825c4aefc748191b31f39848a1b0ef1-cbt/c/694dca12-03d4-832b-842a-e9974106392b

(3) Home / X
https://x.com/home

(2) DailyPapers on X: "Google discovers emergent temporal abstractions in autoregressive models These models learn linearly controllable action representations in their residual streams—activating them executes long-horizon behaviors. This enables Internal RL to solve sparse-reward hierarchical tasks https://t.co/GxOObljGcB" / X
https://x.com/HuggingPapers/status/2004645512079659192

Infinite Context Length with Global but Constant Attention Memory
https://hanlab.mit.edu/blog/infinite-context-length-with-global-but-constant-attention-memory

eseckel/ai-for-grant-writing: A curated list of resources for using LLMs to develop more competitive grant applications.
https://github.com/eseckel/ai-for-grant-writing/

ekwek1/soprano: Soprano: Instant, Ultra-Realistic Text-to-Speech
https://github.com/ekwek1/soprano?tab=readme-ov-file

gemelo-ai/vocos: Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis
https://github.com/gemelo-ai/vocos

Tweet Screenshot Source Explained
https://chatgpt.com/c/694f21a2-6d00-832d-93cd-560c63893afa

sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://github.com/sihyun-yu/REPA?utm_source=chatgpt.com

d9e42b4d7163931f3689d6d6fbaa11d0-Paper-Conference.pdf
https://proceedings.iclr.cc/paper_files/paper/2025/file/d9e42b4d7163931f3689d6d6fbaa11d0-Paper-Conference.pdf?utm_source=chatgpt.com

Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://arxiv.org/pdf/2410.06940

sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://github.com/sihyun-yu/REPA

(1) Instagram
https://www.instagram.com/reels/DRH4OcZj-ys/

Linear Attention for LLMs
https://chatgpt.com/c/69558459-bf90-832b-96a7-4d15cccba19c


