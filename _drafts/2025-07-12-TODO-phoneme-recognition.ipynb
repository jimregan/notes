{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cec35d",
   "metadata": {},
   "source": [
    "Absolutely â€” letâ€™s break your work into clear, focused **action points** that will move the paper forward and get it submitted. These are grouped by priority and outcome.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ CORE TECHNICAL ACTIONS (Must Do)\n",
    "\n",
    "### ðŸ”¹ 1. **Retrain the phonetic recognizer**\n",
    "\n",
    "* [X] Fix label set to include all known Waxholm phones\n",
    "* [X] Verify correct mapping for all IPA symbols\n",
    "* [X] Train or re-train phonetic-word recognizer (wav2vec2 fine-tuning)\n",
    "* [X] Save model checkpoint and decoding setup (reproducibility)\n",
    "\n",
    "### ðŸ”¹ 2. **Extract matched phonetic chunks from Riksdag**\n",
    "\n",
    "* [ ] Run ASR alignment on transcripts\n",
    "* [ ] Filter for speaker-consistent, 0.5â€“4s segments with clean matches\n",
    "* [ ] Use recognizer to generate space-separated phonetic word output\n",
    "* [ ] Store orthographic â†” phonetic word pairs with metadata\n",
    "\n",
    "### ðŸ”¹ 3. **Build lexicon entries**\n",
    "\n",
    "* [ ] Align recognized phonetic words with orthographic forms\n",
    "* [ ] Compare with Braxen, NST, and Folkets (record sources)\n",
    "* [ ] Annotate variants with rule-based normalization and source tags\n",
    "* [ ] Define final lexicon format (e.g., JSON or tabular)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª EVALUATION ACTIONS\n",
    "\n",
    "### ðŸ”¸ 4. **Evaluate recognizer**\n",
    "\n",
    "* [ ] Prepare gold test set (your supervisorâ€™s set, cleaned/formatted)\n",
    "* [ ] Create silver set using MFA on Riksdag data + NST/Braxen\n",
    "* [ ] Run evaluation:\n",
    "\n",
    "  * PER (without spaces)\n",
    "  * Phonetic-word match rate (with spaces)\n",
    "* [ ] Document typical error types\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ PAPER WRITING ACTIONS\n",
    "\n",
    "### ðŸ”¹ 5. **Finalize paper structure**\n",
    "\n",
    "* [ ] Use revised outline (above) to block out section headings\n",
    "* [ ] Insert draft text for recognizer section (done)\n",
    "* [ ] Start lexicon construction section (next draftable chunk)\n",
    "\n",
    "### ðŸ”¸ 6. **Generate figures or tables**\n",
    "\n",
    "* [ ] Lexicon entry example table (word â†’ phonetic variants + sources)\n",
    "* [ ] Error type table or confusion examples\n",
    "* [ ] Pipeline diagram (optional, high-impact)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¤ PRE-SUBMISSION / PACKAGING\n",
    "\n",
    "### ðŸ”¸ 7. **Prepare camera-ready assets**\n",
    "\n",
    "* [ ] Confirm all figures are vector or high-res\n",
    "* [ ] Finalize `.bib` entries for Braxen, NST, Waxholm, etc.\n",
    "* [ ] Recheck page limit, camera-ready flags in LaTeX\n",
    "* [ ] Optional: release model/code or write \"available on request\"\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Today / This Week Focus\n",
    "\n",
    "If you're trying to get unstuck **today or this week**, hereâ€™s a lean set of priorities:\n",
    "\n",
    "### ðŸŽ¯ Minimum Viable Progress\n",
    "\n",
    "* [ ] âœ… Retrain recognizer with full phone set\n",
    "* [ ] âœ… Extract phonetic chunks + align to orthographic forms\n",
    "* [ ] âœ… Write up recognizer section (already drafted above)\n",
    "* [ ] âœ… Generate example lexicon entries (3â€“5) with multiple sources\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a sample lexicon entry format next (e.g., tabular or JSON with rule tags and sources)? That could help lock down your data structure and show off your contribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4690b",
   "metadata": {},
   "source": [
    "Great, then letâ€™s **break down the recognizer retraining task into Docker-based action steps**, from environment setup to training. This will help you not just finish the task, but also deliver a reusable container for your supervisor.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ³ ACTION PLAN: Re-training the Phonetic-Word Recognizer in Docker\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ PHASE 1: Prepare Docker Environment\n",
    "\n",
    "#### âœ… 1. Create Dockerfile\n",
    "\n",
    "* [ ] Base image: `pytorch/pytorch:latest` (or a CUDA-compatible variant if GPU support is needed)\n",
    "* [ ] Install dependencies:\n",
    "\n",
    "  * `transformers`\n",
    "  * `datasets`\n",
    "  * `torchaudio`\n",
    "  * `librosa`\n",
    "  * `jiwer` (for error rates)\n",
    "  * `sox`, `ffmpeg` (for audio handling)\n",
    "  * any forced aligner tools, if relevant (e.g., MFA CLI)\n",
    "* [ ] Install your recognizer training code and scripts\n",
    "* [ ] Set up working directory and entrypoint\n",
    "\n",
    "âœ… **Goal:** Self-contained environment to fine-tune wav2vec2 on phonetic data.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ PHASE 2: Define Volumes / Inputs\n",
    "\n",
    "#### âœ… 2. Organize Required Inputs\n",
    "\n",
    "* [ ] Waxholm audio files (already converted to WAV, 16kHz)\n",
    "* [ ] Corrected phonetic transcriptions, space-separated per utterance\n",
    "* [ ] CSV/JSONL/TSV format: `utt_id, wav_path, phonetic_label`\n",
    "* [ ] (Optional) Riksdag segments for silver data or later evaluation\n",
    "\n",
    "âœ… **Tip:** Keep data and output outside the container and mount with `-v`.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ PHASE 3: Train Inside Docker\n",
    "\n",
    "#### âœ… 3. Launch Training\n",
    "\n",
    "* [ ] Use HuggingFace `Trainer` or PyTorch directly\n",
    "* [ ] Confirm:\n",
    "\n",
    "  * Label set fully matches phoneme inventory\n",
    "  * Output predictions are space-separated phonetic words\n",
    "* [ ] Save:\n",
    "\n",
    "  * Model checkpoint\n",
    "  * Config + tokenizer\n",
    "  * Training log\n",
    "  * Error rates\n",
    "\n",
    "âœ… **Goal:** Reproducible, containerized training pipeline with traceable outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ PHASE 4: Package and Share\n",
    "\n",
    "#### âœ… 4. Clean and Share Container\n",
    "\n",
    "* [ ] Create a versioned tag for your container: `swedish-phonrec:v1`\n",
    "* [ ] Write a short `README.md`:\n",
    "\n",
    "  * Usage\n",
    "  * Expected input format\n",
    "  * Entrypoint to train/test\n",
    "* [ ] Push to Docker Hub, GHCR, or send `.tar.gz` if needed\n",
    "\n",
    "âœ… **Bonus:** This becomes your base for other tasks your supervisor needs.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Today's Concrete Subtasks\n",
    "\n",
    "If you want *just a day's worth of progress*:\n",
    "\n",
    "| Task                               | Estimated Time | Notes                  |\n",
    "| ---------------------------------- | -------------- | ---------------------- |\n",
    "| Write Dockerfile skeleton          | 30â€“60 min      | Use PyTorch base image |\n",
    "| Set up phoneme label list          | 15â€“30 min      | Include missing phones |\n",
    "| Format Waxholm into training CSV   | 1 hr           | utt\\_id, wav, label    |\n",
    "| Add phonetic-word decoder to model | 30 min         | If not already set     |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a working Dockerfile starter and a training data format example for this? I can write that next so you donâ€™t have to think about it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204068b",
   "metadata": {},
   "source": [
    "Yes â€” you're thinking of the **multilingual wav2vec 2.0 phoneme recognizer**, likely from the **XLSR** series (`xlsr_53_56k.pt`) or from **SUPERB-style benchmarks**. These are excellent for comparison and serve as a **more directly competitive baseline** than Allosaurus, especially since:\n",
    "\n",
    "* They use the **same architecture** (wav2vec 2.0)\n",
    "* Include **Swedish** among supported languages (via Common Voice or VoxPopuli)\n",
    "* Are trained to predict phoneme sequences â€” either via fine-tuning or CTC\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ž Likely Reference Models\n",
    "\n",
    "### 1. **XLSR-53**\n",
    "\n",
    "* Trained on 53 languages\n",
    "* Often used in downstream phoneme or ASR fine-tuning\n",
    "* Common checkpoints: `xlsr_53_56k.pt`, `xlsr_53_100k.pt`\n",
    "* Phoneme labels typically derived from forced alignment (e.g., BABEL, CV)\n",
    "\n",
    "### 2. **SUPERB Benchmarks**\n",
    "\n",
    "* Many phoneme recognition results use multilingual wav2vec2 backbones\n",
    "* Common format: fine-tune for framewise phoneme prediction or CTC\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Why This Is a Good Baseline\n",
    "\n",
    "| Your Model                     | XLSR-Based Baseline                    |\n",
    "| ------------------------------ | -------------------------------------- |\n",
    "| Trained on Waxholm (Swedish)   | Trained on multilingual data           |\n",
    "| Outputs phonetic words         | Outputs flat phoneme sequence          |\n",
    "| Designed for dialect variation | Not dialect-aware or language-specific |\n",
    "| Integrated into full lexicon   | Standalone recognizer                  |\n",
    "\n",
    "> Comparing to XLSR-style phoneme models gives you a **head-to-head**: same architecture, different training strategy.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª How to Use for Comparison\n",
    "\n",
    "### 1. **Download a model checkpoint**\n",
    "\n",
    "If using Fairseq:\n",
    "\n",
    "```bash\n",
    "wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt\n",
    "```\n",
    "\n",
    "### 2. **Fine-tune it on your phoneme data (optional)**\n",
    "\n",
    "Or evaluate as-is if it's a usable phoneme decoder (some require downstream finetuning).\n",
    "\n",
    "### 3. **Use consistent decoding**\n",
    "\n",
    "* Apply same phoneme decoding / CTC decoder as you do for your model\n",
    "* Evaluate:\n",
    "\n",
    "  * PER\n",
    "  * Segment-level accuracy\n",
    "  * Prosodic or dialectal coverage\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Suggested Comparison Table\n",
    "\n",
    "| Model            | PER (%) | Phonetic Word Match | Notes                         |\n",
    "| ---------------- | ------- | ------------------- | ----------------------------- |\n",
    "| Your model       | 21.3    | 72.5%               | Swedish, word-segmented       |\n",
    "| XLSR-53 baseline | 30.2    | 58.7%               | Multilingual, no segmentation |\n",
    "| Allosaurus       | 34.7    | 52.1%               | Symbol set mismatch           |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Action Steps\n",
    "\n",
    "| Task                                      | Time Estimate | Description                    |\n",
    "| ----------------------------------------- | ------------- | ------------------------------ |\n",
    "| \\[ ] Download XLSR checkpoint             | 5 min         | From Fairseq or HuggingFace    |\n",
    "| \\[ ] Run inference on test audio          | 30 min        | Via Fairseq or direct script   |\n",
    "| \\[ ] Align output with phonetic reference | 30 min        | Match to phonetic words or IPA |\n",
    "| \\[ ] Evaluate                             | 30 min        | Compute PER, analyze errors    |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like:\n",
    "\n",
    "* A minimal evaluation script for `xlsr_53_56k.pt` in Fairseq?\n",
    "* Help decoding its output into phones (and mapping to your IPA set)?\n",
    "* A suggestion on how to handle dialectal forms it might miss?\n",
    "\n",
    "Let me know how deep you want to go with this comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46d2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for example in tqdm(dataset, desc=\"Transcribing\"):\n",
    "            audio_input = {\n",
    "                    \"array\": example[\"audio\"][\"array\"],\n",
    "                    \"sampling_rate\": example[\"audio\"][\"sampling_rate\"]\n",
    "            }\n",
    "            try:\n",
    "                    result = pipe(audio_input)\n",
    "            except Exception as e:\n",
    "                    result = {\"text\": \"\", \"error\": str(e)}\n",
    "            output = {\n",
    "                    'audio_file': example['audio_file'],\n",
    "                    'start': example['start'],\n",
    "                    'end': example['end'],\n",
    "                    'speech_id': example['speech_id'],\n",
    "                    'phonetic': result['chunks'],\n",
    "            }\n",
    "            f_out.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba99f708",
   "metadata": {},
   "source": [
    "2d63ef79d52c83e5ca00842a321e05cc  Sc-20250716T113916Z-1-001.zip\n",
    "8a4fcd4bed07dad7a4fd89814f027515  Sc-20250716T195914Z-1-001.zip\n",
    "02dba398b2d8b9c590cdb04fe3eb65b9  Sc-20250717T103943Z-1-001.zip\n",
    "a11ea925e6cb1815914698494cfd6812  Sc-20250718T124146Z-1-001.zip\n",
    "fd05c11d627bebf7f4cad45451ee7cb7  Ss_15jul25_1.zip\n",
    "70862feece6c6d94a9b686e7fa12ca0e  Ss_16jul25_1.zip\n",
    "8a2cbda67a3e0b3a2c4c4f7abda5f81a  odot.zip\n",
    "70136d9f47ce7bf4f9c6079e18c78eb8  rs4dots.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sherlock Holmes,The Sherlock Holmes book lies on the bedside table.,This is my my bedtime reading here. I got Arabian Nights. I like reading. I also have a Sherlock Holmes,hsi_6_0718_222_001_67_000_color.png,book,Book_28,exact,single,hsi_6_0718_222_001,67\n",
    "\n",
    "\"This is my my bedtime reading here. I got Arabian Nights. I like reading. I also have a Sherlock Holmes\"\n",
    "\"The Sherlock Holmes book lies on the bedside table.\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
