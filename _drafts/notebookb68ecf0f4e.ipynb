{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544772dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:04:13.687393Z",
     "iopub.status.busy": "2025-11-06T17:04:13.686699Z",
     "iopub.status.idle": "2025-11-06T17:04:31.455227Z",
     "shell.execute_reply": "2025-11-06T17:04:31.454120Z"
    },
    "papermill": {
     "duration": 17.773971,
     "end_time": "2025-11-06T17:04:31.457052",
     "exception": false,
     "start_time": "2025-11-06T17:04:13.683081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\r\n",
      "Preconfiguring packages ...\r\n",
      "Selecting previously unselected package libtext-iconv-perl.\r\n",
      "(Reading database ... 128639 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-libtext-iconv-perl_1.7-7build3_amd64.deb ...\r\n",
      "Unpacking libtext-iconv-perl (1.7-7build3) ...\r\n",
      "Selecting previously unselected package dictionaries-common.\r\n",
      "Preparing to unpack .../1-dictionaries-common_1.28.14_all.deb ...\r\n",
      "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\r\n",
      "Unpacking dictionaries-common (1.28.14) ...\r\n",
      "Selecting previously unselected package hunspell-en-us.\r\n",
      "Preparing to unpack .../2-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\r\n",
      "Unpacking hunspell-en-us (1:2020.12.07-2) ...\r\n",
      "Selecting previously unselected package libhunspell-1.7-0:amd64.\r\n",
      "Preparing to unpack .../3-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\r\n",
      "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\r\n",
      "Selecting previously unselected package hunspell.\r\n",
      "Preparing to unpack .../4-hunspell_1.7.0-4build1_amd64.deb ...\r\n",
      "Unpacking hunspell (1.7.0-4build1) ...\r\n",
      "Selecting previously unselected package libhunspell-dev:amd64.\r\n",
      "Preparing to unpack .../5-libhunspell-dev_1.7.0-4build1_amd64.deb ...\r\n",
      "Unpacking libhunspell-dev:amd64 (1.7.0-4build1) ...\r\n",
      "Setting up libtext-iconv-perl (1.7-7build3) ...\r\n",
      "Setting up dictionaries-common (1.28.14) ...\r\n",
      "Setting up hunspell-en-us (1:2020.12.07-2) ...\r\n",
      "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\r\n",
      "Setting up libhunspell-dev:amd64 (1.7.0-4build1) ...\r\n",
      "Setting up hunspell (1.7.0-4build1) ...\r\n",
      "Processing triggers for man-db (2.10.2-1) ...\r\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
      "\r\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
      "\r\n",
      "Processing triggers for dictionaries-common (1.28.14) ...\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get -qq update\n",
    "!apt-get -qq install -y hunspell git libhunspell-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5daa9c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:04:31.465805Z",
     "iopub.status.busy": "2025-11-06T17:04:31.465486Z",
     "iopub.status.idle": "2025-11-06T17:04:40.938247Z",
     "shell.execute_reply": "2025-11-06T17:04:40.937079Z"
    },
    "papermill": {
     "duration": 9.478956,
     "end_time": "2025-11-06T17:04:40.939996",
     "exception": false,
     "start_time": "2025-11-06T17:04:31.461040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hunspell\r\n",
      "  Downloading hunspell-0.5.5.tar.gz (34 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: hunspell\r\n",
      "  Building wheel for hunspell (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for hunspell: filename=hunspell-0.5.5-cp311-cp311-linux_x86_64.whl size=66313 sha256=2030452264a0eab0b8ccd54afee50c20e1b8be3122809398da7216f9a2d39349\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/41/b3/14ebfe8dfb3116e3f1ab55ff0db766d1ef033b6842ccc67e24\r\n",
      "Successfully built hunspell\r\n",
      "Installing collected packages: hunspell\r\n",
      "Successfully installed hunspell-0.5.5\r\n"
     ]
    }
   ],
   "source": [
    "!pip install hunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42ef632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:04:40.948988Z",
     "iopub.status.busy": "2025-11-06T17:04:40.948615Z",
     "iopub.status.idle": "2025-11-06T17:04:41.067062Z",
     "shell.execute_reply": "2025-11-06T17:04:41.065894Z"
    },
    "papermill": {
     "duration": 0.125287,
     "end_time": "2025-11-06T17:04:41.069013",
     "exception": false,
     "start_time": "2025-11-06T17:04:40.943726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/dictionaries/dictionaries\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PWD/dictionaries/dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1361a41c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T17:04:41.078209Z",
     "iopub.status.busy": "2025-11-06T17:04:41.077890Z",
     "iopub.status.idle": "2025-11-06T17:04:51.929909Z",
     "shell.execute_reply": "2025-11-06T17:04:51.928688Z"
    },
    "papermill": {
     "duration": 10.859101,
     "end_time": "2025-11-06T17:04:51.932050",
     "exception": false,
     "start_time": "2025-11-06T17:04:41.072949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dictionaries'...\r\n",
      "remote: Enumerating objects: 11042, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (1506/1506), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (213/213), done.\u001b[K\r\n",
      "remote: Total 11042 (delta 1318), reused 1303 (delta 1293), pack-reused 9536 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (11042/11042), 101.83 MiB | 18.76 MiB/s, done.\r\n",
      "Resolving deltas: 100% (9492/9492), done.\r\n",
      "Updating files: 100% (570/570), done.\r\n",
      "/kaggle/working\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/wooorm/dictionaries\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7fc8c9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-06T17:04:51.947711Z",
     "iopub.status.busy": "2025-11-06T17:04:51.947374Z",
     "iopub.status.idle": "2025-11-07T01:12:51.362870Z",
     "shell.execute_reply": "2025-11-07T01:12:51.361710Z"
    },
    "papermill": {
     "duration": 29279.425083,
     "end_time": "2025-11-07T01:12:51.364394",
     "exception": false,
     "start_time": "2025-11-06T17:04:51.939311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cze: 410 tokens via cs\n",
      "dan: 1941 tokens via da\n",
      "eng: 19790 tokens via en\n",
      "skip fin: dict not found for ['fi']\n",
      "fre: 5533 tokens via fr\n",
      "gre: 861 tokens via el\n",
      "hun: 398 tokens via hu\n",
      "ita: 3093 tokens via it\n",
      "lat: 3808 tokens via la\n",
      "pol: 753 tokens via pl\n",
      "por: 479 tokens via pt\n",
      "rus: 1362 tokens via ru\n",
      "spa: 2450 tokens via es\n",
      "swe: 704331 tokens via sv\n",
      "tur: 806 tokens via tr\n",
      "ukr: 142 tokens via uk\n",
      "Wrote hunspell_results.tsv with 745,764 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_code</th>\n",
       "      <th>word</th>\n",
       "      <th>status</th>\n",
       "      <th>suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cze</td>\n",
       "      <td>Adamkova</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cze</td>\n",
       "      <td>Allertova</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Albertova, Gallertová, Albertov, Tolerovat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cze</td>\n",
       "      <td>Babiš</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cze</td>\n",
       "      <td>Balazova</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Balasova, Balažova, Balažová, Balážova, Balážo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cze</td>\n",
       "      <td>Banik</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cze</td>\n",
       "      <td>Banska</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bánská, Banka, Baska, Blanska, Banika, Baníka,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cze</td>\n",
       "      <td>Baranka</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bartecko</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bartečko, Barteckou, Bartesko, Bartecký, Barte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bartok</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bartosak</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bartošák, Barto sak, Barto-sak, Bartok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cze</td>\n",
       "      <td>Batovska</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bátovská, Bítovska, Baťovská</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bejlek</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cze</td>\n",
       "      <td>Benice</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bezpecnostni</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bezpečnostní</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bittová</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cze</td>\n",
       "      <td>Blazíková</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Blažíkova, Blažíková, Bazíková, Blaníková, Bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cze</td>\n",
       "      <td>Boleslav</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cze</td>\n",
       "      <td>Boril</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bořil, Borli, Borik, Borl, Briol, Borel, Bortl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cze</td>\n",
       "      <td>Borivoj</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Bořivoj, Borisov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cze</td>\n",
       "      <td>Bouzkova</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_code          word status  \\\n",
       "0        cze      Adamkova     OK   \n",
       "1        cze     Allertova   MISS   \n",
       "2        cze         Babiš     OK   \n",
       "3        cze      Balazova   MISS   \n",
       "4        cze         Banik     OK   \n",
       "5        cze        Banska   MISS   \n",
       "6        cze       Baranka     OK   \n",
       "7        cze      Bartecko   MISS   \n",
       "8        cze        Bartok     OK   \n",
       "9        cze      Bartosak   MISS   \n",
       "10       cze      Batovska   MISS   \n",
       "11       cze        Bejlek     OK   \n",
       "12       cze        Benice     OK   \n",
       "13       cze  Bezpecnostni   MISS   \n",
       "14       cze       Bittová     OK   \n",
       "15       cze     Blazíková   MISS   \n",
       "16       cze      Boleslav     OK   \n",
       "17       cze         Boril   MISS   \n",
       "18       cze       Borivoj   MISS   \n",
       "19       cze      Bouzkova     OK   \n",
       "\n",
       "                                          suggestions  \n",
       "0                                                      \n",
       "1          Albertova, Gallertová, Albertov, Tolerovat  \n",
       "2                                                      \n",
       "3   Balasova, Balažova, Balažová, Balážova, Balážo...  \n",
       "4                                                      \n",
       "5   Bánská, Banka, Baska, Blanska, Banika, Baníka,...  \n",
       "6                                                      \n",
       "7   Bartečko, Barteckou, Bartesko, Bartecký, Barte...  \n",
       "8                                                      \n",
       "9              Bartošák, Barto sak, Barto-sak, Bartok  \n",
       "10                       Bátovská, Bítovska, Baťovská  \n",
       "11                                                     \n",
       "12                                                     \n",
       "13                                       Bezpečnostní  \n",
       "14                                                     \n",
       "15  Blažíkova, Blažíková, Bazíková, Blaníková, Bla...  \n",
       "16                                                     \n",
       "17  Bořil, Borli, Borik, Borl, Briol, Borel, Bortl...  \n",
       "18                                   Bořivoj, Borisov  \n",
       "19                                                     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, glob, pandas as pd, hunspell\n",
    "\n",
    "FILE_DIR = \"/kaggle/input/split-braxen-by-language\"\n",
    "DICT_ROOT = \"/kaggle/working/dictionaries/dictionaries\"  # as you showed\n",
    "OUT_TSV = \"hunspell_results.tsv\"\n",
    "MIN_ENTRIES = 100\n",
    "SKIP_CODES = {\"afr\",\"asi\",\"aus\",\"sla\",\"mix\",\"fisa\"}\n",
    "\n",
    "# find index.aff/index.dic pairs under your DICT_ROOT\n",
    "pairs = {}\n",
    "for aff in glob.glob(os.path.join(DICT_ROOT, \"*\", \"index.aff\")):\n",
    "    d = os.path.dirname(aff)\n",
    "    dic = os.path.join(d, \"index.dic\")\n",
    "    code = os.path.basename(d)\n",
    "    if os.path.isfile(dic):\n",
    "        pairs[code] = (dic, aff)\n",
    "\n",
    "CODE2DICT = {\n",
    "    \"lat\":[\"la\"],\n",
    "    \"swe\":[\"sv\"],\n",
    "    \"nor\":[\"nb\"],\n",
    "    \"dan\":[\"da\"],\n",
    "    \"isl\":[\"is\"],\n",
    "    \"fin\":[\"fi\"],\n",
    "    \"est\":[\"et\"],\n",
    "    \"lav\":[\"lv\"],\n",
    "    \"lit\":[\"lt\"],\n",
    "    \"pol\":[\"pl\"],\n",
    "    \"cze\":[\"cs\"],\n",
    "    \"slk\":[\"sk\"],\n",
    "    \"slv\":[\"sl\"],\n",
    "    \"hrv\":[\"hr\"],\n",
    "    \"srp\":[\"sr-Latn\"],\n",
    "    \"bos\":[\"bs\"],\n",
    "    \"mkd\":[\"mk\"],\n",
    "    \"bul\":[\"bg\"],\n",
    "    \"ukr\":[\"uk\"],\n",
    "    \"rus\":[\"ru\"],\n",
    "    \"deu\":[\"de\"],\n",
    "    \"nld\":[\"nl\",\"dut\"],\n",
    "    \"eng\":[\"en\",\"en-GB\",\"en-CA\",\"en-AU\",\"en-ZA\"],\n",
    "    \"fre\":[\"fr\"],\n",
    "    \"ita\":[\"it\"],\n",
    "    \"spa\":[\"es\",\"es-MX\",\"es-AR\",\"es-CL\",\"es-ES\"],\n",
    "    \"por\":[\"pt\",\"pt-PT\"],\n",
    "    \"rom\":[\"ro\"],\n",
    "    \"hun\":[\"hu\"],\n",
    "    \"tur\":[\"tr\"],\n",
    "    \"gre\":[\"el\"],\n",
    "}\n",
    "\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_][\\w’'\\-\\u2011\\u2013\\u2014]*\", re.UNICODE)\n",
    "def tokenize(t): return WORD_RE.findall(t)\n",
    "def read_text(p):\n",
    "    b = open(p,\"rb\").read()\n",
    "    try: return b.decode(\"utf-8\")\n",
    "    except UnicodeDecodeError: return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "def file_code(p):\n",
    "    b = os.path.basename(p)\n",
    "    return b[len(\"braxen-\"):-4] if b.startswith(\"braxen-\") else b\n",
    "\n",
    "def load_hs(dict_codes):\n",
    "    for c in dict_codes:\n",
    "        if c in pairs:\n",
    "            dic, aff = pairs[c]\n",
    "            return hunspell.HunSpell(dic, aff), c\n",
    "    return None, None\n",
    "\n",
    "files = [os.path.join(FILE_DIR, f) for f in os.listdir(FILE_DIR) if f.startswith(\"braxen-\") and f.endswith(\".txt\")]\n",
    "\n",
    "file_words, file_sizes = {}, {}\n",
    "for p in files:\n",
    "    ws = set(tokenize(read_text(p)))\n",
    "    file_words[p] = ws\n",
    "    file_sizes[p] = len(ws)\n",
    "\n",
    "candidates = []\n",
    "for p, n in file_sizes.items():\n",
    "    code = file_code(p)\n",
    "    if n >= MIN_ENTRIES and code not in SKIP_CODES and code in CODE2DICT:\n",
    "        candidates.append(p)\n",
    "\n",
    "rows = []\n",
    "for p in sorted(candidates):\n",
    "    code = file_code(p)\n",
    "    hs, used = load_hs(CODE2DICT[code])\n",
    "    if not hs:\n",
    "        print(f\"skip {code}: dict not found for {CODE2DICT[code]}\")\n",
    "        continue\n",
    "    words = sorted(file_words[p])\n",
    "    print(f\"{code}: {len(words)} tokens via {used}\")\n",
    "    for w in words:\n",
    "        if not w: \n",
    "            continue\n",
    "        if all(ord(ch) < 128 for ch in w) and len(w) < 2:\n",
    "            continue\n",
    "        wcheck = w.replace(\"ö\",\"ø\").replace(\"Ö\",\"Ø\") if code in {\"nor\",\"dan\"} else w\n",
    "        if hs.spell(wcheck):\n",
    "            rows.append((code, w, \"OK\", \"\"))\n",
    "        else:\n",
    "            sugs = \", \".join(hs.suggest(wcheck))\n",
    "            rows.append((code, w, \"MISS\", sugs))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"file_code\",\"word\",\"status\",\"suggestions\"])\n",
    "df.to_csv(OUT_TSV, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote {OUT_TSV} with {len(df):,} rows\")\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 270049783,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29323.516792,
   "end_time": "2025-11-07T01:12:52.395881",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-06T17:04:08.879089",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
