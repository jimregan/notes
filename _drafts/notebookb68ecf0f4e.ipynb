{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":270049783,"sourceType":"kernelVersion"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install hunspell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:30:53.843930Z","iopub.execute_input":"2025-10-22T22:30:53.844107Z","iopub.status.idle":"2025-10-22T22:30:57.615453Z","shell.execute_reply.started":"2025-10-22T22:30:53.844089Z","shell.execute_reply":"2025-10-22T22:30:57.614521Z"}},"outputs":[{"name":"stdout","text":"Collecting hunspell\n  Using cached hunspell-0.5.5.tar.gz (34 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: hunspell\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for hunspell (setup.py) ... \u001b[?25lerror\n\u001b[31m  ERROR: Failed building wheel for hunspell\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25h  Running setup.py clean for hunspell\nFailed to build hunspell\n\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (hunspell)\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!apt-get -qq update\n!apt-get -qq install -y hunspell git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:32:56.223311Z","iopub.execute_input":"2025-10-22T22:32:56.223654Z","iopub.status.idle":"2025-10-22T22:33:12.053854Z","shell.execute_reply.started":"2025-10-22T22:32:56.223630Z","shell.execute_reply":"2025-10-22T22:33:12.052767Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nPreconfiguring packages ...\nSelecting previously unselected package libtext-iconv-perl.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../libtext-iconv-perl_1.7-7build3_amd64.deb ...\nUnpacking libtext-iconv-perl (1.7-7build3) ...\nSelecting previously unselected package dictionaries-common.\nPreparing to unpack .../dictionaries-common_1.28.14_all.deb ...\nAdding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\nUnpacking dictionaries-common (1.28.14) ...\nSelecting previously unselected package hunspell-en-us.\nPreparing to unpack .../hunspell-en-us_1%3a2020.12.07-2_all.deb ...\nUnpacking hunspell-en-us (1:2020.12.07-2) ...\nSelecting previously unselected package libhunspell-1.7-0:amd64.\nPreparing to unpack .../libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\nUnpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\nSelecting previously unselected package hunspell.\nPreparing to unpack .../hunspell_1.7.0-4build1_amd64.deb ...\nUnpacking hunspell (1.7.0-4build1) ...\nSetting up libtext-iconv-perl (1.7-7build3) ...\nSetting up dictionaries-common (1.28.14) ...\nSetting up hunspell-en-us (1:2020.12.07-2) ...\nSetting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\nSetting up hunspell (1.7.0-4build1) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nProcessing triggers for dictionaries-common (1.28.14) ...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os, re, subprocess, shlex\nimport pandas as pd\nfrom collections import defaultdict\n\n# CONFIG\nFILE_DIR = \"/kaggle/input/split-braxen-by-language\"\nMIN_ENTRIES = 1000\nSKIP_CODES = {\"afr\",\"asi\",\"aus\",\"sla\",\"mix\",\"fisa\"}\nOUT_TSV  = \"hunspell_results.tsv\"\nSAMPLES_PER_FILE = None  # set e.g. 500 to sample per file during testing\n\n# Get dictionaries\nif not os.path.exists(\"dictionaries\"):\n    !git clone -q https://github.com/wooorm/dictionaries.git\n\nDICT_ROOT = os.path.abspath(\"dictionaries\")\n\n# Map your codes -> hunspell dictionary basenames (relative to DICT_ROOT)\n# You can give multiple dicts; they will be combined (e.g. nb+nn, pt+pt-PT)\nCODE2DICT = {\n    \"swe\": [\"sv\"],\n    \"nor\": [\"nb\",\"nn\"],        # Norwegian Bokmål + Nynorsk\n    \"dan\": [\"da\"],\n    \"isl\": [\"is\"],\n    \"fin\": [\"fi\"],\n    \"est\": [\"et\"],\n    \"lav\": [\"lv\"],\n    \"lit\": [\"lt\"],\n    \"pol\": [\"pl\"],\n    \"cze\": [\"cs\"],\n    \"slk\": [\"sk\"],\n    \"slv\": [\"sl\"],\n    \"hrv\": [\"hr\"],\n    \"srp\": [\"sr-Latn\"],        # Latin Serbian\n    \"bos\": [\"bs\"],\n    \"mkd\": [\"mk\"],\n    \"bul\": [\"bg\"],\n    \"ukr\": [\"uk\"],\n    \"rus\": [\"ru\"],\n    \"deu\": [\"de\"],             # de, de-AT, de-CH also available\n    \"nld\": [\"nl\"], \"dut\": [\"nl\"],\n    \"eng\": [\"en\",\"en-GB\",\"en-CA\",\"en-AU\",\"en-ZA\"],  # combine EN variants\n    \"fre\": [\"fr\"],\n    \"ita\": [\"it\"],\n    \"spa\": [\"es\",\"es-MX\",\"es-AR\",\"es-CL\",\"es-ES\"],  # add variants to taste\n    \"por\": [\"pt\",\"pt-PT\"],\n    \"rom\": [\"ro\"],\n    \"hun\": [\"hu\"],\n    \"tur\": [\"tr\"],\n    \"gre\": [\"el\"],\n    \"wel\": [\"cy\"],\n    \"gle\": [\"ga\"],  # if you have it\n    # add more as needed; see `ls dictionaries` for available dirs\n}\n\nWORD_RE = re.compile(r\"[^\\W\\d_][\\w’'\\-\\u2011\\u2013\\u2014]*\", flags=re.UNICODE)\n\ndef tokenize(text: str):\n    return WORD_RE.findall(text)\n\ndef read_text(path):\n    with open(path, \"rb\") as f:\n        b = f.read()\n    try:\n        return b.decode(\"utf-8\")\n    except UnicodeDecodeError:\n        return b.decode(\"utf-8\", errors=\"ignore\")\n\ndef file_code_from_name(path):\n    base = os.path.basename(path)\n    return base[len(\"braxen-\"):-len(\".txt\")] if base.startswith(\"braxen-\") else base\n\ndef list_candidate_files():\n    files = [os.path.join(FILE_DIR, f) for f in os.listdir(FILE_DIR)\n             if f.startswith(\"braxen-\") and f.endswith(\".txt\")]\n    sizes = {}\n    uniqs = {}\n    for p in files:\n        ws = set(tokenize(read_text(p)))\n        sizes[p] = len(ws)\n        uniqs[p] = ws\n    keep = []\n    for p, n in sizes.items():\n        code = file_code_from_name(p)\n        if n >= MIN_ENTRIES and code not in SKIP_CODES and code in CODE2DICT:\n            keep.append((p, uniqs[p]))\n    return keep\n\ndef dict_args_for(code):\n    dicts = CODE2DICT.get(code, [])\n    # Validate presence of .aff/.dic, build full dict basenames for -d\n    ok = []\n    for d in dicts:\n        base = os.path.join(DICT_ROOT, d, d)\n        if os.path.exists(base + \".aff\") and os.path.exists(base + \".dic\"):\n            ok.append(base)\n    return ok\n\ndef run_hunspell(words, dict_bases, code):\n    # hunspell -a -i utf-8 -d dict[,dict2,...]\n    # Note: pass dict_bases comma-separated; hunspell finds .aff/.dic automatically.\n    if not dict_bases:\n        return []\n    cmd = [\"hunspell\",\"-a\",\"-i\",\"utf-8\",\"-d\", \",\".join(dict_bases)]\n    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    # Input: one word per line\n    stdin_data = \"\\n\".join(words) + \"\\n\"\n    out, err = p.communicate(stdin_data, timeout=300)\n\n    results = []\n    # Output format: first line is version/banner, then one response per input token\n    # Lines:\n    #  *  => OK\n    #  +  => OK (root/compounded)\n    #  & word count suggestions: sug, sug, ...\n    #  # => unknown, no suggestions\n    #  ? => guess (rare)\n    lines = out.splitlines()\n    # Drop banner lines (start with '@' or '# version info) until first response marker\n    it = iter(lines)\n    # consume until we see a marker line\n    first_resp_seen = False\n    clean_lines = []\n    for ln in it:\n        if ln and ln[0] in {\"*\",\"+\",\"&\",\"#\",\"?\"}:\n            first_resp_seen = True\n            clean_lines.append(ln)\n            break\n    if first_resp_seen:\n        clean_lines.extend(list(it))\n\n    # Pair responses to inputs 1:1\n    # hunspell returns exactly one response per input token in -a mode.\n    if len(clean_lines) != len(words):\n        # fallback: try to realign by skipping empty/comment lines\n        cl = [ln for ln in clean_lines if ln and ln[0] in {\"*\",\"+\",\"&\",\"#\",\"?\"}]\n        clean_lines = cl + [\"#\"]*(len(words)-len(cl)) if len(cl) < len(words) else cl[:len(words)]\n\n    for w, ln in zip(words, clean_lines):\n        if not ln:\n            results.append((w,\"ERROR\",\"\"))\n            continue\n        tag = ln[0]\n        if tag in {\"*\",\"+\"}:\n            results.append((w,\"OK\",\"\"))\n        elif tag == \"&\":\n            # & word count offset: sug, sug, sug\n            parts = ln.split(\":\")\n            sugs = parts[1].strip() if len(parts) > 1 else \"\"\n            results.append((w,\"SUGGEST\", sugs))\n        elif tag in {\"#\",\"?\"}:\n            results.append((w,\"SUGGEST\",\"\"))  # unknown, no suggestions provided\n        else:\n            results.append((w,\"UNKNOWN\", ln))\n    return results\n\ncandidates = list_candidate_files()\nrows = []\n\nfor path, uniq in candidates:\n    code = file_code_from_name(path)\n\n    # Ignore the Norwegian/Danish ö→ø quirk: replace ö with ø only for checking,\n    # but keep original form in output.\n    def normalize_for_check(w):\n        if code in {\"nor\",\"dan\"}:\n            return w.replace(\"ö\",\"ø\").replace(\"Ö\",\"Ø\")\n        return w\n\n    words = sorted(uniq)\n    if SAMPLES_PER_FILE:\n        words = words[:SAMPLES_PER_FILE]\n\n    dict_bases = dict_args_for(code)\n    if not dict_bases:\n        # no dictionary, skip\n        continue\n\n    # Feed a deduped list to hunspell\n    check_words = [normalize_for_check(w) for w in words]\n    results = run_hunspell(check_words, dict_bases, code)\n\n    # Attach original tokens and file code\n    for orig, (checked, status, sugs) in zip(words, results):\n        rows.append({\n            \"file_code\": code,\n            \"word\": orig,\n            \"status\": status,          # OK | SUGGEST | UNKNOWN | ERROR\n            \"suggestions\": sugs\n        })\n\ndf = pd.DataFrame(rows, columns=[\"file_code\",\"word\",\"status\",\"suggestions\"])\ndf.to_csv(OUT_TSV, sep=\"\\t\", index=False)\nprint(f\"Wrote {OUT_TSV} with {len(df):,} rows\")\n\n# small preview\ndf.head(30)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T22:34:16.814519Z","iopub.execute_input":"2025-10-22T22:34:16.814814Z","iopub.status.idle":"2025-10-22T22:34:20.633621Z","shell.execute_reply.started":"2025-10-22T22:34:16.814794Z","shell.execute_reply":"2025-10-22T22:34:20.632717Z"}},"outputs":[{"name":"stdout","text":"Wrote hunspell_results.tsv with 0 rows\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [file_code, word, status, suggestions]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_code</th>\n      <th>word</th>\n      <th>status</th>\n      <th>suggestions</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5}]}