{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- REAL HUNSPELL + wooorm dictionaries, Kaggle-friendly TSV output ---\n\n!apt-get -qq update\n!apt-get -qq install -y hunspell git >/dev/null\n\nimport os, re, subprocess, shlex\nimport pandas as pd\nfrom collections import defaultdict\n\n# CONFIG\nFILE_DIR = \"/kaggle/input/split-braxen-by-language\"\nMIN_ENTRIES = 1000\nSKIP_CODES = {\"afr\",\"asi\",\"aus\",\"sla\",\"mix\",\"fisa\"}\nOUT_TSV  = \"hunspell_results.tsv\"\nSAMPLES_PER_FILE = None  # set e.g. 500 to sample per file during testing\n\n# Get dictionaries\nif not os.path.exists(\"dictionaries\"):\n    !git clone -q https://github.com/wooorm/dictionaries.git\n\nDICT_ROOT = os.path.abspath(\"dictionaries\")\n\n# Map your codes -> hunspell dictionary basenames (relative to DICT_ROOT)\n# You can give multiple dicts; they will be combined (e.g. nb+nn, pt+pt-PT)\nCODE2DICT = {\n    \"swe\": [\"sv\"],\n    \"nor\": [\"nb\",\"nn\"],        # Norwegian Bokmål + Nynorsk\n    \"dan\": [\"da\"],\n    \"isl\": [\"is\"],\n    \"fin\": [\"fi\"],\n    \"est\": [\"et\"],\n    \"lav\": [\"lv\"],\n    \"lit\": [\"lt\"],\n    \"pol\": [\"pl\"],\n    \"cze\": [\"cs\"],\n    \"slk\": [\"sk\"],\n    \"slv\": [\"sl\"],\n    \"hrv\": [\"hr\"],\n    \"srp\": [\"sr-Latn\"],        # Latin Serbian\n    \"bos\": [\"bs\"],\n    \"mkd\": [\"mk\"],\n    \"bul\": [\"bg\"],\n    \"ukr\": [\"uk\"],\n    \"rus\": [\"ru\"],\n    \"deu\": [\"de\"],             # de, de-AT, de-CH also available\n    \"nld\": [\"nl\"], \"dut\": [\"nl\"],\n    \"eng\": [\"en\",\"en-GB\",\"en-CA\",\"en-AU\",\"en-ZA\"],  # combine EN variants\n    \"fre\": [\"fr\"],\n    \"ita\": [\"it\"],\n    \"spa\": [\"es\",\"es-MX\",\"es-AR\",\"es-CL\",\"es-ES\"],  # add variants to taste\n    \"por\": [\"pt\",\"pt-PT\"],\n    \"rom\": [\"ro\"],\n    \"hun\": [\"hu\"],\n    \"tur\": [\"tr\"],\n    \"gre\": [\"el\"],\n    \"wel\": [\"cy\"],\n    \"gle\": [\"ga\"],  # if you have it\n    # add more as needed; see `ls dictionaries` for available dirs\n}\n\nWORD_RE = re.compile(r\"[^\\W\\d_][\\w’'\\-\\u2011\\u2013\\u2014]*\", flags=re.UNICODE)\n\ndef tokenize(text: str):\n    return WORD_RE.findall(text)\n\ndef read_text(path):\n    with open(path, \"rb\") as f:\n        b = f.read()\n    try:\n        return b.decode(\"utf-8\")\n    except UnicodeDecodeError:\n        return b.decode(\"utf-8\", errors=\"ignore\")\n\ndef file_code_from_name(path):\n    base = os.path.basename(path)\n    return base[len(\"braxen-\"):-len(\".txt\")] if base.startswith(\"braxen-\") else base\n\ndef list_candidate_files():\n    files = [os.path.join(FILE_DIR, f) for f in os.listdir(FILE_DIR)\n             if f.startswith(\"braxen-\") and f.endswith(\".txt\")]\n    sizes = {}\n    uniqs = {}\n    for p in files:\n        ws = set(tokenize(read_text(p)))\n        sizes[p] = len(ws)\n        uniqs[p] = ws\n    keep = []\n    for p, n in sizes.items():\n        code = file_code_from_name(p)\n        if n >= MIN_ENTRIES and code not in SKIP_CODES and code in CODE2DICT:\n            keep.append((p, uniqs[p]))\n    return keep\n\ndef dict_args_for(code):\n    dicts = CODE2DICT.get(code, [])\n    # Validate presence of .aff/.dic, build full dict basenames for -d\n    ok = []\n    for d in dicts:\n        base = os.path.join(DICT_ROOT, d, d)\n        if os.path.exists(base + \".aff\") and os.path.exists(base + \".dic\"):\n            ok.append(base)\n    return ok\n\ndef run_hunspell(words, dict_bases, code):\n    # hunspell -a -i utf-8 -d dict[,dict2,...]\n    # Note: pass dict_bases comma-separated; hunspell finds .aff/.dic automatically.\n    if not dict_bases:\n        return []\n    cmd = [\"hunspell\",\"-a\",\"-i\",\"utf-8\",\"-d\", \",\".join(dict_bases)]\n    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    # Input: one word per line\n    stdin_data = \"\\n\".join(words) + \"\\n\"\n    out, err = p.communicate(stdin_data, timeout=300)\n\n    results = []\n    # Output format: first line is version/banner, then one response per input token\n    # Lines:\n    #  *  => OK\n    #  +  => OK (root/compounded)\n    #  & word count suggestions: sug, sug, ...\n    #  # => unknown, no suggestions\n    #  ? => guess (rare)\n    lines = out.splitlines()\n    # Drop banner lines (start with '@' or '# version info) until first response marker\n    it = iter(lines)\n    # consume until we see a marker line\n    first_resp_seen = False\n    clean_lines = []\n    for ln in it:\n        if ln and ln[0] in {\"*\",\"+\",\"&\",\"#\",\"?\"}:\n            first_resp_seen = True\n            clean_lines.append(ln)\n            break\n    if first_resp_seen:\n        clean_lines.extend(list(it))\n\n    # Pair responses to inputs 1:1\n    # hunspell returns exactly one response per input token in -a mode.\n    if len(clean_lines) != len(words):\n        # fallback: try to realign by skipping empty/comment lines\n        cl = [ln for ln in clean_lines if ln and ln[0] in {\"*\",\"+\",\"&\",\"#\",\"?\"}]\n        clean_lines = cl + [\"#\"]*(len(words)-len(cl)) if len(cl) < len(words) else cl[:len(words)]\n\n    for w, ln in zip(words, clean_lines):\n        if not ln:\n            results.append((w,\"ERROR\",\"\"))\n            continue\n        tag = ln[0]\n        if tag in {\"*\",\"+\"}:\n            results.append((w,\"OK\",\"\"))\n        elif tag == \"&\":\n            # & word count offset: sug, sug, sug\n            parts = ln.split(\":\")\n            sugs = parts[1].strip() if len(parts) > 1 else \"\"\n            results.append((w,\"SUGGEST\", sugs))\n        elif tag in {\"#\",\"?\"}:\n            results.append((w,\"SUGGEST\",\"\"))  # unknown, no suggestions provided\n        else:\n            results.append((w,\"UNKNOWN\", ln))\n    return results\n\ncandidates = list_candidate_files()\nrows = []\n\nfor path, uniq in candidates:\n    code = file_code_from_name(path)\n\n    # Ignore the Norwegian/Danish ö→ø quirk: replace ö with ø only for checking,\n    # but keep original form in output.\n    def normalize_for_check(w):\n        if code in {\"nor\",\"dan\"}:\n            return w.replace(\"ö\",\"ø\").replace(\"Ö\",\"Ø\")\n        return w\n\n    words = sorted(uniq)\n    if SAMPLES_PER_FILE:\n        words = words[:SAMPLES_PER_FILE]\n\n    dict_bases = dict_args_for(code)\n    if not dict_bases:\n        # no dictionary, skip\n        continue\n\n    # Feed a deduped list to hunspell\n    check_words = [normalize_for_check(w) for w in words]\n    results = run_hunspell(check_words, dict_bases, code)\n\n    # Attach original tokens and file code\n    for orig, (checked, status, sugs) in zip(words, results):\n        rows.append({\n            \"file_code\": code,\n            \"word\": orig,\n            \"status\": status,          # OK | SUGGEST | UNKNOWN | ERROR\n            \"suggestions\": sugs\n        })\n\ndf = pd.DataFrame(rows, columns=[\"file_code\",\"word\",\"status\",\"suggestions\"])\ndf.to_csv(OUT_TSV, sep=\"\\t\", index=False)\nprint(f\"Wrote {OUT_TSV} with {len(df):,} rows\")\n\n# small preview\ndf.head(30)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}