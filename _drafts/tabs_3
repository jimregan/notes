404 – Hugging Face
https://huggingface.co/datasets/kth-tmh/mm-conv-motion

CBT - Reel Text Assistance
https://chatgpt.com/g/g-p-6825c4aefc748191b31f39848a1b0ef1-cbt/c/694dca12-03d4-832b-842a-e9974106392b

Lecture 1: The Column Space of A Contains All Vectors Ax - YouTube
https://www.youtube.com/watch?v=YiqIkSHSmyc&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k&index=4

Home / X
https://x.com/home

DailyPapers on X: "Google discovers emergent temporal abstractions in autoregressive models These models learn linearly controllable action representations in their residual streams—activating them executes long-horizon behaviors. This enables Internal RL to solve sparse-reward hierarchical tasks https://t.co/GxOObljGcB" / X
https://x.com/HuggingPapers/status/2004645512079659192

[2512.20605] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning
https://arxiv.org/abs/2512.20605

Tweet Screenshot Source Explained
https://chatgpt.com/c/694f21a2-6d00-832d-93cd-560c63893afa

sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://github.com/sihyun-yu/REPA?utm_source=chatgpt.com

d9e42b4d7163931f3689d6d6fbaa11d0-Paper-Conference.pdf
https://proceedings.iclr.cc/paper_files/paper/2025/file/d9e42b4d7163931f3689d6d6fbaa11d0-Paper-Conference.pdf?utm_source=chatgpt.com

[2410.06940] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://arxiv.org/abs/2410.06940

sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
https://github.com/sihyun-yu/REPA

Linear Attention for LLMs
https://chatgpt.com/c/69558459-bf90-832b-96a7-4d15cccba19c


