
@article{yi_efficiently_2021,
	title = {Efficiently {Fusing} {Pretrained} {Acoustic} and {Linguistic} {Encoders} for {Low}-resource {Speech} {Recognition}},
	volume = {28},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2101.06699},
	doi = {10.1109/LSP.2021.3071668},
	abstract = {End-to-end models have achieved impressive results on the task of automatic speech recognition (ASR). For low-resource ASR tasks, however, labeled data can hardly satisfy the demand of end-to-end models. Self-supervised acoustic pre-training has already shown its amazing ASR performance, while the transcription is still inadequate for language modeling in end-to-end models. In this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a pre-trained linguistic encoder (BERT) into an end-to-end ASR model. The fused model only needs to learn the transfer from speech to language during fine-tuning on limited labeled data. The length of the two modalities is matched by a monotonic attention mechanism without additional parameters. Besides, a fully connected layer is introduced for the hidden mapping between modalities. We further propose a scheduled fine-tuning strategy to preserve and utilize the text context modeling ability of the pre-trained linguistic encoder. Experiments show our effective utilizing of pre-trained modules. Our model achieves better recognition performance on CALLHOME corpus (15 hours) than other end-to-end models.},
	urldate = {2021-06-24},
	journal = {IEEE Signal Processing Letters},
	author = {Yi, Cheng and Zhou, Shiyu and Xu, Bo},
	year = {2021},
	note = {arXiv: 2101.06699},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {788--792},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\U2L8YEI5\\Yi et al. - 2021 - Efficiently Fusing Pretrained Acoustic and Linguis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4YQ7KW78\\2101.html:text/html},
}

@misc{garofolo_john_s_timit_nodate,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	urldate = {2021-06-24},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	doi = {10.35111/17GK-BN40},
	note = {Type: dataset},
}

@book{lopes_phoneme_2011,
	title = {Phoneme {Recognition} on the {TIMIT} {Database}},
	isbn = {978-953-307-996-7},
	abstract = {Open access peer-reviewed chapter},
	language = {en},
	urldate = {2021-06-24},
	publisher = {IntechOpen},
	author = {Lopes, Carla and Perdigao, Fernando},
	month = jun,
	year = {2011},
	doi = {10.5772/17600},
	annote = {Speech recognition based on phones is very attractive since it is inherently free from vocabulary limitations. Large Vocabulary ASR (LVASR) systems’ performance depends on the quality of the phone recognizer. That is why research teams continue developing phone recognizers, in order to enhance their performance as much as possible. Phone recognition is, in fact, a recurrent problem for the speech recognition community.},
	annote = {The database defines the units that can be trained and the success of the training algorithms is highly dependent on the quality and detail of the annotation of those units. Many databases are insufficiently annotated and only a few of them include labels at the phone level. So the reason why the TIMIT database (Garofolo et al., 1990) has become the database most widely used by the phone recognition research community is mainly because it is totally and manually annotated at the phone level.
Phone recognition in TIMIT has more than two decades of intense research behind it and its performance has naturally improved with time. There is a full array of systems, but with regard to evaluation they concentrate on three domains: phone segmentation, phone classification and phone recognition.},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\3UFJM87B\\Lopes and Perdigao - 2011 - Phoneme Recognition on the TIMIT Database.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MFPICHSN\\phoneme-recognition-on-the-timit-database.html:text/html},
}

@article{lee_speaker-independent_1989,
	title = {Speaker-independent phone recognition using hidden {Markov} models},
	volume = {37},
	issn = {0096-3518},
	doi = {10.1109/29.46546},
	abstract = {Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8\% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69\% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems.{\textless}{\textgreater}},
	number = {11},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Lee, K.-F. and Hon, H.-W.},
	month = nov,
	year = {1989},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Acoustics, Context modeling, Databases, Hidden Markov models, Humans, Knowledge engineering, Linear predictive coding, Maximum likelihood decoding, Natural languages, Speech recognition},
	pages = {1641--1648},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LV8ANX62\\citations.html:text/html},
}

@article{belinkov_analyzing_2017,
	title = {Analyzing {Hidden} {Representations} in {End}-to-{End} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/b069b3415151fa7217e870017374de7c-Abstract.html},
	language = {en},
	urldate = {2021-06-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Belinkov, Yonatan and Glass, James},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\B3UY4WM9\\Belinkov and Glass - 2017 - Analyzing Hidden Representations in End-to-End Aut.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\TIWCYTWD\\b069b3415151fa7217e870017374de7c-Abstract.html:text/html},
}

@article{webb_ensemble_2019,
	title = {To {Ensemble} or {Not} {Ensemble}: {When} does {End}-{To}-{End} {Training} {Fail}?},
	shorttitle = {To {Ensemble} or {Not} {Ensemble}},
	url = {http://arxiv.org/abs/1902.04422},
	doi = {10.13140/RG.2.2.28091.46880},
	abstract = {End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue-are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where over-parameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.},
	urldate = {2021-06-25},
	journal = {arXiv:1902.04422 [cs, stat]},
	author = {Webb, Andrew M. and Reynolds, Charles and Chen, Wenlin and Reeve, Henry and Iliescu, Dan-Andrei and Lujan, Mikel and Brown, Gavin},
	year = {2019},
	note = {arXiv: 1902.04422},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Code: https://github.com/grey-area/modular-loss-experiments. Preprint updated to reflect version accepted for publication at ECML},
	annote = {Comment: Code: https://github.com/grey-area/modular-loss-experiments. Preprint updated to reflect version accepted for publication at ECML},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HL7TZ2VG\\Webb et al. - 2019 - To Ensemble or Not Ensemble When does End-To-End .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\R26CGEA9\\1902.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\43W6WF6I\\Webb et al. - 2019 - To Ensemble or Not Ensemble When does End-To-End .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7GHF8B4Z\\1902.html:text/html},
}

@article{hsu_hubert_2021,
	title = {{HuBERT}: {Self}-{Supervised} {Speech} {Representation} {Learning} by {Masked} {Prediction} of {Hidden} {Units}},
	shorttitle = {{HuBERT}},
	abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
	urldate = {2021-06-27},
	journal = {arXiv:2106.07447 [cs, eess]},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07447},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LLMZ2XAV\\Hsu et al. - 2021 - HuBERT Self-Supervised Speech Representation Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2BDF9N6J\\2106.html:text/html},
}

@article{zhou_syllable-based_2018,
	title = {Syllable-{Based} {Sequence}-to-{Sequence} {Speech} {Recognition} with the {Transformer} in {Mandarin} {Chinese}},
	url = {http://arxiv.org/abs/1804.10752},
	abstract = {Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attention-based model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the-art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of {\textbackslash}emph\{\$28.77{\textbackslash}\%\$\}, which is competitive to the state-of-the-art CER of \$28.0{\textbackslash}\%\$ by the joint CTC-attention based encoder-decoder network.},
	urldate = {2021-06-27},
	journal = {arXiv:1804.10752 [cs, eess]},
	author = {Zhou, Shiyu and Dong, Linhao and Xu, Shuang and Xu, Bo},
	month = jun,
	year = {2018},
	note = {arXiv: 1804.10752},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: accepted by INTERSPEECH2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4RMQJFCS\\Zhou et al. - 2018 - Syllable-Based Sequence-to-Sequence Speech Recogni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EPIH7EXJ\\1804.html:text/html},
}

@inproceedings{hejtmanek_using_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Syllables} as {Acoustic} {Units} for {Spontaneous} {Speech} {Recognition}},
	isbn = {978-3-642-15760-8},
	doi = {10.1007/978-3-642-15760-8_38},
	abstract = {In this work, we deal with advanced context-dependent automatic speech recognition (ASR) of Czech spontaneous talk using hidden Markov models (HMM). Context-dependent units (e.g. triphones, diphones) in ASR systems provide significant improvement against simple non-context-dependent units. However, for spontaneous speech recognition we had to overcome some very challenging tasks. For one, the number of syllables compared to the size of spontaneous speech corpus makes the usage of context-dependent units very difficult. The main part of this article shows problems and procedures to effectively build and use a syllable-based ASR with the LASER (ASR system developed at Department of Computer Science and Engineering, Faculty of Applied Sciences). The procedures are usable with virtual any modern ASR.},
	language = {en},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Hejtmánek, Jan},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2010},
	keywords = {Acoustic Model, Automatic Speech Recognition, Hide Markov Model, Speech Recognition, Spontaneous Speech},
	pages = {299--305},
}

@article{fujimura_syllable_1975,
	title = {Syllable as a {Unit} of {Speech} {Recognition}},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162631},
	abstract = {Basic problems involved in automatic recognition of continuous speech are discussed with reference to the recently developed template matching technique using dynamic programming. Irregularities in phonetic manifestations of phonemes are discussed and it is argued that the syllable, phonologically redefined, will serve as the effective minimal unit in the time domain. English syllable structures are discussed from this point of view using the notions of "syllable features" and "vowel affinity."},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Fujimura, O.},
	month = feb,
	year = {1975},
	keywords = {Acoustics, Automatic speech recognition, Dentistry, Detectors, Dynamic programming, Isolation technology, Liquids, Speech analysis, Speech recognition, Tail},
	pages = {82--87},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2GFTFRKM\\authors.html:text/html},
}

@article{pratap_mls_2020,
	title = {{MLS}: {A} {Large}-{Scale} {Multilingual} {Dataset} for {Speech} {Research}},
	shorttitle = {{MLS}},
	url = {http://arxiv.org/abs/2012.03411},
	doi = {10.21437/Interspeech.2020-2826},
	abstract = {This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.},
	urldate = {2021-06-29},
	journal = {Interspeech 2020},
	author = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
	month = oct,
	year = {2020},
	note = {arXiv: 2012.03411},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {2757--2761},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\9JMDBDWR\\Pratap et al. - 2020 - MLS A Large-Scale Multilingual Dataset for Speech.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\QR4LRLNP\\2012.html:text/html},
}

@inproceedings{punjabi_joint_2021,
	title = {Joint {ASR} and {Language} {Identification} {Using} {RNN}-{T}: {An} {Efficient} {Approach} to {Dynamic} {Language} {Switching}},
	shorttitle = {Joint {ASR} and {Language} {Identification} {Using} {RNN}-{T}},
	doi = {10.1109/ICASSP39728.2021.9413734},
	abstract = {Conventional dynamic language switching enables seamless multilingual interactions by running several monolingual ASR systems in parallel and triggering the appropriate downstream components using a standalone language identification (LID) service. Since this solution is neither scalable nor cost- and memory-efficient, especially for on-device applications, we propose end-to-end, streaming, joint ASR-LID architectures based on the recurrent neural network transducer framework. Two key formulations are explored: (1) joint training using a unified output space for ASR and LID vocabularies, and (2) joint training viewed as multi-task optimization. We also evaluate the benefit of using auxiliary language information obtained on-the-fly from an acoustic LID classifier. Experiments with the English-Hindi language pair show that: (a) multi-task architectures perform better overall, and (b) the best joint architecture surpasses monolingual ASR (6.4–9.2\% word error rate reduction) and acoustic LID (53.9–56.1\% error rate reduction) baselines while reducing the overall memory footprint by up to 46\%.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Punjabi, Surabhi and Arsikere, Harish and Raeesy, Zeynab and Chandak, Chander and Bhave, Nikhil and Bansal, Ankish and Müller, Markus and Murillo, Sergio and Rastrow, Ariya and Stolcke, Andreas and Droppo, Jasha and Garimella, Sri and Maas, Roland and Hans, Mat and Mouchtaris, Athanasios and Kunzmann, Siegfried},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {code switching, Computer architecture, Error analysis, joint modeling, language identification, multilingual, recurrent neural network transducer, Recurrent neural networks, Switches, Training, Transducers, Vocabulary},
	pages = {7218--7222},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JRLSUL8J\\9413734.html:text/html},
}

@inproceedings{doshi_extending_2021,
	title = {Extending {Parrotron}: {An} {End}-to-{End}, {Speech} {Conversion} and {Speech} {Recognition} {Model} for {Atypical} {Speech}},
	shorttitle = {Extending {Parrotron}},
	doi = {10.1109/ICASSP39728.2021.9414644},
	abstract = {We present an extended Parrotron model: a single, end-to-end network that enables voice conversion and recognition simultaneously. Input spectrograms are transformed to output spectrograms in the voice of a predetermined target speaker while also generating hypotheses in a target vocabulary. We study the performance of this novel architecture, which jointly predicts speech and text, on atypical (e.g. dysarthric) speech. We show that with as little as an hour of atypical speech, speaker adaptation can yield a 77\% relative reduction in Word Error Rate (WER), measured by ASR performance on the converted speech. We also show that data augmentation using a customized synthesizer built on atypical speech can provide an additional 10\% relative improvement over the best speaker-adapted model. Finally, we show how these methods generalize across 8 types of atypical speech for a range of speech impairment severities.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Doshi, Rohan and Chen, Youzheng and Jiang, Liyang and Zhang, Xia and Biadsy, Fadi and Ramabhadran, Bhuvana and Chu, Fang and Rosenberg, Andrew and Moreno, Pedro J.},
	month = jun,
	year = {2021},
	keywords = {Adaptation models, Conferences, Error analysis, Measurement uncertainty, sequence-to-sequence model, speech impairments, speech normalization, speech recognition, Speech recognition, Synthesizers, Vocabulary, voice conversion},
	pages = {6988--6992},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JLNLC79V\\9414644.html:text/html},
}

@inproceedings{li_towards_2019,
	title = {Towards {Code}-switching {ASR} for {End}-to-end {CTC} {Models}},
	doi = {10.1109/ICASSP.2019.8683223},
	abstract = {Although great progress has been made on end-to-end (E2E) models for monolingual and multilingual automatic speech recognition (ASR), there is no successful study for E2E models on the challenging intra-sentential code-switching (CS) ASR task to our best knowledge. In this paper, we propose an approach for CS ASR using E2E connectionist temporal classification (CTC) models. We use a frame-level language identification model to linearly adjust the posteriors of an E2E CTC model. We evaluate the proposed method on Microsoft live Chinese Cortana data with 7000 hours Chinese and English monolingual data and 300 hours CS data as the training data. Trained with only monolingual data without observing any CS data, the proposed method can obtain up to 6.3\% relative word error rate (WER) reduction. In the scenario of training with both monolingual and CS data, the proposed method can get up to 4.2\% relative WER improvement. This approach can also maintain comparable performance on a Chinese test set compared with baseline models.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Ke and Li, Jinyu and Ye, Guoli and Zhao, Rui and Gong, Yifan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Speech recognition, language identification, Recurrent neural networks, ASR, code-switching, CTC, Data models, Decoding, end-to-end, Predictive models},
	pages = {6076--6080},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ATTLXWWT\\8683223.html:text/html},
}

@article{naowarat_reducing_2021,
	title = {Reducing {Spelling} {Inconsistencies} in {Code}-{Switching} {ASR} using {Contextualized} {CTC} {Loss}},
	url = {http://arxiv.org/abs/2005.07920},
	abstract = {Code-Switching (CS) remains a challenge for Automatic Speech Recognition (ASR), especially character-based models. With the combined choice of characters from multiple languages, the outcome from character-based models suffers from phoneme duplication, resulting in language-inconsistent spellings. We propose Contextualized Connectionist Temporal Classification (CCTC) loss to encourage spelling consistencies of a character-based non-autoregressive ASR which allows for faster inference. The CCTC loss conditions the main prediction on the predicted contexts to ensure language consistency in the spellings. In contrast to existing CTC-based approaches, CCTC loss does not require frame-level alignments, since the context ground truth is obtained from the model's estimated path. Compared to the same model trained with regular CTC loss, our method consistently improved the ASR performance on both CS and monolingual corpora.},
	urldate = {2021-06-29},
	journal = {arXiv:2005.07920 [cs, eess]},
	author = {Naowarat, Burin and Kongthaworn, Thananchai and Karunratanakul, Korrawe and Wu, Sheng Hui and Chuangsuwanich, Ekapol},
	month = jun,
	year = {2021},
	note = {arXiv: 2005.07920},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: ICASSP 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\WCLBKTE4\\Naowarat et al. - 2021 - Reducing Spelling Inconsistencies in Code-Switchin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\D3PBEJCQ\\2005.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How many syllables exist in the {American} {English} language? - {Quora}},
	url = {https://www.quora.com/How-many-syllables-exist-in-the-American-English-language},
	urldate = {2021-06-29},
	annote = {I'm going to give you a very specific number, 15,831. That is how many syllables there are in the English language.A syllable is a vowel sound with or without consonants.Girl can be pronounced as one or two syllables grl or grr-ell. Same with boy, boy-yee. People from Australia get two syllables out of the word no, somehow.Let's count them all, shall we?No, that's been done already, here's a link: Page on nyu.eduIt provides a very thorough look at what a syllable is and how many there are (and maybe how many there should be).Hope this helps.},
	file = {How many syllables exist in the American English language? - Quora:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8HCXBKRZ\\How-many-syllables-exist-in-the-American-English-language.html:text/html},
}

@inproceedings{rieger_idiosyncratic_2001,
	title = {Idiosyncratic fillers in the speech of bilinguals},
	volume = {DISS'01},
	url = {https://www.isca-speech.org/archive_open/diss_01/dis1_081.html},
	urldate = {2021-06-29},
	author = {Rieger, Caroline L.},
	year = {2001},
	pages = {81--84},
	file = {DISS'01 Abstract\: Rieger, Caroline L.:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7WUDW5FF\\dis1_081.html:text/html},
}

@article{musgrave_metric_2020,
	title = {A {Metric} {Learning} {Reality} {Check}},
	url = {http://arxiv.org/abs/2003.08505},
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best.},
	urldate = {2021-06-29},
	journal = {arXiv:2003.08505 [cs]},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	month = sep,
	year = {2020},
	note = {arXiv: 2003.08505},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Visit https://www.github.com/KevinMusgrave/powerful-benchmarker for supplementary material, including the source code, configuration files, log files, and interactive bayesian optimization plots},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4DDYLHPA\\Musgrave et al. - 2020 - A Metric Learning Reality Check.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MUCR9S4H\\2003.html:text/html},
}

@article{marchand_automatic_2009,
	title = {Automatic {Syllabification} in {English}: {A} {Comparison} of {Different} {Algorithms}},
	volume = {52},
	issn = {0023-8309},
	shorttitle = {Automatic {Syllabification} in {English}},
	doi = {10.1177/0023830908099881},
	abstract = {Automatic syllabification of words is challenging, not least because the syllable is not easy to define precisely. Consequently, no accepted standard algorithm for automatic syllabification exists. There are two broad approaches: rule-based and data-driven. The rule-based method effectively embodies some theoretical position regarding the syllable, whereas the data-driven paradigm tries to infer “new” syllabifications from examples assumed to be correctly syllabified already. This article compares the performance of several variants of the two basic approaches. Given the problems of definition, it is difficult to determine a correct syllabification in all cases and so to establish the quality of the “gold standard” corpus used either to evaluate quantitatively the output of an automatic algorithm or as the example-set on which data-driven methods crucially depend. Thus, we look for consensus in the entries in multiple lexical databases of pre-syllabified words. In this work, we have used two independent lexicons, and extracted from them the same 18,016 words with their corresponding (possibly different) syllabifications. We have also created a third lexicon corresponding to the 13,594 words that share the same syllabifications in these two sources. As well as two rule-based approaches (Hammond's and Fisher's implementation of Kahn's), three data-driven techniques are evaluated: a look-up procedure, an exemplar-based generalization technique, and syllabification by analogy (SbA). The results on the three databases show consistent and robust patterns. First, the data-driven techniques outperform the rule-based systems in word and juncture accuracies by a very significant margin but require training data and are slower. Second, syllabification in the pronunciation domain is easier than in the spelling domain. Finally, best results are consistently obtained with SbA.},
	language = {en},
	number = {1},
	urldate = {2021-07-02},
	journal = {Language and Speech},
	author = {Marchand, Yannick and Adsett, Connie R. and Damper, Robert I.},
	month = mar,
	year = {2009},
	keywords = {analogy, computational linguistics, corpus linguistics, rule-based systems, speech technology, syllabification},
	pages = {1--27},
	file = {Accepted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\KIUS6D3Q\\Marchand et al. - 2009 - Automatic Syllabification in English A Comparison.pdf:application/pdf},
}

@inproceedings{zhang_learning_1997,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {English} syllabification for words},
	isbn = {978-3-540-69612-4},
	doi = {10.1007/3-540-63614-5_17},
	abstract = {This paper presents the LE-SR (Learning English Syllabification Rules learning system, which learns English syllabification rules using a symbolic pattern recognition approach. LE-SR was tested on NTC2, a 20,000 English word pronouncing dictionary. The ten-fold accuracy ranged from 94.55\% to 96.05\% for words and 96.81\% to 97.71\% for syllables.The frequency of the rule usage indicates that most of the syllabification (86.21\%) in NTC2 is covered by only 0.64\% of the syllabification rules produced by LE-SR, while exceptions (2.45\%) require 76.77\% of the syllabification rules. This experimental result is consistent with the linguistics literature. Relatively few rules cover the vast majority of cases, but a considerable number of exceptions must be handled individually.The learned rules can be used to divide any English word into syllables. Based on syllables, the English stress rules can be learned. Syllabification and stress greatly influence the naturalness of the output of a text-to-speech system. Existing text-to-speech systems have either obtained syllabification from a dictionary or used a few hand coded rules. Using machine learning approach to obtain these rules is a step towards producing natural speech sounding for text-to-speech systems.},
	language = {en},
	booktitle = {Foundations of {Intelligent} {Systems}},
	publisher = {Springer},
	author = {Zhang, Jian and Hamilton, Howard J.},
	editor = {Raś, Zbigniew W. and Skowron, Andrzej},
	year = {1997},
	keywords = {English Word, Natural Speech, Performance Element, Rule Usage, Vowel Sound},
	pages = {177--186},
}

@article{goslin_comparing_2007,
	title = {Comparing {French} syllabification in preliterate children and adults},
	volume = {28},
	doi = {10.1017/S0142716407070178},
	abstract = {The influence of development and literacy upon syllabification in French was evaluated by comparing the segmental behavior of 4- to 5-year-old preliterate children and adults using a pause insertion task. Participants were required to repeat bisyllabic words such as “fourmi” (ant) by inserting a pause between its two syllabic components (/fur/-/mi/). In the first experiment we tested segmentation over a range of 49 double intervocalic consonant clusters. A similar general segmentation behavior was observed in both age groups, with a pattern that fit the predictions from a legality principle-based model of syllabification. Experiment 2 revealed that opacity between phonological and orthographic representations lead to increased ambisyllabic responses and a reduction in segmentation consistency in adults. In total, these findings indicate that syllabic forms are consistently represented from an early age, but that segmentation in metalinguistic tasks is susceptible to contamination from spelling and etymological knowledge.},
	language = {en},
	number = {2},
	urldate = {2021-07-03},
	journal = {Applied Psycholinguistics},
	author = {Goslin, Jeremy and Floccia, Caroline},
	month = apr,
	year = {2007},
	pages = {341--367},
	file = {Full Text:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\BRW3JKHF\\Goslin and Floccia - 2007 - Comparing French syllabification in preliterate ch.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\C4GN9FZT\\C3CD62BF8020CBF33E607D1ECBEC7591.html:text/html},
}

@article{fallows_experimental_1981,
	title = {Experimental evidence for {English} syllabification and syllable structure},
	volume = {17},
	issn = {1469-7742, 0022-2267},
	doi = {10.1017/S0022226700007027},
	abstract = {Most phonologists regard the syllable as a unit of language. In recent years, partly in reaction to Chomsky's and Halle's neglect of it in The sound pattern of English, much work has been done to incorporate the syllable into phonological theory (Anderson \& Jones, 1974; Bailey, 1978; Hoard, 1971; Hooper, 1972, 1974, 1976, 1978; Kahn, 1976; Pulgram, 1970; Rudes, 1977; Vennemann, 1972).Syllable theories have been based on evidence from phonetics, phonological processes, prosody, language change, child language acquisition, and language universals. The purpose of this experimental study is (a) to contribute empirical evidence about the nature of the syllable from native speakers' actual syllabifications of words and (b) to determine how this evidence reflects on the syllable theories already proposed.Syllable studies have focused on two major questions: (1) what is the structure of the syllable and (2) how are words divided into syllables (syllabified). These questions are obviously related, and the answers to each have implications for the other. This study was designed to elicit data directly addressing both questions.},
	language = {en},
	number = {2},
	urldate = {2021-07-03},
	journal = {Journal of Linguistics},
	author = {Fallows, Deborah},
	month = sep,
	year = {1981},
	pages = {309--317},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2RZVF5RQ\\E9E39A4D30B053C19700402CD7A28DA6.html:text/html},
}

@article{baker_trainable_1979,
	title = {Trainable grammars for speech recognition},
	volume = {65},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.2017061},
	doi = {10.1121/1.2017061},
	number = {S1},
	urldate = {2021-07-04},
	journal = {The Journal of the Acoustical Society of America},
	author = {Baker, J. K.},
	month = jun,
	year = {1979},
	pages = {S132--S132},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\I75QYALW\\Baker - 1979 - Trainable grammars for speech recognition.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\Z5DLTTC8\\1.html:text/html},
}

@article{hadian_flat-start_2018,
	title = {Flat-{Start} {Single}-{Stage} {Discriminatively} {Trained} {HMM}-{Based} {Models} for {ASR}},
	volume = {26},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2018.2848701},
	abstract = {In recent years, end-to-end approaches to automatic speech recognition have received considerable attention as they are much faster in terms of preparing resources. However, conventional multistage approaches, which rely on a pipeline of training hidden Markov models (HMM)-GMM models and tree-building steps still give the state-of-the-art results on most databases. In this study, we investigate flat-start one-stage training of neural networks using lattice-free maximum mutual information (LF-MMI) objective function with HMM for large vocabulary continuous speech recognition. We thoroughly look into different issues that arise in such a setup and propose a standalone system, which achieves word error rates (WER) comparable with that of the state-of-the-art multi-stage systems while being much faster to prepare. We propose to use full biphones to enable flat-start context-dependent (CD) modeling and show through experiments that our CD modeling approach can be almost as effective as regular tree-based CD modeling. We show that our flat-start LF-MMI setup together with this tree-free CD modeling technique achieves 10 to 25 \% relative WER reduction compared to other end-to-end methods on well-known databases. The improvements are larger for smaller databases.},
	number = {11},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hadian, Hossein and Sameti, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = nov,
	year = {2018},
	keywords = {automatic speech recognition, Context modeling, Databases, Decoding, flat-start, hidden Markov models, Hidden Markov models, Lattice-free, Linear programming, maximum mutual information, Neural networks, single-stage, Training},
	pages = {1949--1961},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LJVW5XRV\\8387866.html:text/html},
}

@article{baker_dragon_1975,
	title = {The {DRAGON} system–{An} overview},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162650},
	abstract = {This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model–that of a probabilistic function of a Markov process–is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities.},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Baker, J.},
	month = feb,
	year = {1975},
	keywords = {Acoustic measurements, Automatic speech recognition, Data mining, Error analysis, Markov processes, Power system modeling, Power system reliability, Random variables, Speech recognition, Vocabulary},
	pages = {24--29},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7ZUX56CQ\\1162650.html:text/html},
}

@inproceedings{krantz_language-agnostic_2019,
	title = {Language-{Agnostic} {Syllabification} with {Neural} {Sequence} {Labeling}},
	doi = {10.1109/ICMLA.2019.00141},
	abstract = {The identification of syllables within phonetic sequences is known as syllabification. This task is thought to play an important role in natural language understanding, speech production, and the development of speech recognition systems. The concept of the syllable is cross-linguistic, though formal definitions are rarely agreed upon, even within a language. In response, data-driven syllabification methods have been developed to learn from syllabified examples. These methods often employ classical machine learning sequence labeling models. In recent years, recurrence-based neural networks have been shown to perform increasingly well for sequence labeling tasks such as named entity recognition (NER), part of speech (POS) tagging, and chunking. We present a novel approach to the syllabification problem which leverages modern neural network techniques. Our network is constructed with long short-term memory (LSTM) cells, a convolutional component, and a conditional random field (CRF) output layer. Existing syllabification approaches are rarely evaluated across multiple language families. To demonstrate cross-linguistic generalizability, we show that the network is competitive with state of the art systems in syllabifying English, Dutch, Italian, French, Manipuri, and Basque datasets.},
	booktitle = {2019 18th {IEEE} {International} {Conference} {On} {Machine} {Learning} {And} {Applications} ({ICMLA})},
	author = {Krantz, Jacob and Dulin, Maxwell and De Palma, Paul},
	month = dec,
	year = {2019},
	keywords = {Hidden Markov models, Labeling, Natural language processing, Neural networks, Phonetics, Speech recognition, Supervised learning, Tagging, Task analysis},
	pages = {804--810},
	annote = {Github: https://github.com/jacobkrantz/lstm-syllabify},
	annote = {https://arxiv.org/abs/1909.13362},
	file = {Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JBFIJQPI\\Krantz et al. - 2019 - Language-Agnostic Syllabification with Neural Sequ.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\VCHKMGCW\\8999232.html:text/html},
}

@inproceedings{sak_learning_2015,
	title = {Learning acoustic frame labeling for speech recognition with recurrent neural networks},
	doi = {10.1109/ICASSP.2015.7178778},
	abstract = {We explore alternative acoustic modeling techniques for large vocabulary speech recognition using Long Short-Term Memory recurrent neural networks. For an acoustic frame labeling task, we compare the conventional approach of cross-entropy (CE) training using fixed forced-alignments of frames and labels, with the Connectionist Temporal Classification (CTC) method proposed for labeling unsegmented sequence data. We demonstrate that the latter can be implemented with finite state transducers. We experiment with phones and context dependent HMM states as acoustic modeling units. We also investigate the effect of context in acoustic input by training unidirectional and bidirectional LSTM RNN models. We show that a bidirectional LSTM RNN CTC model using phone units can perform as well as an LSTM RNN model trained with CE using HMM state alignments. Finally, we also show the effect of sequence discriminative training on these models and show the first results for sMBR training of CTC models.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Sak, Haşim and Senior, Andrew and Rao, Kanishka and İrsoy, Ozan and Graves, Alex and Beaufays, Françoise and Schalkwyk, Johan},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {acoustic modeling, Acoustics, Context modeling, CTC, Gold, Hidden Markov models, LSTM, Neural networks, RNN, Speech recognition, Training},
	pages = {4280--4284},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\AU75FQED\\7178778.html:text/html},
}

@inproceedings{soltau_neural_2017,
	title = {Neural {Speech} {Recognizer}: {Acoustic}-to-{Word} {LSTM} {Model} for {Large} {Vocabulary} {Speech} {Recognition}},
	shorttitle = {Neural {Speech} {Recognizer}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1566.html},
	doi = {10.21437/Interspeech.2017-1566},
	language = {en},
	urldate = {2021-07-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Soltau, Hagen and Liao, Hank and Sak, Haşim},
	month = aug,
	year = {2017},
	pages = {3707--3711},
	file = {Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\PPI3J34N\\Soltau et al. - 2017 - Neural Speech Recognizer Acoustic-to-Word LSTM Mo.pdf:application/pdf},
}

@inproceedings{prabhavalkar_comparison_2017,
	title = {A {Comparison} of {Sequence}-to-{Sequence} {Models} for {Speech} {Recognition}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0233.html},
	doi = {10.21437/Interspeech.2017-233},
	language = {en},
	urldate = {2021-07-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Prabhavalkar, Rohit and Rao, Kanishka and Sainath, Tara N. and Li, Bo and Johnson, Leif and Jaitly, Navdeep},
	month = aug,
	year = {2017},
	pages = {939--943},
}

@book{saussure_course_1959,
	address = {New York},
	title = {Course in general linguistics.},
	language = {eng},
	publisher = {Philosophical Library},
	author = {Saussure, Ferdinand de},
	translator = {Baskin, Wade},
	year = {1959},
	keywords = {Comparative linguistics, Language and languages},
	annote = {IA: courseingenerall00saus},
	annote = {Open Library ID: OL23291521M},
}

@book{ladefoged_course_2011,
	edition = {6th},
	title = {A {Course} in {Phonetics}},
	language = {English},
	author = {Ladefoged, Peter and Johnson, Keith},
	year = {2011},
	annote = {OCLC: 613523782},
}

@incollection{wells_syllabification_2019,
	edition = {4},
	title = {Syllabification and {Allophony}},
	isbn = {978-0-429-49039-2},
	abstract = {In this chapter, the author discusses the system he arrived at through writing his Longman Pronunciation Dictionary, a system based on the allophones of sounds in different positions in the syllable. For A. C. Gimson the syllable is relevant mainly as a possible phonetic category or as a category to which phonotactic constraints may be referred. He is sceptical of the first, and for the second prefers the word. Yet English has a fair number of important allophonic rules which can best be described by specifying ‘syllable boundary’ as part of the conditioning environment. It is this fact which makes syllabification phonologically relevant. If allophonic rules are to be allowed to refer to syllable boundaries as part of their conditioning environments, one need a principled way of specifying the location of such boundaries. The only cases in English where immediately adjacent syllables have equal grade are those involving weak vowels.},
	booktitle = {Practical {English} {Phonetics} and {Phonology}},
	publisher = {Routledge},
	author = {Wells, J. C.},
	year = {2019},
}

@incollection{kingston_role_1990,
	address = {Cambridge},
	series = {Papers in {Laboratory} {Phonology}},
	title = {The role of the sonority cycle in core syllabification},
	volume = {1},
	isbn = {978-0-521-36238-2},
	abstract = {IntroductionOne of the major concerns of laboratory phonology is that of determining the nature of the transition between discrete phonological structure (conventionally, “phonology”) and its expression in terms of nondiscrete physical or psychoacoustic parameters (conventionally, “phonetics”). A considerable amount of research has been devoted to determining where this transition lies, and to what extent the rule types and representational systems needed to characterize the two levels may differ (see Keating 1985 for an overview). For instance, it is an empirical question to what extent the assignment of phonetic parameters to strings of segments (phonemes, tones, etc.) depends upon increasingly rich representational structures of the sort provided by autosegmental and metrical phonology, or upon real-time realization rules – or indeed upon some combination of the two, as many are coming to believe. We are only beginning to assess the types of evidence that can decide questions of this sort, and a complete and fully adequate theory of the phonetics/phonology interface remains to be worked out. A new synthesis of the methodology of phonology and phonetics, integrating results from the physical, biological and cognitive sciences, is required if we are to make significant progress in this area.The present study examines one question of traditional interest to both phoneticians and phonologists, with roots that go deep into modern linguistic theory. Many linguists have noted the existence of cross-linguistic preferences for certain types of syllable structures and syllable contacts. These have been the subject of descriptive studies and surveys such as that of Greenberg (1978), which have brought to light a number of generalizations suggesting that certain syllable types are less complex or less marked than others across languages.},
	urldate = {2021-07-05},
	booktitle = {Papers in {Laboratory} {Phonology}: {Volume} 1: {Between} the {Grammar} and {Physics} of {Speech}},
	publisher = {Cambridge University Press},
	author = {Clements, G. N.},
	editor = {Kingston, John and Beckman, Mary E.},
	year = {1990},
	note = {doi: 10.1017/CBO9780511627736.017},
	pages = {283--333},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\87YF7QTB\\B5908538E8FA8A13D4B4EBACAE3EF061.html:text/html},
}

@book{katamba_introduction_1989,
	address = {London; New York},
	title = {An introduction to phonology},
	isbn = {978-0-582-29150-8},
	abstract = {A practical introduction to generative phonology for the novice. The work reflects the trends towards scrutinizing the nature of phonological representations and the relationship between phonology and other grammatical components.},
	language = {English},
	publisher = {Longman},
	author = {Katamba, Francis},
	year = {1989},
	note = {OCLC: 18463593},
}

@book{pulgram_syllable_2019,
	title = {Syllable, {Word}, {Nexus}, {Cursus}},
	isbn = {978-3-11-081544-3},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110815443/html},
	abstract = {Syllable, Word, Nexus, Cursus by Ernst Pulgram was published on March 18, 2019 by De Gruyter Mouton.},
	language = {en},
	urldate = {2021-07-05},
	publisher = {De Gruyter Mouton},
	author = {Pulgram, Ernst},
	month = mar,
	year = {2019},
	note = {Publication Title: Syllable, Word, Nexus, Cursus},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\KQ8WT9Z7\\html.html:text/html},
}

@misc{baayen_r_h_celex2_1995,
	title = {{CELEX2}},
	url = {https://catalog.ldc.upenn.edu/LDC96L14},
	urldate = {2021-07-06},
	publisher = {Linguistic Data Consortium},
	author = {Baayen, R H. and Piepenbrock, R and Gulikers, L},
	year = {1995},
	note = {doi: 10.35111/GS6S-GM48},
}

@inproceedings{tachbelie_morpheme-based_2010,
	address = {Universiti Sains, Penang, Malaysia},
	title = {Morpheme-based automatic speech recognition for a morphologically rich language - {Amharic}},
	author = {Tachbelie, Martha Yifiru and Abate, Solomon Teferra and Menzel, Wolfgang},
	month = may,
	year = {2010},
	pages = {68--73},
}

@inproceedings{guijarrubia_morpheme-based_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Morpheme-{Based} {Automatic} {Speech} {Recognition} of {Basque}},
	isbn = {978-3-642-02172-5},
	doi = {10.1007/978-3-642-02172-5_50},
	abstract = {In this work, we focus on studying a morpheme-based speech recognition system for Basque, an highly inflected language that is official language in the Basque Country (northern Spain). Two different techniques are presented to decompose the words into their morphological units. The morphological units are then integrated into an Automatic Speech Recognition System, and those systems are then compared to a word-based approach in terms of accuracy and processing speed. Results show that whereas the morpheme-based approaches perform similarly from an accuracy point of view, they can be significantly faster than the word-based system when applied to a weather-forecast task.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer},
	author = {Guijarrubia, Víctor G. and Torres, M. Inés and Justo, Raquel},
	editor = {Araujo, Helder and Mendonça, Ana Maria and Pinho, Armando J. and Torres, María Inés},
	year = {2009},
	keywords = {Morphological operations, Speech recognition},
	pages = {386--393},
}

@inproceedings{abera_tigrinya_2020,
	title = {Tigrinya {Automatic} {Speech} recognition with {Morpheme} based recognition units},
	doi = {10.18653/v1/2020.winlp-1.12},
	abstract = {Hafte Abera, Sebsibe Hailemariam. Proceedings of the The Fourth Widening Natural Language Processing Workshop. 2020.},
	language = {en-us},
	urldate = {2021-07-06},
	author = {Abera, Hafte and Hailemariam, Sebsibe},
	month = jul,
	year = {2020},
	pages = {46--50},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\XJKL4VPM\\2020.winlp-1.12.html:text/html},
}

@article{jafri_concatenative_2021,
	title = {Concatenative {Speech} {Recognition} using {Morphemes}},
	volume = {12},
	issn = {2156-5570},
	doi = {10.14569/IJACSA.2021.0120378},
	abstract = {This paper adopts a novel sub-lexical approach to construct viable continuous speech recognition systems with scalable vocabulary that use the components of words to form the elements of pronunciation dictionaries and recognition lattices. The proposed Concatenative ASR family utilizes combination rules between morphemes (prefixes, stems, and suffixes), along with their theoretical grammatical categories. The constrained structure reduces invalid words by using grammar rules governing agglutination of affixes with stems, while having a large vocabulary space and hence fewer out-of-vocabulary words. In pursuing this approach, the project develops automatic speech recognition (ASR) parameterized models, designs parameter values, constructs and implements ASR systems, and analyzes the characteristics of these systems. The project designs parameter values in the context of Arabic to yield a subset hierarchy of vocabularies of the ASR systems facilitating meaningful analysis. It investigates the characteristics of the ASR systems with respect to vocabulary, recognition lattice, dictionary, and word error rate (WER). In the experiments, the standard Word ASR model has the best characteristics for vocabulary of up to five thousand words and the Concatenative ASR family is most appropriate for vocabulary of up to half a million words. The paper shows that the approach used encompasses fundamentally different processes of word formation and thus is applicable to languages that exhibit concatenative word-formation processes.},
	language = {en},
	number = {3},
	urldate = {2021-07-06},
	journal = {International Journal of Advanced Computer Science and Applications (IJACSA)},
	author = {Jafri, Afshan},
	month = jul,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LHJPRJ5I\\Jafri - 2021 - Concatenative Speech Recognition using Morphemes.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\L3VJYZRS\\ViewPaper.html:text/html},
}

@article{huckvale_using_2002,
	title = {Using phonologically-constrained morphological analysis in continuous speech recognition},
	volume = {16},
	issn = {0885-2308},
	doi = {10.1006/csla.2001.0187},
	abstract = {This article describes investigations into the use of phonologically-constrained morphological analysis (PCMA) in language modelling for continuous speech recognition. PCMA provides a means for modelling text as a sequence of morphemes in a way that retains compatibility with the linear concatenative model of pronunciation used in conventional decoders. Experiments were performed in English exploiting the 100-million-word British National Corpus as source material. We show that PCMA leads to smaller but more generative pronunciation lexicons, and that it does not weaken the quality of the acoustic decoding measured in terms of recognition lattices. For trigram language models, perplexity figures are poorer for PCMA over words, as might be expected given the reduction in sentence span. However recognition results show small improvements in accuracy under some conditions, particularly when morph lattices are decoded with word-trigram models. We explore the capabilities for PCMA across vocabulary size, language model training size, and post-processing strategy. The best results show a 16\% relative reduction in word error rate.},
	language = {en},
	number = {2},
	urldate = {2021-07-06},
	journal = {Computer Speech \& Language},
	author = {Huckvale, Mark and Fang, Alex Chengyu},
	month = apr,
	year = {2002},
	pages = {165--181},
	annote = {https://www.sciencedirect.com/science/article/pii/S0885230801901871},
	file = {ScienceDirect Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8QZEDPTL\\S0885230801901871.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	shorttitle = {Deep {Speech}},
	url = {http://arxiv.org/abs/1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2021-07-06},
	journal = {arXiv:1412.5567 [cs]},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5567},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4PGJFLPC\\Hannun et al. - 2014 - Deep Speech Scaling up end-to-end speech recognit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4GM52WB8\\1412.html:text/html},
}

@inproceedings{graves_towards_2014,
	series = {{ICML}'14},
	title = {Towards {End}-to-{End} {Speech} {Recognition} with {Recurrent} {Neural} {Networks}},
	abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3\% on the Wall Street Journal corpus with no prior linguistic information, 21.9\% with only a lexicon of allowed words, and 8.2\% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7\%.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Graves, Alex and Jaitly, Navdeep},
	year = {2014},
	pages = {II--1764--II--1772},
}

@inproceedings{li_acoustic--word_2017,
	title = {Acoustic-to-word model without {OOV}},
	doi = {10.1109/ASRU.2017.8268924},
	abstract = {Recently, the acoustic-to-word model based on the Connectionist Temporal Classification (CTC) criterion was shown as a natural end-to-end model directly targeting words as output units. However, this type of word-based CTC model suffers from the out-of-vocabulary (OOV) issue as it can only model limited number of words in the output layer and maps all the remaining words into an OOV output node. Therefore, such word-based CTC model can only recognize the frequent words modeled by the network output nodes. It also cannot easily handle the hot-words which emerge after the model is trained. In this study, we improve the acoustic-to-word model with a hybrid CTC model which can predict both words and characters at the same time. With a shared-hidden-layer structure and modular design, the alignments of words generated from the word-based CTC and the character-based CTC are synchronized. Whenever the acoustic-to-word model emits an OOV token, we back off that OOV segment to the word output generated from the character-based CTC, hence solving the OOV or hot-words issue. Evaluated on a Microsoft Cortana voice assistant task, the proposed model can reduce the errors introduced by the OOV output token in the acoustic-to-word model by 30\%.},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Li, Jinyu and Ye, Guoli and Zhao, Rui and Droppo, Jasha and Gong, Yifan},
	month = dec,
	year = {2017},
	keywords = {acoustic-to-word, Acoustics, CTC, Decoding, hybrid, LSTM, OOV, Predictive models, Speech, Task analysis, Training, Training data},
	pages = {111--117},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ZCMMBJ62\\8268924.html:text/html;Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7J25JARJ\\Li et al. - 2017 - Acoustic-to-word model without OOV.pdf:application/pdf},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {https://aclanthology.org/P16-1162},
	doi = {10.18653/v1/P16-1162},
	urldate = {2021-07-06},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = aug,
	year = {2016},
	pages = {1715--1725},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\WMRB3AZV\\Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@inproceedings{dong_cif_2020,
	title = {{CIF}: {Continuous} {Integrate}-{And}-{Fire} for {End}-{To}-{End} {Speech} {Recognition}},
	shorttitle = {{CIF}},
	doi = {10.1109/ICASSP40776.2020.9054250},
	abstract = {In this paper, we propose a novel soft and monotonic alignment mechanism used for sequence transduction. It is inspired by the integrate-and-fire model in spiking neural networks and employed in the encoder-decoder framework consists of continuous functions, thus being named as: Continuous Integrate-and-Fire (CIF). Applied to the ASR task, CIF not only shows a concise calculation, but also supports online recognition and acoustic boundary positioning, thus suitable for various ASR scenarios. Several support strategies are also proposed to alleviate the unique problems of CIF-based model. With the joint action of these methods, the CIF-based model shows competitive performance. Notably, it achieves a word error rate (WER) of 2.86\% on the test-clean of Librispeech and creates new state-of-the-art result on Mandarin telephone ASR benchmark.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Dong, Linhao and Xu, Bo},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {acoustic boundary positioning, Acoustics, Computational modeling, continuous integrate-and-fire, Decoding, end-to-end model, Hidden Markov models, online speech recognition, Prediction algorithms, Predictive models, soft and monotonic alignment, Training},
	pages = {6079--6083},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EHI4XADJ\\9054250.html:text/html;Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\RS5WL2ST\\Dong and Xu - 2020 - CIF Continuous Integrate-And-Fire for End-To-End .pdf:application/pdf},
}

@inproceedings{zeyer_ctc_2017,
	title = {{CTC} in the {Context} of {Generalized} {Full}-{Sum} {HMM} {Training}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1073.html},
	doi = {10.21437/Interspeech.2017-1073},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Zeyer, Albert and Beck, Eugen and Schlüter, Ralf and Ney, Hermann},
	month = aug,
	year = {2017},
	pages = {944--948},
	file = {Full Text:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4VLPK2EA\\Zeyer et al. - 2017 - CTC in the Context of Generalized Full-Sum HMM Tra.pdf:application/pdf},
}

@inproceedings{bisani_investigations_2002,
	title = {Investigations on {Joint}-{Multigram} {Models} for {Grapheme}-to-{Phoneme} {Conversion}},
	booktitle = {Seventh {International} {Conference} on {Spoken} {Language} {Processing}},
	author = {Bisani, Maximilian and Ney, Hermann},
	year = {2002},
	pages = {105--108},
}

@article{novak_phonetisaurus_2016,
	title = {Phonetisaurus: {Exploring} grapheme-to-phoneme conversion with joint n-gram models in the {WFST} framework},
	volume = {22},
	shorttitle = {Phonetisaurus},
	doi = {10.1017/S1351324915000315},
	abstract = {This paper provides an analysis of several practical issues related to the theory and implementation of Grapheme-to-Phoneme (G2P) conversion systems utilizing the Weighted Finite-State Transducer paradigm. The paper addresses issues related to system accuracy, training time and practical implementation. The focus is on joint n-gram models which have proven to provide an excellent trade-off between system accuracy and training complexity. The paper argues in favor of simple, productive approaches to G2P, which favor a balance between training time, accuracy and model complexity. The paper also introduces the first instance of using joint sequence RnnLMs directly for G2P conversion, and achieves new state-of-the-art performance via ensemble methods combining RnnLMs and n-gram based models. In addition to detailed descriptions of the approach, minor yet novel implementation solutions, and experimental results, the paper introduces Phonetisaurus, a fully-functional, flexible, open-source, BSD-licensed G2P conversion toolkit, which leverages the OpenFst library. The work is intended to be accessible to a broad range of readers.},
	language = {en},
	number = {6},
	urldate = {2021-07-06},
	journal = {Natural Language Engineering},
	author = {Novak, Josef Robert and Minematsu, Nobuaki and Hirose, Keikichi},
	month = nov,
	year = {2016},
	pages = {907--938},
	annote = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/phonetisaurus-exploring-graphemetophoneme-conversion-with-joint-ngram-models-in-the-wfst-framework/F1160C3866842F0B707924EB30B8E753},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EKTASUU3\\F1160C3866842F0B707924EB30B8E753.html:text/html},
}

@inproceedings{povey_kaldi_2011,
	title = {The {Kaldi} {Speech} {Recognition} {Toolkit}},
	abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {Infoscience},
	publisher = {IEEE Signal Processing Society},
	author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
	year = {2011},
	annote = {https://infoscience.epfl.ch/record/192584},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ELZ7VJIP\\Povey et al. - 2011 - The Kaldi Speech Recognition Toolkit.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ML2XWJYD\\192584.html:text/html},
}

@misc{baevski_unsupervised_2021,
	title = {Unsupervised {Speech} {Recognition}},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
	year = {2021},
	note = {arXiv: 2105.11084},
}

@article{hannun_differentiable_2020,
	title = {Differentiable {Weighted} {Finite}-{State} {Transducers}},
	journal = {arXiv:2010.01003},
	author = {Hannun, Awni and Pratap, Vineel and Kahn, Jacob and Hsu, Wei-Ning},
	year = {2020},
}
