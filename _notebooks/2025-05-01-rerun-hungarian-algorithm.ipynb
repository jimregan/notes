{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Re-)run Hugarian algorithm\n",
    "\n",
    "> \"Because it stops after a few hundred\"\n",
    "\n",
    "- branch: master\n",
    "- hidden: true\n",
    "- categories: [hsi, munkres]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuation/tidying of [this notebook]({% post_url 2025-04-28-filter-by-wer %})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/tmp/procced2.1.json\") as inf:\n",
    "    a = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered1 = [x for x in a if not \"discarded\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_done = []\n",
    "with open(\"/tmp/assignment_short.csv\") as tsvf:\n",
    "    for line in tsvf:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"syntitem\"):\n",
    "            continue\n",
    "        if not line:\n",
    "            continue\n",
    "        already_done.append(line.split(\",\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [x for x in filtered1 if not x[\"fileid\"] in already_done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1412, 1405)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered), len(filtered1), len(already_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "L = np.load('bvh_pt_lengths.npy',allow_pickle=True)\n",
    "framerate = 120\n",
    "point_length = {}\n",
    "for thing in L:\n",
    "    item = list(thing.keys())[0].split('/')[-1]\n",
    "    item = item.replace('.bvh','')\n",
    "    point_length[item] = list(thing.values())[0]/framerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_length = {}\n",
    "for item in filtered:\n",
    "    synth_length[item[\"fileid\"]] = item[\"duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Anna's names from my list\n",
    "synth_pre = {}\n",
    "synth_post = {}\n",
    "synth_data = {}\n",
    "synth_times = []\n",
    "\n",
    "for item in filtered:\n",
    "    fileid = item[\"fileid\"]\n",
    "    dem_start = float(item[\"determiner_start\"])\n",
    "    dem_end = float(item[\"determiner_end\"])\n",
    "    duration = float(item[\"duration\"])\n",
    "\n",
    "    synth_pre[fileid] = dem_start\n",
    "    synth_post[fileid] = dem_end - dem_start\n",
    "    synth_data[fileid] = (dem_start, duration)\n",
    "    synth_times.append((duration, dem_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "def find_demonstrative_index(expression, demonstratives):\n",
    "    words = expression.split(' ')\n",
    "    # Iterate over each word to find if it matches any of the demonstratives\n",
    "    for i, word in enumerate(words):\n",
    "        if word in demonstratives:\n",
    "            return i  # Return the index of the first matching demonstrative\n",
    "    return -1  # Return -1 if no demonstratives are found\n",
    "demonstratives = ['this','that','one','those','these','there','here']\n",
    "\n",
    "files = glob('/tmp/tsv_pt_segments/*.tsv')\n",
    "print(len(files))\n",
    "words_to_exclude = ['left', 'right', 'middle', 'back']\n",
    "files = [f for f in files if not any(word in f for word in words_to_exclude)]\n",
    "pt_times = []\n",
    "pt_names = []\n",
    "pt_pre = {}\n",
    "pt_post = {}\n",
    "pt_data = {}\n",
    "for fn in files:\n",
    "    temp_list = []\n",
    "    item = fn.split('/')[-1].split('.tsv')[0]\n",
    "    with open(fn) as f:\n",
    "        with open(fn) as f:\n",
    "            for line in f:\n",
    "                t0, t1, wrd = line.strip().split('\\t')\n",
    "                t0, t1 = float(t0), float(t1)\n",
    "                temp_list.append([t0, t1, wrd])\n",
    "        df = pd.DataFrame(temp_list, columns=['t0','t1','wrd'])\n",
    "        expr = ' '.join(df['wrd'])\n",
    "        index  = find_demonstrative_index(expr, demonstratives)\n",
    "        dem_time = df['t0'].iloc[index]\n",
    "        total_time = point_length[item]\n",
    "        pt_times.append((total_time, dem_time))\n",
    "        pt_names.append(item)\n",
    "        pt_pre[item] = dem_time\n",
    "        pt_post[item] = point_length[item] - pt_pre[item]\n",
    "        pt_data[item] = (dem_time,total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.78866667 10.826      14.506      12.49       11.31666667 13.642\n",
      " 11.562     ]\n",
      "mean_length 12.447333333333333\n",
      "std_dev_length 1.2232631872973312\n",
      "max 14.506\n",
      "min 10.826\n",
      "percentile_75_length 13.215333333333334\n",
      "cutoff_time_syn 12.49\n",
      "percentile_75_length 13.215333333333334\n",
      "cutoff_time_pt 4.158333333333333\n",
      "len(pt_data) 1147\n",
      "len(synth_data) 7\n"
     ]
    }
   ],
   "source": [
    "# Convert the list to a numpy array for statistical calculations\n",
    "synth_lengths_array = np.array([x[0] for x in synth_times])\n",
    "print(synth_lengths_array)\n",
    "lengths_array = np.array(synth_lengths_array)\n",
    "\n",
    "lengths_array_pt = np.array([x[0] for x in pt_times])\n",
    "# Calculate the mean and standard deviation\n",
    "mean_length = np.mean(lengths_array)\n",
    "std_dev_length = np.std(lengths_array)\n",
    "print(\"mean_length\", mean_length)\n",
    "print(\"std_dev_length\", std_dev_length)\n",
    "print(\"max\", np.max(lengths_array))\n",
    "print(\"min\", np.min(lengths_array))\n",
    "\n",
    "# Calculate the 50th (median) and 75th percentiles\n",
    "median_length_syn = np.percentile(lengths_array, 50)\n",
    "percentile_75_length = np.percentile(lengths_array, 75)\n",
    "print(\"percentile_75_length\", percentile_75_length)\n",
    "\n",
    "cutoff_time_syn = median_length_syn\n",
    "print(\"cutoff_time_syn\", cutoff_time_syn)\n",
    "\n",
    "median_length_pt = np.percentile(lengths_array_pt, 50)\n",
    "percentile_75_length = np.percentile(lengths_array, 75)\n",
    "print(\"percentile_75_length\", percentile_75_length)\n",
    "\n",
    "cutoff_time_pt = median_length_pt\n",
    "print(\"cutoff_time_pt\", cutoff_time_pt)\n",
    "\n",
    "print(\"len(pt_data)\", len(pt_data))\n",
    "print(\"len(synth_data)\", len(synth_data))\n",
    "\n",
    "short_pt = {k:v for k,v in pt_data.items() if v[1]<cutoff_time_pt}\n",
    "short_synth = {k:v for k,v in synth_data.items() if v[1]<cutoff_time_syn}\n",
    "long_pt = {k:v for k,v in pt_data.items() if v[1]>=cutoff_time_pt}\n",
    "long_synth = {k:v for k,v in synth_data.items() if v[1]>=cutoff_time_syn}\n",
    "\n",
    "synth_pre_short = {k:v for k,v in synth_pre.items() if k in short_synth}\n",
    "synth_post_short = {k:v for k,v in synth_post.items() if k in short_synth}\n",
    "pt_pre_short = {k:v for k,v in pt_pre.items() if k in short_pt}\n",
    "pt_post_short = {k:v for k,v in pt_post.items() if k in short_pt}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpre\u001b[49m(pt_pre\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre' is not defined"
     ]
    }
   ],
   "source": [
    "len(pt_pre.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(pt_pre.keys()) 573\n",
      "list(pt_pre.keys())[:] 573\n",
      "list(pt_pre.keys()) 573\n",
      "list(pt_pre.keys())[:] 573\n",
      "list(pt_pre.keys()) 573\n",
      "list(pt_pre.keys())[:] 573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 573)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make distance matrix\n",
    "D = []\n",
    "synth_pre = synth_pre_short\n",
    "synth_post = synth_post_short\n",
    "pt_pre = pt_pre_short\n",
    "pt_post = pt_post_short\n",
    "\n",
    "for syntitem in list(synth_pre.keys())[:]:\n",
    "    row = []\n",
    "    print(\"list(pt_pre.keys())\", len(list(pt_pre.keys())))\n",
    "    print(\"list(pt_pre.keys())[:]\", len(list(pt_pre.keys())[:]))\n",
    "    for pointitem in list(pt_pre.keys())[:]:\n",
    "        syntpre = synth_pre[syntitem]\n",
    "        syntpost = synth_post[syntitem]\n",
    "        pointpre = pt_pre[pointitem]\n",
    "        pointpost = pt_post[pointitem]\n",
    "\n",
    "        cost = abs(syntpre-pointpre) + abs(syntpost-pointpost)\n",
    "        if synth_length[syntitem] > point_length[pointitem]:\n",
    "            cost *= 10\n",
    "        # penalize if synt starts before or ends after point\n",
    "        if syntpre > pointpre:\n",
    "            cost *= 2\n",
    "        if syntpost > pointpost:\n",
    "            cost *= 2\n",
    "        row.append(cost)\n",
    "    \n",
    "    D.append(row)\n",
    "dd = np.array(D)\n",
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: munkres in /Users/joregan/opt/anaconda3/envs/nst-tts/lib/python3.10/site-packages (1.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "m = Munkres()\n",
    "assignment_re = m.compute(D)\n",
    "f = open('assignment_short.csv','w')\n",
    "f.write('syntitem,pointitem,offset\\n')\n",
    "for pair in assignment_re:\n",
    "    syntidx,pointidx = pair\n",
    "    syntitem = list(synth_pre.keys())[syntidx]\n",
    "    pointitem = list(pt_pre.keys())[pointidx]\n",
    "    f.write('{},{},{}\\n'.format(syntitem, pointitem, pt_pre[pointitem]-synth_pre[syntitem]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = []\n",
    "with open(\"/tmp/assignment_short.csv\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"syntitem\"):\n",
    "            continue\n",
    "        parts = line.split(\",\")\n",
    "        selected.append(parts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "with open(\"/tmp/procced2.1.json\") as inf:\n",
    "    newdata = json.load(inf)\n",
    "for item in newdata:\n",
    "    if item[\"fileid\"] in selected:\n",
    "        filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg -i /tmp/output/st2_mirrored.10512_10854_1.23stretch.mp4 -i groundinggpt-generated-speech/hsi_4_0717_211_001_main__ggpt__682.wav -filter_complex \"[1:a]atrim=start=0.8624999999999999[aud]\" -map 0:v -map \"[aud]\" -c:v copy -c:a aac output_minus.mp4\n",
    "\n",
    "with open(\"/tmp/assigned_extended_all.csv\") as f, open(\"/tmp/ffmpeg-runner.sh\", \"w\") as outf:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"syntitem\"):\n",
    "            continue\n",
    "        parts = line.split(\",\")\n",
    "        \n",
    "        outf.write(f\"ffmpeg -i /tmp/output/{parts[1]}.mp4 -i /tmp/groundinggpt-generated-speech/{parts[0]}.wav\")\n",
    "        time = float(parts[2])\n",
    "        if time < 0.0:\n",
    "            outf.write(f\" -filter_complex \\\"[1:a]atrim=start={time}[aud]\\\" -map 0:v -map \\\"[aud]\\\"\")\n",
    "        else:\n",
    "            itime = int(time * 1000.0)\n",
    "            outf.write(f\" -filter_complex \\\"[1:a]adelay={itime}|{itime}[aud]\\\" -map 0:v -map \\\"[aud]\\\"\")\n",
    "        outf.write(f\" -c:v copy -c:a aac /tmp/output_minus/{parts[0]}.mp4\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict = {x[\"fileid\"]: x for x in filtered1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_motion = []\n",
    "filter_seen = True\n",
    "\n",
    "with open(\"/tmp/assignment_short.csv\") as f, open(\"/tmp/assigned_extended_all.csv\", \"w\") as outf:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"syntitem\"):\n",
    "            outf.write(line + \",person,room,topic\\n\")\n",
    "            continue\n",
    "        parts = line.split(\",\")\n",
    "        cur = filtered_dict[parts[0]]\n",
    "        if filter_seen and parts[1] in seen_motion:\n",
    "            continue\n",
    "        seen_motion.append(parts[1])\n",
    "        outf.write(f\"{line},{cur['person']},{cur['room']},{cur['topic']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
