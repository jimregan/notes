{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foinse scraper pieces, ctd\n",
    "\n",
    "> Scraping Foinse, from the Wayback Machine\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [irish, scraper, foinse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Continued]({% post_url 2021-09-27-foinse_scraper_pieces %})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skhj7IflwQHk"
   },
   "outputs": [],
   "source": [
    "link = \"http://web.archive.org/web/20171209002240/http://www.foinse.ie/sport/eile/6412-an-dornalai-john-joe-nevin-rangaithe-ag-uimhir-a-haon-anois\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9XVXKEgIwUbd"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VcTxKOf14_3W"
   },
   "outputs": [],
   "source": [
    "def extract_summary(inlist):\n",
    "    if len(inlist) > 2:\n",
    "        if inlist[-2] == \"Did you understand this story? Here are the main points:\":\n",
    "            return inlist[-1]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2iLwYYet5ath"
   },
   "outputs": [],
   "source": [
    "def filter_para_list(inlist):\n",
    "    out = []\n",
    "    for para in inlist:\n",
    "        if para == \"\":\n",
    "            continue\n",
    "        elif para.strip() == \"Foinse - News as Gaeilge\":\n",
    "            return out\n",
    "        elif para.strip() == \"Did you understand this story? Here are the main points:\":\n",
    "            return out\n",
    "        else:\n",
    "            out.append(para)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "W9XJdLGL39i-"
   },
   "outputs": [],
   "source": [
    "def get_content(url, text=\"\"):\n",
    "    out = {}\n",
    "    if text:\n",
    "        page_content = text\n",
    "    else:\n",
    "        page = requests.get(url)\n",
    "        if page.status_code != 200:\n",
    "            return {}\n",
    "        page_content = page.text\n",
    "\n",
    "    soup = BeautifulSoup(page_content, \"lxml\")\n",
    "\n",
    "    content = soup.find(\"div\", {\"class\": \"item-page\"})\n",
    "    if not content:\n",
    "        content = soup.find(\"div\", {\"id\": \"ja-main\"})\n",
    "    if not content:\n",
    "        return {}\n",
    "    \n",
    "    breadcrumbs = soup.find(\"div\", {\"class\": \"ja-breadcrums\"})\n",
    "    if breadcrumbs:\n",
    "        here = breadcrumbs.find(\"a\", {\"class\": \"pathway\"})\n",
    "        if not here:\n",
    "            here = breadcrumbs.find(\"span\", {\"class\": \"pathway\"})\n",
    "        if here:\n",
    "            out[\"category\"] = here.text.strip()\n",
    "    \n",
    "    # junk\n",
    "    jc = content.find(\"div\", {\"id\": \"jc\"})\n",
    "    if jc:\n",
    "        jc.extract()\n",
    "    pagenav = content.find(\"ul\", {\"class\": \"pagenav\"})\n",
    "    if pagenav:\n",
    "        pagenav.extract()\n",
    "    for js in content.find_all(\"script\", {\"type\": \"text/javascript\"}):\n",
    "        js.extract()\n",
    "\n",
    "    h2 = content.find(\"h2\")\n",
    "    if h2:\n",
    "        title = h2.text.strip()\n",
    "        if title:\n",
    "            out[\"title\"] = title\n",
    "        h2.extract()\n",
    "\n",
    "    h1 = content.find(\"h1\")\n",
    "    if h1:\n",
    "        heading = h1.text.strip()\n",
    "        if heading:\n",
    "            out[\"subcategory\"] = heading\n",
    "        h1.extract()\n",
    "\n",
    "    published_tag = content.find(\"dd\", {\"class\": \"published\"})\n",
    "    if not published_tag:\n",
    "        published_tag = content.find(\"span\", {\"class\": \"createdate\"})\n",
    "    if published_tag:\n",
    "        out[\"published\"] = published_tag.text.strip()\n",
    "\n",
    "    author_tag = content.find(\"dd\", {\"class\": \"createdby\"})\n",
    "    if not author_tag:\n",
    "        author_tag = content.find(\"span\", {\"class\": \"createby\"})\n",
    "    if author_tag:\n",
    "        out[\"author\"] = author_tag.text.strip()\n",
    "    artinfo = content.find(\"dl\", {\"class\": \"article-info\"})\n",
    "    if not artinfo:\n",
    "        artinfo = content.find(\"div\", {\"class\": \"article-meta\"})\n",
    "    if artinfo:\n",
    "        artinfo.extract()\n",
    "\n",
    "    paragraphs_tags = content.find_all(\"p\")\n",
    "    paragraphs = [p.text.replace(\"\\xa0\", \" \").strip() for p in paragraphs_tags]\n",
    "    out[\"text\"] = paragraphs\n",
    "    \n",
    "    raw_text = content.text\n",
    "    \n",
    "    raw_out = []\n",
    "    for raw_line in raw_text.split(\"\\n\"):\n",
    "        line = raw_line.replace(\"\\xa0\", \" \").strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        raw_out.append(line)\n",
    "    if paragraphs != raw_out:\n",
    "        out[\"text\"] = raw_out\n",
    "        \n",
    "    summary = extract_summary(out[\"text\"])\n",
    "    if summary:\n",
    "        out[\"summary\"] = summary\n",
    "    out[\"text\"] = filter_para_list(out[\"text\"])\n",
    "\n",
    "    vocab_list = []\n",
    "    for vocab in content.find_all(\"a\", {\"class\": \"glossarylink\"}):\n",
    "        item = {}\n",
    "        item[\"en\"] = vocab.get(\"title\").strip()\n",
    "        item[\"ga\"] = vocab.text.strip()\n",
    "        vocab_list.append(item)\n",
    "    out[\"vocab\"] = vocab_list\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(link)\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"lxml\")\n",
    "\n",
    "content = soup.find(\"div\", {\"class\": \"item-page\"})\n",
    "if not content:\n",
    "    print(\"Empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/jim/Playing/foinseunpacked\"\n",
    "file = open(f\"{BASE_DIR}/attempt1\", \"r\")\n",
    "pages = []\n",
    "for link in file.readlines():\n",
    "    pages.append(link.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foinse_data = []\n",
    "with open(\"/home/jim/foinse-bad.txt\", \"w\") as bad_list:\n",
    "    for page in pages:\n",
    "        print(page)\n",
    "        page_path = BASE_DIR + page.strip()[6:]\n",
    "        with open(page_path, \"r\") as pagef:\n",
    "            plines = pagef.readlines()\n",
    "            ptext = \"\\n\".join(plines)\n",
    "        content = get_content(page_path, ptext)\n",
    "        if content:\n",
    "            foinse_data.append(content)\n",
    "        else:\n",
    "            bad_list.write(page + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('foinse.json', 'w') as outfile:\n",
    "    json.dump(foinse_data, outfile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "foinse-scraper-pieces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
