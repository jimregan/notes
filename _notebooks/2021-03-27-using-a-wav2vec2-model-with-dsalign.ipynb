{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Using a wav2vec2 model with DSAlign.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "85aab658733e4c6aaa70316ad1486bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_645d349cf21c4382a8c978fcbea0af41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a288f831bfe458bb1698fe9c85edae2",
              "IPY_MODEL_6fed060131f34086812cd5567e7538cc"
            ]
          }
        },
        "645d349cf21c4382a8c978fcbea0af41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a288f831bfe458bb1698fe9c85edae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2cc66ae29b7c47d389e713e363b91c38",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 18,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 18,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03e934bb2747474db84b7d80c7e99c9b"
          }
        },
        "6fed060131f34086812cd5567e7538cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6eafddb79621458d8294887bdfa036fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 18/18 [00:18&lt;00:00,  1.04s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e929ea1d96140f999a5ba1ccab14696"
          }
        },
        "2cc66ae29b7c47d389e713e363b91c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03e934bb2747474db84b7d80c7e99c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6eafddb79621458d8294887bdfa036fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e929ea1d96140f999a5ba1ccab14696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqfxFfZSbTeM"
      },
      "source": [
        "# \"Using a wav2vec2 model with DSAlign\"\n",
        "> \"Speech recognition with wav2vec2, as input to the DSAlign aligner\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- comments: true\n",
        "- categories: [wav2vec2, dsalign]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo0Hy3GcHQit"
      },
      "source": [
        "#collapse-hide\n",
        "%%capture\n",
        "!pip install librosa webrtcvad"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRGZSK6eHTLq"
      },
      "source": [
        "The VAD wrapper is taken from [PyTorch Speaker Verification](https://github.com/HarryVolek/PyTorch_Speaker_Verification), which is in turn is based on [py-webrtcvad](https://github.com/wiseman/py-webrtcvad/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5f0-XyBH2Gz"
      },
      "source": [
        "#collapse-hide\n",
        "# VAD wrapper is taken from PyTorch Speaker Verification:\n",
        "# https://github.com/HarryVolek/PyTorch_Speaker_Verification\n",
        "# Copyright (c) 2019, HarryVolek\n",
        "# License: BSD-3-Clause\n",
        "# based on https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n",
        "# Copyright (c) 2016 John Wiseman\n",
        "# License: MIT\n",
        "import collections\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import sys\n",
        "import librosa\n",
        "import wave\n",
        "\n",
        "import webrtcvad\n",
        "\n",
        "#from hparam import hparam as hp\n",
        "sr = 16000\n",
        "\n",
        "def read_wave(path, sr):\n",
        "    \"\"\"Reads a .wav file.\n",
        "    Takes the path, and returns (PCM audio data, sample rate).\n",
        "    Assumes sample width == 2\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "    data, _ = librosa.load(path, sr)\n",
        "    assert len(data.shape) == 1\n",
        "    assert sr in (8000, 16000, 32000, 48000)\n",
        "    return data, pcm_data\n",
        "    \n",
        "class Frame(object):\n",
        "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    \"\"\"Generates audio frames from PCM audio data.\n",
        "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
        "    the sample rate.\n",
        "    Yields Frames of the requested duration.\n",
        "    \"\"\"\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms,\n",
        "                  padding_duration_ms, vad, frames):\n",
        "    \"\"\"Filters out non-voiced audio frames.\n",
        "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
        "    the voiced audio.\n",
        "    Uses a padded, sliding window algorithm over the audio frames.\n",
        "    When more than 90% of the frames in the window are voiced (as\n",
        "    reported by the VAD), the collector triggers and begins yielding\n",
        "    audio frames. Then the collector waits until 90% of the frames in\n",
        "    the window are unvoiced to detrigger.\n",
        "    The window is padded at the front and back to provide a small\n",
        "    amount of silence or the beginnings/endings of speech around the\n",
        "    voiced frames.\n",
        "    Arguments:\n",
        "    sample_rate - The audio sample rate, in Hz.\n",
        "    frame_duration_ms - The frame duration in milliseconds.\n",
        "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
        "    vad - An instance of webrtcvad.Vad.\n",
        "    frames - a source of audio frames (sequence or generator).\n",
        "    Returns: A generator that yields PCM audio data.\n",
        "    \"\"\"\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    # We use a deque for our sliding window/ring buffer.\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
        "    # NOTTRIGGERED state.\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
        "            # the ring buffer are voiced frames, then enter the\n",
        "            # TRIGGERED state.\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                start = ring_buffer[0][0].timestamp\n",
        "                # We want to yield all the audio we see from now until\n",
        "                # we are NOTTRIGGERED, but we have to start with the\n",
        "                # audio that's already in the ring buffer.\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            # We're in the TRIGGERED state, so collect the audio data\n",
        "            # and add it to the ring buffer.\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            # If more than 90% of the frames in the ring buffer are\n",
        "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
        "            # audio we've collected.\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = False\n",
        "                yield (start, frame.timestamp + frame.duration)\n",
        "                ring_buffer.clear()\n",
        "                voiced_frames = []\n",
        "    # If we have any leftover voiced audio when we run out of input,\n",
        "    # yield it.\n",
        "    if voiced_frames:\n",
        "        yield (start, frame.timestamp + frame.duration)\n",
        "\n",
        "\n",
        "def VAD_chunk(aggressiveness, path):\n",
        "    audio, byte_audio = read_wave(path, sr)\n",
        "    vad = webrtcvad.Vad(int(aggressiveness))\n",
        "    frames = frame_generator(20, byte_audio, sr)\n",
        "    frames = list(frames)\n",
        "    times = vad_collector(sr, 20, 200, vad, frames)\n",
        "    speech_times = []\n",
        "    speech_segs = []\n",
        "    for i, time in enumerate(times):\n",
        "        start = np.round(time[0],decimals=2)\n",
        "        end = np.round(time[1],decimals=2)\n",
        "        j = start\n",
        "        while j + .4 < end:\n",
        "            end_j = np.round(j+.4,decimals=2)\n",
        "            speech_times.append((j, end_j))\n",
        "            speech_segs.append(audio[int(j*sr):int(end_j*sr)])\n",
        "            j = end_j\n",
        "        else:\n",
        "            speech_times.append((j, end))\n",
        "            speech_segs.append(audio[int(j*sr):int(end*sr)])\n",
        "    return speech_times, speech_segs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7gdrly0IpRR"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1prY_wtxrgW"
      },
      "source": [
        "I'm going to use a video from YouTube as my input, so first I need to install youtube-dl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWLCFV4dIwdi"
      },
      "source": [
        "%%capture\n",
        "!pip install youtube-dl"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YrfhSDx_It"
      },
      "source": [
        "I've selected [this video](https://www.youtube.com/watch?v=VRg-a0qSGa8) because it's a speech by the President of Ireland (and so copyright-free as a matter of public record), it has subtitles (in Irish, though listed as English), and the subtitles are quite faithful to what was spoken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FcsjkvxI91-"
      },
      "source": [
        "%%capture\n",
        "!youtube-dl --all-subs -o '%(id)s' VRg-a0qSGa8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNzzdTLd1NMC"
      },
      "source": [
        "The audio needs to be a 16k wav, so I'm converting it with ffmpeg. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGeU9bqOKxfd"
      },
      "source": [
        "%%capture\n",
        "!ffmpeg -i VRg-a0qSGa8.mkv -acodec pcm_s16le -ac 1 -ar 16000 VRg-a0qSGa8.wav"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFZCbsO20RNq"
      },
      "source": [
        "Next, I'm using the ```VAD_chunk()``` function to get the start and end times, and audio segements of each part of the video with speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhShQO9fLBo4"
      },
      "source": [
        "times, segs = VAD_chunk(3, 'VRg-a0qSGa8.wav')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3FPhqIa0ks7"
      },
      "source": [
        "The `wav2vec2` models generally perform badly on short input, so `vad_concat()` concatenates the segments, as well as the times (for DSAlign)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohIO-cG0LNPY"
      },
      "source": [
        "#collapse-show\n",
        "# Based on code from PyTorch Speaker Verification:\n",
        "# https://github.com/HarryVolek/PyTorch_Speaker_Verification\n",
        "# Copyright (c) 2019, HarryVolek\n",
        "# Additions Copyright (c) 2021, Jim O'Regan\n",
        "# License: MIT\n",
        "import numpy as np\n",
        "\n",
        "# wav2vec2's max duration is 40 seconds, using 39 by default\n",
        "# to be a little safer\n",
        "def vad_concat(times, segs, max_duration=39.0):\n",
        "    \"\"\"\n",
        "    Concatenate continuous times and their segments, where the end time\n",
        "    of a segment is the same as the start time of the next\n",
        "        Parameters:\n",
        "            times: list of tuple (start, end)\n",
        "            segs: list of segments (audio frames)\n",
        "            max_duration: maximum duration of the resulting concatenated\n",
        "                segments; the kernel size of wav2vec2 is 40 seconds, so\n",
        "                the default max_duration is 39, to ensure the resulting\n",
        "                list of segments will fit\n",
        "        Returns:\n",
        "            concat_times: list of tuple (start, end)\n",
        "            concat_segs: list of segments (audio frames)\n",
        "    \"\"\"\n",
        "    absolute_maximum=40.0\n",
        "    if max_duration > absolute_maximum:\n",
        "        raise Exception('`max_duration` {:.2f} larger than kernel size (40 seconds)'.format(max_duration))\n",
        "    # we take 0.0 to mean \"don't concatenate\"\n",
        "    do_concat = (max_duration != 0.0)\n",
        "    concat_seg = []\n",
        "    concat_times = []\n",
        "    seg_concat = segs[0]\n",
        "    time_concat = times[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        can_concat = (times[i+1][1] - time_concat[0]) < max_duration\n",
        "        if time_concat[1] == times[i+1][0] and do_concat and can_concat:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "            time_concat = (time_concat[0], times[i+1][1])\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "            concat_times.append(time_concat)\n",
        "            time_concat = times[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "        concat_times.append(time_concat)\n",
        "    return concat_times, concat_seg"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOI8L_psURXV"
      },
      "source": [
        "ntimes, nsegs = vad_concat(times, segs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trkB11-R02-M"
      },
      "source": [
        "Next, I'm putting the data into a dict that Huggingface datasets can read:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phPPgOi5WDoF"
      },
      "source": [
        "starts = [s[0] for s in ntimes]\n",
        "ends = [s[1] for s in ntimes]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMYDkNeow2Pa"
      },
      "source": [
        "dset = {'start': starts,\n",
        "        'end': ends,\n",
        "        'speech': nsegs}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZufqkUX6Im"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPSklhnxN24"
      },
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(dset)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRM84NFJxWRf",
        "outputId": "201b2764-ec04-4bb8-ce4e-5e6e9ab75ccb"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['start', 'end', 'speech'],\n",
              "    num_rows: 137\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H13dhzTl1nxH"
      },
      "source": [
        "Now, the data is ready to plug into my wav2vec2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLn4iw__1tn3"
      },
      "source": [
        "%%capture\n",
        "!pip install -q transformers"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIfFtegAqIQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df32cf57-d234-4e33-b311-1f9fb9959f03"
      },
      "source": [
        "%%capture\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "# load model and tokenizer\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"jimregan/wav2vec2-large-xlsr-irish-basic\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"jimregan/wav2vec2-large-xlsr-irish-basic\")\n",
        "model.to(\"cuda\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N27cTq2rqNqV"
      },
      "source": [
        "#collapse-hide\n",
        "def speech_file_to_array_fn(batch):\n",
        "    import torchaudio\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    batch[\"speech\"] = speech_array[0].numpy()\n",
        "    batch[\"sampling_rate\"] = sampling_rate\n",
        "    batch[\"target_text\"] = batch[\"sentence\"]\n",
        "    return batch\n",
        "def evaluate(batch):\n",
        "  import torch\n",
        "  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
        "\n",
        "  pred_ids = torch.argmax(logits, dim=-1)\n",
        "  batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
        "  return batch"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "85aab658733e4c6aaa70316ad1486bd2",
            "645d349cf21c4382a8c978fcbea0af41",
            "7a288f831bfe458bb1698fe9c85edae2",
            "6fed060131f34086812cd5567e7538cc",
            "2cc66ae29b7c47d389e713e363b91c38",
            "03e934bb2747474db84b7d80c7e99c9b",
            "6eafddb79621458d8294887bdfa036fa",
            "5e929ea1d96140f999a5ba1ccab14696"
          ]
        },
        "id": "O0sn8DfJdxOs",
        "outputId": "08a069cf-9816-46d6-b02a-95001e44af3a"
      },
      "source": [
        "result = dataset.map(evaluate, batched=True, batch_size=8)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85aab658733e4c6aaa70316ad1486bd2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0UuCr7rgUjo"
      },
      "source": [
        "speechless = result.remove_columns(['speech'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skxECvmYgmx3"
      },
      "source": [
        "d=speechless.to_dict()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwHf2t0tguBP"
      },
      "source": [
        "tlog = list()\n",
        "for i in range(0, len(d['end']) - 1):\n",
        "  out = dict()\n",
        "  out['start'] = d['start'][i]\n",
        "  out['end'] = d['end'][i]\n",
        "  out['transcript'] = d['pred_strings'][i]\n",
        "  tlog.append(out)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3hJjR0ihikx"
      },
      "source": [
        "import json\n",
        "with open('/content/VRg-a0qSGa8.tlog', 'w') as outfile:\n",
        "    json.dump(tlog, outfile)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjrT1Aeqmzi5"
      },
      "source": [
        "Next, I'm extracting the text content from the vtt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNdOgJdCj0TD",
        "outputId": "d80c1450-765e-42a0-868f-b53bc677ecb2"
      },
      "source": [
        "!pip install webvtt-py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: webvtt-py in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from webvtt-py) (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcawkwNpj327"
      },
      "source": [
        "def get_vtt_text(filename):\n",
        "  import webvtt\n",
        "  out = list()\n",
        "  for sub in webvtt.read(filename):\n",
        "    out.append(sub.text)\n",
        "  return ' '.join(out)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9CRqWZMk6OH"
      },
      "source": [
        "text = get_vtt_text('/content/VRg-a0qSGa8.en.vtt')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyTxDZm8onfr"
      },
      "source": [
        "I can do some normalisation now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vBIjjrnnOmw"
      },
      "source": [
        "text = text.replace('1901', 'naoi dÃ©ag is a haon')\n",
        "text = text.replace('2021', 'fiche is fiche is a haon')\n",
        "text = text.replace('Covid-19', 'covid a naoi dÃ©ag')\n",
        "text = text.replace('fiche fiche haon', 'fiche is fiche is a haon')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TugaXN_tm6kb"
      },
      "source": [
        "I want sentences, so I'm going to use mosestokenizer to split the text (there aren't any specific abbreviations in this video, so the English splitter works fine. YMMV.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWYFUg9ulzgS"
      },
      "source": [
        "%%capture\n",
        "!pip install mosestokenizer\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zas7H8U5hg5K"
      },
      "source": [
        "The actual moses tokeniser has sentence splitting support for Irish, but the Python version was forked before that; we don't actually need any specific support for Irish here, so we can just use English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_zi73iamK7A"
      },
      "source": [
        "from mosestokenizer import MosesSentenceSplitter\n",
        "with MosesSentenceSplitter('en') as splitsents:\n",
        "  sents = splitsents([text])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYO6Q1iQmiwV"
      },
      "source": [
        "with open('/content/VRg-a0qSGa8.txt', 'w') as outfile:\n",
        "  outfile.writelines(['\\n'.join(sents)])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCnedg6yvRGX"
      },
      "source": [
        "DSAlign requires an alphabet (1 character per line), so create that first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2-cheYRvZ8i"
      },
      "source": [
        "alpha=\"aÃ¡bcdeÃ©fghiÃ­jklmnoÃ³pqrstuÃºvwxyz'-\"\n",
        "alpha_chars = [char for char in alpha]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvJSAKh0vteL"
      },
      "source": [
        "with open('/content/ga.alphabet', 'w') as outfile:\n",
        "  outfile.writelines(['\\n'.join(alpha_chars)])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMHkaflyvIHt"
      },
      "source": [
        "Now, to install DSAlign and its dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j36pyC-3r8VZ"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/mozilla/DSAlign"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge9QlSGKu-b1"
      },
      "source": [
        "%%capture\n",
        "!apt-get install sox"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qPaxkH9tBds"
      },
      "source": [
        "%%capture\n",
        "import os\n",
        "os.chdir('DSAlign')\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTAmbZ5wwW_L"
      },
      "source": [
        "Now, I'm ready to align:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp8TO2OKtKQe",
        "outputId": "f959d3c1-1599-43e6-c291-5df75f306bbe"
      },
      "source": [
        "!bin/align.sh --force --tlog /content/VRg-a0qSGa8.tlog --script /content/VRg-a0qSGa8.txt --aligned /content/VRg-a0qSGa8.aligned --text-meaningful-newlines --alphabet /content/ga.alphabet "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin/align.sh: line 3: /content/DSAlign/venv/bin/activate: No such file or directory\n",
            "INFO:root:Aligning\n",
            " 1 of 1 : 100.00% (elapsed: 00:00:04, speed: 0.25 it/s, ETA: 00:00:00)\n",
            "INFO:root:Aligned 24 fragments\n",
            "INFO:root:Dropped 112 fragments 466.67%:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5nsf4gew7oX"
      },
      "source": [
        "24 out of 136 fragments isn't great, but it's quite good considering the WER of the model (43.7%); the next step would be to add the aligned data to the training set, retrain, and repeat."
      ]
    }
  ]
}