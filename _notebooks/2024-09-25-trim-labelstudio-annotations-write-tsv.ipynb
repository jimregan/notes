{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim LabelStudio annotations\n",
    "\n",
    "> \"to match trimmed audio\"\n",
    "\n",
    "- branch: master\n",
    "- hidden: true\n",
    "- comments: false\n",
    "- categories: [labelstudio, trim, csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- normalise text for inline marks ([breath], etc.)\n",
    "- get annotation IDs in a better way (currently there's a hardcoded list)\n",
    "- do something better with truncated entries (skip?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slurpfile(filename) -> str:\n",
    "    with open(filename) as inf:\n",
    "        return inf.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API key is just read from `label_studio_mine` in the current directory (relative to the notebook). It's available under \"Accounts & Settings\" in the user menu, top right of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "timecode_dir = \"/Users/joregan/timecode_cut\"\n",
    "output_dir = \"/tmp/textgrid_cut\"\n",
    "host = \"http://130.237.3.107:8080/api/\"\n",
    "api_token: str = slurpfile(\"../_drafts/label_studio_mine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "timecode_path = Path(timecode_dir)\n",
    "output_path = Path(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timecode_offsets(filename):\n",
    "    if type(filename) == Path:\n",
    "        filename = str(filename)\n",
    "\n",
    "    with open(filename) as inf:\n",
    "        lines = [l.strip() for l in inf.readlines()]\n",
    "        assert lines[0] == \",Frame,Time (Seconds),TimeCode\", f\"CSV file ({filename}) seems to be incorrect\"\n",
    "        p_start = lines[1].split(\",\")\n",
    "        start = float(p_start[2])\n",
    "        p_end = lines[-1].split(\",\")\n",
    "        end = float(p_end[2])\n",
    "        return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {api_token}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXME: need a better way to get these than hardcoding a list, but that will take a bunch of reading API docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDS = [\n",
    "    89,\n",
    "    163,\n",
    "    164,\n",
    "    165,\n",
    "    166,\n",
    "    167,\n",
    "    168,\n",
    "    169,\n",
    "    170,\n",
    "    171,\n",
    "    172,\n",
    "    173,\n",
    "    174,\n",
    "    175,\n",
    "    176,\n",
    "    177,\n",
    "    178,\n",
    "    223,\n",
    "    224,\n",
    "    225,\n",
    "    226,\n",
    "    227,\n",
    "    228,\n",
    "    230,\n",
    "    231,\n",
    "    232,\n",
    "    233,\n",
    "    234,\n",
    "    235,\n",
    "    236,\n",
    "    237,\n",
    "    238,\n",
    "    239,\n",
    "    240,\n",
    "    241,\n",
    "    264,\n",
    "\n",
    "    286,\n",
    "    297,\n",
    "    295,\n",
    "    298,\n",
    "    290,\n",
    "    287,\n",
    "    285,\n",
    "    282,\n",
    "    281,\n",
    "    280,\n",
    "    279,\n",
    "    278,\n",
    "    277,\n",
    "    276,\n",
    "    275,\n",
    "    273,\n",
    "    271,\n",
    "    272,\n",
    "    289,\n",
    "    291,\n",
    "    292,\n",
    "    265,\n",
    "    288,\n",
    "    293,\n",
    "    299,\n",
    "    303,\n",
    "    304,\n",
    "    302,\n",
    "    267,\n",
    "    270,\n",
    "    266,\n",
    "    284,\n",
    "    162,\n",
    "    161,\n",
    "    229\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task_id):\n",
    "    ep = f\"{host}tasks/{task_id}\"\n",
    "    req = requests.get(ep, headers=headers)\n",
    "    if req.status_code != 200:\n",
    "        return {}\n",
    "    data = json.loads(req.text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation(annot_it):\n",
    "    ep = f\"{host}annotations/{annot_it}\"\n",
    "    req = requests.get(ep, headers=headers)\n",
    "    assert req.status_code == 200\n",
    "    data = json.loads(req.text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_annotation(264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_labels(data):\n",
    "    combined = {}\n",
    "\n",
    "    if \"result\" in data:\n",
    "        for res in data[\"result\"]:\n",
    "            if not res[\"id\"] in combined:\n",
    "                combined[res[\"id\"]] = res\n",
    "            else:\n",
    "                if \"text\" in res[\"value\"]:\n",
    "                    combined[res[\"id\"]][\"value\"][\"text\"] = res[\"value\"][\"text\"]\n",
    "                elif \"labels\" in res[\"value\"]:\n",
    "                    combined[res[\"id\"]][\"value\"][\"labels\"] = res[\"value\"][\"labels\"]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "OK_STARTS = [\n",
    "    \"hsi_7_0719_210_003_inter\",\n",
    "    \"hsi_4_0717_211_001_main\",\n",
    "    \"hsi_7_0719_211_004_main\",\n",
    "    \"hsi_7_0719_210_002_main\",\n",
    "    \"hsi_7_0719_211_002_main\"\n",
    "]\n",
    "OK_ENDS = [\n",
    "    \"hsi_3_0715_209_006_main\",\n",
    "    \"hsi_3_0715_227_003_main\",\n",
    "    \"hsi_7_0719_222_002_inter\",\n",
    "    \"hsi_7_0719_227_002_main\",\n",
    "    \"hsi_7_0719_211_002_main\",\n",
    "    \"hsi_6_0718_209_002_main\",\n",
    "    \"hsi_6_0718_209_001_main\",\n",
    "    \"hsi_6_0718_210_001_main\"\n",
    "]\n",
    "\n",
    "DELETE_ENDS = [\n",
    "    \"hsi_7_0719_222_004_inter\",\n",
    "    \"hsi_6_0718_209_001_inter\"\n",
    "]\n",
    "\n",
    "DELETE_STARTS = [\n",
    "    \"hsi_5_0718_209_001_main\",\n",
    "]\n",
    "\n",
    "FIXES_START = {\n",
    "    \"hsi_3_0715_210_010_main\": \"o welcome\",\n",
    "    \"hsi_6_0718_209_001_inter\": \"Hi. It's like it looks like a very nice place you have here.\",\n",
    "    \"hsi_7_0719_227_002_inter\": \"Yeah that's a very nice apartment you have here.\",\n",
    "    \"hsi_7_0719_222_002_main\": \"you know, it was really expensive property.\"\n",
    "}\n",
    "\n",
    "FIXES_END = {\n",
    "    \"hsi_7_0719_209_001_inter\": \"Okay, but yeah, yeah\",\n",
    "    \"hsi_7_0719_210_001_main\": \"sometimes when I am reading and I feel that I don't have that much light [smack] I just turn that on. But uh otherwise I just keep this setup here with the small lamp and the chair and I just read something have a glass of wine or something that's real\",\n",
    "    \"hsi_7_0719_222_004_main\": \"But maybe maybe if you if you put a small table, like two small tables next, I mean, i- on both sides of the fireplace\",\n",
    "    \"hsi_5_0718_222_003_main\": \"so so and i was thinking of the bins there, uh, do you need one of the bins because you could take one if you want to yes of course take it it's you can have it because i was anyway trying to so so yeah okay\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_times_write_tsv(data):\n",
    "    task = data[\"task\"]\n",
    "    task_data = get_task(task)\n",
    "    if \"data\" in task_data and \"audio\" in task_data[\"data\"]:\n",
    "        orig_file = task_data[\"data\"][\"audio\"]\n",
    "        parts = orig_file.split(\"/\")\n",
    "        orig_file = parts[-1]\n",
    "    if orig_file:\n",
    "        out_part = orig_file.replace(\".wav\", \".csv\")\n",
    "        orig_file = out_part.replace(\"_main\", \"\").replace(\"_inter\", \"\")\n",
    "    else:\n",
    "        return []\n",
    "    tsv_file = timecode_path / orig_file\n",
    "    out_file = output_path / out_part\n",
    "\n",
    "    if not tsv_file.exists():\n",
    "        return []\n",
    "    start, end = get_timecode_offsets(str(tsv_file))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    combined = combine_labels(data)\n",
    "\n",
    "    for item in combined:\n",
    "        val = combined[item][\"value\"]\n",
    "        if not \"labels\" in val:\n",
    "            continue\n",
    "        if not \"Speech\" in val[\"labels\"]:\n",
    "            continue\n",
    "        e_start = val[\"start\"]\n",
    "        e_end = val[\"end\"]\n",
    "        text = val[\"text\"]\n",
    "        if len(text) > 1:\n",
    "            for t in text:\n",
    "                t = t.strip()\n",
    "                if not (t.startswith(\"/\") and t.endswith(\"/\")):\n",
    "                    text = t\n",
    "        else:\n",
    "            if text[0].startswith(\"/\"):\n",
    "                text = None\n",
    "            else:\n",
    "                text = text[0]\n",
    "\n",
    "        new_start = e_start - start\n",
    "        new_end = e_end - start\n",
    "\n",
    "        if text is None:\n",
    "            continue\n",
    "\n",
    "        out_stem = out_part.replace(\".csv\", \"\")\n",
    "\n",
    "        if new_end < 0.0:\n",
    "            continue\n",
    "        elif e_start >= end and e_end > end:\n",
    "            continue\n",
    "        elif new_start < 0.0 and new_end > 0.0:\n",
    "            if text != \"\":\n",
    "                if out_stem in FIXES_START:\n",
    "                    text = FIXES_START[out_stem]\n",
    "                if out_stem in DELETE_STARTS:\n",
    "                    continue\n",
    "                if not (out_stem in OK_STARTS or out_stem in FIXES_START):\n",
    "                    print(\"Warning\", out_part, \"truncating entry\", e_start, e_end, text)\n",
    "            if results == []:\n",
    "                results.append((0.0, new_end, text))\n",
    "            else:\n",
    "                print(\"Shouldn't have existing entries!!\", out_part, e_start, e_end, text)\n",
    "                results.append((0.0, new_end, text))\n",
    "        elif e_start >= start and e_end <= end:\n",
    "            results.append((new_start, new_end, text))\n",
    "        elif e_start <= end and e_end > end:\n",
    "            if text != \"\":\n",
    "                if out_stem in DELETE_ENDS:\n",
    "                    continue\n",
    "                if out_stem in FIXES_END:\n",
    "                    text = FIXES_END[out_stem]\n",
    "                if not (out_stem in OK_ENDS or out_stem in FIXES_END):\n",
    "                    print(\"Warning\", out_part, \"truncating entry\", e_start, e_end, text)\n",
    "            results.append((new_start, new_end, text))\n",
    "        else:\n",
    "            print(\"There should be no default case\", out_part, e_start, e_end, text)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "    with open(out_file, \"w\") as outf:\n",
    "        for res in sorted_results:\n",
    "            outf.write(\"\\t\".join([str(x) for x in list(res)]) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcription in IDS:\n",
    "    data = get_annotation(transcription)\n",
    "    if not \"task\" in data:\n",
    "        print(\"Error with task\", transcription)\n",
    "        continue\n",
    "    else:\n",
    "        adjust_times_write_tsv(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
