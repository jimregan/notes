{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CLDR to OpenFST\n",
        "\n",
        "> \"ChatGPT generated code; only basic part works\"\n",
        "\n",
        "- categories: [cldr, openfst, chatgpt]\n",
        "- branch: master\n",
        "- hidden: true\n",
        "- badges: true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtW5rX6n2WjU",
        "outputId": "d5a06676-e003-499a-dc1f-e333f47eb41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pynini in /usr/local/lib/python3.12/dist-packages (2.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pynini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__HGtWPx2X_7"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import pynini\n",
        "from pynini import *\n",
        "from pynini.lib import pynutil\n",
        "from pynini.lib import utf8\n",
        "\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes like \\\\u0259 and \\\\U0001F600.\"\"\"\n",
        "    def rpl4(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    def rpl8(m):\n",
        "        return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    return s\n",
        "\n",
        "def _expand_char_class(cls: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Expand a simple bracket class like [abc] or [a-z].\n",
        "    This is intentionally minimal; it does not handle nested classes, properties, or set ops.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(cls):\n",
        "        if i + 2 < len(cls) and cls[i+1] == \"-\":\n",
        "            start = ord(cls[i])\n",
        "            end = ord(cls[i+2])\n",
        "            for cp in range(start, end + 1):\n",
        "                out.append(chr(cp))\n",
        "            i += 3\n",
        "        else:\n",
        "            out.append(cls[i])\n",
        "            i += 1\n",
        "    return out\n",
        "\n",
        "def _tokenize_pattern(pat: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a very small subset of CLDR pattern syntax:\n",
        "    - Literal characters\n",
        "    - Character classes [..]\n",
        "    Returns a list of alternatives (strings) if the pattern is a single char-class;\n",
        "    otherwise returns [pat] (treated as a literal string).\n",
        "    \"\"\"\n",
        "    pat = pat.strip()\n",
        "    if len(pat) >= 2 and pat[0] == \"[\" and pat[-1] == \"]\":\n",
        "        inner = _decode_escapes(pat[1:-1])\n",
        "        alts = _expand_char_class(inner)\n",
        "        return alts\n",
        "    return [_decode_escapes(pat)]\n",
        "\n",
        "def _string_map(pairs: List[Tuple[str, str]]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a union of literal string transductions, determinize & minimize.\n",
        "    \"\"\"\n",
        "    t = pynini.string_map(pairs)\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "_RULE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<lhs>.+?)\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?)\n",
        "    ;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    lhs: str\n",
        "    rhs: str\n",
        "    op: str\n",
        "\n",
        "def parse_cldr_rules_simple(text: str) -> List[Rule]:\n",
        "    \"\"\"\n",
        "    Parse a subset of CLDR rule lines:\n",
        "      LHS > RHS ;\n",
        "      LHS < RHS ;\n",
        "      LHS <> RHS ;\n",
        "    Strips comments (# ...) and blank lines.\n",
        "    Ignores directives (:: ... ;)\n",
        "    \"\"\"\n",
        "    rules: List[Rule] = []\n",
        "    for raw in text.splitlines():\n",
        "        line = raw.strip()\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "        if line.startswith(\"::\"):\n",
        "            # ignore directives in this simple converter\n",
        "            continue\n",
        "        m = _RULE_RE.match(line)\n",
        "        if not m:\n",
        "            # Not supported yet; skip quietly so you can still test quickly.\n",
        "            continue\n",
        "        lhs = m.group(\"lhs\").strip()\n",
        "        rhs = m.group(\"rhs\").strip()\n",
        "        op = m.group(\"op\")\n",
        "        rules.append(Rule(lhs=lhs, rhs=rhs, op=op))\n",
        "    return rules\n",
        "\n",
        "\n",
        "def build_transducer_from_rules(rules: List[Rule]) -> Fst:\n",
        "    \"\"\"\n",
        "    Build a transducer from simple (context-free) CLDR rules.\n",
        "    Uses cdrewrite with utf8.VALID_UTF8.closure() so non-matching chars pass through.\n",
        "    \"\"\"\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "\n",
        "    for r in rules:\n",
        "        lhs_alts = _tokenize_pattern(r.lhs)\n",
        "        rhs_alts = _tokenize_pattern(r.rhs)\n",
        "\n",
        "        def add_pairs(A: List[str], B: List[str]):\n",
        "            if len(A) == len(B):\n",
        "                for x, y in zip(A, B):\n",
        "                    pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "            else:\n",
        "                for x in A:\n",
        "                    for y in B:\n",
        "                        pairs.append((_decode_escapes(x), _decode_escapes(y)))\n",
        "\n",
        "        if r.op == \">\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "        elif r.op == \"<\":\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "        elif r.op == \"<>\":\n",
        "            add_pairs(lhs_alts, rhs_alts)\n",
        "            add_pairs(rhs_alts, lhs_alts)\n",
        "\n",
        "    sigma_star = utf8.VALID_UTF8_CHAR.closure()\n",
        "\n",
        "    if not pairs:\n",
        "        # Identity FST if no rules\n",
        "        return pynini.cdrewrite(pynini.cross(\"\", \"\"), \"\", \"\", sigma_star)\n",
        "\n",
        "    # Sequentially compose cdrewrite rules\n",
        "    t = None\n",
        "    for src, tgt in pairs:\n",
        "        rule = pynini.cdrewrite(\n",
        "            pynini.cross(src, tgt),\n",
        "            \"\", \"\", sigma_star\n",
        "        )\n",
        "        t = rule if t is None else (t @ rule)\n",
        "\n",
        "    t.optimize()\n",
        "    return t\n",
        "\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    \"\"\"\n",
        "    Create an acceptor for a language prefix token like '<eng>'.\n",
        "    We allow it as literal at the beginning and then delete it from output:\n",
        "      '<eng>' x ...  -> ... (so the prefix doesn't appear in the output)\n",
        "    \"\"\"\n",
        "    # Literal token\n",
        "    token_acceptor = pynini.accep(lang_token)\n",
        "    # We want to delete it from output: cross(lang_token, \"\").\n",
        "    return pynini.cross(lang_token, \"\")\n",
        "\n",
        "\n",
        "def build_multilingual_transducer(\n",
        "    lang_to_rules_text: Dict[str, str],\n",
        "    token_fmt: str = \"<{lang}>\",\n",
        ") -> Fst:\n",
        "    \"\"\"\n",
        "    For each language:\n",
        "      - parse rules\n",
        "      - build transducer\n",
        "      - prepend a required language token (deleted on output)\n",
        "    Then union all.\n",
        "    \"\"\"\n",
        "    unified = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        rules = parse_cldr_rules_simple(text)\n",
        "        t = build_transducer_from_rules(rules)\n",
        "        pref = prefix_language_token(token_fmt.format(lang=lang))\n",
        "        lang_t = pref + t  # concatenation; prefix must come first\n",
        "        lang_t.optimize()\n",
        "        unified = lang_t if unified is None else (unified | lang_t)\n",
        "\n",
        "    if unified is None:\n",
        "        # No rules: accept anything and echo it (after removing an imaginary token)\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "    unified.optimize()\n",
        "    # Determinize/minimize for speed\n",
        "    unified = pynini.determinize(unified).minimize()\n",
        "    return unified\n",
        "\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "\n",
        "def demo():\n",
        "    cldr_en = r\"\"\"\n",
        "        # English-ish toy rules\n",
        "        th > θ ;\n",
        "        sh > ʃ ;\n",
        "        ch > t͡ʃ ;\n",
        "        ng > ŋ ;\n",
        "        [aeiou] > a ;   # silly vowel collapse to 'a'\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        # Spanish-ish toy rules\n",
        "        ll > ʎ ;\n",
        "        ñ > ɲ ;\n",
        "        qu > k ;\n",
        "        c > k ;\n",
        "        z > s ;\n",
        "        [aeiou] > a ;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\n",
        "        \"eng\": cldr_en,\n",
        "        \"spa\": cldr_es,\n",
        "    }\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    test_inputs = [\n",
        "        \"<eng>thing\",\n",
        "        \"<eng>mashing\",\n",
        "        \"<spa>llama\",\n",
        "        \"<spa>quiza\",\n",
        "    ]\n",
        "\n",
        "    def apply(input_str: str) -> str:\n",
        "        lattice = pynini.compose(input_str, fst)\n",
        "        if lattice.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "        try:\n",
        "            return pynini.shortestpath(lattice, 1).string()\n",
        "        except Exception:\n",
        "            return \"<no-output>\"\n",
        "\n",
        "    for s in test_inputs:\n",
        "        print(s, \"->\", apply(s))\n",
        "\n",
        "    print(\"Wrote combined FST to multilang.fst\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99J8j-Ct3Djl",
        "outputId": "b0d2f7c2-5ac0-4564-b522-5d0e49cce29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<eng>thing -> θiŋ\n",
            "<eng>mashing -> maʃiŋ\n",
            "<spa>llama -> ʎama\n",
            "<spa>quiza -> kasa\n",
            "Wrote combined FST to multilang.fst\n"
          ]
        }
      ],
      "source": [
        "demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Xm09hR72On1z",
        "outputId": "c0e7f544-1294-4709-b37b-36e0c0c31137"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Failed to determinize/minimize final language cascade. Error: Operation failed\n",
            "Warning: Failed to determinize/minimize final language cascade. Error: Operation failed\n"
          ]
        },
        {
          "ename": "FstOpError",
          "evalue": "Operation failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFstOpError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2617454326.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2617454326.py\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mlang_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcldr_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spa\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcldr_es\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0mfst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multilingual_transducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_fmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<{lang}>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0msave_fst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilang.fst\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2617454326.py\u001b[0m in \u001b[0;36mbuild_multilingual_transducer\u001b[0;34m(lang_to_rules_text, token_fmt)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0munified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpynini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransducer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpynini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterminize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_fst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mextensions/_pynini.pyx\u001b[0m in \u001b[0;36m_pynini._1arg_patch.patch\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mextensions/_pywrapfst.pyx\u001b[0m in \u001b[0;36m_pywrapfst.determinize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mextensions/_pywrapfst.pyx\u001b[0m in \u001b[0;36m_pywrapfst.determinize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mextensions/_pywrapfst.pyx\u001b[0m in \u001b[0;36m_pywrapfst._init_MutableFst\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFstOpError\u001b[0m: Operation failed"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import sys\n",
        "import io\n",
        "\n",
        "import pynini\n",
        "from pynini import Fst\n",
        "from pynini.lib import utf8\n",
        "\n",
        "_U_HEX = re.compile(r\"\\\\u([0-9A-Fa-f]{4})\")\n",
        "_U_HEX_LONG = re.compile(r\"\\\\U([0-9A-Fa-f]{6,8})\")\n",
        "_ESCAPED = re.compile(r\"\\\\([][\\\\/\\-^(){}_.*+?|])\")\n",
        "\n",
        "def _decode_escapes(s: str) -> str:\n",
        "    \"\"\"Decodes CLDR-style escapes.\"\"\"\n",
        "    def rpl4(m): return chr(int(m.group(1), 16))\n",
        "    def rpl8(m): return chr(int(m.group(1), 16))\n",
        "    s = _U_HEX.sub(rpl4, s)\n",
        "    s = _U_HEX_LONG.sub(rpl8, s)\n",
        "    s = _ESCAPED.sub(lambda m: m.group(1), s)\n",
        "    return s\n",
        "\n",
        "def _char_range(a: str, b: str) -> List[str]:\n",
        "    return [chr(cp) for cp in range(ord(a), ord(b) + 1)]\n",
        "\n",
        "def _parse_unicode_set(text: str) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"Simple parser for CLDR-style character sets like [a-z] or [^abc].\"\"\"\n",
        "    assert text.startswith(\"[\") and text.endswith(\"]\"), \"not a set\"\n",
        "    inner = text[1:-1]\n",
        "    neg = inner.startswith(\"^\")\n",
        "    if neg: inner = inner[1:]\n",
        "    items: List[str] = []\n",
        "    i = 0\n",
        "    def read_char(ix: int) -> Tuple[str, int]:\n",
        "        if ix < len(inner) and inner[ix] == \"\\\\\":\n",
        "            m4 = _U_HEX.match(inner, ix)\n",
        "            if m4: return (chr(int(m4.group(1), 16)), m4.end())\n",
        "            m8 = _U_HEX_LONG.match(inner, ix)\n",
        "            if m8: return (chr(int(m8.group(1), 16)), m8.end())\n",
        "            if ix + 1 < len(inner): return (inner[ix + 1], ix + 2)\n",
        "            return (\"\\\\\", ix + 1)\n",
        "        return (inner[ix], ix + 1)\n",
        "    while i < len(inner):\n",
        "        c1, j = read_char(i)\n",
        "        if j < len(inner) - 1 and inner[j] == \"-\" and j + 1 < len(inner):\n",
        "            c2, k = read_char(j + 1)\n",
        "            items.extend(_char_range(c1, c2))\n",
        "            i = k\n",
        "        else:\n",
        "            items.append(c1)\n",
        "            i = j\n",
        "    return (neg, items)\n",
        "\n",
        "def _acceptor(s: str) -> Fst:\n",
        "    return pynini.accep(s, token_type=\"utf8\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Rule:\n",
        "    # CLDR syntax: May contain embedded context markers like '}' or '{'\n",
        "    raw_lhs: str\n",
        "    rhs: str\n",
        "    op: str\n",
        "\n",
        "    # Pynini syntax: Explicit L and R contexts (parsed from raw_lhs/rhs if markers are found)\n",
        "    lhs: str = \"\"   # X (The target string being rewritten)\n",
        "    left: str = \"\"  # L (The preceding context)\n",
        "    right: str = \"\" # R (The following context)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Directive:\n",
        "    kind: str\n",
        "    payload: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class VarAssign:\n",
        "    name: str\n",
        "    expr: str\n",
        "\n",
        "Line = Tuple[str, object]\n",
        "\n",
        "# CLDR rule format: [CONTEXT] TARGET [CONTEXT] OP RESULT ;\n",
        "# E.g.: [ei] } c > s; (Rewrite c to s if preceded by [ei])\n",
        "_CLDR_RULE_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<raw_lhs>.+?) # Everything before the operator\n",
        "    \\s*\n",
        "    (?P<op><>|>|<)\n",
        "    \\s*\n",
        "    (?P<rhs>.+?) # Everything after the operator until the semicolon\n",
        "    \\s*;\n",
        "    \\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "_VAR_RE = re.compile(r\"\"\"\n",
        "    ^\\s*\n",
        "    (?P<name>\\$[A-Za-z0-9_]+)\n",
        "    \\s*=\\s*\n",
        "    (?P<expr>.+?)\n",
        "    \\s*;?\\s*$\n",
        "\"\"\", re.VERBOSE)\n",
        "\n",
        "def _extract_cldr_context_parts(raw_lhs: str) -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Transforms a raw CLDR LHS string (which may contain '}' or '{' markers)\n",
        "    into the three Pynini rewrite components: L (Left Context), X (LHS Target), R (Right Context).\n",
        "\n",
        "    CLDR conventions:\n",
        "    1. L } X: Lookbehind: L is context, X is target. R is empty.\n",
        "    2. X { R: Lookahead: R is context, X is target. L is empty.\n",
        "    3. L / X _ R: Full Context (ignored for this version, assumes L}X or X{R).\n",
        "\n",
        "    Returns: (L_ctx_str, X_target_str, R_ctx_str)\n",
        "    \"\"\"\n",
        "    s = raw_lhs.strip()\n",
        "\n",
        "    if s.count('}') > 1 or s.count('{') > 1 or (s.count('}') == 1 and s.count('{') == 1):\n",
        "        raise ValueError(f\"Ambiguous context markers in CLDR rule LHS: {raw_lhs}\")\n",
        "\n",
        "    if '}' in s:\n",
        "        # Case 1: L } X (Lookbehind)\n",
        "        parts = s.split('}', 1)\n",
        "        L_ctx = parts[0].strip()\n",
        "        X_target = parts[1].strip()\n",
        "        return (L_ctx, X_target, \"\")\n",
        "\n",
        "    if '{' in s:\n",
        "        # Case 2: X { R (Lookahead)\n",
        "        parts = s.split('{', 1)\n",
        "        X_target = parts[0].strip()\n",
        "        R_ctx = parts[1].strip()\n",
        "        return (\"\", X_target, R_ctx)\n",
        "\n",
        "    # Case 3: Simple X > Y rule (no context markers)\n",
        "    return (\"\", s, \"\")\n",
        "\n",
        "\n",
        "def _parse_line(line: str) -> Optional[Line]:\n",
        "    s = line.strip()\n",
        "    if not s or s.startswith(\"#\"):\n",
        "        return None\n",
        "\n",
        "    mvar = _VAR_RE.match(s)\n",
        "    if mvar and mvar.group(\"expr\").strip():\n",
        "        name = mvar.group(\"name\")\n",
        "        expr = mvar.group(\"expr\").strip().rstrip(';')\n",
        "        return (\"var\", VarAssign(name=name, expr=expr))\n",
        "\n",
        "    if s.startswith(\"::\"):\n",
        "        body = s[2:].strip()\n",
        "        if body.lower() == \"null;\": return (\"dir\", Directive(kind=\"null\"))\n",
        "        if body.lower() == \"nfd;\": return (\"dir\", Directive(kind=\"nfd\"))\n",
        "        if body.lower() == \"nfc;\": return (\"dir\", Directive(kind=\"nfc\"))\n",
        "        if body.startswith(\"[\") and body.endswith(\";\"):\n",
        "            payload = body[:-1].strip()\n",
        "            return (\"dir\", Directive(kind=\"filter\", payload=payload))\n",
        "        return (\"dir\", Directive(kind=\"unknown\", payload=body))\n",
        "\n",
        "    m = _CLDR_RULE_RE.match(s)\n",
        "    if m:\n",
        "        raw_lhs = m.group(\"raw_lhs\").strip()\n",
        "        rhs_core = m.group(\"rhs\").strip()\n",
        "        op = m.group(\"op\")\n",
        "\n",
        "        try:\n",
        "            # New logic: Parse L, X, R from the raw LHS string\n",
        "            L_ctx_str, X_target_str, R_ctx_str = _extract_cldr_context_parts(raw_lhs)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error parsing CLDR rule '{s}': {e}\", file=sys.stderr)\n",
        "            return None\n",
        "\n",
        "        # Populate the Rule object with the Pynini components\n",
        "        return (\"rule\", Rule(\n",
        "            raw_lhs=raw_lhs,\n",
        "            rhs=rhs_core,\n",
        "            op=op,\n",
        "            lhs=X_target_str,\n",
        "            left=L_ctx_str,\n",
        "            right=R_ctx_str\n",
        "        ))\n",
        "    return None\n",
        "\n",
        "def parse_cldr(text: str) -> List[Line]:\n",
        "    out: List[Line] = []\n",
        "    for raw in text.splitlines():\n",
        "        p = _parse_line(raw)\n",
        "        if p: out.append(p)\n",
        "    return out\n",
        "\n",
        "class Env:\n",
        "    def __init__(self) -> None:\n",
        "        self.vars: Dict[str, Fst] = {}\n",
        "        self.sigma: Fst = utf8.VALID_UTF8_CHAR.optimize()\n",
        "        self.sigma_star: Fst = self.sigma.closure().optimize()\n",
        "        I_char = pynini.cross(self.sigma, self.sigma)\n",
        "        self.I_sigma_star: Fst = I_char.closure().optimize()\n",
        "\n",
        "    def get(self, name: str) -> Fst:\n",
        "        if name not in self.vars:\n",
        "            raise KeyError(f\"Undefined variable {name}\")\n",
        "        return self.vars[name]\n",
        "\n",
        "def _compile_atom(expr: str, env: Env) -> Fst:\n",
        "    expr = expr.strip()\n",
        "    if expr.startswith(\"$\"):\n",
        "        return env.get(expr)\n",
        "\n",
        "    # CLDR Boundary Support: Treat ^ (start) and $ (end) as empty string acceptors\n",
        "    if expr == \"^\" or expr == \"$\":\n",
        "        return _acceptor(\"\")\n",
        "\n",
        "    if expr.startswith(\"[\") and expr.endswith(\"]\"):\n",
        "        neg, items = _parse_unicode_set(expr)\n",
        "        if not items:\n",
        "            return env.sigma if neg else pynini.Fst()\n",
        "        u = pynini.union(*(_acceptor(ch) for ch in items)).optimize()\n",
        "        if neg:\n",
        "            return (env.sigma - u).optimize()\n",
        "        return u\n",
        "\n",
        "    return _acceptor(_decode_escapes(expr))\n",
        "\n",
        "def _compile_seq(expr: Optional[str], env: Env) -> Fst:\n",
        "    \"\"\"Compiles a sequence of atoms (literals, sets, variables, boundaries) into an Fst acceptor.\"\"\"\n",
        "    if not expr: return _acceptor(\"\")\n",
        "    s = expr\n",
        "    parts: List[str] = []\n",
        "    i = 0\n",
        "    cur = []\n",
        "    depth = 0\n",
        "\n",
        "    # Simple tokenizer logic\n",
        "    while i < len(s):\n",
        "        ch = s[i]\n",
        "        is_escape_char = (ch == '\\\\') and (i + 1 < len(s) and s[i+1].isalpha())\n",
        "\n",
        "        # Token separation logic\n",
        "        if ch == \"[\" and not is_escape_char: depth += 1\n",
        "        elif ch == \"]\" and depth > 0 and not is_escape_char: depth -= 1\n",
        "        elif ch.isspace() and depth == 0:\n",
        "            if cur: parts.append(\"\".join(cur)); cur = []\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        cur.append(ch)\n",
        "        i += 1\n",
        "        if is_escape_char and i < len(s):\n",
        "            pass\n",
        "\n",
        "    if cur: parts.append(\"\".join(cur))\n",
        "    if not parts: return _acceptor(\"\")\n",
        "\n",
        "    # Composition of FST atoms\n",
        "    fst = _compile_atom(parts[0], env)\n",
        "    for p in parts[1:]:\n",
        "        fst = fst + _compile_atom(p, env)\n",
        "    return fst.optimize()\n",
        "\n",
        "def compile_lines(lines: List[Line]) -> Fst:\n",
        "    env = Env()\n",
        "\n",
        "    vars_to_compile: List[VarAssign] = []\n",
        "    for kind, payload in lines:\n",
        "        if kind == \"var\":\n",
        "            vars_to_compile.append(payload) # type: ignore[arg-type]\n",
        "\n",
        "    for va in vars_to_compile:\n",
        "        env.vars[va.name] = _compile_seq(va.expr, env).optimize()\n",
        "\n",
        "    all_rules: List[Rule] = []\n",
        "    for kind, payload in lines:\n",
        "        if kind == \"rule\":\n",
        "            all_rules.append(payload) # type: ignore[arg-type]\n",
        "\n",
        "    cascade = env.I_sigma_star\n",
        "\n",
        "    for r in all_rules:\n",
        "        # X is the target (r.lhs), Y is the replacement (r.rhs)\n",
        "        X_target = _compile_seq(r.lhs, env)\n",
        "        Y_replacement = _compile_seq(r.rhs, env)\n",
        "\n",
        "        # L and R are the context FSTs (r.left, r.right)\n",
        "        L_ctx = _compile_seq(r.left, env)\n",
        "        R_ctx = _compile_seq(r.right, env)\n",
        "\n",
        "        def add_rule(X: Fst, Y: Fst, L: Fst, R: Fst, raw_rule: str):\n",
        "            nonlocal cascade\n",
        "\n",
        "            if X.num_states() == 0:\n",
        "                print(f\"Warning: Empty LHS FST for rule '{raw_rule}'. Skipping.\")\n",
        "                return\n",
        "\n",
        "            # Create the rewriting transducer (tau = X -> Y)\n",
        "            try:\n",
        "                rewrite = pynini.cdrewrite(\n",
        "                    pynini.cross(X, Y),\n",
        "                    L, R,\n",
        "                    env.sigma_star\n",
        "                ).optimize()\n",
        "            except pynini.FstOpError as e:\n",
        "                print(f\"Severe Warning: Rewrite failed for rule '{raw_rule}'. Error: {e}\", file=sys.stderr)\n",
        "                return\n",
        "\n",
        "            composed = cascade @ rewrite\n",
        "            cascade = composed.optimize()\n",
        "\n",
        "        # The rule structure requires L, X, R to be correctly assigned before calling add_rule\n",
        "        # This is handled by _extract_cldr_context_parts\n",
        "        if r.op == \">\":\n",
        "            add_rule(X_target, Y_replacement, L_ctx, R_ctx, r.raw_lhs + r.op + r.rhs)\n",
        "        elif r.op == \"<\":\n",
        "            # Inverse: Y->X / R_rev _ L_rev\n",
        "            add_rule(Y_replacement, X_target, R_ctx, L_ctx, r.raw_lhs + r.op + r.rhs)\n",
        "        elif r.op == \"<>\":\n",
        "            add_rule(X_target, Y_replacement, L_ctx, R_ctx, r.raw_lhs + r.op + r.rhs)\n",
        "            add_rule(Y_replacement, X_target, R_ctx, L_ctx, r.raw_lhs + r.op + r.rhs)\n",
        "\n",
        "    try:\n",
        "        cascade = pynini.determinize(cascade).minimize()\n",
        "    except pynini.FstOpError as e:\n",
        "        print(f\"Warning: Failed to determinize/minimize final language cascade. Error: {e}\", file=sys.stderr)\n",
        "        cascade.optimize()\n",
        "\n",
        "    return cascade\n",
        "\n",
        "def prefix_language_token(lang_token: str) -> Fst:\n",
        "    return pynini.cross(_acceptor(lang_token), _acceptor(\"\"))\n",
        "\n",
        "def build_multilingual_transducer(lang_to_rules_text: Dict[str, str], token_fmt: str = \"<{lang}>\") -> Fst:\n",
        "    unified: Optional[Fst] = None\n",
        "    for lang, text in lang_to_rules_text.items():\n",
        "        lines = parse_cldr(text)\n",
        "        t = compile_lines(lines)\n",
        "        lt = prefix_language_token(token_fmt.format(lang=lang)) + t\n",
        "        lt.optimize()\n",
        "        unified = lt if unified is None else (unified | lt)\n",
        "    if unified is None:\n",
        "        unified = pynini.transducer(\"\", \"\")\n",
        "\n",
        "    return pynini.determinize(unified).minimize()\n",
        "\n",
        "def save_fst(fst: Fst, path: str) -> None:\n",
        "    fst.write(path)\n",
        "\n",
        "def demo():\n",
        "    cldr_en = r\"\"\"\n",
        "        $V = [aeiou] ;\n",
        "        # Simple rewrite (no context)\n",
        "        th > θ ;\n",
        "        sh > ʃ ;\n",
        "        ng > ŋ ;\n",
        "        ch > t͡ʃ ;\n",
        "    \"\"\"\n",
        "\n",
        "    cldr_es = r\"\"\"\n",
        "        $V = [aeiou] ;\n",
        "        $E_OR_I = [ei];\n",
        "        :: [a-zñ] ;\n",
        "\n",
        "        # 1. CLDR Lookahead (Soft C): Rewrite 'c' to 's' if FOLLOWED by 'e' or 'i'.\n",
        "        # X { R > Y: Target X=c, R=E_OR_I, Y=s.\n",
        "        c { $E_OR_I > s;\n",
        "\n",
        "        # 2. CLDR Lookbehind: Rewrite 's' to 'z' if PRECEDED by a vowel.\n",
        "        # L } X > Y: L=V, X=s, Y=z.\n",
        "        $V } s > z;\n",
        "\n",
        "        # 3. CLDR Lookbehind (Boundary): Rewrite ll to lambda if preceded by start of string\n",
        "        ^ } ll > ʎ;\n",
        "\n",
        "        # 4. Simple rewrites\n",
        "        ñ > ɲ ;\n",
        "        qu > k ;\n",
        "\n",
        "        ::Null;\n",
        "    \"\"\"\n",
        "\n",
        "    lang_rules = {\"eng\": cldr_en, \"spa\": cldr_es}\n",
        "\n",
        "    fst = build_multilingual_transducer(lang_rules, token_fmt=\"<{lang}>\")\n",
        "    save_fst(fst, \"multilang.fst\")\n",
        "\n",
        "    tests = [\n",
        "        \"<eng>thing\",      # th -> θ\n",
        "        \"<eng>mashing\",    # sh -> ʃ, ng -> ŋ\n",
        "\n",
        "        # SPA tests using CLDR syntax:\n",
        "        \"<spa>cama\",     # Rule 1: 'c' followed by 'a' (no match) -> cama. Rule 2: 's' not present. -> cama\n",
        "        \"<spa>cita\",     # Rule 1: c { [ei] -> s: sita. Rule 2: 's' preceded by 'i' ($V } s): sita -> siza. Final: siza\n",
        "        \"<spa>ceca\",     # Rule 1: c { [ei] -> s: seca. Rule 2: 's' preceded by 'e' ($V } s): seca -> seza. Final: seza\n",
        "        \"<spa>sopa\",     # Rule 2: 's' not preceded by vowel (at start) -> sopa\n",
        "        \"<spa>mesa\",     # Rule 2: $V } s -> z: meza\n",
        "        \"<spa>acacia\",   # 2nd 'c' followed by 'i' (Rule 1): acasia. 1st 's' preceded by 'a' (Rule 2): acazia. Final: acazia\n",
        "        \"<spa>llave\",    # Rule 3: ^ } ll -> ʎ: ʎave\n",
        "    ]\n",
        "\n",
        "    def apply(s: str) -> str:\n",
        "        lat = pynini.compose(_acceptor(s), fst)\n",
        "        if lat.start() == pynini.NO_STATE_ID:\n",
        "            return \"<no-path>\"\n",
        "\n",
        "        raw_output = pynini.shortestpath(lat, 1).string(token_type=\"utf8\")\n",
        "        if isinstance(raw_output, bytes):\n",
        "            return raw_output.decode(\"utf-8\")\n",
        "        return raw_output\n",
        "\n",
        "    print(\"--- Transliteration Results ---\")\n",
        "    for t in tests:\n",
        "        result = apply(t)\n",
        "        print(f\"{t} -> {result}\")\n",
        "\n",
        "    print(\"Wrote multilang.fst\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRC-w_q2oS6m",
        "outputId": "4e9e609d-89f2-43fa-82fb-03f00cd00d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<eng>thing -> <no-path>\n",
            "<eng>mashing -> <no-path>\n",
            "<spa>llama -> <no-path>\n",
            "<spa>quiza -> kasa\n",
            "<spa>cita -> sata\n",
            "<spa>cuna -> kana\n",
            "Wrote multilang.fst\n"
          ]
        }
      ],
      "source": [
        "demo()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
