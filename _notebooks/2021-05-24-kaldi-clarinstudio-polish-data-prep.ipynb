{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ac39e0",
   "metadata": {},
   "source": [
    "# Training Kaldi on Kaggle: Data Prep\n",
    "\n",
    "> \"Training Kaldi on Kaggle needs to be split into steps\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- hidden: true\n",
    "- categories: [kaggle, kaldi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ordinary-insider",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:12:04.730215Z",
     "iopub.status.busy": "2021-05-24T21:12:04.729202Z",
     "iopub.status.idle": "2021-05-24T21:12:04.740033Z",
     "shell.execute_reply": "2021-05-24T21:12:04.739414Z",
     "shell.execute_reply.started": "2021-05-24T21:10:29.833263Z"
    },
    "papermill": {
     "duration": 0.026421,
     "end_time": "2021-05-24T21:12:04.740188",
     "exception": false,
     "start_time": "2021-05-24T21:12:04.713767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt\n"
     ]
    }
   ],
   "source": [
    "%cd /opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-preliminary",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-24T21:12:04.770386Z",
     "iopub.status.busy": "2021-05-24T21:12:04.769656Z",
     "iopub.status.idle": "2021-05-24T21:12:37.407297Z",
     "shell.execute_reply": "2021-05-24T21:12:37.406744Z",
     "shell.execute_reply.started": "2021-05-24T21:10:33.473835Z"
    },
    "papermill": {
     "duration": 32.655149,
     "end_time": "2021-05-24T21:12:37.407453",
     "exception": false,
     "start_time": "2021-05-24T21:12:04.752304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!tar xvf /kaggle/input/extract-prebuilt-kaldi-from-docker/kaldi.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extra-jurisdiction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:12:37.437633Z",
     "iopub.status.busy": "2021-05-24T21:12:37.436803Z",
     "iopub.status.idle": "2021-05-24T21:12:37.442078Z",
     "shell.execute_reply": "2021-05-24T21:12:37.442663Z",
     "shell.execute_reply.started": "2021-05-24T18:50:35.686028Z"
    },
    "papermill": {
     "duration": 0.023666,
     "end_time": "2021-05-24T21:12:37.442899",
     "exception": false,
     "start_time": "2021-05-24T21:12:37.419233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/kaldi/egs\n"
     ]
    }
   ],
   "source": [
    "%cd kaldi/egs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unusual-allowance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:12:37.488931Z",
     "iopub.status.busy": "2021-05-24T21:12:37.487919Z",
     "iopub.status.idle": "2021-05-24T21:12:41.638582Z",
     "shell.execute_reply": "2021-05-24T21:12:41.637796Z",
     "shell.execute_reply.started": "2021-05-24T18:50:39.248768Z"
    },
    "papermill": {
     "duration": 4.179655,
     "end_time": "2021-05-24T21:12:41.638719",
     "exception": false,
     "start_time": "2021-05-24T21:12:37.459064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ClarinStudioKaldi'...\r\n",
      "remote: Enumerating objects: 778, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\r\n",
      "remote: Total 778 (delta 0), reused 0 (delta 0), pack-reused 775\u001b[K\r\n",
      "Receiving objects: 100% (778/778), 35.26 MiB | 19.96 MiB/s, done.\r\n",
      "Resolving deltas: 100% (262/262), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/danijel3/ClarinStudioKaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exact-personal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:12:41.672618Z",
     "iopub.status.busy": "2021-05-24T21:12:41.672027Z",
     "iopub.status.idle": "2021-05-24T21:12:41.676641Z",
     "shell.execute_reply": "2021-05-24T21:12:41.677241Z",
     "shell.execute_reply.started": "2021-05-24T18:50:45.624159Z"
    },
    "papermill": {
     "duration": 0.024315,
     "end_time": "2021-05-24T21:12:41.677466",
     "exception": false,
     "start_time": "2021-05-24T21:12:41.653151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/kaldi/egs/ClarinStudioKaldi\n"
     ]
    }
   ],
   "source": [
    "%cd ClarinStudioKaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "endless-german",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:12:41.715875Z",
     "iopub.status.busy": "2021-05-24T21:12:41.712054Z",
     "iopub.status.idle": "2021-05-24T21:14:56.865599Z",
     "shell.execute_reply": "2021-05-24T21:14:56.864999Z",
     "shell.execute_reply.started": "2021-05-24T18:50:45.632869Z"
    },
    "papermill": {
     "duration": 135.173399,
     "end_time": "2021-05-24T21:14:56.865754",
     "exception": false,
     "start_time": "2021-05-24T21:12:41.692355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!conda install -c bioconda perl-perlio-gzip -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decimal-neighbor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:14:56.897546Z",
     "iopub.status.busy": "2021-05-24T21:14:56.896970Z",
     "iopub.status.idle": "2021-05-24T21:14:56.900922Z",
     "shell.execute_reply": "2021-05-24T21:14:56.901358Z",
     "shell.execute_reply.started": "2021-05-24T18:53:22.616551Z"
    },
    "papermill": {
     "duration": 0.021245,
     "end_time": "2021-05-24T21:14:56.901533",
     "exception": false,
     "start_time": "2021-05-24T21:14:56.880288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib:/opt/kaldi/tools/openfst-1.6.7/lib:/opt/kaldi/src/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "excited-effects",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:14:56.933304Z",
     "iopub.status.busy": "2021-05-24T21:14:56.932769Z",
     "iopub.status.idle": "2021-05-24T21:14:58.383260Z",
     "shell.execute_reply": "2021-05-24T21:14:58.382793Z",
     "shell.execute_reply.started": "2021-05-24T18:53:22.626466Z"
    },
    "papermill": {
     "duration": 1.467208,
     "end_time": "2021-05-24T21:14:58.383397",
     "exception": false,
     "start_time": "2021-05-24T21:14:56.916189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat path.sh|sed -e 's/~\\/apps/\\/opt/' > tmp\n",
    "!mv tmp path.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compatible-miracle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:14:58.417255Z",
     "iopub.status.busy": "2021-05-24T21:14:58.416428Z",
     "iopub.status.idle": "2021-05-24T21:14:59.133227Z",
     "shell.execute_reply": "2021-05-24T21:14:59.132648Z",
     "shell.execute_reply.started": "2021-05-24T18:53:24.098924Z"
    },
    "papermill": {
     "duration": 0.735703,
     "end_time": "2021-05-24T21:14:59.133355",
     "exception": false,
     "start_time": "2021-05-24T21:14:58.397652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo > local_clarin/clarin_pl_clean.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "controversial-collection",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:14:59.172791Z",
     "iopub.status.busy": "2021-05-24T21:14:59.171761Z",
     "iopub.status.idle": "2021-05-24T21:15:00.597007Z",
     "shell.execute_reply": "2021-05-24T21:15:00.597486Z",
     "shell.execute_reply.started": "2021-05-24T18:53:24.820866Z"
    },
    "papermill": {
     "duration": 1.449936,
     "end_time": "2021-05-24T21:15:00.597667",
     "exception": false,
     "start_time": "2021-05-24T21:14:59.147731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/data\n",
    "!ln -s /kaggle/working/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "radical-algebra",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-24T21:15:00.630059Z",
     "iopub.status.busy": "2021-05-24T21:15:00.629377Z",
     "iopub.status.idle": "2021-05-24T21:15:00.638746Z",
     "shell.execute_reply": "2021-05-24T21:15:00.638056Z",
     "shell.execute_reply.started": "2021-05-24T19:29:00.984873Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026978,
     "end_time": "2021-05-24T21:15:00.638923",
     "exception": false,
     "start_time": "2021-05-24T21:15:00.611945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/kaldi/tools/phonetisaurus-g2p/src/scripts/phonetisaurus-apply\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/kaldi/tools/phonetisaurus-g2p/src/scripts/phonetisaurus-apply\n",
    "#!/usr/bin/env python\n",
    "# -*- mode: python; coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os, logging, subprocess, time, re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import tempfile\n",
    "\n",
    "class G2PModelTester () :\n",
    "    \"\"\"G2P Model training wrapper class.\n",
    "\n",
    "    Phonetisaurus G2P modeling training wrapper class.\n",
    "    This wraps the alignment, joint n-gram training, and ARPA to\n",
    "    WFST conversion steps into one command.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, model, **kwargs) :\n",
    "        self.model = model\n",
    "        self.lexicon_file = kwargs.get (\"lexicon\", None)\n",
    "        self.nbest = kwargs.get (\"nbest\", 1)\n",
    "        self.thresh = kwargs.get (\"thresh\", 99)\n",
    "        self.beam = kwargs.get (\"beam\", 10000)\n",
    "        self.greedy = kwargs.get (\"greedy\", False)\n",
    "        self.accumulate = kwargs.get (\"accumulate\", False)\n",
    "        self.pmass = kwargs.get (\"pmass\", 0.0)\n",
    "        self.probs = kwargs.get (\"probs\", False)\n",
    "        self.verbose = kwargs.get (\"verbose\", False)\n",
    "        self.logger = self.setupLogger ()\n",
    "\n",
    "    def setupLogger (self) :\n",
    "        \"\"\"Setup the logger and logging level.\n",
    "\n",
    "        Setup the logger and logging level.  We only support\n",
    "        verbose and non-verbose mode.\n",
    "\n",
    "        Args:\n",
    "            verbose (bool): Verbose mode, or not.\n",
    "\n",
    "        Returns:\n",
    "            Logger: A configured logger instance.\n",
    "        \"\"\"\n",
    "\n",
    "        level = logging.DEBUG if self.verbose else logging.INFO\n",
    "        logging.basicConfig (\n",
    "            level=level,\n",
    "            format=\"\\033[94m%(levelname)s:%(name)s:\"\\\n",
    "            \"%(asctime)s\\033[0m:  %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "\n",
    "        return logging.getLogger (\"phonetisaurus-apply\")\n",
    "\n",
    "    def _loadLexicon (self) :\n",
    "        \"\"\"Load the lexicon from a file.\n",
    "\n",
    "        Load the reference lexicon from a file, and store it\n",
    "        in a defaultdict (list).\n",
    "        \"\"\"\n",
    "\n",
    "        _lexicon = defaultdict (list)\n",
    "        if not self.lexicon_file :\n",
    "            return _lexicon\n",
    "\n",
    "        self.logger.debug (\"Loading lexicon from file...\")\n",
    "        with open (self.lexicon_file, \"r\") as ifp :\n",
    "            for line in ifp :\n",
    "                # py2py3 compatbility,\n",
    "                if sys.version_info[0] < 3:\n",
    "                    line = line.decode(\"utf8\").strip ()\n",
    "                else:\n",
    "                    line = line.strip ()\n",
    "                word, pron = re.split (r\"\\t\", line, 1)\n",
    "                _lexicon [word].append (pron)\n",
    "\n",
    "        return _lexicon\n",
    "\n",
    "    def checkPhonetisaurusConfig (self) :\n",
    "        \"\"\"Run some basic checks before training.\n",
    "\n",
    "        Run some basic checks regarding the $PATH, environment,\n",
    "        and provided data before starting training.\n",
    "\n",
    "        Raises:\n",
    "            EnvironmentError: raised if binaries are not found.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug (\"Checking command configuration...\")\n",
    "        for program in [\"phonetisaurus-g2pfst\"] :\n",
    "            if not self.which (program) :\n",
    "                raise EnvironmentError(\"Phonetisaurus command, '{0}', \"\\\n",
    "                    \"not found in path.\".format (program))\n",
    "\n",
    "        if self.lexicon_file and not os.path.exists (self.lexicon_file) :\n",
    "            self.logger.error (\"Could not find provided lexicon file.\")\n",
    "            sys.exit (1)\n",
    "\n",
    "        for key,val in sorted (vars (self).items ()) :\n",
    "            self.logger.debug (u\"{0}:  {1}\".format (key, val))\n",
    "\n",
    "        self.lexicon = self._loadLexicon ()\n",
    "\n",
    "        return\n",
    "\n",
    "    def which (self, program) :\n",
    "        \"\"\"Basic 'which' implementation for python.\n",
    "\n",
    "        Basic 'which' implementation for python from stackoverflow:\n",
    "          * https://stackoverflow.com/a/377028/6739158\n",
    "\n",
    "        Args:\n",
    "            program (str): The program name to search the $PATH for.\n",
    "\n",
    "        Returns:\n",
    "            path/None: The path to the executable, or None.\n",
    "        \"\"\"\n",
    "\n",
    "        def is_exe (fpath) :\n",
    "            return os.path.isfile (fpath) and os.access (fpath, os.X_OK)\n",
    "\n",
    "        fpath, fname = os.path.split (program)\n",
    "        if fpath:\n",
    "            if is_exe (program):\n",
    "                return program\n",
    "        else:\n",
    "            for path in os.environ[\"PATH\"].split (os.pathsep) :\n",
    "                path = path.strip ('\"')\n",
    "                exe_file = os.path.join (path, program)\n",
    "                if is_exe (exe_file):\n",
    "                    return exe_file\n",
    "\n",
    "        return None\n",
    "\n",
    "    def makeG2PCommand (self, word_list) :\n",
    "        \"\"\"Build the G2P command.\n",
    "\n",
    "        Build the G2P command from the provided arguments.\n",
    "\n",
    "        Returns:\n",
    "            list: The command in subprocess list format.\n",
    "        \"\"\"\n",
    "\n",
    "        command = [\n",
    "            u\"phonetisaurus-g2pfst\",\n",
    "            u\"--model={0}\".format (self.model),\n",
    "            u\"--nbest={0}\".format (self.nbest),\n",
    "            u\"--beam={0}\".format (self.beam),\n",
    "            u\"--thresh={0}\".format (self.thresh),\n",
    "            u\"--accumulate={0}\".format (str (self.accumulate).lower ()),\n",
    "            u\"--pmass={0}\".format (self.pmass),\n",
    "            u\"--nlog_probs={0}\".format (str(not self.probs).lower ()),\n",
    "            u\"--wordlist={0}\".format (word_list)\n",
    "        ]\n",
    "\n",
    "        self.logger.debug (u\" \".join (command))\n",
    "\n",
    "        return command\n",
    "\n",
    "    def runG2PCommand (self, word_list_file) :\n",
    "        \"\"\"Generate and run the actual G2P command.\n",
    "\n",
    "        Generate and run the actual G2P command.  Each synthesized\n",
    "        entry will be yielded back on-the-fly via the subprocess\n",
    "        stdout readline method.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        g2p_command = self.makeG2PCommand (word_list_file)\n",
    "\n",
    "        self.logger.debug (\"Applying G2P model...\")\n",
    "\n",
    "        with open (os.devnull, \"w\") as devnull :\n",
    "            proc = subprocess.Popen (\n",
    "                g2p_command,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=devnull if not self.verbose else None\n",
    "            )\n",
    "\n",
    "            for line in proc.stdout :\n",
    "                parts = re.split (r\"\\t\", line.decode (\"utf8\").strip ())\n",
    "                if not len (parts) == 3 :\n",
    "                    self.logger.warning (\n",
    "                        u\"No pronunciation for word: '{0}'\".format (parts [0])\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                yield parts\n",
    "\n",
    "        return\n",
    "\n",
    "    def applyG2POnly (self, word_list_file) :\n",
    "        \"\"\"Apply the G2P model to a word list.\n",
    "\n",
    "        Apply the G2P model to a word list.  No filtering or application\n",
    "        of a reference lexicon is used here.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        for word, score, pron in self.runG2PCommand (word_list_file) :\n",
    "            line = u\"\"\n",
    "            if self.verbose :\n",
    "                line = u\"{0}\\t{1:.2f}\\t{2}\".format (\n",
    "                    word, float (score), pron\n",
    "                )\n",
    "            else :\n",
    "                line = u\"{0}\\t{1}\".format (word, pron)\n",
    "            # py2py3 compatbility,\n",
    "            if sys.version_info[0] < 3:\n",
    "                print (line.encode (\"utf8\"))\n",
    "            else :\n",
    "                print (line)\n",
    "\n",
    "        return\n",
    "\n",
    "    def applyG2PWithLexicon (self, word_list_file) :\n",
    "        \"\"\"Apply the G2P model to a word list, combined with lexicon.\n",
    "\n",
    "        Apply the G2P model to a word list, but combine this with\n",
    "        a reference lexicon.  Words for which a reference entry exists\n",
    "        will not be sent to the G2P, unless the additional '--greedy'\n",
    "        flag is set to True.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        target_lexicon = defaultdict (list)\n",
    "        tmpwordlist = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "\n",
    "        #First, find any words in the target list for which we already\n",
    "        # have a canonical pronunciation in the reference lexicon.\n",
    "        with open (word_list_file, \"r\") as ifp :\n",
    "            for word in ifp :\n",
    "                # py2py3 compatbility,\n",
    "                if sys.version_info[0] < 3:\n",
    "                    word = word.decode (\"utf8\").strip ()\n",
    "                else:\n",
    "                    word = word.strip () # already in 'utf8'.\n",
    "                if word in self.lexicon :\n",
    "                    target_lexicon [word] = [(0.0,pron)\n",
    "                                             for pron in self.lexicon [word]]\n",
    "                    #In greedy mode we still send words to the G2P, even\n",
    "                    # if we have canonical entries in the reference lexicon.\n",
    "                    if self.greedy :\n",
    "                        print (word.encode (\"utf8\"), file=tmpwordlist)\n",
    "                else :\n",
    "                    # py2py3 compatbility,\n",
    "                    if sys.version_info[0] < 3:\n",
    "                        print (word.encode (\"utf8\"), file=tmpwordlist)\n",
    "                    else:\n",
    "                        print (word, file=tmpwordlist)\n",
    "        tmpwordlist.close ()\n",
    "\n",
    "        #Second, iterate through the G2P output, and filter against\n",
    "        # any possible duplicates previously found in the reference lexicon.\n",
    "        for word, score, pron in self.runG2PCommand (tmpwordlist.name) :\n",
    "            prons = set ([p for s,p in target_lexicon [word]])\n",
    "            if pron in prons :\n",
    "                continue\n",
    "            target_lexicon [word].append ((score, pron))\n",
    "\n",
    "        #Finally, sort everything that is left and print it.\n",
    "        for word in sorted (target_lexicon.keys ()) :\n",
    "            for score, pron in target_lexicon [word] :\n",
    "                line = u\"\"\n",
    "                if self.verbose :\n",
    "                    line = u\"{0}\\t{1:.2f}\\t{2}\".format (\n",
    "                        word, float (score), pron\n",
    "                    )\n",
    "                else :\n",
    "                    line = u\"{0}\\t{1}\".format (word, pron)\n",
    "                # py2py3 compatbility,\n",
    "                if sys.version_info[0] < 3:\n",
    "                    print (line.encode (\"utf8\"))\n",
    "                else :\n",
    "                    print (line)\n",
    "\n",
    "        os.unlink (tmpwordlist.name)\n",
    "        return\n",
    "\n",
    "    def ApplyG2PModel (self, word_list_file) :\n",
    "        \"\"\"Apply the G2P model to a word list.\n",
    "\n",
    "        Apply the G2P model to a word list.\n",
    "\n",
    "        Args:\n",
    "            word_list_file (str): The input word list.\n",
    "        \"\"\"\n",
    "        self.checkPhonetisaurusConfig ()\n",
    "\n",
    "        if not os.path.exists (word_list_file) \\\n",
    "           or not os.path.isfile (word_list_file) :\n",
    "            raise IOError(\"Word list file not found.\")\n",
    "\n",
    "        if len (self.lexicon) == 0 :\n",
    "            self.applyG2POnly (word_list_file)\n",
    "        else :\n",
    "            self.applyG2PWithLexicon (word_list_file)\n",
    "\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    import sys, argparse\n",
    "\n",
    "    example = \"{0} --model train/model.fst --word test\".format (sys.argv [0])\n",
    "\n",
    "    parser  = argparse.ArgumentParser (description=example)\n",
    "    parser.add_argument (\"--model\", \"-m\", help=\"Phonetisaurus G2P fst model.\",\n",
    "                         required=True)\n",
    "    parser.add_argument (\"--lexicon\", \"-l\", help=\"Optional reference lexicon.\",\n",
    "                         required=False)\n",
    "    parser.add_argument (\"--nbest\", \"-n\", help=\"Maximum number of hypotheses \"\n",
    "                         \"to produce.  Overridden if --pmass is set.\",\n",
    "                         default=1, type=int)\n",
    "    parser.add_argument (\"--beam\", \"-b\", help=\"Search 'beam'.\",\n",
    "                         default=10000, type=int)\n",
    "    parser.add_argument (\"--thresh\", \"-t\", help=\"Pruning threshold for n-best.\",\n",
    "                         default=99.0, type=float)\n",
    "    parser.add_argument (\"--greedy\", \"-g\", help=\"Use the G2P even if a \"\n",
    "                         \"reference lexicon has been provided.\", default=False,\n",
    "                         action=\"store_true\")\n",
    "    parser.add_argument (\"--accumulate\", \"-a\", help=\"Accumulate probabilities \"\n",
    "                         \"across unique pronunciations.\", default=False,\n",
    "                         action=\"store_true\")\n",
    "    parser.add_argument (\"--pmass\", \"-p\", help=\"Select the maximum number of \"\n",
    "                         \"hypotheses summing to P total mass for a word.\",\n",
    "                         default=0.0, type=float)\n",
    "    parser.add_argument (\"--probs\", \"-pr\", help=\"Print exp(-val) \"\n",
    "                         \"instead of default -log values.\", default=False,\n",
    "                         action=\"store_true\")\n",
    "    parser.add_argument (\"--word_list\", \"-wl\", help=\"Input word or word list to apply \"\n",
    "                        \"G2P model to.\", type=str)\n",
    "\n",
    "    parser.add_argument (\"--verbose\", \"-v\", help=\"Verbose mode.\",\n",
    "                         default=False, action=\"store_true\")\n",
    "    args = parser.parse_args ()\n",
    "\n",
    "    tester = G2PModelTester (\n",
    "        args.model,\n",
    "        **{key:val for key,val in args.__dict__.items ()\n",
    "           if not key in [\"model\",\"word_list\"]}\n",
    "    )\n",
    "\n",
    "    tester.ApplyG2PModel (args.word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "still-scanning",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:15:00.673148Z",
     "iopub.status.busy": "2021-05-24T21:15:00.672391Z",
     "iopub.status.idle": "2021-05-24T21:15:01.398628Z",
     "shell.execute_reply": "2021-05-24T21:15:01.398148Z",
     "shell.execute_reply.started": "2021-05-24T19:29:14.641765Z"
    },
    "papermill": {
     "duration": 0.744689,
     "end_time": "2021-05-24T21:15:01.398762",
     "exception": false,
     "start_time": "2021-05-24T21:15:00.654073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!chmod a+x /opt/kaldi/tools/phonetisaurus-g2p/src/scripts/phonetisaurus-apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "saved-framework",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-24T21:15:01.433048Z",
     "iopub.status.busy": "2021-05-24T21:15:01.432433Z",
     "iopub.status.idle": "2021-05-24T21:15:01.436728Z",
     "shell.execute_reply": "2021-05-24T21:15:01.437109Z"
    },
    "papermill": {
     "duration": 0.024231,
     "end_time": "2021-05-24T21:15:01.437262",
     "exception": false,
     "start_time": "2021-05-24T21:15:01.413031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_clarin/clarin_prepare_dict.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_clarin/clarin_prepare_dict.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Copyright 2010-2012 Microsoft Corporation  \n",
    "#           2012-2014 Johns Hopkins University (Author: Daniel Povey)\n",
    "#                2015 Guoguo Chen\n",
    "\n",
    "# Modified 2017 Danijel Korzinek\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#  http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n",
    "# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n",
    "# MERCHANTABLITY OR NON-INFRINGEMENT.\n",
    "# See the Apache 2 License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Call this script from one level above, e.g. from the s3/ directory.  It puts\n",
    "# its output in data/local/.\n",
    "\n",
    "# The parts of the output of this that will be needed are\n",
    "# [in data/local/dict/ ]\n",
    "# lexicon.txt\n",
    "# extra_questions.txt\n",
    "# nonsilence_phones.txt\n",
    "# optional_silence.txt\n",
    "# silence_phones.txt\n",
    "\n",
    "# run this from ../\n",
    "\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    ". utils/parse_options.sh || exit 1;\n",
    "\n",
    ". ./path.sh\n",
    "\n",
    "if [ $# -ne 2 ]; then\n",
    "  echo \"Usage: ./local/prepare_lang.sh <word_list> <dict_dir>\"\n",
    "  echo \"Creates a folder <dict_dir> with lexicon derived from\"\n",
    "  echo \"  word list <word_list>.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "word_list=$1\n",
    "dir=$2\n",
    "\n",
    "mkdir -p $dir\n",
    "\n",
    "# Make phones symbol-table (adding in silence and verbal and non-verbal noises at this point).\n",
    "# We are adding suffixes _B, _E, _S for beginning, ending, and singleton phones.\n",
    "\n",
    "# silence phones, one per line.\n",
    "(echo sil) > $dir/silence_phones.txt\n",
    "echo sil > $dir/optional_silence.txt\n",
    "\n",
    "# nonsilence phones; on each line is a list of phones that correspond\n",
    "# really to the same base phone.\n",
    "printf \"I\\nS\\nZ\\na\\nb\\nd\\ndZ\\ndz\\ndzi\\ne\\nen\\nf\\ng\\ni\\nj\\nk\\nl\\nm\\nn\\nni\\no\\non\\np\\nr\\ns\\nsi\\nt\\ntS\\nts\\ntsi\\nu\\nv\\nw\\nx\\nz\\nzi\\n\" > $dir/nonsilence_phones.txt\n",
    "\n",
    "# A few extra questions that will be added to those obtained by automatically clustering\n",
    "# the \"real\" phones.  These ask about stress; there's also one for silence.\n",
    "cat $dir/silence_phones.txt| awk '{printf(\"%s \", $1);} END{printf \"\\n\";}' > $dir/extra_questions.txt || exit 1;\n",
    "cat $dir/nonsilence_phones.txt | perl -e 'while(<>){ foreach $p (split(\" \", $_)) {\n",
    "  $p =~ m:^([^\\d]+)(\\d*)$: || die \"Bad phone $_\"; $q{$2} .= \"$p \"; } } foreach $l (values %q) {print \"$l\\n\";}' \\\n",
    " >> $dir/extra_questions.txt || exit 1;\n",
    "\n",
    "#Transcribe the wordlist\n",
    "export LD_LIBRARY_PATH=$KALDI_ROOT/tools/openfst/lib\n",
    "export PATH=$PATH:/opt/kaldi/tools/phonetisaurus-g2p/\n",
    "/opt/kaldi/tools/phonetisaurus-g2p/src/scripts/phonetisaurus-apply --model local_clarin/model.fst --lexicon local_clarin/lexicon.txt --word_list $word_list -p 0.8 > $dir/lexicon_raw_nosil.txt || exit 1\n",
    "\n",
    "sort -u $dir/lexicon_raw_nosil.txt -o $dir/lexicon_raw_nosil.txt\n",
    "\n",
    "# Add the silences, noises etc.\n",
    "# the sort | uniq is to remove a duplicated pron.\n",
    "# lexicon.txt is without the _B, _E, _S, _I markers.\n",
    "(echo -e '<unk>\\tsil' ) | \\\n",
    " cat - $dir/lexicon_raw_nosil.txt | sort -u > $dir/lexicon.txt || exit 1;\n",
    "\n",
    "# Cleanup\n",
    "rm -f $dir/lexiconp.txt\n",
    "rm -f $dir/lexicon_raw_nosil.txt\n",
    "\n",
    "echo \"Dictionary preparation succeeded\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-lingerie",
   "metadata": {
    "papermill": {
     "duration": 0.01425,
     "end_time": "2021-05-24T21:15:01.466248",
     "exception": false,
     "start_time": "2021-05-24T21:15:01.451998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spiritual-sperm",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-24T21:15:01.500848Z",
     "iopub.status.busy": "2021-05-24T21:15:01.500059Z",
     "iopub.status.idle": "2021-05-24T21:15:01.504156Z",
     "shell.execute_reply": "2021-05-24T21:15:01.503623Z"
    },
    "papermill": {
     "duration": 0.023458,
     "end_time": "2021-05-24T21:15:01.504276",
     "exception": false,
     "start_time": "2021-05-24T21:15:01.480818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_clarin/clarin_pl_data_prep.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_clarin/clarin_pl_data_prep.sh\n",
    "#!/bin/bash\n",
    "\n",
    ". ./path.sh\n",
    "\n",
    "#you can change this here, if you want it on a different partition, for example\n",
    "AUDIO_DL_PATH=audio\n",
    "\n",
    "if [ ! -d $AUDIO_DL_PATH ] ; then mkdir -p $AUDIO_DL_PATH ; fi\n",
    "pushd $AUDIO_DL_PATH\n",
    "if [ ! -f audio.tar.gz ] ; then\n",
    "\techo \"Downloading audio from the Clarin-pl website (~4.6GB)...\"\n",
    "\tcurl -O http://mowa.clarin-pl.eu/korpusy/audio.tar.gz\n",
    "else\n",
    "\techo \"File already downloaded! Checking if download is consistent...\"\n",
    "\tcurl -O http://mowa.clarin-pl.eu/korpusy/audio.md5sum\n",
    "\tif ! md5sum -c audio.md5sum ; then\n",
    "\t\techo \"Download doesn't match the one on the server! \"\n",
    "\t\techo \"Erase the audio.tar.gz file (and audio folder) and run this script again!\"\n",
    "\t\texit -1\n",
    "    fi\n",
    "fi\n",
    "\n",
    "if [ ! -d audio\t] ; then\n",
    "\techo \"Extracting files...\"\n",
    "\ttar xf audio.tar.gz\n",
    "else\n",
    "\techo \"Files already extracted?\"\n",
    "\techo \"Remove the audio dir to extract them again...\"\n",
    "fi\n",
    "popd\n",
    "\n",
    "if [ ! -d data ] ; then mkdir data ; fi\n",
    "\n",
    "echo Generating file lists using proper paths...\n",
    "python3 local_clarin/generate_lists.py $AUDIO_DL_PATH/audio data local_clarin\n",
    "\n",
    "echo Generating spk2utt...\n",
    "utils/utt2spk_to_spk2utt.pl data/train/utt2spk > data/train/spk2utt\n",
    "utils/utt2spk_to_spk2utt.pl data/test/utt2spk > data/test/spk2utt\n",
    "utils/utt2spk_to_spk2utt.pl data/dev/utt2spk > data/dev/spk2utt\n",
    "\n",
    "echo Preparing dictionary...\n",
    "if [ ! -d data/local ] ; then mkdir data/local ; fi\n",
    "\n",
    "cut -f2- -d' ' < data/train/text | tr ' ' '\\n' | sort -u > data/local/train.wlist\n",
    "if [ x\"$(which ngram)\" != x\"\" ]\n",
    "then\n",
    "\tngram -lm local_clarin/arpa.lm.gz -unk -write-vocab data/local/lm.wlist\n",
    "else\n",
    "\tperl local_clarin/extract_vocab.pl local_clarin/arpa.lm.gz > data/local/lm.wlist\n",
    "fi\n",
    "tail -n +5 data/local/lm.wlist | cat data/local/train.wlist - | sort -u > data/local/all.wlist\n",
    "if [ ! -f local_clarin/model.fst ] ; then gunzip -c local_clarin/model.fst.gz > local_clarin/model.fst ; fi\n",
    "local_clarin/clarin_prepare_dict.sh data/local/all.wlist data/local/dict_nosp || exit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bibliographic-leone",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-05-24T21:15:01.538895Z",
     "iopub.status.busy": "2021-05-24T21:15:01.537998Z",
     "iopub.status.idle": "2021-05-24T21:15:01.542461Z",
     "shell.execute_reply": "2021-05-24T21:15:01.541948Z",
     "shell.execute_reply.started": "2021-05-24T17:53:11.754933Z"
    },
    "papermill": {
     "duration": 0.023478,
     "end_time": "2021-05-24T21:15:01.542600",
     "exception": false,
     "start_time": "2021-05-24T21:15:01.519122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing runmfcc.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile runmfcc.sh\n",
    "#!/bin/bash\n",
    "\n",
    ". ./path.sh ## set the paths in this file correctly!\n",
    "\n",
    "# link to scripts from the standard Kaldi distribution\n",
    "# we try to use these as much as possible\n",
    "if [ ! -f $KALDI_ROOT/egs/wsj/s5/conf ] ; then ln -s $KALDI_ROOT/egs/wsj/s5/conf ; fi\n",
    "if [ ! -f $KALDI_ROOT/egs/wsj/s5/local ] ; then ln -s $KALDI_ROOT/egs/wsj/s5/local ; fi\n",
    "if [ ! -f $KALDI_ROOT/egs/wsj/s5/utils ] ; then ln -s $KALDI_ROOT/egs/wsj/s5/utils ; fi\n",
    "if [ ! -f $KALDI_ROOT/egs/wsj/s5/steps ] ; then ln -s $KALDI_ROOT/egs/wsj/s5/steps ; fi\n",
    "\n",
    "# exits script if error occurs anywhere\n",
    "# you might not want to do this for interactive shells.\n",
    "set -e\n",
    "\n",
    "export nj=40 ##number of concurrent processes\n",
    "export nj_test=30 ## number of concurrent processes for test has to be <=30\n",
    "\n",
    "# This is a shell script, but it's recommended that you run the commands one by\n",
    "# one by copying and pasting into the shell.\n",
    "\n",
    "#run some initial data preparation (look at the file for more details):\n",
    "local_clarin/clarin_pl_data_prep.sh\n",
    "\n",
    "#prepare the lang directory\n",
    "utils/prepare_lang.sh data/local/dict_nosp \"<unk>\" data/local/tmp_nosp data/lang_nosp\n",
    "\n",
    "#make G.fst\n",
    "utils/format_lm.sh data/lang_nosp local_clarin/arpa.lm.gz data/local/dict_nosp/lexicon.txt data/lang_nosp_test\n",
    "\n",
    "# Make normalized MFCC features.\n",
    "steps/make_mfcc.sh --nj $nj data/train\n",
    "steps/compute_cmvn_stats.sh data/train\n",
    "steps/make_mfcc.sh --nj $nj data/test\n",
    "steps/compute_cmvn_stats.sh data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-progressive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T21:15:01.580260Z",
     "iopub.status.busy": "2021-05-24T21:15:01.576257Z",
     "iopub.status.idle": "2021-05-24T21:22:23.695991Z",
     "shell.execute_reply": "2021-05-24T21:22:23.696412Z"
    },
    "papermill": {
     "duration": 442.139009,
     "end_time": "2021-05-24T21:22:23.696621",
     "exception": false,
     "start_time": "2021-05-24T21:15:01.557612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bash runmfcc.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 628.019848,
   "end_time": "2021-05-24T21:22:25.400102",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-24T21:11:57.380254",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
