{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.diarize import DiarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HF_DIR = Path.home() / \".huggingface\"\n",
    "HF_TOKEN = HF_DIR / \"token\"\n",
    "TOKEN = \"\"\n",
    "if HF_DIR.is_dir() and HF_TOKEN.exists():\n",
    "    with open(str(HF_TOKEN)) as hf_tok:\n",
    "        TOKEN = hf_tok.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_pipe = DiarizationPipeline(use_auth_token=TOKEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = Path(\"/home/joregan/hsi/audio\")\n",
    "MFA_DIR = \"/home/joregan/hsi_mfa\"\n",
    "TSV_DIR = \"/home/joregan/hsi_segments\"\n",
    "EG = AUDIO_PATH / \"hsi_3_0715_227_001_inter-002.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.audio import load_audio, SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is just to confirm that the output of `merge_chunks` is similar in terms of timestamps to the diarisation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "# from whisperx.vad import load_vad_model, merge_chunks\n",
    "# from whisperx.audio import load_audio, SAMPLE_RATE\n",
    "\n",
    "# # https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L336\n",
    "# default_vad_options = {\n",
    "#     \"vad_onset\": 0.500,\n",
    "#     \"vad_offset\": 0.363\n",
    "# }\n",
    "\n",
    "# audio = load_audio(str(EG))\n",
    "\n",
    "# chunk_size = 30\n",
    "\n",
    "# # https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L186\n",
    "# vad_model = load_vad_model(torch.device(DEVICE), use_auth_token=None, **default_vad_options)\n",
    "# vad_segments = vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "# vad_segments = merge_chunks(\n",
    "#     vad_segments,\n",
    "#     chunk_size,\n",
    "#     onset=default_vad_options[\"vad_onset\"],\n",
    "#     offset=default_vad_options[\"vad_offset\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.008532423208191127,\n",
       " 'end': 27.329351535836178,\n",
       " 'segments': [(0.008532423208191127, 2.841296928327645),\n",
       "  (5.6058020477815695, 10.179180887372015),\n",
       "  (11.527303754266212, 12.022184300341298),\n",
       "  (23.523890784982935, 27.329351535836178)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vad_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarised_chunks(filename):\n",
    "    diar_res = diar_pipe(filename, num_speakers=2)\n",
    "    res = []\n",
    "    for idx, diar_seg in diar_res.iterrows():\n",
    "        res.append({\n",
    "            \"start\": diar_seg[\"start\"],\n",
    "            \"end\": diar_seg[\"end\"],\n",
    "            \"segments\": [(diar_seg[\"start\"], diar_seg[\"end\"])],\n",
    "            \"speaker\": diar_seg[\"speaker\"]\n",
    "        })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def write_wave(filename, data):\n",
    "    data_denorm = data * 32768.0\n",
    "    data16 = data_denorm.astype(np.int16)\n",
    "    output = wave.open(filename, \"w\")\n",
    "    # pcm_s16le, single channel\n",
    "    output.setnchannels(1)\n",
    "    output.setsampwidth(2)\n",
    "    output.setframerate(16000)\n",
    "    output.writeframes(data16.tobytes())\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for my reference\n",
    "_FORMATS = \"\"\"\n",
    "hsi_N_NNNN_NNN_NNN-mic.wav\n",
    "hsi_N_NNNN_NNN_NNN-micN-NNN.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_NNN_main.wav\n",
    "hsi_N_NNNN_NNN_inter.wav\n",
    "hsi_N_NNNN_NNN_main.wav\n",
    "\"\"\"\n",
    "def get_speaker_id(filename, detected_speaker):\n",
    "    detected_speaker = detected_speaker.replace(\"SPEAKER_\", \"\")\n",
    "    if \"inter\" in filename or \"mic2\" in filename:\n",
    "        part = \"inter\"\n",
    "    elif \"main\" in filename or \"mic1\" in filename:\n",
    "        part = \"main\"\n",
    "    elif filename.endswith(\"-mic.wav\"):\n",
    "        # one file\n",
    "        part = \"inter\"\n",
    "    pieces = filename.split(\"_\")\n",
    "    return f\"hsi_{pieces[1]}_{part}_{detected_speaker}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory(speaker_id, base_dir=\"/home/joregan/hsi_mfa\"):\n",
    "    base_path = Path(base_dir)\n",
    "    if not base_path.is_dir():\n",
    "        base_path.mkdir()\n",
    "    speaker_path = base_path / speaker_id\n",
    "    if not speaker_path.is_dir():\n",
    "        speaker_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from whisperx.types import TranscriptionResult\n",
    "from typing import List, Union\n",
    "import faster_whisper\n",
    "from whisperx.asr import find_numeral_symbol_tokens, SingleSegment\n",
    "\n",
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L173\n",
    "def transcribe(\n",
    "    self, audio: Union[str, np.ndarray], batch_size=None, num_workers=0, language=None, task=None, chunk_size=30, print_progress = False, combined_progress=False\n",
    ") -> TranscriptionResult:\n",
    "    filename = audio\n",
    "    if isinstance(audio, str):\n",
    "        audio = load_audio(audio)\n",
    "\n",
    "    def data(audio, segments):\n",
    "        for seg in segments:\n",
    "            f1 = int(seg['start'] * SAMPLE_RATE)\n",
    "            f2 = int(seg['end'] * SAMPLE_RATE)\n",
    "            if (seg['end'] - seg['start']) < 30.0:\n",
    "                yield {'inputs': audio[f1:f2]}\n",
    "\n",
    "    # vad_segments = self.vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
    "    # vad_segments = merge_chunks(\n",
    "    #     vad_segments,\n",
    "    #     chunk_size,\n",
    "    #     onset=self._vad_params[\"vad_onset\"],\n",
    "    #     offset=self._vad_params[\"vad_offset\"],\n",
    "    # )\n",
    "    vad_segments = get_diarised_chunks(filename)\n",
    "    if self.tokenizer is None:\n",
    "        language = language or self.detect_language(audio)\n",
    "        task = task or \"transcribe\"\n",
    "        self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                            self.model.model.is_multilingual, task=task,\n",
    "                                                            language=language)\n",
    "    else:\n",
    "        language = language or self.tokenizer.language_code\n",
    "        task = task or self.tokenizer.task\n",
    "        if task != self.tokenizer.task or language != self.tokenizer.language_code:\n",
    "            self.tokenizer = faster_whisper.tokenizer.Tokenizer(self.model.hf_tokenizer,\n",
    "                                                                self.model.model.is_multilingual, task=task,\n",
    "                                                                language=language)\n",
    "            \n",
    "    if self.suppress_numerals:\n",
    "        previous_suppress_tokens = self.options.suppress_tokens\n",
    "        numeral_symbol_tokens = find_numeral_symbol_tokens(self.tokenizer)\n",
    "        print(f\"Suppressing numeral and symbol tokens\")\n",
    "        new_suppressed_tokens = numeral_symbol_tokens + self.options.suppress_tokens\n",
    "        new_suppressed_tokens = list(set(new_suppressed_tokens))\n",
    "        self.options = self.options._replace(suppress_tokens=new_suppressed_tokens)\n",
    "\n",
    "    segments: List[SingleSegment] = []\n",
    "    batch_size = batch_size or self._batch_size\n",
    "    total_segments = len(vad_segments)\n",
    "    for idx, out in enumerate(self.__call__(data(audio, vad_segments), batch_size=batch_size, num_workers=num_workers)):\n",
    "        if print_progress:\n",
    "            base_progress = ((idx + 1) / total_segments) * 100\n",
    "            percent_complete = base_progress / 2 if combined_progress else base_progress\n",
    "            print(f\"Progress: {percent_complete:.2f}%...\")\n",
    "        text = out['text']\n",
    "        if batch_size in [0, 1, None]:\n",
    "            text = text[0]\n",
    "        segments.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"start\": round(vad_segments[idx]['start'], 3),\n",
    "                \"end\": round(vad_segments[idx]['end'], 3),\n",
    "                \"speaker\": vad_segments[idx]['speaker']\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # revert the tokenizer if multilingual inference is enabled\n",
    "    if self.preset_language is None:\n",
    "        self.tokenizer = None\n",
    "\n",
    "    # revert suppressed tokens if suppress_numerals is enabled\n",
    "    if self.suppress_numerals:\n",
    "        self.options = self.options._replace(suppress_tokens=previous_suppress_tokens)\n",
    "\n",
    "    return {\"segments\": segments, \"language\": language}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L300\n",
    "\n",
    "default_asr_options =  {\n",
    "    \"beam_size\": 5,\n",
    "    \"best_of\": 5,\n",
    "    \"patience\": 1,\n",
    "    \"length_penalty\": 1,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"temperatures\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    \"compression_ratio_threshold\": 2.4,\n",
    "    \"log_prob_threshold\": -1.0,\n",
    "    \"no_speech_threshold\": 0.6,\n",
    "    \"condition_on_previous_text\": False,\n",
    "    \"prompt_reset_on_temperature\": 0.5,\n",
    "    \"initial_prompt\": None,\n",
    "    \"prefix\": None,\n",
    "    \"suppress_blank\": True,\n",
    "    \"suppress_tokens\": [-1],\n",
    "    \"without_timestamps\": True,\n",
    "    \"max_initial_timestamp\": 0.0,\n",
    "    \"word_timestamps\": False,\n",
    "    \"prepend_punctuations\": \"\\\"'“¿([{-\",\n",
    "    \"append_punctuations\": \"\\\"'.。,，!！?？:：”)]}、\",\n",
    "    \"suppress_numerals\": False,\n",
    "    \"max_new_tokens\": None,\n",
    "    \"clip_timestamps\": None,\n",
    "    \"hallucination_silence_threshold\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_asr_options[\"initial_prompt\"] = \"Yeah, so, uh... we were... we were umm going there a hundred percent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import types\n",
    "\n",
    "compute_type = \"float16\"\n",
    "batch_size = 16\n",
    "model = whisperx.load_model(\"large-v2\", DEVICE, asr_options=default_asr_options, language=\"en\", compute_type=compute_type)\n",
    "model.transcribe = types.MethodType(transcribe, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text + \" \"\n",
    "    # https://github.com/m-bain/whisperX/blob/58f00339af7dcc9705ef49d97a1f40764b7cf555/whisperx/asr.py#L320C1-L321C53\n",
    "    prepend_punctuations = \"\\\"'“¿([{-\"\n",
    "    append_punctuations = \"\\\"'.。,，!！?？:：”)]}、\"\n",
    "    text = text.replace(\"...\", \"\")\n",
    "    for punct in prepend_punctuations:\n",
    "        text = text.replace(f\" {punct}\", \" \")\n",
    "    for punct in append_punctuations:\n",
    "        text = text.replace(f\"{punct} \", \" \")\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mfa(filename, audio, segment, base_path):\n",
    "    seg_id = get_speaker_id(filename, segment['speaker'])\n",
    "    ensure_directory(seg_id, base_path)\n",
    "    filestem = Path(filename).stem\n",
    "    output_base = f\"{filestem}__{segment['start']}_{segment['end']}\"\n",
    "    f1 = int(segment['start'] * SAMPLE_RATE)\n",
    "    f2 = int(segment['end'] * SAMPLE_RATE)\n",
    "    audio_segment = audio[f1:f2]\n",
    "    clean = clean_text(segment['text'])\n",
    "    base_path_path = Path(base_path)\n",
    "    text_filename = str(base_path_path / seg_id / f\"{output_base}.txt\")\n",
    "    with open(text_filename, \"w\") as txtf:\n",
    "        txtf.write(clean)\n",
    "    wave_filename = str(base_path_path / seg_id / f\"{output_base}.wav\")\n",
    "    write_wave(wave_filename, audio_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filepath(filepath: Path, mfa_dir, tsv_dir):\n",
    "    filename = str(filepath)\n",
    "    result = model.transcribe(audio=filename, batch_size=batch_size)\n",
    "    full_audio = load_audio(filename)\n",
    "    barefilename = filepath.stem\n",
    "    tsv_path = Path(tsv_dir)\n",
    "    if not tsv_path.is_dir():\n",
    "        tsv_path.mkdir()\n",
    "\n",
    "    tsv_file = str(tsv_path / f\"{barefilename}_segments.tsv\")\n",
    "    with open(tsv_file, \"w\") as tsvf:\n",
    "        tsvf.write(\"filename\\tstart\\tend\\tspeaker_id\\ttext\\n\")\n",
    "        for segment in result['segments']:\n",
    "            write_mfa(filename, full_audio, segment, mfa_dir)\n",
    "            tsvf.write(f\"{barefilename}.wav\\t{segment['start']}\\t{segment['end']}\\t{segment['speaker']}\\t{segment['text'].strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wavfile in AUDIO_PATH.glob(\"*.wav\"):\n",
    "    if \"timecode\" in str(wavfile):\n",
    "        continue\n",
    "    process_filepath(wavfile, MFA_DIR, TSV_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
