{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kashubian.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0YeSLuH2aP5"
      },
      "source": [
        "# \"Running wav2vec2 for Polish on Kashubian\"\n",
        "> \"How does it fare with closely related languages?\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- comments: true\n",
        "- categories: [wav2vec2, kashubian]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiMBByvsGeKH"
      },
      "source": [
        "%%capture\n",
        "import requests \n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ7wm-C4Gi48"
      },
      "source": [
        "URL='http://www.miesiecznikpomerania.pl/audio'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRNWXPMeGnsG"
      },
      "source": [
        "req = requests.get(URL)\n",
        "soup = BeautifulSoup(req.content, 'html.parser')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CifJIhLQG0-w"
      },
      "source": [
        "contents = list()\n",
        "for part in soup.find_all('div', class_='sp-accordion-inner'):\n",
        "  out = {}\n",
        "  audtag = part.find('audio')\n",
        "  source = audtag.find('source')\n",
        "  out['audio'] = 'http://www.miesiecznikpomerania.pl{}'.format(source['src'])\n",
        "  audtag.decompose()\n",
        "  out['text'] = part.text.strip()\n",
        "  contents.append(out)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOsadzzUbExt"
      },
      "source": [
        "for c in contents:\n",
        "  !echo {c['audio']} >> input"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bpzq8qNd_3t"
      },
      "source": [
        "%%capture\n",
        "!cat input|sort|uniq > input.sorted\n",
        "!wget -i input.sorted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgwWapQtxcw0"
      },
      "source": [
        "!cat input.sorted|grep -v uczba_5_Miedzy_niebem_a_ziemia_-_Najo_uczba|awk '{print \"http://web.archive.org/web/\" $0}' > input.wayback"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMd70oOyFDW"
      },
      "source": [
        "%%capture\n",
        "!wget -i input.wayback"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5DhVwh_-sra"
      },
      "source": [
        "import json\n",
        "with open('data.json', 'w') as outfile:\n",
        "    json.dump(contents, outfile)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geNB2x34hjpU"
      },
      "source": [
        "%%capture\n",
        "!for i in 1[57]*.ogg;do ffmpeg -i \"$i\" -acodec pcm_s16le -ac 1 -ar 16000 \"$i.wav\";done"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhVsVOlKwepr"
      },
      "source": [
        "%%capture\n",
        "!pip install librosa webrtcvad"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9oo0q7jkyao"
      },
      "source": [
        "# VAD wrapper is taken from PyTorch Speaker Verification:\n",
        "# https://github.com/HarryVolek/PyTorch_Speaker_Verification\n",
        "# Copyright (c) 2019, HarryVolek\n",
        "# License: BSD-3-Clause\n",
        "# based on https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n",
        "# Copyright (c) 2016 John Wiseman\n",
        "# License: MIT\n",
        "import collections\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import sys\n",
        "import librosa\n",
        "import wave\n",
        "\n",
        "import webrtcvad\n",
        "\n",
        "#from hparam import hparam as hp\n",
        "sr = 16000\n",
        "\n",
        "def read_wave(path, sr):\n",
        "    \"\"\"Reads a .wav file.\n",
        "    Takes the path, and returns (PCM audio data, sample rate).\n",
        "    Assumes sample width == 2\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "    data, _ = librosa.load(path, sr)\n",
        "    assert len(data.shape) == 1\n",
        "    assert sr in (8000, 16000, 32000, 48000)\n",
        "    return data, pcm_data\n",
        "    \n",
        "class Frame(object):\n",
        "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    \"\"\"Generates audio frames from PCM audio data.\n",
        "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
        "    the sample rate.\n",
        "    Yields Frames of the requested duration.\n",
        "    \"\"\"\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms,\n",
        "                  padding_duration_ms, vad, frames):\n",
        "    \"\"\"Filters out non-voiced audio frames.\n",
        "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
        "    the voiced audio.\n",
        "    Uses a padded, sliding window algorithm over the audio frames.\n",
        "    When more than 90% of the frames in the window are voiced (as\n",
        "    reported by the VAD), the collector triggers and begins yielding\n",
        "    audio frames. Then the collector waits until 90% of the frames in\n",
        "    the window are unvoiced to detrigger.\n",
        "    The window is padded at the front and back to provide a small\n",
        "    amount of silence or the beginnings/endings of speech around the\n",
        "    voiced frames.\n",
        "    Arguments:\n",
        "    sample_rate - The audio sample rate, in Hz.\n",
        "    frame_duration_ms - The frame duration in milliseconds.\n",
        "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
        "    vad - An instance of webrtcvad.Vad.\n",
        "    frames - a source of audio frames (sequence or generator).\n",
        "    Returns: A generator that yields PCM audio data.\n",
        "    \"\"\"\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    # We use a deque for our sliding window/ring buffer.\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
        "    # NOTTRIGGERED state.\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
        "            # the ring buffer are voiced frames, then enter the\n",
        "            # TRIGGERED state.\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                start = ring_buffer[0][0].timestamp\n",
        "                # We want to yield all the audio we see from now until\n",
        "                # we are NOTTRIGGERED, but we have to start with the\n",
        "                # audio that's already in the ring buffer.\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            # We're in the TRIGGERED state, so collect the audio data\n",
        "            # and add it to the ring buffer.\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            # If more than 90% of the frames in the ring buffer are\n",
        "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
        "            # audio we've collected.\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = False\n",
        "                yield (start, frame.timestamp + frame.duration)\n",
        "                ring_buffer.clear()\n",
        "                voiced_frames = []\n",
        "    # If we have any leftover voiced audio when we run out of input,\n",
        "    # yield it.\n",
        "    if voiced_frames:\n",
        "        yield (start, frame.timestamp + frame.duration)\n",
        "\n",
        "\n",
        "def VAD_chunk(aggressiveness, path):\n",
        "    audio, byte_audio = read_wave(path, sr)\n",
        "    vad = webrtcvad.Vad(int(aggressiveness))\n",
        "    frames = frame_generator(20, byte_audio, sr)\n",
        "    frames = list(frames)\n",
        "    times = vad_collector(sr, 20, 200, vad, frames)\n",
        "    speech_times = []\n",
        "    speech_segs = []\n",
        "    for i, time in enumerate(times):\n",
        "        start = np.round(time[0],decimals=2)\n",
        "        end = np.round(time[1],decimals=2)\n",
        "        j = start\n",
        "        while j + .4 < end:\n",
        "            end_j = np.round(j+.4,decimals=2)\n",
        "            speech_times.append((j, end_j))\n",
        "            speech_segs.append(audio[int(j*sr):int(end_j*sr)])\n",
        "            j = end_j\n",
        "        else:\n",
        "            speech_times.append((j, end))\n",
        "            speech_segs.append(audio[int(j*sr):int(end*sr)])\n",
        "    return speech_times, speech_segs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwpMLzH6k8Hv"
      },
      "source": [
        "# Based on code from PyTorch Speaker Verification:\n",
        "# https://github.com/HarryVolek/PyTorch_Speaker_Verification\n",
        "# Copyright (c) 2019, HarryVolek\n",
        "# Additions Copyright (c) 2021, Jim O'Regan\n",
        "# License: MIT\n",
        "import numpy as np\n",
        "\n",
        "# wav2vec2's max duration is 40 seconds, using 39 by default\n",
        "# to be a little safer\n",
        "def vad_concat(times, segs, max_duration=39.0):\n",
        "    \"\"\"\n",
        "    Concatenate continuous times and their segments, where the end time\n",
        "    of a segment is the same as the start time of the next\n",
        "        Parameters:\n",
        "            times: list of tuple (start, end)\n",
        "            segs: list of segments (audio frames)\n",
        "            max_duration: maximum duration of the resulting concatenated\n",
        "                segments; the kernel size of wav2vec2 is 40 seconds, so\n",
        "                the default max_duration is 39, to ensure the resulting\n",
        "                list of segments will fit\n",
        "        Returns:\n",
        "            concat_times: list of tuple (start, end)\n",
        "            concat_segs: list of segments (audio frames)\n",
        "    \"\"\"\n",
        "    absolute_maximum=40.0\n",
        "    if max_duration > absolute_maximum:\n",
        "        raise Exception('`max_duration` {:.2f} larger than kernel size (40 seconds)'.format(max_duration))\n",
        "    # we take 0.0 to mean \"don't concatenate\"\n",
        "    do_concat = (max_duration != 0.0)\n",
        "    concat_seg = []\n",
        "    concat_times = []\n",
        "    seg_concat = segs[0]\n",
        "    time_concat = times[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        can_concat = (times[i+1][1] - time_concat[0]) < max_duration\n",
        "        if time_concat[1] == times[i+1][0] and do_concat and can_concat:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "            time_concat = (time_concat[0], times[i+1][1])\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "            concat_times.append(time_concat)\n",
        "            time_concat = times[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "        concat_times.append(time_concat)\n",
        "    return concat_times, concat_seg"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzccsIklFkV"
      },
      "source": [
        "def make_dataset(concat_times, concat_segs):\n",
        "  starts = [s[0] for s in concat_times]\n",
        "  ends = [s[1] for s in concat_times]\n",
        "  return {'start': starts,\n",
        "          'end': ends,\n",
        "          'speech': concat_segs}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIbDIIkFmQJu",
        "outputId": "e89e38f5-6bc3-4a4b-ab71-2318c916b6df"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "def vad_to_dataset(path):\n",
        "  t,s = VAD_chunk(3, path)\n",
        "  ct, cs = vad_concat(t, s)\n",
        "  dset = make_dataset(ct, cs)\n",
        "  return Dataset.from_dict(dset)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 23.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 21.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 16.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 81kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 92kB 16.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 112kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 122kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 133kB 17.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 143kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 153kB 17.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 163kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 174kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 184kB 17.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, fsspec, xxhash, datasets\n",
            "Successfully installed datasets-1.5.0 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLuEz02ZlSvC"
      },
      "source": [
        "%%capture\n",
        "!pip install -q transformers"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hac9-TrHmyaM",
        "outputId": "cb77792e-c5b4-4140-e2af-4dd099154c67"
      },
      "source": [
        "%%capture\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "# load model and tokenizer\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"mbien/wav2vec2-large-xlsr-polish\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"mbien/wav2vec2-large-xlsr-polish\")\n",
        "model.to(\"cuda\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPoyTvuHm_gC"
      },
      "source": [
        "def speech_file_to_array_fn(batch):\n",
        "    import torchaudio\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    batch[\"speech\"] = speech_array[0].numpy()\n",
        "    batch[\"sampling_rate\"] = sampling_rate\n",
        "    batch[\"target_text\"] = batch[\"sentence\"]\n",
        "    return batch\n",
        "def evaluate(batch):\n",
        "  import torch\n",
        "  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
        "\n",
        "  pred_ids = torch.argmax(logits, dim=-1)\n",
        "  batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
        "  return batch"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgjOpU7ynmpT"
      },
      "source": [
        "import json\n",
        "def process_wave(filename):\n",
        "  dataset = vad_to_dataset(filename)\n",
        "  result = dataset.map(evaluate, batched=True, batch_size=16)\n",
        "  speechless = result.remove_columns(['speech'])\n",
        "  d=speechless.to_dict()\n",
        "  tlog = list()\n",
        "  for i in range(0, len(d['end']) - 1):\n",
        "    out = dict()\n",
        "    out['start'] = d['start'][i]\n",
        "    out['end'] = d['end'][i]\n",
        "    out['transcript'] = d['pred_strings'][i]\n",
        "    tlog.append(out)\n",
        "  with open('{}.tlog'.format(filename), 'w') as outfile:\n",
        "    json.dump(tlog, outfile)\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRmfyBqpoZeD"
      },
      "source": [
        "import glob\n",
        "for f in glob.glob('./24*.wav'):\n",
        "  process_wave(f)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeHvI5e37q1Z",
        "outputId": "f9122205-f8a0-4b5b-c516-f9e6933f2d9b"
      },
      "source": [
        "!ls *json|zip tlogs.zip -@"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: data.json (deflated 71%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}