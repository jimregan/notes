{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using a wav2vec2 model with DSAlign.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "666eeda0a34243f984cc34187436d83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af2de6b30ed74907a41ae8d8521b1be3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3669149f07ab4c2193ea3db72a4a6187",
              "IPY_MODEL_8516e813bdc745efb0f932e6035f6e34"
            ]
          }
        },
        "af2de6b30ed74907a41ae8d8521b1be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3669149f07ab4c2193ea3db72a4a6187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6742df6564ec4bdd87891307c1068d92",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 18,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 18,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_617d764425ea480aa96ff18b196207f5"
          }
        },
        "8516e813bdc745efb0f932e6035f6e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_921527056a944ebf81ea0a903118fb16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 18/18 [00:17&lt;00:00,  1.03ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b81655193e3646029295d03303356fd2"
          }
        },
        "6742df6564ec4bdd87891307c1068d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "617d764425ea480aa96ff18b196207f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "921527056a944ebf81ea0a903118fb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b81655193e3646029295d03303356fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo0Hy3GcHQit"
      },
      "source": [
        "!pip install librosa webrtcvad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRGZSK6eHTLq"
      },
      "source": [
        "The VAD wrapper is taken from [PyTorch Speaker Verification](https://github.com/HarryVolek/PyTorch_Speaker_Verification), which is BSD 2-clause (expand the tab below to check)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux8SX2YrHoCh"
      },
      "source": [
        "# Licence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRyoNkF0HdWh"
      },
      "source": [
        "**BSD** 3-Clause License\n",
        "\n",
        "Copyright (c) 2019, HarryVolek\n",
        "\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice, this\n",
        "  list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice,\n",
        "  this list of conditions and the following disclaimer in the documentation\n",
        "  and/or other materials provided with the distribution.\n",
        "\n",
        "* Neither the name of the copyright holder nor the names of its\n",
        "  contributors may be used to endorse or promote products derived from\n",
        "  this software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwZjkeQzHzOC"
      },
      "source": [
        "# VAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5f0-XyBH2Gz"
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Dec 18 16:22:41 2018\n",
        "\n",
        "@author: Harry\n",
        "Modified from https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import sys\n",
        "import librosa\n",
        "import wave\n",
        "\n",
        "import webrtcvad\n",
        "\n",
        "#from hparam import hparam as hp\n",
        "sr = 16000\n",
        "\n",
        "def read_wave(path, sr):\n",
        "    \"\"\"Reads a .wav file.\n",
        "    Takes the path, and returns (PCM audio data, sample rate).\n",
        "    Assumes sample width == 2\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "    data, _ = librosa.load(path, sr)\n",
        "    assert len(data.shape) == 1\n",
        "    assert sr in (8000, 16000, 32000, 48000)\n",
        "    return data, pcm_data\n",
        "    \n",
        "class Frame(object):\n",
        "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    \"\"\"Generates audio frames from PCM audio data.\n",
        "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
        "    the sample rate.\n",
        "    Yields Frames of the requested duration.\n",
        "    \"\"\"\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms,\n",
        "                  padding_duration_ms, vad, frames):\n",
        "    \"\"\"Filters out non-voiced audio frames.\n",
        "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
        "    the voiced audio.\n",
        "    Uses a padded, sliding window algorithm over the audio frames.\n",
        "    When more than 90% of the frames in the window are voiced (as\n",
        "    reported by the VAD), the collector triggers and begins yielding\n",
        "    audio frames. Then the collector waits until 90% of the frames in\n",
        "    the window are unvoiced to detrigger.\n",
        "    The window is padded at the front and back to provide a small\n",
        "    amount of silence or the beginnings/endings of speech around the\n",
        "    voiced frames.\n",
        "    Arguments:\n",
        "    sample_rate - The audio sample rate, in Hz.\n",
        "    frame_duration_ms - The frame duration in milliseconds.\n",
        "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
        "    vad - An instance of webrtcvad.Vad.\n",
        "    frames - a source of audio frames (sequence or generator).\n",
        "    Returns: A generator that yields PCM audio data.\n",
        "    \"\"\"\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    # We use a deque for our sliding window/ring buffer.\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
        "    # NOTTRIGGERED state.\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
        "            # the ring buffer are voiced frames, then enter the\n",
        "            # TRIGGERED state.\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                start = ring_buffer[0][0].timestamp\n",
        "                # We want to yield all the audio we see from now until\n",
        "                # we are NOTTRIGGERED, but we have to start with the\n",
        "                # audio that's already in the ring buffer.\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            # We're in the TRIGGERED state, so collect the audio data\n",
        "            # and add it to the ring buffer.\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            # If more than 90% of the frames in the ring buffer are\n",
        "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
        "            # audio we've collected.\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = False\n",
        "                yield (start, frame.timestamp + frame.duration)\n",
        "                ring_buffer.clear()\n",
        "                voiced_frames = []\n",
        "    # If we have any leftover voiced audio when we run out of input,\n",
        "    # yield it.\n",
        "    if voiced_frames:\n",
        "        yield (start, frame.timestamp + frame.duration)\n",
        "\n",
        "\n",
        "def VAD_chunk(aggressiveness, path):\n",
        "    audio, byte_audio = read_wave(path, sr)\n",
        "    vad = webrtcvad.Vad(int(aggressiveness))\n",
        "    frames = frame_generator(20, byte_audio, sr)\n",
        "    frames = list(frames)\n",
        "    times = vad_collector(sr, 20, 200, vad, frames)\n",
        "    speech_times = []\n",
        "    speech_segs = []\n",
        "    for i, time in enumerate(times):\n",
        "        start = np.round(time[0],decimals=2)\n",
        "        end = np.round(time[1],decimals=2)\n",
        "        j = start\n",
        "        while j + .4 < end:\n",
        "            end_j = np.round(j+.4,decimals=2)\n",
        "            speech_times.append((j, end_j))\n",
        "            speech_segs.append(audio[int(j*sr):int(end_j*sr)])\n",
        "            j = end_j\n",
        "        else:\n",
        "            speech_times.append((j, end))\n",
        "            speech_segs.append(audio[int(j*sr):int(end*sr)])\n",
        "    return speech_times, speech_segs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjmykjXyIMPV"
      },
      "source": [
        "(From ```dvector_create.py``` in the same package)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myog5EDPH6C_"
      },
      "source": [
        "import numpy as np\n",
        "def concat_segs(times, segs):\n",
        "    #Concatenate continuous voiced segments\n",
        "    concat_seg = []\n",
        "    seg_concat = segs[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        if times[i][1] == times[i+1][0]:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "    return concat_seg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7gdrly0IpRR"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1prY_wtxrgW"
      },
      "source": [
        "I'm going to use a video from YouTube as my input, so first I need to install youtube-dl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWLCFV4dIwdi",
        "outputId": "222b56f3-27c1-4cf1-a6e5-8c8b25bd1e6b"
      },
      "source": [
        "!pip install youtube-dl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtube-dl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/f7/e51b361789bd2085a2e9d9e3eaa3070c10b755dff3623c1ae5222acee9e0/youtube_dl-2021.3.25-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.9MB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: youtube-dl\n",
            "Successfully installed youtube-dl-2021.3.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5YrfhSDx_It"
      },
      "source": [
        "I've selected [this video](https://www.youtube.com/watch?v=VRg-a0qSGa8) because it's a speech by the President of Ireland (and so copyright-free as a matter of public record), it has subtitles (in Irish, though listed as English), and the subtitles are quite faithful to what was spoken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FcsjkvxI91-"
      },
      "source": [
        "!youtube-dl --all-subs -o '%(id)s' VRg-a0qSGa8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNzzdTLd1NMC"
      },
      "source": [
        "The audio needs to be a 16k wav, so I'm converting it with ffmpeg. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGeU9bqOKxfd"
      },
      "source": [
        "!ffmpeg -i VRg-a0qSGa8.mkv -acodec pcm_s16le -ac 1 -ar 16000 VRg-a0qSGa8.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFZCbsO20RNq"
      },
      "source": [
        "Next, I'm using the ```VAD_chunk()``` function to get the start and end times, and audio segements of each part of the video with speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhShQO9fLBo4"
      },
      "source": [
        "times, segs = VAD_chunk(3, 'VRg-a0qSGa8.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3FPhqIa0ks7"
      },
      "source": [
        "The ```concat_segs()``` function from above merges the audio segments, but not the times, so I've adapted it here to do both:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohIO-cG0LNPY"
      },
      "source": [
        "import numpy as np\n",
        "def concat_both(times, segs):\n",
        "    \"\"\"Concatenate continuous times and their segments\"\"\"\n",
        "    concat_seg = []\n",
        "    concat_times = []\n",
        "    seg_concat = segs[0]\n",
        "    time_concat = times[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        if time_concat[1] == times[i+1][0]:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "            time_concat = (time_concat[0], times[i+1][1])\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "            concat_times.append(time_concat)\n",
        "            time_concat = times[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "        concat_times.append(time_concat)\n",
        "    return concat_times, concat_seg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOI8L_psURXV"
      },
      "source": [
        "ntimes, nsegs = concat_both(times, segs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trkB11-R02-M"
      },
      "source": [
        "Next, I'm putting the data into a dict that Huggingface datasets can read:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phPPgOi5WDoF"
      },
      "source": [
        "starts = [s[0] for s in ntimes]\n",
        "ends = [s[1] for s in ntimes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMYDkNeow2Pa"
      },
      "source": [
        "dset = {'start': starts,\n",
        "        'end': ends,\n",
        "        'speech': nsegs}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZufqkUX6Im"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPSklhnxN24"
      },
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(dset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRM84NFJxWRf",
        "outputId": "46d12732-e825-4799-fd28-ffc0d097598b"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['start', 'end', 'speech'],\n",
              "    num_rows: 137\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H13dhzTl1nxH"
      },
      "source": [
        "Now, the data is ready to plug into my wav2vec2 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLn4iw__1tn3",
        "outputId": "716433c5-5546-4e55-ce65-ae23e5d50508"
      },
      "source": [
        "! pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0MB 7.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 37.2MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 43.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIfFtegAqIQC"
      },
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "# load model and tokenizer\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"jimregan/wav2vec2-large-xlsr-irish-basic\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"jimregan/wav2vec2-large-xlsr-irish-basic\")\n",
        "model.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N27cTq2rqNqV"
      },
      "source": [
        "def speech_file_to_array_fn(batch):\n",
        "    import torchaudio\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    batch[\"speech\"] = speech_array[0].numpy()\n",
        "    batch[\"sampling_rate\"] = sampling_rate\n",
        "    batch[\"target_text\"] = batch[\"sentence\"]\n",
        "    return batch\n",
        "def evaluate(batch):\n",
        "  import torch\n",
        "  inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
        "\n",
        "  pred_ids = torch.argmax(logits, dim=-1)\n",
        "  batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
        "  return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "666eeda0a34243f984cc34187436d83c",
            "af2de6b30ed74907a41ae8d8521b1be3",
            "3669149f07ab4c2193ea3db72a4a6187",
            "8516e813bdc745efb0f932e6035f6e34",
            "6742df6564ec4bdd87891307c1068d92",
            "617d764425ea480aa96ff18b196207f5",
            "921527056a944ebf81ea0a903118fb16",
            "b81655193e3646029295d03303356fd2"
          ]
        },
        "id": "O0sn8DfJdxOs",
        "outputId": "036f99b8-bf6f-46ca-a5f0-9c6cc9925d1c"
      },
      "source": [
        "result = dataset.map(evaluate, batched=True, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "666eeda0a34243f984cc34187436d83c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0UuCr7rgUjo"
      },
      "source": [
        "speechless = result.remove_columns(['speech'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skxECvmYgmx3"
      },
      "source": [
        "d=speechless.to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwHf2t0tguBP"
      },
      "source": [
        "tlog = list()\n",
        "for i in range(0, len(d['end']) - 1):\n",
        "  out = dict()\n",
        "  out['start'] = d['start'][i]\n",
        "  out['end'] = d['end'][i]\n",
        "  out['transcript'] = d['pred_strings'][i]\n",
        "  tlog.append(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3hJjR0ihikx"
      },
      "source": [
        "import json\n",
        "with open('/content/VRg-a0qSGa8.tlog', 'w') as outfile:\n",
        "    json.dump(tlog, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjrT1Aeqmzi5"
      },
      "source": [
        "Next, I'm extracting the text content from the vtt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNdOgJdCj0TD",
        "outputId": "ad6f0651-7ac0-42a5-e3b4-ae1a1f48d8f2"
      },
      "source": [
        "!pip install webvtt-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting webvtt-py\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/83/115b001f8c79f9580834faf214062b1ff69f61c62ab1a5c3c1e5e347d4a3/webvtt_py-0.4.6-py3-none-any.whl\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from webvtt-py) (0.6.2)\n",
            "Installing collected packages: webvtt-py\n",
            "Successfully installed webvtt-py-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcawkwNpj327"
      },
      "source": [
        "def get_vtt_text(filename):\n",
        "  import webvtt\n",
        "  out = list()\n",
        "  for sub in webvtt.read(filename):\n",
        "    out.append(sub.text)\n",
        "  return ' '.join(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9CRqWZMk6OH"
      },
      "source": [
        "text = get_vtt_text('/content/VRg-a0qSGa8.en.vtt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyTxDZm8onfr"
      },
      "source": [
        "I can do some normalisation now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vBIjjrnnOmw"
      },
      "source": [
        "text = text.replace('1901', 'naoi dÃ©ag is a haon')\n",
        "text = text.replace('2021', 'fiche is fiche is a haon')\n",
        "text = text.replace('Covid-19', 'covid a naoi dÃ©ag')\n",
        "text = text.replace('fiche fiche haon', 'fiche is fiche is a haon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TugaXN_tm6kb"
      },
      "source": [
        "I want sentences, so I'm going to use mosestokenizer to split the text (there aren't any specific abbreviations in this video, so the English splitter works fine. YMMV.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWYFUg9ulzgS"
      },
      "source": [
        "!pip install mosestokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_zi73iamK7A"
      },
      "source": [
        "from mosestokenizer import MosesSentenceSplitter\n",
        "with MosesSentenceSplitter('en') as splitsents:\n",
        "  sents = splitsents([text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYO6Q1iQmiwV"
      },
      "source": [
        "with open('/content/VRg-a0qSGa8.txt', 'w') as outfile:\n",
        "  outfile.writelines(['\\n'.join(sents)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCnedg6yvRGX"
      },
      "source": [
        "DSAlign requires an alphabet (1 character per line), so create that first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2-cheYRvZ8i"
      },
      "source": [
        "alpha=\"aÃ¡bcdeÃ©fghiÃ­jklmnoÃ³pqrstuÃºvwxyz'\"\n",
        "alpha_chars = [char for char in alpha]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvJSAKh0vteL"
      },
      "source": [
        "with open('/content/ga.alphabet', 'w') as outfile:\n",
        "  outfile.writelines(['\\n'.join(alpha_chars)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMHkaflyvIHt"
      },
      "source": [
        "Now, to install DSAlign and its dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j36pyC-3r8VZ"
      },
      "source": [
        "!git clone https://github.com/mozilla/DSAlign"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge9QlSGKu-b1"
      },
      "source": [
        "!apt-get install sox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qPaxkH9tBds"
      },
      "source": [
        "import os\n",
        "os.chdir('DSAlign')\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTAmbZ5wwW_L"
      },
      "source": [
        "Now, I'm ready to align:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp8TO2OKtKQe",
        "outputId": "86639ff6-7dba-4ffd-fe65-bf649a5a4731"
      },
      "source": [
        "!bin/align.sh --tlog /content/VRg-a0qSGa8.tlog --script /content/VRg-a0qSGa8.txt --aligned /content/VRg-a0qSGa8.aligned --text-meaningful-newlines --alphabet /content/ga.alphabet "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin/align.sh: line 3: /content/DSAlign/venv/bin/activate: No such file or directory\n",
            "INFO:root:Aligning\n",
            " 1 of 1 : 100.00% (elapsed: 00:00:04, speed: 0.25 it/s, ETA: 00:00:00)\n",
            "INFO:root:Aligned 24 fragments\n",
            "INFO:root:Dropped 112 fragments 466.67%:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5nsf4gew7oX"
      },
      "source": [
        "24 out of 136 fragments isn't great, but it's quite good considering the WER of the model (43.7%); the next step would be to add the aligned data to the training set, retrain, and repeat."
      ]
    }
  ]
}