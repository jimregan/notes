{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /","metadata":{"execution":{"iopub.status.busy":"2021-05-27T08:45:21.86057Z","iopub.execute_input":"2021-05-27T08:45:21.860878Z","iopub.status.idle":"2021-05-27T08:45:21.872691Z","shell.execute_reply.started":"2021-05-27T08:45:21.860805Z","shell.execute_reply":"2021-05-27T08:45:21.871694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!tar xvf /kaggle/input/extract-cuda-from-kaldi-docker/cuda.tar","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /opt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo Untarring Kaldi","metadata":{"execution":{"iopub.status.busy":"2021-05-27T08:49:38.58714Z","iopub.execute_input":"2021-05-27T08:49:38.587503Z","iopub.status.idle":"2021-05-27T08:49:39.222724Z","shell.execute_reply.started":"2021-05-27T08:49:38.587474Z","shell.execute_reply":"2021-05-27T08:49:39.221766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!tar xvf /kaggle/input/extract-prebuilt-kaldi-from-docker/kaldi.tar","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-27T08:49:41.295471Z","iopub.execute_input":"2021-05-27T08:49:41.29592Z","iopub.status.idle":"2021-05-27T08:50:19.584365Z","shell.execute_reply.started":"2021-05-27T08:49:41.295876Z","shell.execute_reply":"2021-05-27T08:50:19.583342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd kaldi/egs","metadata":{"execution":{"iopub.status.busy":"2021-05-27T08:50:19.586138Z","iopub.execute_input":"2021-05-27T08:50:19.586496Z","iopub.status.idle":"2021-05-27T08:50:19.592778Z","shell.execute_reply.started":"2021-05-27T08:50:19.586455Z","shell.execute_reply":"2021-05-27T08:50:19.591901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/danijel3/ClarinStudioKaldi","metadata":{"execution":{"iopub.status.busy":"2021-05-27T08:50:19.594692Z","iopub.execute_input":"2021-05-27T08:50:19.595208Z","iopub.status.idle":"2021-05-27T08:50:26.778557Z","shell.execute_reply.started":"2021-05-27T08:50:19.595169Z","shell.execute_reply":"2021-05-27T08:50:26.777632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ClarinStudioKaldi","metadata":{"execution":{"iopub.status.busy":"2021-05-27T08:50:26.782255Z","iopub.execute_input":"2021-05-27T08:50:26.782521Z","iopub.status.idle":"2021-05-27T08:50:26.79085Z","shell.execute_reply.started":"2021-05-27T08:50:26.782487Z","shell.execute_reply":"2021-05-27T08:50:26.789927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo getting cudatoolkit 10.0","metadata":{"execution":{"iopub.status.busy":"2021-05-27T08:50:26.793829Z","iopub.execute_input":"2021-05-27T08:50:26.794189Z","iopub.status.idle":"2021-05-27T08:50:27.436374Z","shell.execute_reply.started":"2021-05-27T08:50:26.79416Z","shell.execute_reply":"2021-05-27T08:50:27.435437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib:/opt/kaldi/tools/openfst-1.6.7/lib:/opt/kaldi/src/lib:/usr/local/cuda-10.0/targets/x86_64-linux/lib/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat path.sh|sed -e 's/~\\/apps/\\/opt/' > tmp\n!mv tmp path.sh","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:41:52.08403Z","iopub.execute_input":"2021-05-25T17:41:52.084244Z","iopub.status.idle":"2021-05-25T17:41:53.564637Z","shell.execute_reply.started":"2021-05-25T17:41:52.084223Z","shell.execute_reply":"2021-05-25T17:41:53.563351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo > local_clarin/clarin_pl_clean.sh","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:41:53.566993Z","iopub.execute_input":"2021-05-25T17:41:53.567274Z","iopub.status.idle":"2021-05-25T17:41:54.310366Z","shell.execute_reply.started":"2021-05-25T17:41:53.567245Z","shell.execute_reply":"2021-05-25T17:41:54.309073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ln -s ../wsj/s5/steps\n!ln -s ../wsj/s5/conf\n!ln -s ../wsj/s5/local\n!ln -s ../wsj/s5/utils","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:41:54.312197Z","iopub.execute_input":"2021-05-25T17:41:54.312497Z","iopub.status.idle":"2021-05-25T17:41:57.204431Z","shell.execute_reply.started":"2021-05-25T17:41:54.312456Z","shell.execute_reply":"2021-05-25T17:41:57.203233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo \"copying data and exp from last run\" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/kaldi-clarinstudio-polish-ivectors/data /kaggle/working/\n!cp -r /kaggle/input/kaldi-clarinstudio-polish-ivectors/exp /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:41:57.205908Z","iopub.execute_input":"2021-05-25T17:41:57.206207Z","iopub.status.idle":"2021-05-25T17:43:40.957574Z","shell.execute_reply.started":"2021-05-25T17:41:57.206175Z","shell.execute_reply":"2021-05-25T17:43:40.955985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ln -s /kaggle/working/exp\n!ln -s /kaggle/working/data","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:43:40.959576Z","iopub.execute_input":"2021-05-25T17:43:40.959823Z","iopub.status.idle":"2021-05-25T17:43:42.406563Z","shell.execute_reply.started":"2021-05-25T17:43:40.959798Z","shell.execute_reply":"2021-05-25T17:43:42.405312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp exp/tri3b/35.mdl exp/tri3b/final.mdl\n!cp exp/tri3b/35.occs exp/tri3b/final.occs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo Setup done","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile run.sh\n. path.sh\n\nexport nj=40 ##number of concurrent processes\nexport nj_test=30 ## number of concurrent processes for test has to be <=30\n\n./local_clarin/clarin_tdnn.sh\nsteps/nnet3/decode.sh --nj $nj_test --num-threads 4 --online-ivector-dir exp/nnet3/ivectors_test_hires \\\n          exp/tri3b/graph data/test_hires exp/nnet3/tdnn1a_sp/decode\n./steps/oracle_wer.sh data/test_hires data/lang exp/nnet3/tdnn1a_sp/decode\n./steps/lmrescore_const_arpa.sh data/lang_test data/lang_carpa data/test_hires exp/nnet3/tdnn1a_sp/decode exp/nnet3/tdnn1a_sp/decode_rs\n\n# Same as above but using the chain framework - trains about the same, works much faster as has lower WER\n./local_clarin/clarin_chain_tdnn.sh\n./steps/oracle_wer.sh data/test_hires data/lang exp/chain/tdnn1f_sp/decode\n./steps/lmrescore_const_arpa.sh data/lang_test data/lang_carpa data/test_hires exp/chain/tdnn1f_sp/decode exp/chain/tdnn1f_sp/decode_rs\n\n# Getting results\nfind exp -name best_wer | while read f ; do cat $f ; done | sort -k2nr\nfind exp -name oracle_wer | while read f ; do echo -n \"$f: \" ; cat $f ; done | sort -k2nr","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:43:42.408143Z","iopub.execute_input":"2021-05-25T17:43:42.408431Z","iopub.status.idle":"2021-05-25T17:43:47.132173Z","shell.execute_reply.started":"2021-05-25T17:43:42.408401Z","shell.execute_reply":"2021-05-25T17:43:47.131178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile local_clarin/clarin_chain_tdnn.sh\n#!/bin/bash\n\nset -e -o pipefail\n\n# First the options that are passed through to run_ivector_common.sh\n# (some of which are also used in this script directly).\nstage=0\nnj=40\ntrain_set=train\ntest_set=test\ngmm=tri3b        # this is the source gmm-dir that we'll use for alignments; it\n                 # should have alignments for the specified training data.\nnum_threads_ubm=40\nnnet3_affix=       # affix for exp dirs, e.g. it was _cleaned in tedlium.\n\n# Options which are not passed through to run_ivector_common.sh\naffix=1f  #affix for TDNN+LSTM directory e.g. \"1a\" or \"1b\", in case we change the configuration.\ncommon_egs_dir=\nreporting_email=\n\n# LSTM/chain options\ntrain_stage=-10\nxent_regularize=0.1\n\n# training chunk-options\nchunk_width=140,100,160\n# we don't need extra left/right context for TDNN systems.\nchunk_left_context=0\nchunk_right_context=0\n\n# training options\nsrand=0\nremove_egs=true\n\n#decode options\ntest_online_decoding=false  # if true, it will run the last decoding stage.\n\n# End configuration section.\necho \"$0 $@\"  # Print the command line for logging\n\n\n. ./cmd.sh\n. ./path.sh\n. ./utils/parse_options.sh\n\n\nif ! cuda-compiled; then\n  cat <<EOF && exit 1\nThis script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\nIf you want to use GPUs (and have them), go to src/, and configure and make on a machine\nwhere \"nvcc\" is installed.\nEOF\nfi\n\n#local/nnet3/run_ivector_common.sh \\\n#  --stage $stage --nj $nj \\\n#  --train-set $train_set --gmm $gmm \\\n#  --test-sets $test_set \\\n#  --num-threads-ubm $num_threads_ubm \\\n#  --nnet3-affix \"$nnet3_affix\"\n\n\n\ngmm_dir=exp/${gmm}\nali_dir=exp/${gmm}_ali_${train_set}_sp\nlat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\ndir=exp/chain${nnet3_affix}/tdnn${affix}_sp\ntrain_data_dir=data/${train_set}_sp_hires\ntrain_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\nlores_train_data_dir=data/${train_set}_sp\n\n# note: you don't necessarily have to change the treedir name\n# each time you do a new experiment-- only if you change the\n# configuration in a way that affects the tree.\ntree_dir=exp/chain${nnet3_affix}/tree_a_sp\n# the 'lang' directory is created by this script.\n# If you create such a directory with a non-standard topology\n# you should probably name it differently.\nlang=data/lang_chain\n\nfor f in $train_data_dir/feats.scp $train_ivector_dir/ivector_online.scp \\\n    $lores_train_data_dir/feats.scp $ali_dir/ali.1.gz; do\n  [ ! -f $f ] && echo \"$0: expected file $f to exist\" && exit 1\ndone\n\n\nif [ $stage -le 12 ]; then\n  echo \"$0: creating lang directory $lang with chain-type topology\"\n  # Create a version of the lang/ directory that has one state per phone in the\n  # topo file. [note, it really has two states.. the first one is only repeated\n  # once, the second one has zero or more repeats.]\n  if [ -d $lang ]; then\n    if [ $lang/L.fst -nt data/lang/L.fst ]; then\n      echo \"$0: $lang already exists, not overwriting it; continuing\"\n    else\n      echo \"$0: $lang already exists and seems to be older than data/lang...\"\n      echo \" ... not sure what to do.  Exiting.\"\n      exit 1;\n    fi\n  else\n    cp -r data/lang $lang\n    silphonelist=$(cat $lang/phones/silence.csl) || exit 1;\n    nonsilphonelist=$(cat $lang/phones/nonsilence.csl) || exit 1;\n    # Use our special topology... note that later on may have to tune this\n    # topology.\n    steps/nnet3/chain/gen_topo.py $nonsilphonelist $silphonelist >$lang/topo\n  fi\nfi\n\nif [ $stage -le 13 ]; then\n  # Get the alignments as lattices (gives the chain training more freedom).\n  # use the same num-jobs as the alignments\n  steps/align_fmllr_lats.sh --nj 100 --cmd \"$train_cmd\" ${lores_train_data_dir} \\\n    data/lang $gmm_dir $lat_dir\n  rm $lat_dir/fsts.*.gz # save space\nfi\n\nif [ $stage -le 14 ]; then\n  # Build a tree using our new topology.  We know we have alignments for the\n  # speed-perturbed data (local/nnet3/run_ivector_common.sh made them), so use\n  # those.  The num-leaves is always somewhat less than the num-leaves from\n  # the GMM baseline.\n   if [ -f $tree_dir/final.mdl ]; then\n     echo \"$0: $tree_dir/final.mdl already exists, refusing to overwrite it.\"\n     exit 1;\n  fi\n  steps/nnet3/chain/build_tree.sh \\\n    --frame-subsampling-factor 3 \\\n    --context-opts \"--context-width=2 --central-position=1\" \\\n    --cmd \"$train_cmd\" 3500 ${lores_train_data_dir} \\\n    $lang $ali_dir $tree_dir\nfi\n\n\nif [ $stage -le 15 ]; then\n  mkdir -p $dir\n  echo \"$0: creating neural net configs using the xconfig parser\";\n\n  num_targets=$(tree-info $tree_dir/tree |grep num-pdfs|awk '{print $2}')\n  learning_rate_factor=$(echo \"print(0.5/$xent_regularize)\" | python)\n\n  mkdir -p $dir/configs\n  cat <<EOF > $dir/configs/network.xconfig\n  input dim=100 name=ivector\n  input dim=40 name=input\n\n  # please note that it is important to have input layer with the name=input\n  # as the layer immediately preceding the fixed-affine-layer to enable\n  # the use of short notation for the descriptor\n  fixed-affine-layer name=lda input=Append(-2,-1,0,1,2,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat\n\n  # the first splicing is moved before the lda layer, so no splicing here\n  relu-renorm-layer name=tdnn1 dim=512\n  relu-renorm-layer name=tdnn2 dim=512 input=Append(-1,0,1)\n  relu-renorm-layer name=tdnn3 dim=512 input=Append(-1,0,1)\n  relu-renorm-layer name=tdnn4 dim=512 input=Append(-3,0,3)\n  relu-renorm-layer name=tdnn5 dim=512 input=Append(-3,0,3)\n  relu-renorm-layer name=tdnn6 dim=512 input=Append(-6,-3,0)\n\n  ## adding the layers for chain branch\n  relu-renorm-layer name=prefinal-chain dim=512 target-rms=0.5\n  output-layer name=output include-log-softmax=false dim=$num_targets max-change=1.5\n\n  # adding the layers for xent branch\n  # This block prints the configs for a separate output that will be\n  # trained with a cross-entropy objective in the 'chain' models... this\n  # has the effect of regularizing the hidden parts of the model.  we use\n  # 0.5 / args.xent_regularize as the learning rate factor- the factor of\n  # 0.5 / args.xent_regularize is suitable as it means the xent\n  # final-layer learns at a rate independent of the regularization\n  # constant; and the 0.5 was tuned so as to make the relative progress\n  # similar in the xent and regular final layers.\n  relu-renorm-layer name=prefinal-xent input=tdnn6 dim=512 target-rms=0.5\n  output-layer name=output-xent dim=$num_targets learning-rate-factor=$learning_rate_factor max-change=1.5\nEOF\n  steps/nnet3/xconfig_to_configs.py --xconfig-file $dir/configs/network.xconfig --config-dir $dir/configs/\nfi\n\n\nif [ $stage -le 16 ]; then\n  if [[ $(hostname -f) == *.clsp.jhu.edu ]] && [ ! -d $dir/egs/storage ]; then\n    utils/create_split_dir.pl \\\n     /export/b0{3,4,5,6}/$USER/kaldi-data/egs/wsj-$(date +'%m_%d_%H_%M')/s5/$dir/egs/storage $dir/egs/storage\n  fi\n\n  steps/nnet3/chain/train.py --stage=$train_stage \\\n    --cmd=\"$decode_cmd\" \\\n    --feat.online-ivector-dir=$train_ivector_dir \\\n    --feat.cmvn-opts=\"--norm-means=false --norm-vars=false\" \\\n    --chain.xent-regularize $xent_regularize \\\n    --chain.leaky-hmm-coefficient=0.1 \\\n    --chain.l2-regularize=0.00005 \\\n    --chain.apply-deriv-weights=false \\\n    --chain.lm-opts=\"--num-extra-lm-states=2000\" \\\n    --trainer.srand=$srand \\\n    --trainer.max-param-change=2.0 \\\n    --trainer.num-epochs=4 \\\n    --trainer.frames-per-iter=3000000 \\\n    --trainer.optimization.num-jobs-initial=4 \\\n    --trainer.optimization.num-jobs-final=4 \\\n    --trainer.optimization.initial-effective-lrate=0.001 \\\n    --trainer.optimization.final-effective-lrate=0.0001 \\\n    --trainer.optimization.shrink-value=1.0 \\\n    --trainer.optimization.proportional-shrink=60.0 \\\n    --trainer.num-chunk-per-minibatch=256,128,64 \\\n    --trainer.optimization.momentum=0.0 \\\n    --egs.chunk-width=$chunk_width \\\n    --egs.chunk-left-context=$chunk_left_context \\\n    --egs.chunk-right-context=$chunk_right_context \\\n    --egs.chunk-left-context-initial=0 \\\n    --egs.chunk-right-context-final=0 \\\n    --egs.dir=\"$common_egs_dir\" \\\n    --egs.opts=\"--frames-overlap-per-eg 0\" \\\n    --cleanup.remove-egs=$remove_egs \\\n    --use-gpu=true \\\n    --reporting.email=\"$reporting_email\" \\\n    --feat-dir=$train_data_dir \\\n    --tree-dir=$tree_dir \\\n    --lat-dir=$lat_dir \\\n    --dir=$dir  || exit 1;\nfi\n\nif [ $stage -le 17 ]; then\n  # The reason we are using data/lang here, instead of $lang, is just to\n  # emphasize that it's not actually important to give mkgraph.sh the\n  # lang directory with the matched topology (since it gets the\n  # topology file from the model).  So you could give it a different\n  # lang directory, one that contained a wordlist and LM of your choice,\n  # as long as phones.txt was compatible.\n\n  utils/lang/check_phones_compatible.sh \\\n    data/lang_test/phones.txt $lang/phones.txt\n  utils/mkgraph.sh \\\n    --self-loop-scale 1.0 data/lang_test \\\n    $tree_dir $tree_dir/graph || exit 1;\n\nfi\n\nif [ $stage -le 18 ]; then\n  frames_per_chunk=$(echo $chunk_width | cut -d, -f1)\n  nspk=$(wc -l <data/${test_set}_hires/spk2utt)\n  steps/nnet3/decode.sh \\\n          --acwt 1.0 --post-decode-acwt 10.0 \\\n          --extra-left-context $chunk_left_context \\\n          --extra-right-context $chunk_right_context \\\n          --extra-left-context-initial 0 \\\n          --extra-right-context-final 0 \\\n          --frames-per-chunk $frames_per_chunk \\\n          --nj $nspk --cmd \"$decode_cmd\"  --num-threads 4 \\\n          --online-ivector-dir exp/nnet3${nnet3_affix}/ivectors_${test_set}_hires \\\n          $tree_dir/graph data/${test_set}_hires ${dir}/decode || exit 1\nfi\n\n\nexit 0;","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!bash run.sh","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:48:05.419759Z","iopub.execute_input":"2021-05-25T17:48:05.420122Z"},"trusted":true},"execution_count":null,"outputs":[]}]}