<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">wav2vec-u notes</h1><p class="page-description">tl;dr - wav2vec-u is difficult to get running</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-05T00:00:00-05:00" itemprop="datePublished">
        Jun 5, 2021
      </time>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#wav2vec-u">wav2vec-u</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#the-skippable-blah">The skippable blah</a></li>
<li class="toc-entry toc-h2"><a href="#caveat">Caveat</a></li>
<li class="toc-entry toc-h2"><a href="#step-0-data">Step 0: Data</a></li>
<li class="toc-entry toc-h2"><a href="#step-01-ltrwrdphn-files">Step 0.1: ltr/wrd/phn files</a></li>
<li class="toc-entry toc-h2"><a href="#step-02-preparing-tsvs">Step 0.2: Preparing TSVs</a></li>
<li class="toc-entry toc-h2"><a href="#step-1-vadsilence-trimming">Step 1: VAD/Silence trimming</a></li>
<li class="toc-entry toc-h2"><a href="#step-2-prepare_audiosh">Step 2: prepare_audio.sh</a></li>
<li class="toc-entry toc-h2"><a href="#step-3-prepare_textsh">Step 3: prepare_text.sh</a></li>
<li class="toc-entry toc-h2"><a href="#step-4-gan-training">Step 4: GAN training</a></li>
<li class="toc-entry toc-h2"><a href="#fin">Fin</a></li>
</ul><h2 id="the-skippable-blah">
<a class="anchor" href="#the-skippable-blah" aria-hidden="true"><span class="octicon octicon-link"></span></a>The skippable blah</h2>

<p><a href="https://ai.facebook.com/research/publications/unsupervised-speech-recognition">wav2vec unsupervised</a> has caught a bit of attention.</p>

<p>There has been a mixed bag of expectations: there was <a href="https://ai.facebook.com/blog/wav2vec-unsupervised-speech-recognition-without-supervision/">a blog post</a>, they even had a video:</p>

<div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">Facebook AI‚Äôs new open source speech recognition model, wav2vec Unsupervised, uses no transcribed data at all. We‚Äôve tested it on many languages, such as Swahili, that have proven challenging for other systems. <br>Learn more in our blog post here: <a href="https://t.co/b6ic50AsM6">https://t.co/b6ic50AsM6</a> <a href="https://t.co/x3Tx9nxq5i">pic.twitter.com/x3Tx9nxq5i</a></p>‚Äî Meta AI (@MetaAI) <a href="https://twitter.com/MetaAI/status/1399750058883444738?ref_src=twsrc%5Etfw">June 1, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>So, a few people have had the expectation that it would be quite a bit easier than it turned out to.</p>

<p>I‚Äôve been beating my head against multiple walls for over a decade, with various pieces of research software for various purposes, so my expectations were a little different. Just looking at the <a href="https://github.com/pytorch/fairseq/tree/master/examples/wav2vec/unsupervised">directory</a>, the third subdirectory is <code class="language-plaintext highlighter-rouge">kaldi_self_train</code>, which is the first red flag: this will not be easy. Scrolling down, among the first instructions are zsh scripts. zsh is a great shell, and using it as a shell was a sign of sophistication in the late 90s, but it isn‚Äôt the most universal shell, so if your scripts are zsh scripts, that‚Äôs a pretty good sign you‚Äôve never tried to run them on a second computer. That said, trying to use any kind of software on Linux in the late 90s involved some sort of beating of heads against walls, so that contributes too.</p>

<p>I like Kaggle. A lot. I like the workflow, and being able to use the output of one notebook as the input to another. I like being able to run something, and not have to babysit it in case it disconnects, like with Colab. So I‚Äôve tried to do as much of this as possible on Kaggle.</p>

<p>But the GPU images on Kaggle are seriously broken. It could be by design: the handful of things I‚Äôve tried that are run purely as a notebook seem to work well. Maybe conda is deliberately cobbled, maybe it‚Äôs unintentional, but it fails more often than not. So anything that involves using a GPU: switch to Colab.</p>

<h2 id="caveat">
<a class="anchor" href="#caveat" aria-hidden="true"><span class="octicon octicon-link"></span></a>Caveat</h2>

<p>These are my notes for my own use, because once I‚Äôve done a full trial run, I have some data I want to try out. I‚Äôm deliberately not adding additional text, mostly because I want the trial run to go as quickly as possible; that‚Äôs failing for other reasons, but such is life.</p>

<p>What I‚Äôve done was based on my understanding, or best guess. I can guarantee that I was not smoking any illegal substances, but considering where I live, I can‚Äôt guarantee that there was no second-hand smoke.</p>

<p>There is <a href="https://github.com/pytorch/fairseq/issues/3591">an issue</a> on fairseq‚Äôs github where Alexei Baevski has said that better instructions are coming in a week or so, so maybe bide your time; he also offered to answer questions on that thread, so if you have questions, your best bet is to ask there.</p>

<h2 id="step-0-data">
<a class="anchor" href="#step-0-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 0: Data</h2>

<p>In an ideal world, Kaggle‚Äôs dataset uploader would Do The Right Thing when given a link to a zip file, or, rather, one of two Right Things: just download it, or download and unzip. Instead, it creates a directory for every file in the zip.</p>

<p>ü§¶</p>

<p>Cool, I‚Äôll just do that in a <a href="/notes/kaggle/wav2vec-u/2021/05/25/download-common-voice-swedish.html">notebook</a>.</p>

<p>wav2vec-u (and just about everything else in the world of ASR, ever) needs audio sampled at 16 kHz, and uses <code class="language-plaintext highlighter-rouge">soundfile</code>, so MP3s are not welcome, so I‚Äôll do that in <a href="/notes/kaggle/wav2vec-u/2021/05/25/common-voice-swedish-16bit-wav.html">another notebook</a>.</p>

<h2 id="step-01-ltrwrdphn-files">
<a class="anchor" href="#step-01-ltrwrdphn-files" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 0.1: ltr/wrd/phn files</h2>

<p>Preparing these files is mentioned in passing, as though they‚Äôre self-explanatory. Which they are, if you happen to have played with phoneme-based ASR as well as wav2letter. So, not really.</p>

<p>What I ended up doing is <a href="/notes/kaggle/wav2vec-u/2021/05/26/wav2vec-u-cv-swedish-prep-ltr-phn-wrd.html">this</a>; I <em>should have</em> changed the tab separation in the <code class="language-plaintext highlighter-rouge">dict.*</code> files to a space, because that‚Äôs what‚Äôs usually given to Kaldi, but IIRC, it handles tab. So change:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>paste /tmp/$i.wl /tmp/$i.wl.phn &gt; dict.$i
</code></pre></div></div>

<p>to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>paste /tmp/$i.wl /tmp/$i.wl.phn | tr '\t' ' ' &gt; dict.$i
</code></pre></div></div>

<p>or equivalent.</p>

<p>Caveat: I can‚Äôt say for sure if these are actually correct outputs, and I‚Äôm not even sure they‚Äôre actually used by default, aside from the <code class="language-plaintext highlighter-rouge">prepare_audio.sh</code> script dying if they‚Äôre missing.</p>

<p>There are some notes in <a href="/notes/kaggle/wav2vec-u/2021/05/26/wav2vec-u-cv-swedish-prep-ltr-phn-wrd.html">that notebook</a> where I tracked down and corrected for espeak‚Äôs language switching; feel free to ignore that if you‚Äôre not borderline OCD.</p>

<h2 id="step-02-preparing-tsvs">
<a class="anchor" href="#step-02-preparing-tsvs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 0.2: Preparing TSVs</h2>

<p>Another thing that‚Äôs glossed over a bit is the TSV files, which are more pseudo-TSV.</p>

<p>‚ÄúSimilar to wav2vec 2.0‚Äù is more-or-less true, in that you can figure it out if you look at <a href="https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/libri_labels.py">this script</a>; basically, the format is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/my/audio/
file1.wav	[number of frames]
file2.wav	[number of frames]
... (etc.)
</code></pre></div></div>

<p>I used <a href="/notes/kaggle/wav2vec-u/2021/05/25/wav2vec-u-cv-swedish-tsv.html">this notebook</a> to convert Common Voice TSV to these pseudo-TSVs, but the number of frames aren‚Äôt read by anything, so you can get away with a file list, as long as the first line is the path.</p>

<h2 id="step-1-vadsilence-trimming">
<a class="anchor" href="#step-1-vadsilence-trimming" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 1: VAD/Silence trimming</h2>

<p>There‚Äôs a passing mention of rVAD, like it‚Äôs a common piece of software that you should just be able to install. It‚Äôs not: it‚Äôs <a href="https://github.com/zhenghuatan/rVADfast">here</a>. Or, you know, save yourself the trouble and copy the relevant steps from the <a href="/notes/kaggle/wav2vec-u/2021/05/25/wav2vec-u-cv-swedish-vads.html">notebook</a></p>

<p>This went fairly smoothly; I wrestled with Kaggle a bit, but I think any problems here were of my own creation.</p>

<h2 id="step-2-prepare_audiosh">
<a class="anchor" href="#step-2-prepare_audiosh" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 2: <code class="language-plaintext highlighter-rouge">prepare_audio.sh</code>
</h2>

<p>My notebook for <a href="/notes/kaggle/wav2vec-u/2021/05/27/wav2vec-u-cv-swedish-audio.html">prepare_audio.sh</a> is quite short; basically, you need the <code class="language-plaintext highlighter-rouge">dict.*</code>, <code class="language-plaintext highlighter-rouge">*.wrd</code>, <code class="language-plaintext highlighter-rouge">*.ltr</code>, and <code class="language-plaintext highlighter-rouge">*.phn</code> files from Step 0.1, and:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install npy-append-array faiss-gpu
</code></pre></div></div>

<p>(also, possibly, <code class="language-plaintext highlighter-rouge">apt install zsh</code>)</p>

<p>Also: wow! This used GPU on Kaggle, and actually worked.</p>

<h2 id="step-3-prepare_textsh">
<a class="anchor" href="#step-3-prepare_textsh" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 3: <code class="language-plaintext highlighter-rouge">prepare_text.sh</code>
</h2>

<p>This needs Kaldi to compile FSTs. On Kaggle, I used <a href="/notes/kaggle/kaldi/wav2vec-u/2021/05/15/extract-prebuilt-kaldi-from-docker.html">this notebook</a> to extract a pre-built version from the official docker images. DNN parts won‚Äôt run, because they‚Äôre compiled for an earlier version of CUDA, but they‚Äôre not necessary for this step.</p>

<p>If you‚Äôre using Colab, <a href="https://stackoverflow.com/questions/49771968/is-it-possible-to-install-kaldi-on-google-colab">this question</a> on Stack Overflow is for you:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install kora -q
import kora.install.kaldi
</code></pre></div></div>

<p>The version of Kaldi there is also from the official docker image (that‚Äôs where I got the idea), but it also downloads and unpacks it for you. Which is nice.</p>

<p>My notebook for running <code class="language-plaintext highlighter-rouge">prepare_text.sh</code> has more notes than usual: <a href="/notes/kaggle/wav2vec-u/2021/05/26/wav2vec-u-cv-swedish-text-prep.html">check it out</a></p>

<h2 id="step-4-gan-training">
<a class="anchor" href="#step-4-gan-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 4: GAN training</h2>

<p>This doesn‚Äôt work on Kaggle, because GPU. It does, however, run on CPU‚Äîalbeit 8-9 times slower‚Äîso I‚Äôve been chaining together calls, starting with <a href="/notes/kaggle/wav2vec-u/2021/06/01/wav2vec-u-cv-swedish-gan-cpu1.html">this</a>, leading up to (at the time of writing) <a href="/notes/kaggle/wav2vec-u/2021/06/05/wav2vec-u-cv-swedish-gan-cpu8.html">this</a>.</p>

<p>The good news is, it runs fine on Colab: <a href="/notes/kaggle/colab/wav2vec-u/2021/05/30/wav2vec-u-cv-swedish-gan.html">notebook here</a>.</p>

<p>(By ‚Äúfine‚Äù, I mean ‚Äúwith <a href="https://github.com/pytorch/fairseq/pull/3569">this patch</a> added, running from <a href="https://github.com/jimregan/fairseq/tree/issue3581">this branch</a> where everything has been moved around.‚Äù Close enough.)</p>

<h2 id="fin">
<a class="anchor" href="#fin" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fin</h2>

<p>I‚Äôm still waiting for GAN training to finish, so I can‚Äôt comment on anything else.</p>


  </div><a class="u-url" href="/notes/wav2vec-u/2021/06/05/wav2vec-u-notes.html" hidden></a>
</article>