
@article{yi_efficiently_2021,
	title = {Efficiently {Fusing} {Pretrained} {Acoustic} and {Linguistic} {Encoders} for {Low}-resource {Speech} {Recognition}},
	volume = {28},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2101.06699},
	doi = {10.1109/LSP.2021.3071668},
	abstract = {End-to-end models have achieved impressive results on the task of automatic speech recognition (ASR). For low-resource ASR tasks, however, labeled data can hardly satisfy the demand of end-to-end models. Self-supervised acoustic pre-training has already shown its amazing ASR performance, while the transcription is still inadequate for language modeling in end-to-end models. In this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a pre-trained linguistic encoder (BERT) into an end-to-end ASR model. The fused model only needs to learn the transfer from speech to language during fine-tuning on limited labeled data. The length of the two modalities is matched by a monotonic attention mechanism without additional parameters. Besides, a fully connected layer is introduced for the hidden mapping between modalities. We further propose a scheduled fine-tuning strategy to preserve and utilize the text context modeling ability of the pre-trained linguistic encoder. Experiments show our effective utilizing of pre-trained modules. Our model achieves better recognition performance on CALLHOME corpus (15 hours) than other end-to-end models.},
	urldate = {2021-06-24},
	journal = {IEEE Signal Processing Letters},
	author = {Yi, Cheng and Zhou, Shiyu and Xu, Bo},
	year = {2021},
	note = {arXiv: 2101.06699},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {788--792},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\U2L8YEI5\\Yi et al. - 2021 - Efficiently Fusing Pretrained Acoustic and Linguis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4YQ7KW78\\2101.html:text/html},
}

@misc{garofolo_john_s_timit_nodate,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	urldate = {2021-06-24},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	doi = {10.35111/17GK-BN40},
	note = {Type: dataset},
}

@book{lopes_phoneme_2011,
	title = {Phoneme {Recognition} on the {TIMIT} {Database}},
	isbn = {978-953-307-996-7},
	abstract = {Open access peer-reviewed chapter},
	language = {en},
	urldate = {2021-06-24},
	publisher = {IntechOpen},
	author = {Lopes, Carla and Perdigao, Fernando},
	month = jun,
	year = {2011},
	doi = {10.5772/17600},
	annote = {Speech recognition based on phones is very attractive since it is inherently free from vocabulary limitations. Large Vocabulary ASR (LVASR) systems’ performance depends on the quality of the phone recognizer. That is why research teams continue developing phone recognizers, in order to enhance their performance as much as possible. Phone recognition is, in fact, a recurrent problem for the speech recognition community.},
	annote = {The database defines the units that can be trained and the success of the training algorithms is highly dependent on the quality and detail of the annotation of those units. Many databases are insufficiently annotated and only a few of them include labels at the phone level. So the reason why the TIMIT database (Garofolo et al., 1990) has become the database most widely used by the phone recognition research community is mainly because it is totally and manually annotated at the phone level.
Phone recognition in TIMIT has more than two decades of intense research behind it and its performance has naturally improved with time. There is a full array of systems, but with regard to evaluation they concentrate on three domains: phone segmentation, phone classification and phone recognition.},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\3UFJM87B\\Lopes and Perdigao - 2011 - Phoneme Recognition on the TIMIT Database.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MFPICHSN\\phoneme-recognition-on-the-timit-database.html:text/html},
}

@article{hsu_hubert_2021,
	title = {{HuBERT}: {Self}-{Supervised} {Speech} {Representation} {Learning} by {Masked} {Prediction} of {Hidden} {Units}},
	shorttitle = {{HuBERT}},
	abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
	urldate = {2021-06-27},
	journal = {arXiv:2106.07447 [cs, eess]},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07447},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LLMZ2XAV\\Hsu et al. - 2021 - HuBERT Self-Supervised Speech Representation Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2BDF9N6J\\2106.html:text/html},
}

@inproceedings{hejtmanek_using_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Syllables} as {Acoustic} {Units} for {Spontaneous} {Speech} {Recognition}},
	isbn = {978-3-642-15760-8},
	doi = {10.1007/978-3-642-15760-8_38},
	abstract = {In this work, we deal with advanced context-dependent automatic speech recognition (ASR) of Czech spontaneous talk using hidden Markov models (HMM). Context-dependent units (e.g. triphones, diphones) in ASR systems provide significant improvement against simple non-context-dependent units. However, for spontaneous speech recognition we had to overcome some very challenging tasks. For one, the number of syllables compared to the size of spontaneous speech corpus makes the usage of context-dependent units very difficult. The main part of this article shows problems and procedures to effectively build and use a syllable-based ASR with the LASER (ASR system developed at Department of Computer Science and Engineering, Faculty of Applied Sciences). The procedures are usable with virtual any modern ASR.},
	language = {en},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Hejtmánek, Jan},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2010},
	keywords = {Acoustic Model, Automatic Speech Recognition, Hide Markov Model, Speech Recognition, Spontaneous Speech},
	pages = {299--305},
}

@article{fujimura_syllable_1975,
	title = {Syllable as a {Unit} of {Speech} {Recognition}},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162631},
	abstract = {Basic problems involved in automatic recognition of continuous speech are discussed with reference to the recently developed template matching technique using dynamic programming. Irregularities in phonetic manifestations of phonemes are discussed and it is argued that the syllable, phonologically redefined, will serve as the effective minimal unit in the time domain. English syllable structures are discussed from this point of view using the notions of "syllable features" and "vowel affinity."},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Fujimura, O.},
	month = feb,
	year = {1975},
	keywords = {Acoustics, Automatic speech recognition, Dentistry, Detectors, Dynamic programming, Isolation technology, Liquids, Speech analysis, Speech recognition, Tail},
	pages = {82--87},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2GFTFRKM\\authors.html:text/html},
}

@article{marchand_automatic_2009,
	title = {Automatic {Syllabification} in {English}: {A} {Comparison} of {Different} {Algorithms}},
	volume = {52},
	issn = {0023-8309},
	shorttitle = {Automatic {Syllabification} in {English}},
	doi = {10.1177/0023830908099881},
	abstract = {Automatic syllabification of words is challenging, not least because the syllable is not easy to define precisely. Consequently, no accepted standard algorithm for automatic syllabification exists. There are two broad approaches: rule-based and data-driven. The rule-based method effectively embodies some theoretical position regarding the syllable, whereas the data-driven paradigm tries to infer “new” syllabifications from examples assumed to be correctly syllabified already. This article compares the performance of several variants of the two basic approaches. Given the problems of definition, it is difficult to determine a correct syllabification in all cases and so to establish the quality of the “gold standard” corpus used either to evaluate quantitatively the output of an automatic algorithm or as the example-set on which data-driven methods crucially depend. Thus, we look for consensus in the entries in multiple lexical databases of pre-syllabified words. In this work, we have used two independent lexicons, and extracted from them the same 18,016 words with their corresponding (possibly different) syllabifications. We have also created a third lexicon corresponding to the 13,594 words that share the same syllabifications in these two sources. As well as two rule-based approaches (Hammond's and Fisher's implementation of Kahn's), three data-driven techniques are evaluated: a look-up procedure, an exemplar-based generalization technique, and syllabification by analogy (SbA). The results on the three databases show consistent and robust patterns. First, the data-driven techniques outperform the rule-based systems in word and juncture accuracies by a very significant margin but require training data and are slower. Second, syllabification in the pronunciation domain is easier than in the spelling domain. Finally, best results are consistently obtained with SbA.},
	language = {en},
	number = {1},
	urldate = {2021-07-02},
	journal = {Language and Speech},
	author = {Marchand, Yannick and Adsett, Connie R. and Damper, Robert I.},
	month = mar,
	year = {2009},
	keywords = {analogy, computational linguistics, corpus linguistics, rule-based systems, speech technology, syllabification},
	pages = {1--27},
	file = {Accepted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\KIUS6D3Q\\Marchand et al. - 2009 - Automatic Syllabification in English A Comparison.pdf:application/pdf},
}

@article{goslin_comparing_2007,
	title = {Comparing {French} syllabification in preliterate children and adults},
	volume = {28},
	doi = {10.1017/S0142716407070178},
	abstract = {The influence of development and literacy upon syllabification in French was evaluated by comparing the segmental behavior of 4- to 5-year-old preliterate children and adults using a pause insertion task. Participants were required to repeat bisyllabic words such as “fourmi” (ant) by inserting a pause between its two syllabic components (/fur/-/mi/). In the first experiment we tested segmentation over a range of 49 double intervocalic consonant clusters. A similar general segmentation behavior was observed in both age groups, with a pattern that fit the predictions from a legality principle-based model of syllabification. Experiment 2 revealed that opacity between phonological and orthographic representations lead to increased ambisyllabic responses and a reduction in segmentation consistency in adults. In total, these findings indicate that syllabic forms are consistently represented from an early age, but that segmentation in metalinguistic tasks is susceptible to contamination from spelling and etymological knowledge.},
	language = {en},
	number = {2},
	urldate = {2021-07-03},
	journal = {Applied Psycholinguistics},
	author = {Goslin, Jeremy and Floccia, Caroline},
	month = apr,
	year = {2007},
	pages = {341--367},
	file = {Full Text:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\BRW3JKHF\\Goslin and Floccia - 2007 - Comparing French syllabification in preliterate ch.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\C4GN9FZT\\C3CD62BF8020CBF33E607D1ECBEC7591.html:text/html},
}

@article{fallows_experimental_1981,
	title = {Experimental evidence for {English} syllabification and syllable structure},
	volume = {17},
	issn = {1469-7742, 0022-2267},
	doi = {10.1017/S0022226700007027},
	abstract = {Most phonologists regard the syllable as a unit of language. In recent years, partly in reaction to Chomsky's and Halle's neglect of it in The sound pattern of English, much work has been done to incorporate the syllable into phonological theory (Anderson \& Jones, 1974; Bailey, 1978; Hoard, 1971; Hooper, 1972, 1974, 1976, 1978; Kahn, 1976; Pulgram, 1970; Rudes, 1977; Vennemann, 1972).Syllable theories have been based on evidence from phonetics, phonological processes, prosody, language change, child language acquisition, and language universals. The purpose of this experimental study is (a) to contribute empirical evidence about the nature of the syllable from native speakers' actual syllabifications of words and (b) to determine how this evidence reflects on the syllable theories already proposed.Syllable studies have focused on two major questions: (1) what is the structure of the syllable and (2) how are words divided into syllables (syllabified). These questions are obviously related, and the answers to each have implications for the other. This study was designed to elicit data directly addressing both questions.},
	language = {en},
	number = {2},
	urldate = {2021-07-03},
	journal = {Journal of Linguistics},
	author = {Fallows, Deborah},
	month = sep,
	year = {1981},
	pages = {309--317},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2RZVF5RQ\\E9E39A4D30B053C19700402CD7A28DA6.html:text/html},
}

@article{hadian_flat-start_2018,
	title = {Flat-{Start} {Single}-{Stage} {Discriminatively} {Trained} {HMM}-{Based} {Models} for {ASR}},
	volume = {26},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2018.2848701},
	abstract = {In recent years, end-to-end approaches to automatic speech recognition have received considerable attention as they are much faster in terms of preparing resources. However, conventional multistage approaches, which rely on a pipeline of training hidden Markov models (HMM)-GMM models and tree-building steps still give the state-of-the-art results on most databases. In this study, we investigate flat-start one-stage training of neural networks using lattice-free maximum mutual information (LF-MMI) objective function with HMM for large vocabulary continuous speech recognition. We thoroughly look into different issues that arise in such a setup and propose a standalone system, which achieves word error rates (WER) comparable with that of the state-of-the-art multi-stage systems while being much faster to prepare. We propose to use full biphones to enable flat-start context-dependent (CD) modeling and show through experiments that our CD modeling approach can be almost as effective as regular tree-based CD modeling. We show that our flat-start LF-MMI setup together with this tree-free CD modeling technique achieves 10 to 25 \% relative WER reduction compared to other end-to-end methods on well-known databases. The improvements are larger for smaller databases.},
	number = {11},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hadian, Hossein and Sameti, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = nov,
	year = {2018},
	keywords = {automatic speech recognition, Context modeling, Databases, Decoding, flat-start, hidden Markov models, Hidden Markov models, Lattice-free, Linear programming, maximum mutual information, Neural networks, single-stage, Training},
	pages = {1949--1961},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LJVW5XRV\\8387866.html:text/html},
}

@article{baker_dragon_1975,
	title = {The {DRAGON} system–{An} overview},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162650},
	abstract = {This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model–that of a probabilistic function of a Markov process–is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities.},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Baker, J.},
	month = feb,
	year = {1975},
	keywords = {Acoustic measurements, Automatic speech recognition, Data mining, Error analysis, Markov processes, Power system modeling, Power system reliability, Random variables, Speech recognition, Vocabulary},
	pages = {24--29},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7ZUX56CQ\\1162650.html:text/html},
}

@inproceedings{krantz_language-agnostic_2019,
	title = {Language-{Agnostic} {Syllabification} with {Neural} {Sequence} {Labeling}},
	doi = {10.1109/ICMLA.2019.00141},
	abstract = {The identification of syllables within phonetic sequences is known as syllabification. This task is thought to play an important role in natural language understanding, speech production, and the development of speech recognition systems. The concept of the syllable is cross-linguistic, though formal definitions are rarely agreed upon, even within a language. In response, data-driven syllabification methods have been developed to learn from syllabified examples. These methods often employ classical machine learning sequence labeling models. In recent years, recurrence-based neural networks have been shown to perform increasingly well for sequence labeling tasks such as named entity recognition (NER), part of speech (POS) tagging, and chunking. We present a novel approach to the syllabification problem which leverages modern neural network techniques. Our network is constructed with long short-term memory (LSTM) cells, a convolutional component, and a conditional random field (CRF) output layer. Existing syllabification approaches are rarely evaluated across multiple language families. To demonstrate cross-linguistic generalizability, we show that the network is competitive with state of the art systems in syllabifying English, Dutch, Italian, French, Manipuri, and Basque datasets.},
	booktitle = {2019 18th {IEEE} {International} {Conference} {On} {Machine} {Learning} {And} {Applications} ({ICMLA})},
	author = {Krantz, Jacob and Dulin, Maxwell and De Palma, Paul},
	month = dec,
	year = {2019},
	keywords = {Hidden Markov models, Labeling, Natural language processing, Neural networks, Phonetics, Speech recognition, Supervised learning, Tagging, Task analysis},
	pages = {804--810},
	annote = {Github: https://github.com/jacobkrantz/lstm-syllabify},
	annote = {https://arxiv.org/abs/1909.13362},
	file = {Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JBFIJQPI\\Krantz et al. - 2019 - Language-Agnostic Syllabification with Neural Sequ.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\VCHKMGCW\\8999232.html:text/html},
}

@inproceedings{soltau_neural_2017,
	title = {Neural {Speech} {Recognizer}: {Acoustic}-to-{Word} {LSTM} {Model} for {Large} {Vocabulary} {Speech} {Recognition}},
	shorttitle = {Neural {Speech} {Recognizer}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1566.html},
	doi = {10.21437/Interspeech.2017-1566},
	language = {en},
	urldate = {2021-07-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Soltau, Hagen and Liao, Hank and Sak, Haşim},
	month = aug,
	year = {2017},
	pages = {3707--3711},
	file = {Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\PPI3J34N\\Soltau et al. - 2017 - Neural Speech Recognizer Acoustic-to-Word LSTM Mo.pdf:application/pdf},
}

@book{saussure_course_1959,
	address = {New York},
	title = {Course in general linguistics.},
	language = {eng},
	publisher = {Philosophical Library},
	author = {Saussure, Ferdinand de},
	translator = {Baskin, Wade},
	year = {1959},
	keywords = {Comparative linguistics, Language and languages},
	annote = {IA: courseingenerall00saus},
	annote = {Open Library ID: OL23291521M},
}

@book{ladefoged_course_2011,
	edition = {6th},
	title = {A {Course} in {Phonetics}},
	language = {English},
	author = {Ladefoged, Peter and Johnson, Keith},
	year = {2011},
	annote = {OCLC: 613523782},
}

@incollection{wells_syllabification_2019,
	edition = {4},
	title = {Syllabification and {Allophony}},
	isbn = {978-0-429-49039-2},
	abstract = {In this chapter, the author discusses the system he arrived at through writing his Longman Pronunciation Dictionary, a system based on the allophones of sounds in different positions in the syllable. For A. C. Gimson the syllable is relevant mainly as a possible phonetic category or as a category to which phonotactic constraints may be referred. He is sceptical of the first, and for the second prefers the word. Yet English has a fair number of important allophonic rules which can best be described by specifying ‘syllable boundary’ as part of the conditioning environment. It is this fact which makes syllabification phonologically relevant. If allophonic rules are to be allowed to refer to syllable boundaries as part of their conditioning environments, one need a principled way of specifying the location of such boundaries. The only cases in English where immediately adjacent syllables have equal grade are those involving weak vowels.},
	booktitle = {Practical {English} {Phonetics} and {Phonology}},
	publisher = {Routledge},
	author = {Wells, J. C.},
	year = {2019},
}

@incollection{kingston_role_1990,
	address = {Cambridge},
	series = {Papers in {Laboratory} {Phonology}},
	title = {The role of the sonority cycle in core syllabification},
	volume = {1},
	isbn = {978-0-521-36238-2},
	abstract = {IntroductionOne of the major concerns of laboratory phonology is that of determining the nature of the transition between discrete phonological structure (conventionally, “phonology”) and its expression in terms of nondiscrete physical or psychoacoustic parameters (conventionally, “phonetics”). A considerable amount of research has been devoted to determining where this transition lies, and to what extent the rule types and representational systems needed to characterize the two levels may differ (see Keating 1985 for an overview). For instance, it is an empirical question to what extent the assignment of phonetic parameters to strings of segments (phonemes, tones, etc.) depends upon increasingly rich representational structures of the sort provided by autosegmental and metrical phonology, or upon real-time realization rules – or indeed upon some combination of the two, as many are coming to believe. We are only beginning to assess the types of evidence that can decide questions of this sort, and a complete and fully adequate theory of the phonetics/phonology interface remains to be worked out. A new synthesis of the methodology of phonology and phonetics, integrating results from the physical, biological and cognitive sciences, is required if we are to make significant progress in this area.The present study examines one question of traditional interest to both phoneticians and phonologists, with roots that go deep into modern linguistic theory. Many linguists have noted the existence of cross-linguistic preferences for certain types of syllable structures and syllable contacts. These have been the subject of descriptive studies and surveys such as that of Greenberg (1978), which have brought to light a number of generalizations suggesting that certain syllable types are less complex or less marked than others across languages.},
	urldate = {2021-07-05},
	booktitle = {Papers in {Laboratory} {Phonology}: {Volume} 1: {Between} the {Grammar} and {Physics} of {Speech}},
	publisher = {Cambridge University Press},
	author = {Clements, G. N.},
	editor = {Kingston, John and Beckman, Mary E.},
	year = {1990},
	note = {doi: 10.1017/CBO9780511627736.017},
	pages = {283--333},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\87YF7QTB\\B5908538E8FA8A13D4B4EBACAE3EF061.html:text/html},
}

@book{katamba_introduction_1989,
	address = {London; New York},
	title = {An introduction to phonology},
	isbn = {978-0-582-29150-8},
	abstract = {A practical introduction to generative phonology for the novice. The work reflects the trends towards scrutinizing the nature of phonological representations and the relationship between phonology and other grammatical components.},
	language = {English},
	publisher = {Longman},
	author = {Katamba, Francis},
	year = {1989},
	note = {OCLC: 18463593},
}

@misc{baayen_r_h_celex2_1995,
	title = {{CELEX2}},
	url = {https://catalog.ldc.upenn.edu/LDC96L14},
	urldate = {2021-07-06},
	publisher = {Linguistic Data Consortium},
	author = {Baayen, R H. and Piepenbrock, R and Gulikers, L},
	year = {1995},
	note = {doi: 10.35111/GS6S-GM48},
}

@inproceedings{tachbelie_morpheme-based_2010,
	address = {Universiti Sains, Penang, Malaysia},
	title = {Morpheme-based automatic speech recognition for a morphologically rich language - {Amharic}},
	author = {Tachbelie, Martha Yifiru and Abate, Solomon Teferra and Menzel, Wolfgang},
	month = may,
	year = {2010},
	pages = {68--73},
}

@inproceedings{guijarrubia_morpheme-based_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Morpheme-{Based} {Automatic} {Speech} {Recognition} of {Basque}},
	isbn = {978-3-642-02172-5},
	doi = {10.1007/978-3-642-02172-5_50},
	abstract = {In this work, we focus on studying a morpheme-based speech recognition system for Basque, an highly inflected language that is official language in the Basque Country (northern Spain). Two different techniques are presented to decompose the words into their morphological units. The morphological units are then integrated into an Automatic Speech Recognition System, and those systems are then compared to a word-based approach in terms of accuracy and processing speed. Results show that whereas the morpheme-based approaches perform similarly from an accuracy point of view, they can be significantly faster than the word-based system when applied to a weather-forecast task.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer},
	author = {Guijarrubia, Víctor G. and Torres, M. Inés and Justo, Raquel},
	editor = {Araujo, Helder and Mendonça, Ana Maria and Pinho, Armando J. and Torres, María Inés},
	year = {2009},
	keywords = {Morphological operations, Speech recognition},
	pages = {386--393},
}

@inproceedings{abera_tigrinya_2020,
	title = {Tigrinya {Automatic} {Speech} recognition with {Morpheme} based recognition units},
	doi = {10.18653/v1/2020.winlp-1.12},
	abstract = {Hafte Abera, Sebsibe Hailemariam. Proceedings of the The Fourth Widening Natural Language Processing Workshop. 2020.},
	language = {en-us},
	urldate = {2021-07-06},
	author = {Abera, Hafte and Hailemariam, Sebsibe},
	month = jul,
	year = {2020},
	pages = {46--50},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\XJKL4VPM\\2020.winlp-1.12.html:text/html},
}

@article{jafri_concatenative_2021,
	title = {Concatenative {Speech} {Recognition} using {Morphemes}},
	volume = {12},
	issn = {2156-5570},
	doi = {10.14569/IJACSA.2021.0120378},
	abstract = {This paper adopts a novel sub-lexical approach to construct viable continuous speech recognition systems with scalable vocabulary that use the components of words to form the elements of pronunciation dictionaries and recognition lattices. The proposed Concatenative ASR family utilizes combination rules between morphemes (prefixes, stems, and suffixes), along with their theoretical grammatical categories. The constrained structure reduces invalid words by using grammar rules governing agglutination of affixes with stems, while having a large vocabulary space and hence fewer out-of-vocabulary words. In pursuing this approach, the project develops automatic speech recognition (ASR) parameterized models, designs parameter values, constructs and implements ASR systems, and analyzes the characteristics of these systems. The project designs parameter values in the context of Arabic to yield a subset hierarchy of vocabularies of the ASR systems facilitating meaningful analysis. It investigates the characteristics of the ASR systems with respect to vocabulary, recognition lattice, dictionary, and word error rate (WER). In the experiments, the standard Word ASR model has the best characteristics for vocabulary of up to five thousand words and the Concatenative ASR family is most appropriate for vocabulary of up to half a million words. The paper shows that the approach used encompasses fundamentally different processes of word formation and thus is applicable to languages that exhibit concatenative word-formation processes.},
	language = {en},
	number = {3},
	urldate = {2021-07-06},
	journal = {International Journal of Advanced Computer Science and Applications (IJACSA)},
	author = {Jafri, Afshan},
	month = jul,
	year = {2021},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LHJPRJ5I\\Jafri - 2021 - Concatenative Speech Recognition using Morphemes.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\L3VJYZRS\\ViewPaper.html:text/html},
}

@article{huckvale_using_2002,
	title = {Using phonologically-constrained morphological analysis in continuous speech recognition},
	volume = {16},
	issn = {0885-2308},
	doi = {10.1006/csla.2001.0187},
	abstract = {This article describes investigations into the use of phonologically-constrained morphological analysis (PCMA) in language modelling for continuous speech recognition. PCMA provides a means for modelling text as a sequence of morphemes in a way that retains compatibility with the linear concatenative model of pronunciation used in conventional decoders. Experiments were performed in English exploiting the 100-million-word British National Corpus as source material. We show that PCMA leads to smaller but more generative pronunciation lexicons, and that it does not weaken the quality of the acoustic decoding measured in terms of recognition lattices. For trigram language models, perplexity figures are poorer for PCMA over words, as might be expected given the reduction in sentence span. However recognition results show small improvements in accuracy under some conditions, particularly when morph lattices are decoded with word-trigram models. We explore the capabilities for PCMA across vocabulary size, language model training size, and post-processing strategy. The best results show a 16\% relative reduction in word error rate.},
	language = {en},
	number = {2},
	urldate = {2021-07-06},
	journal = {Computer Speech \& Language},
	author = {Huckvale, Mark and Fang, Alex Chengyu},
	month = apr,
	year = {2002},
	pages = {165--181},
	annote = {https://www.sciencedirect.com/science/article/pii/S0885230801901871},
	file = {ScienceDirect Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8QZEDPTL\\S0885230801901871.html:text/html},
}

@article{hannun_deep_2014,
	title = {Deep {Speech}: {Scaling} up end-to-end speech recognition},
	shorttitle = {Deep {Speech}},
	url = {http://arxiv.org/abs/1412.5567},
	abstract = {We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a "phoneme." Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
	urldate = {2021-07-06},
	journal = {arXiv:1412.5567 [cs]},
	author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5567},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4PGJFLPC\\Hannun et al. - 2014 - Deep Speech Scaling up end-to-end speech recognit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4GM52WB8\\1412.html:text/html},
}

@inproceedings{graves_towards_2014,
	series = {{ICML}'14},
	title = {Towards {End}-to-{End} {Speech} {Recognition} with {Recurrent} {Neural} {Networks}},
	abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3\% on the Wall Street Journal corpus with no prior linguistic information, 21.9\% with only a lexicon of allowed words, and 8.2\% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7\%.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Graves, Alex and Jaitly, Navdeep},
	year = {2014},
	pages = {II--1764--II--1772},
}

@inproceedings{li_acoustic--word_2017,
	title = {Acoustic-to-word model without {OOV}},
	doi = {10.1109/ASRU.2017.8268924},
	abstract = {Recently, the acoustic-to-word model based on the Connectionist Temporal Classification (CTC) criterion was shown as a natural end-to-end model directly targeting words as output units. However, this type of word-based CTC model suffers from the out-of-vocabulary (OOV) issue as it can only model limited number of words in the output layer and maps all the remaining words into an OOV output node. Therefore, such word-based CTC model can only recognize the frequent words modeled by the network output nodes. It also cannot easily handle the hot-words which emerge after the model is trained. In this study, we improve the acoustic-to-word model with a hybrid CTC model which can predict both words and characters at the same time. With a shared-hidden-layer structure and modular design, the alignments of words generated from the word-based CTC and the character-based CTC are synchronized. Whenever the acoustic-to-word model emits an OOV token, we back off that OOV segment to the word output generated from the character-based CTC, hence solving the OOV or hot-words issue. Evaluated on a Microsoft Cortana voice assistant task, the proposed model can reduce the errors introduced by the OOV output token in the acoustic-to-word model by 30\%.},
	booktitle = {2017 {IEEE} {Automatic} {Speech} {Recognition} and {Understanding} {Workshop} ({ASRU})},
	author = {Li, Jinyu and Ye, Guoli and Zhao, Rui and Droppo, Jasha and Gong, Yifan},
	month = dec,
	year = {2017},
	keywords = {acoustic-to-word, Acoustics, CTC, Decoding, hybrid, LSTM, OOV, Predictive models, Speech, Task analysis, Training, Training data},
	pages = {111--117},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ZCMMBJ62\\8268924.html:text/html;Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7J25JARJ\\Li et al. - 2017 - Acoustic-to-word model without OOV.pdf:application/pdf},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {https://aclanthology.org/P16-1162},
	doi = {10.18653/v1/P16-1162},
	urldate = {2021-07-06},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = aug,
	year = {2016},
	pages = {1715--1725},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\WMRB3AZV\\Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@inproceedings{dong_cif_2020,
	title = {{CIF}: {Continuous} {Integrate}-{And}-{Fire} for {End}-{To}-{End} {Speech} {Recognition}},
	shorttitle = {{CIF}},
	doi = {10.1109/ICASSP40776.2020.9054250},
	abstract = {In this paper, we propose a novel soft and monotonic alignment mechanism used for sequence transduction. It is inspired by the integrate-and-fire model in spiking neural networks and employed in the encoder-decoder framework consists of continuous functions, thus being named as: Continuous Integrate-and-Fire (CIF). Applied to the ASR task, CIF not only shows a concise calculation, but also supports online recognition and acoustic boundary positioning, thus suitable for various ASR scenarios. Several support strategies are also proposed to alleviate the unique problems of CIF-based model. With the joint action of these methods, the CIF-based model shows competitive performance. Notably, it achieves a word error rate (WER) of 2.86\% on the test-clean of Librispeech and creates new state-of-the-art result on Mandarin telephone ASR benchmark.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Dong, Linhao and Xu, Bo},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {acoustic boundary positioning, Acoustics, Computational modeling, continuous integrate-and-fire, Decoding, end-to-end model, Hidden Markov models, online speech recognition, Prediction algorithms, Predictive models, soft and monotonic alignment, Training},
	pages = {6079--6083},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EHI4XADJ\\9054250.html:text/html;Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\RS5WL2ST\\Dong and Xu - 2020 - CIF Continuous Integrate-And-Fire for End-To-End .pdf:application/pdf},
}

@inproceedings{zeyer_ctc_2017,
	title = {{CTC} in the {Context} of {Generalized} {Full}-{Sum} {HMM} {Training}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1073.html},
	doi = {10.21437/Interspeech.2017-1073},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Zeyer, Albert and Beck, Eugen and Schlüter, Ralf and Ney, Hermann},
	month = aug,
	year = {2017},
	pages = {944--948},
	file = {Full Text:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4VLPK2EA\\Zeyer et al. - 2017 - CTC in the Context of Generalized Full-Sum HMM Tra.pdf:application/pdf},
}

@inproceedings{bisani_investigations_2002,
	title = {Investigations on {Joint}-{Multigram} {Models} for {Grapheme}-to-{Phoneme} {Conversion}},
	booktitle = {Seventh {International} {Conference} on {Spoken} {Language} {Processing}},
	author = {Bisani, Maximilian and Ney, Hermann},
	year = {2002},
	pages = {105--108},
}

@article{novak_phonetisaurus_2016,
	title = {Phonetisaurus: {Exploring} grapheme-to-phoneme conversion with joint n-gram models in the {WFST} framework},
	volume = {22},
	shorttitle = {Phonetisaurus},
	doi = {10.1017/S1351324915000315},
	abstract = {This paper provides an analysis of several practical issues related to the theory and implementation of Grapheme-to-Phoneme (G2P) conversion systems utilizing the Weighted Finite-State Transducer paradigm. The paper addresses issues related to system accuracy, training time and practical implementation. The focus is on joint n-gram models which have proven to provide an excellent trade-off between system accuracy and training complexity. The paper argues in favor of simple, productive approaches to G2P, which favor a balance between training time, accuracy and model complexity. The paper also introduces the first instance of using joint sequence RnnLMs directly for G2P conversion, and achieves new state-of-the-art performance via ensemble methods combining RnnLMs and n-gram based models. In addition to detailed descriptions of the approach, minor yet novel implementation solutions, and experimental results, the paper introduces Phonetisaurus, a fully-functional, flexible, open-source, BSD-licensed G2P conversion toolkit, which leverages the OpenFst library. The work is intended to be accessible to a broad range of readers.},
	language = {en},
	number = {6},
	urldate = {2021-07-06},
	journal = {Natural Language Engineering},
	author = {Novak, Josef Robert and Minematsu, Nobuaki and Hirose, Keikichi},
	month = nov,
	year = {2016},
	pages = {907--938},
	annote = {https://www.cambridge.org/core/journals/natural-language-engineering/article/abs/phonetisaurus-exploring-graphemetophoneme-conversion-with-joint-ngram-models-in-the-wfst-framework/F1160C3866842F0B707924EB30B8E753},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EKTASUU3\\F1160C3866842F0B707924EB30B8E753.html:text/html},
}

@inproceedings{povey_kaldi_2011,
	title = {The {Kaldi} {Speech} {Recognition} {Toolkit}},
	abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {Infoscience},
	publisher = {IEEE Signal Processing Society},
	author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
	year = {2011},
	annote = {https://infoscience.epfl.ch/record/192584},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ELZ7VJIP\\Povey et al. - 2011 - The Kaldi Speech Recognition Toolkit.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ML2XWJYD\\192584.html:text/html},
}

@misc{baevski_unsupervised_2021,
	title = {Unsupervised {Speech} {Recognition}},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
	year = {2021},
	note = {arXiv: 2105.11084},
}

@article{hannun_differentiable_2020,
	title = {Differentiable {Weighted} {Finite}-{State} {Transducers}},
	journal = {arXiv:2010.01003},
	author = {Hannun, Awni and Pratap, Vineel and Kahn, Jacob and Hsu, Wei-Ning},
	year = {2020},
}
