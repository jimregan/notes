
@article{cychosz_spectral_2019,
	title = {Spectral and temporal measures of coarticulation in child speech},
	volume = {146},
	issn = {0001-4966},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6934419/},
	doi = {10.1121/1.5139201},
	abstract = {Speech produced by children is characterized by a high fundamental frequency which complicates measurement of vocal tract resonances, and hence coarticulation. Here two whole-spectrum measures of coarticulation are validated, one temporal and one spectral, that are less sensitive to these challenges. Using these measures, consonant-vowel coarticulation is calculated in the speech of a large sample of 4-year-old children. The measurements replicate known lingual coarticulatory findings from the literature, demonstrating the utility of these acoustic measures of coarticulation in speakers of all ages.},
	number = {6},
	urldate = {2021-06-24},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cychosz, Margaret and Edwards, Jan R. and Munson, Benjamin and Johnson, Keith},
	month = dec,
	year = {2019},
	pmid = {31893765},
	pmcid = {PMC6934419},
	pages = {EL516--EL522},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7TWMCTA8\\Cychosz et al. - 2019 - Spectral and temporal measures of coarticulation i.pdf:application/pdf},
}

@article{yi_efficiently_2021,
	title = {Efficiently {Fusing} {Pretrained} {Acoustic} and {Linguistic} {Encoders} for {Low}-resource {Speech} {Recognition}},
	volume = {28},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2101.06699},
	doi = {10.1109/LSP.2021.3071668},
	abstract = {End-to-end models have achieved impressive results on the task of automatic speech recognition (ASR). For low-resource ASR tasks, however, labeled data can hardly satisfy the demand of end-to-end models. Self-supervised acoustic pre-training has already shown its amazing ASR performance, while the transcription is still inadequate for language modeling in end-to-end models. In this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a pre-trained linguistic encoder (BERT) into an end-to-end ASR model. The fused model only needs to learn the transfer from speech to language during fine-tuning on limited labeled data. The length of the two modalities is matched by a monotonic attention mechanism without additional parameters. Besides, a fully connected layer is introduced for the hidden mapping between modalities. We further propose a scheduled fine-tuning strategy to preserve and utilize the text context modeling ability of the pre-trained linguistic encoder. Experiments show our effective utilizing of pre-trained modules. Our model achieves better recognition performance on CALLHOME corpus (15 hours) than other end-to-end models.},
	urldate = {2021-06-24},
	journal = {IEEE Signal Processing Letters},
	author = {Yi, Cheng and Zhou, Shiyu and Xu, Bo},
	year = {2021},
	note = {arXiv: 2101.06699},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {788--792},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\U2L8YEI5\\Yi et al. - 2021 - Efficiently Fusing Pretrained Acoustic and Linguis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4YQ7KW78\\2101.html:text/html},
}

@misc{noauthor_190511235_nodate,
	title = {[1905.11235] {CIF}: {Continuous} {Integrate}-and-{Fire} for {End}-to-{End} {Speech} {Recognition}},
	url = {https://arxiv.org/abs/1905.11235},
	urldate = {2021-06-24},
	file = {[1905.11235] CIF\: Continuous Integrate-and-Fire for End-to-End Speech Recognition:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8VCYNEYJ\\1905.html:text/html},
}

@misc{garofolo_john_s_timit_nodate,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	urldate = {2021-06-24},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	doi = {10.35111/17GK-BN40},
	note = {Type: dataset},
}

@book{lopes_phoneme_2011,
	title = {Phoneme {Recognition} on the {TIMIT} {Database}},
	isbn = {978-953-307-996-7},
	url = {https://www.intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database},
	abstract = {Open access peer-reviewed chapter},
	language = {en},
	urldate = {2021-06-24},
	publisher = {IntechOpen},
	author = {Lopes, Carla and Perdigao, Fernando},
	month = jun,
	year = {2011},
	doi = {10.5772/17600},
	note = {Publication Title: Speech Technologies},
	annote = {Speech recognition based on phones is very attractive since it is inherently free from vocabulary limitations. Large Vocabulary ASR (LVASR) systems’ performance depends on the quality of the phone recognizer. That is why research teams continue developing phone recognizers, in order to enhance their performance as much as possible. Phone recognition is, in fact, a recurrent problem for the speech recognition community.},
	annote = {The database defines the units that can be trained and the success of the training algorithms is highly dependent on the quality and detail of the annotation of those units. Many databases are insufficiently annotated and only a few of them include labels at the phone level. So the reason why the TIMIT database (Garofolo et al., 1990) has become the database most widely used by the phone recognition research community is mainly because it is totally and manually annotated at the phone level.
Phone recognition in TIMIT has more than two decades of intense research behind it and its performance has naturally improved with time. There is a full array of systems, but with regard to evaluation they concentrate on three domains: phone segmentation, phone classification and phone recognition.},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\3UFJM87B\\Lopes and Perdigao - 2011 - Phoneme Recognition on the TIMIT Database.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MFPICHSN\\phoneme-recognition-on-the-timit-database.html:text/html},
}

@article{lee_speaker-independent_1989,
	title = {Speaker-independent phone recognition using hidden {Markov} models},
	volume = {37},
	issn = {0096-3518},
	doi = {10.1109/29.46546},
	abstract = {Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8\% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69\% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems.{\textless}{\textgreater}},
	number = {11},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Lee, K.-F. and Hon, H.-W.},
	month = nov,
	year = {1989},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Acoustics, Context modeling, Databases, Hidden Markov models, Humans, Knowledge engineering, Linear predictive coding, Maximum likelihood decoding, Natural languages, Speech recognition},
	pages = {1641--1648},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LV8ANX62\\citations.html:text/html},
}

@article{belinkov_analyzing_2017,
	title = {Analyzing {Hidden} {Representations} in {End}-to-{End} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/b069b3415151fa7217e870017374de7c-Abstract.html},
	language = {en},
	urldate = {2021-06-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Belinkov, Yonatan and Glass, James},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\B3UY4WM9\\Belinkov and Glass - 2017 - Analyzing Hidden Representations in End-to-End Aut.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\TIWCYTWD\\b069b3415151fa7217e870017374de7c-Abstract.html:text/html},
}

@article{webb_ensemble_2019,
	title = {To {Ensemble} or {Not} {Ensemble}: {When} does {End}-{To}-{End} {Training} {Fail}?},
	shorttitle = {To {Ensemble} or {Not} {Ensemble}},
	url = {http://arxiv.org/abs/1902.04422},
	doi = {10.13140/RG.2.2.28091.46880},
	abstract = {End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue-are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where over-parameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.},
	urldate = {2021-06-25},
	journal = {arXiv:1902.04422 [cs, stat]},
	author = {Webb, Andrew M. and Reynolds, Charles and Chen, Wenlin and Reeve, Henry and Iliescu, Dan-Andrei and Lujan, Mikel and Brown, Gavin},
	year = {2019},
	note = {arXiv: 1902.04422},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Code: https://github.com/grey-area/modular-loss-experiments. Preprint updated to reflect version accepted for publication at ECML},
	annote = {Comment: Code: https://github.com/grey-area/modular-loss-experiments. Preprint updated to reflect version accepted for publication at ECML},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HL7TZ2VG\\Webb et al. - 2019 - To Ensemble or Not Ensemble When does End-To-End .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\R26CGEA9\\1902.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\43W6WF6I\\Webb et al. - 2019 - To Ensemble or Not Ensemble When does End-To-End .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7GHF8B4Z\\1902.html:text/html},
}

@inproceedings{moore_usemisuse_2019,
	title = {On the {Use}/{Misuse} of the {Term} ‘{Phoneme}’},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/2711.html},
	doi = {10.21437/Interspeech.2019-2711},
	language = {en},
	urldate = {2021-06-27},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Moore, Roger K. and Skidmore, Lucy},
	month = sep,
	year = {2019},
	pages = {2340--2344},
	file = {Moore and Skidmore - 2019 - On the UseMisuse of the Term ‘Phoneme’.pdf:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\K2BEGFCJ\\Moore and Skidmore - 2019 - On the UseMisuse of the Term ‘Phoneme’.pdf:application/pdf},
}

@misc{noauthor_eigenvectors_nodate,
	title = {Eigenvectors and eigenvalues {\textbar} {Chapter} 14, {Essence} of linear algebra - {YouTube}},
	url = {https://www.youtube.com/watch?v=PFDu9oVAE-g},
	urldate = {2021-06-27},
}

@article{hsu_hubert_2021,
	title = {{HuBERT}: {Self}-{Supervised} {Speech} {Representation} {Learning} by {Masked} {Prediction} of {Hidden} {Units}},
	shorttitle = {{HuBERT}},
	url = {http://arxiv.org/abs/2106.07447},
	abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
	urldate = {2021-06-27},
	journal = {arXiv:2106.07447 [cs, eess]},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07447},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LLMZ2XAV\\Hsu et al. - 2021 - HuBERT Self-Supervised Speech Representation Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2BDF9N6J\\2106.html:text/html},
}

@article{zhou_syllable-based_2018,
	title = {Syllable-{Based} {Sequence}-to-{Sequence} {Speech} {Recognition} with the {Transformer} in {Mandarin} {Chinese}},
	url = {http://arxiv.org/abs/1804.10752},
	abstract = {Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attention-based model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the-art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of {\textbackslash}emph\{\$28.77{\textbackslash}\%\$\}, which is competitive to the state-of-the-art CER of \$28.0{\textbackslash}\%\$ by the joint CTC-attention based encoder-decoder network.},
	urldate = {2021-06-27},
	journal = {arXiv:1804.10752 [cs, eess]},
	author = {Zhou, Shiyu and Dong, Linhao and Xu, Shuang and Xu, Bo},
	month = jun,
	year = {2018},
	note = {arXiv: 1804.10752},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: accepted by INTERSPEECH2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4RMQJFCS\\Zhou et al. - 2018 - Syllable-Based Sequence-to-Sequence Speech Recogni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EPIH7EXJ\\1804.html:text/html},
}

@inproceedings{hejtmanek_using_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Syllables} as {Acoustic} {Units} for {Spontaneous} {Speech} {Recognition}},
	isbn = {978-3-642-15760-8},
	doi = {10.1007/978-3-642-15760-8_38},
	abstract = {In this work, we deal with advanced context-dependent automatic speech recognition (ASR) of Czech spontaneous talk using hidden Markov models (HMM). Context-dependent units (e.g. triphones, diphones) in ASR systems provide significant improvement against simple non-context-dependent units. However, for spontaneous speech recognition we had to overcome some very challenging tasks. For one, the number of syllables compared to the size of spontaneous speech corpus makes the usage of context-dependent units very difficult. The main part of this article shows problems and procedures to effectively build and use a syllable-based ASR with the LASER (ASR system developed at Department of Computer Science and Engineering, Faculty of Applied Sciences). The procedures are usable with virtual any modern ASR.},
	language = {en},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Hejtmánek, Jan},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2010},
	keywords = {Acoustic Model, Automatic Speech Recognition, Hide Markov Model, Speech Recognition, Spontaneous Speech},
	pages = {299--305},
}

@article{fujimura_syllable_1975,
	title = {Syllable as a unit of speech recognition},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162631},
	abstract = {Basic problems involved in automatic recognition of continuous speech are discussed with reference to the recently developed template matching technique using dynamic programming. Irregularities in phonetic manifestations of phonemes are discussed and it is argued that the syllable, phonologically redefined, will serve as the effective minimal unit in the time domain. English syllable structures are discussed from this point of view using the notions of "syllable features" and "vowel affinity."},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Fujimura, O.},
	month = feb,
	year = {1975},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Acoustics, Speech recognition, Automatic speech recognition, Dentistry, Detectors, Dynamic programming, Isolation technology, Liquids, Speech analysis, Tail},
	pages = {82--87},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2GFTFRKM\\authors.html:text/html},
}

@article{pratap_mls_2020,
	title = {{MLS}: {A} {Large}-{Scale} {Multilingual} {Dataset} for {Speech} {Research}},
	shorttitle = {{MLS}},
	url = {http://arxiv.org/abs/2012.03411},
	doi = {10.21437/Interspeech.2020-2826},
	abstract = {This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.},
	urldate = {2021-06-29},
	journal = {Interspeech 2020},
	author = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
	month = oct,
	year = {2020},
	note = {arXiv: 2012.03411},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {2757--2761},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\9JMDBDWR\\Pratap et al. - 2020 - MLS A Large-Scale Multilingual Dataset for Speech.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\QR4LRLNP\\2012.html:text/html},
}

@inproceedings{punjabi_joint_2021,
	title = {Joint {ASR} and {Language} {Identification} {Using} {RNN}-{T}: {An} {Efficient} {Approach} to {Dynamic} {Language} {Switching}},
	shorttitle = {Joint {ASR} and {Language} {Identification} {Using} {RNN}-{T}},
	doi = {10.1109/ICASSP39728.2021.9413734},
	abstract = {Conventional dynamic language switching enables seamless multilingual interactions by running several monolingual ASR systems in parallel and triggering the appropriate downstream components using a standalone language identification (LID) service. Since this solution is neither scalable nor cost- and memory-efficient, especially for on-device applications, we propose end-to-end, streaming, joint ASR-LID architectures based on the recurrent neural network transducer framework. Two key formulations are explored: (1) joint training using a unified output space for ASR and LID vocabularies, and (2) joint training viewed as multi-task optimization. We also evaluate the benefit of using auxiliary language information obtained on-the-fly from an acoustic LID classifier. Experiments with the English-Hindi language pair show that: (a) multi-task architectures perform better overall, and (b) the best joint architecture surpasses monolingual ASR (6.4–9.2\% word error rate reduction) and acoustic LID (53.9–56.1\% error rate reduction) baselines while reducing the overall memory footprint by up to 46\%.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Punjabi, Surabhi and Arsikere, Harish and Raeesy, Zeynab and Chandak, Chander and Bhave, Nikhil and Bansal, Ankish and Müller, Markus and Murillo, Sergio and Rastrow, Ariya and Stolcke, Andreas and Droppo, Jasha and Garimella, Sri and Maas, Roland and Hans, Mat and Mouchtaris, Athanasios and Kunzmann, Siegfried},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {code switching, Computer architecture, Error analysis, joint modeling, language identification, multilingual, recurrent neural network transducer, Recurrent neural networks, Switches, Training, Transducers, Vocabulary},
	pages = {7218--7222},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JRLSUL8J\\9413734.html:text/html},
}

@inproceedings{doshi_extending_2021,
	title = {Extending {Parrotron}: {An} {End}-to-{End}, {Speech} {Conversion} and {Speech} {Recognition} {Model} for {Atypical} {Speech}},
	shorttitle = {Extending {Parrotron}},
	doi = {10.1109/ICASSP39728.2021.9414644},
	abstract = {We present an extended Parrotron model: a single, end-to-end network that enables voice conversion and recognition simultaneously. Input spectrograms are transformed to output spectrograms in the voice of a predetermined target speaker while also generating hypotheses in a target vocabulary. We study the performance of this novel architecture, which jointly predicts speech and text, on atypical (e.g. dysarthric) speech. We show that with as little as an hour of atypical speech, speaker adaptation can yield a 77\% relative reduction in Word Error Rate (WER), measured by ASR performance on the converted speech. We also show that data augmentation using a customized synthesizer built on atypical speech can provide an additional 10\% relative improvement over the best speaker-adapted model. Finally, we show how these methods generalize across 8 types of atypical speech for a range of speech impairment severities.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Doshi, Rohan and Chen, Youzheng and Jiang, Liyang and Zhang, Xia and Biadsy, Fadi and Ramabhadran, Bhuvana and Chu, Fang and Rosenberg, Andrew and Moreno, Pedro J.},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech recognition, Error analysis, Vocabulary, Adaptation models, Conferences, Measurement uncertainty, sequence-to-sequence model, speech impairments, speech normalization, speech recognition, Synthesizers, voice conversion},
	pages = {6988--6992},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JLNLC79V\\9414644.html:text/html},
}

@inproceedings{li_towards_2019,
	title = {Towards {Code}-switching {ASR} for {End}-to-end {CTC} {Models}},
	doi = {10.1109/ICASSP.2019.8683223},
	abstract = {Although great progress has been made on end-to-end (E2E) models for monolingual and multilingual automatic speech recognition (ASR), there is no successful study for E2E models on the challenging intra-sentential code-switching (CS) ASR task to our best knowledge. In this paper, we propose an approach for CS ASR using E2E connectionist temporal classification (CTC) models. We use a frame-level language identification model to linearly adjust the posteriors of an E2E CTC model. We evaluate the proposed method on Microsoft live Chinese Cortana data with 7000 hours Chinese and English monolingual data and 300 hours CS data as the training data. Trained with only monolingual data without observing any CS data, the proposed method can obtain up to 6.3\% relative word error rate (WER) reduction. In the scenario of training with both monolingual and CS data, the proposed method can get up to 4.2\% relative WER improvement. This approach can also maintain comparable performance on a Chinese test set compared with baseline models.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Ke and Li, Jinyu and Ye, Guoli and Zhao, Rui and Gong, Yifan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Speech recognition, language identification, Recurrent neural networks, ASR, code-switching, CTC, Data models, Decoding, end-to-end, Predictive models},
	pages = {6076--6080},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ATTLXWWT\\8683223.html:text/html},
}

@article{naowarat_reducing_2021,
	title = {Reducing {Spelling} {Inconsistencies} in {Code}-{Switching} {ASR} using {Contextualized} {CTC} {Loss}},
	url = {http://arxiv.org/abs/2005.07920},
	abstract = {Code-Switching (CS) remains a challenge for Automatic Speech Recognition (ASR), especially character-based models. With the combined choice of characters from multiple languages, the outcome from character-based models suffers from phoneme duplication, resulting in language-inconsistent spellings. We propose Contextualized Connectionist Temporal Classification (CCTC) loss to encourage spelling consistencies of a character-based non-autoregressive ASR which allows for faster inference. The CCTC loss conditions the main prediction on the predicted contexts to ensure language consistency in the spellings. In contrast to existing CTC-based approaches, CCTC loss does not require frame-level alignments, since the context ground truth is obtained from the model's estimated path. Compared to the same model trained with regular CTC loss, our method consistently improved the ASR performance on both CS and monolingual corpora.},
	urldate = {2021-06-29},
	journal = {arXiv:2005.07920 [cs, eess]},
	author = {Naowarat, Burin and Kongthaworn, Thananchai and Karunratanakul, Korrawe and Wu, Sheng Hui and Chuangsuwanich, Ekapol},
	month = jun,
	year = {2021},
	note = {arXiv: 2005.07920},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: ICASSP 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\WCLBKTE4\\Naowarat et al. - 2021 - Reducing Spelling Inconsistencies in Code-Switchin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\D3PBEJCQ\\2005.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How many syllables exist in the {American} {English} language? - {Quora}},
	url = {https://www.quora.com/How-many-syllables-exist-in-the-American-English-language},
	urldate = {2021-06-29},
	annote = {I'm going to give you a very specific number, 15,831. That is how many syllables there are in the English language.A syllable is a vowel sound with or without consonants.Girl can be pronounced as one or two syllables grl or grr-ell. Same with boy, boy-yee. People from Australia get two syllables out of the word no, somehow.Let's count them all, shall we?No, that's been done already, here's a link: Page on nyu.eduIt provides a very thorough look at what a syllable is and how many there are (and maybe how many there should be).Hope this helps.},
	file = {How many syllables exist in the American English language? - Quora:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8HCXBKRZ\\How-many-syllables-exist-in-the-American-English-language.html:text/html},
}

@misc{noauthor_wayback_2013,
	title = {Wayback {Machine}},
	url = {http://web.archive.org/web/20130914173723/http://semarch.linguistics.fas.nyu.edu/barker/Syllables/index.txt},
	urldate = {2021-06-29},
	month = sep,
	year = {2013},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\C5WUSTPG\\index.html:text/html},
}

@article{gao_visualvoice_2021,
	title = {{VisualVoice}: {Audio}-{Visual} {Speech} {Separation} with {Cross}-{Modal} {Consistency}},
	shorttitle = {{VisualVoice}},
	url = {http://arxiv.org/abs/2101.03149},
	abstract = {We introduce a new approach for audio-visual speech separation. Given a video, the goal is to extract the speech associated with a face in spite of simultaneous background sounds and/or other human speakers. Whereas existing methods focus on learning the alignment between the speaker's lip movements and the sounds they generate, we propose to leverage the speaker's face appearance as an additional prior to isolate the corresponding vocal qualities they are likely to produce. Our approach jointly learns audio-visual speech separation and cross-modal speaker embeddings from unlabeled video. It yields state-of-the-art results on five benchmark datasets for audio-visual speech separation and enhancement, and generalizes well to challenging real-world videos of diverse scenarios. Our video results and code: http://vision.cs.utexas.edu/projects/VisualVoice/.},
	urldate = {2021-06-29},
	journal = {arXiv:2101.03149 [cs, eess]},
	author = {Gao, Ruohan and Grauman, Kristen},
	month = apr,
	year = {2021},
	note = {arXiv: 2101.03149},
	keywords = {Computer Science - Sound, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: In CVPR 2021. Project page: http://vision.cs.utexas.edu/projects/VisualVoice/},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\XE6RDDT6\\Gao and Grauman - 2021 - VisualVoice Audio-Visual Speech Separation with C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\CVI3R4E5\\2101.html:text/html},
}

@inproceedings{rieger_idiosyncratic_2001,
	title = {Idiosyncratic fillers in the speech of bilinguals},
	volume = {DISS'01},
	url = {https://www.isca-speech.org/archive_open/diss_01/dis1_081.html},
	urldate = {2021-06-29},
	author = {Rieger, Caroline L.},
	year = {2001},
	pages = {81--84},
	file = {DISS'01 Abstract\: Rieger, Caroline L.:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7WUDW5FF\\dis1_081.html:text/html},
}

@article{daghaghi_accelerating_2021,
	title = {Accelerating {SLIDE} {Deep} {Learning} on {Modern} {CPUs}: {Vectorization}, {Quantizations}, {Memory} {Optimizations}, and {More}},
	shorttitle = {Accelerating {SLIDE} {Deep} {Learning} on {Modern} {CPUs}},
	url = {http://arxiv.org/abs/2103.10891},
	abstract = {Deep learning implementations on CPUs (Central Processing Units) are gaining more traction. Enhanced AI capabilities on commodity x86 architectures are commercially appealing due to the reuse of existing hardware and virtualization ease. A notable work in this direction is the SLIDE system. SLIDE is a C++ implementation of a sparse hash table based back-propagation, which was shown to be significantly faster than GPUs in training hundreds of million parameter neural models. In this paper, we argue that SLIDE's current implementation is sub-optimal and does not exploit several opportunities available in modern CPUs. In particular, we show how SLIDE's computations allow for a unique possibility of vectorization via AVX (Advanced Vector Extensions)-512. Furthermore, we highlight opportunities for different kinds of memory optimization and quantizations. Combining all of them, we obtain up to 7x speedup in the computations on the same hardware. Our experiments are focused on large (hundreds of millions of parameters) recommendation and NLP models. Our work highlights several novel perspectives and opportunities for implementing randomized algorithms for deep learning on modern CPUs. We provide the code and benchmark scripts at https://github.com/RUSH-LAB/SLIDE},
	urldate = {2021-06-29},
	journal = {arXiv:2103.10891 [cs]},
	author = {Daghaghi, Shabnam and Meisburger, Nicholas and Zhao, Mengnan and Wu, Yong and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.10891},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\BSYXYDSD\\Daghaghi et al. - 2021 - Accelerating SLIDE Deep Learning on Modern CPUs V.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HDNRPP74\\2103.html:text/html},
}

@article{musgrave_metric_2020,
	title = {A {Metric} {Learning} {Reality} {Check}},
	url = {http://arxiv.org/abs/2003.08505},
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best.},
	urldate = {2021-06-29},
	journal = {arXiv:2003.08505 [cs]},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	month = sep,
	year = {2020},
	note = {arXiv: 2003.08505},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Visit https://www.github.com/KevinMusgrave/powerful-benchmarker for supplementary material, including the source code, configuration files, log files, and interactive bayesian optimization plots},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4DDYLHPA\\Musgrave et al. - 2020 - A Metric Learning Reality Check.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MUCR9S4H\\2003.html:text/html},
}

@article{jang_univnet_2021,
	title = {{UnivNet}: {A} {Neural} {Vocoder} with {Multi}-{Resolution} {Spectrogram} {Discriminators} for {High}-{Fidelity} {Waveform} {Generation}},
	shorttitle = {{UnivNet}},
	url = {http://arxiv.org/abs/2106.07889},
	abstract = {Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.},
	urldate = {2021-06-29},
	journal = {arXiv:2106.07889 [cs, eess]},
	author = {Jang, Won and Lim, Dan and Yoon, Jaesam and Kim, Bongwan and Kim, Juntae},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07889},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted to INTERSPEECH 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HW6DH2C7\\Jang et al. - 2021 - UnivNet A Neural Vocoder with Multi-Resolution Sp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\D5XSACFF\\2106.html:text/html},
}
