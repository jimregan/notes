
@article{cychosz_spectral_2019,
	title = {Spectral and temporal measures of coarticulation in child speech},
	volume = {146},
	issn = {0001-4966},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6934419/},
	doi = {10.1121/1.5139201},
	abstract = {Speech produced by children is characterized by a high fundamental frequency which complicates measurement of vocal tract resonances, and hence coarticulation. Here two whole-spectrum measures of coarticulation are validated, one temporal and one spectral, that are less sensitive to these challenges. Using these measures, consonant-vowel coarticulation is calculated in the speech of a large sample of 4-year-old children. The measurements replicate known lingual coarticulatory findings from the literature, demonstrating the utility of these acoustic measures of coarticulation in speakers of all ages.},
	number = {6},
	urldate = {2021-06-24},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cychosz, Margaret and Edwards, Jan R. and Munson, Benjamin and Johnson, Keith},
	month = dec,
	year = {2019},
	pmid = {31893765},
	pmcid = {PMC6934419},
	pages = {EL516--EL522},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7TWMCTA8\\Cychosz et al. - 2019 - Spectral and temporal measures of coarticulation i.pdf:application/pdf},
}

@article{yi_efficiently_2021,
	title = {Efficiently {Fusing} {Pretrained} {Acoustic} and {Linguistic} {Encoders} for {Low}-resource {Speech} {Recognition}},
	volume = {28},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2101.06699},
	doi = {10.1109/LSP.2021.3071668},
	abstract = {End-to-end models have achieved impressive results on the task of automatic speech recognition (ASR). For low-resource ASR tasks, however, labeled data can hardly satisfy the demand of end-to-end models. Self-supervised acoustic pre-training has already shown its amazing ASR performance, while the transcription is still inadequate for language modeling in end-to-end models. In this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a pre-trained linguistic encoder (BERT) into an end-to-end ASR model. The fused model only needs to learn the transfer from speech to language during fine-tuning on limited labeled data. The length of the two modalities is matched by a monotonic attention mechanism without additional parameters. Besides, a fully connected layer is introduced for the hidden mapping between modalities. We further propose a scheduled fine-tuning strategy to preserve and utilize the text context modeling ability of the pre-trained linguistic encoder. Experiments show our effective utilizing of pre-trained modules. Our model achieves better recognition performance on CALLHOME corpus (15 hours) than other end-to-end models.},
	urldate = {2021-06-24},
	journal = {IEEE Signal Processing Letters},
	author = {Yi, Cheng and Zhou, Shiyu and Xu, Bo},
	year = {2021},
	note = {arXiv: 2101.06699},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {788--792},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\U2L8YEI5\\Yi et al. - 2021 - Efficiently Fusing Pretrained Acoustic and Linguis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4YQ7KW78\\2101.html:text/html},
}

@misc{noauthor_190511235_nodate,
	title = {[1905.11235] {CIF}: {Continuous} {Integrate}-and-{Fire} for {End}-to-{End} {Speech} {Recognition}},
	url = {https://arxiv.org/abs/1905.11235},
	urldate = {2021-06-24},
	file = {[1905.11235] CIF\: Continuous Integrate-and-Fire for End-to-End Speech Recognition:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8VCYNEYJ\\1905.html:text/html},
}

@misc{garofolo_john_s_timit_nodate,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	url = {https://catalog.ldc.upenn.edu/LDC93S1},
	urldate = {2021-06-24},
	publisher = {Linguistic Data Consortium},
	author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
	doi = {10.35111/17GK-BN40},
	note = {Type: dataset},
}

@book{lopes_phoneme_2011,
	title = {Phoneme {Recognition} on the {TIMIT} {Database}},
	isbn = {978-953-307-996-7},
	url = {https://www.intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database},
	abstract = {Open access peer-reviewed chapter},
	language = {en},
	urldate = {2021-06-24},
	publisher = {IntechOpen},
	author = {Lopes, Carla and Perdigao, Fernando},
	month = jun,
	year = {2011},
	doi = {10.5772/17600},
	note = {Publication Title: Speech Technologies},
	annote = {Speech recognition based on phones is very attractive since it is inherently free from vocabulary limitations. Large Vocabulary ASR (LVASR) systems’ performance depends on the quality of the phone recognizer. That is why research teams continue developing phone recognizers, in order to enhance their performance as much as possible. Phone recognition is, in fact, a recurrent problem for the speech recognition community.},
	annote = {The database defines the units that can be trained and the success of the training algorithms is highly dependent on the quality and detail of the annotation of those units. Many databases are insufficiently annotated and only a few of them include labels at the phone level. So the reason why the TIMIT database (Garofolo et al., 1990) has become the database most widely used by the phone recognition research community is mainly because it is totally and manually annotated at the phone level.
Phone recognition in TIMIT has more than two decades of intense research behind it and its performance has naturally improved with time. There is a full array of systems, but with regard to evaluation they concentrate on three domains: phone segmentation, phone classification and phone recognition.},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\3UFJM87B\\Lopes and Perdigao - 2011 - Phoneme Recognition on the TIMIT Database.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MFPICHSN\\phoneme-recognition-on-the-timit-database.html:text/html},
}

@article{lee_speaker-independent_1989,
	title = {Speaker-independent phone recognition using hidden {Markov} models},
	volume = {37},
	issn = {0096-3518},
	doi = {10.1109/29.46546},
	abstract = {Hidden Markov modeling is extended to speaker-independent phone recognition. Using multiple codebooks of various linear-predictive-coding (LPC) parameters and discrete hidden Markov models (HMMs) the authors obtain a speaker-independent phone recognition accuracy of 58.8-73.8\% on the TIMIT database, depending on the type of acoustic and language models used. In comparison, the performance of expert spectrogram readers is only 69\% without use of higher level knowledge. The authors introduce the co-occurrence smoothing algorithm, which enables accurate recognition even with very limited training data. Since the results were evaluated on a standard database, they can be used as benchmarks to evaluate future systems.{\textless}{\textgreater}},
	number = {11},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Lee, K.-F. and Hon, H.-W.},
	month = nov,
	year = {1989},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Acoustics, Context modeling, Databases, Hidden Markov models, Humans, Knowledge engineering, Linear predictive coding, Maximum likelihood decoding, Natural languages, Speech recognition},
	pages = {1641--1648},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LV8ANX62\\citations.html:text/html},
}

@article{belinkov_analyzing_2017,
	title = {Analyzing {Hidden} {Representations} in {End}-to-{End} {Automatic} {Speech} {Recognition} {Systems}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/b069b3415151fa7217e870017374de7c-Abstract.html},
	language = {en},
	urldate = {2021-06-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Belinkov, Yonatan and Glass, James},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\B3UY4WM9\\Belinkov and Glass - 2017 - Analyzing Hidden Representations in End-to-End Aut.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\TIWCYTWD\\b069b3415151fa7217e870017374de7c-Abstract.html:text/html},
}

@article{webb_ensemble_2019,
	title = {To {Ensemble} or {Not} {Ensemble}: {When} does {End}-{To}-{End} {Training} {Fail}?},
	shorttitle = {To {Ensemble} or {Not} {Ensemble}},
	url = {http://arxiv.org/abs/1902.04422},
	doi = {10.13140/RG.2.2.28091.46880},
	abstract = {End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue-are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where over-parameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.},
	urldate = {2021-06-25},
	journal = {arXiv:1902.04422 [cs, stat]},
	author = {Webb, Andrew M. and Reynolds, Charles and Chen, Wenlin and Reeve, Henry and Iliescu, Dan-Andrei and Lujan, Mikel and Brown, Gavin},
	year = {2019},
	note = {arXiv: 1902.04422},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Code: https://github.com/grey-area/modular-loss-experiments. Preprint updated to reflect version accepted for publication at ECML},
	annote = {Comment: Code: https://github.com/grey-area/modular-loss-experiments. Preprint updated to reflect version accepted for publication at ECML},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HL7TZ2VG\\Webb et al. - 2019 - To Ensemble or Not Ensemble When does End-To-End .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\R26CGEA9\\1902.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\43W6WF6I\\Webb et al. - 2019 - To Ensemble or Not Ensemble When does End-To-End .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7GHF8B4Z\\1902.html:text/html},
}

@inproceedings{moore_usemisuse_2019,
	title = {On the {Use}/{Misuse} of the {Term} ‘{Phoneme}’},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/2711.html},
	doi = {10.21437/Interspeech.2019-2711},
	language = {en},
	urldate = {2021-06-27},
	booktitle = {Interspeech 2019},
	publisher = {ISCA},
	author = {Moore, Roger K. and Skidmore, Lucy},
	month = sep,
	year = {2019},
	pages = {2340--2344},
	file = {Moore and Skidmore - 2019 - On the UseMisuse of the Term ‘Phoneme’.pdf:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\K2BEGFCJ\\Moore and Skidmore - 2019 - On the UseMisuse of the Term ‘Phoneme’.pdf:application/pdf},
}

@misc{noauthor_eigenvectors_nodate,
	title = {Eigenvectors and eigenvalues {\textbar} {Chapter} 14, {Essence} of linear algebra - {YouTube}},
	url = {https://www.youtube.com/watch?v=PFDu9oVAE-g},
	urldate = {2021-06-27},
}

@article{hsu_hubert_2021,
	title = {{HuBERT}: {Self}-{Supervised} {Speech} {Representation} {Learning} by {Masked} {Prediction} of {Hidden} {Units}},
	shorttitle = {{HuBERT}},
	url = {http://arxiv.org/abs/2106.07447},
	abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
	urldate = {2021-06-27},
	journal = {arXiv:2106.07447 [cs, eess]},
	author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07447},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LLMZ2XAV\\Hsu et al. - 2021 - HuBERT Self-Supervised Speech Representation Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2BDF9N6J\\2106.html:text/html},
}

@article{zhou_syllable-based_2018,
	title = {Syllable-{Based} {Sequence}-to-{Sequence} {Speech} {Recognition} with the {Transformer} in {Mandarin} {Chinese}},
	url = {http://arxiv.org/abs/1804.10752},
	abstract = {Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attention-based model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the-art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of {\textbackslash}emph\{\$28.77{\textbackslash}\%\$\}, which is competitive to the state-of-the-art CER of \$28.0{\textbackslash}\%\$ by the joint CTC-attention based encoder-decoder network.},
	urldate = {2021-06-27},
	journal = {arXiv:1804.10752 [cs, eess]},
	author = {Zhou, Shiyu and Dong, Linhao and Xu, Shuang and Xu, Bo},
	month = jun,
	year = {2018},
	note = {arXiv: 1804.10752},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: accepted by INTERSPEECH2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4RMQJFCS\\Zhou et al. - 2018 - Syllable-Based Sequence-to-Sequence Speech Recogni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\EPIH7EXJ\\1804.html:text/html},
}

@inproceedings{hejtmanek_using_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Syllables} as {Acoustic} {Units} for {Spontaneous} {Speech} {Recognition}},
	isbn = {978-3-642-15760-8},
	doi = {10.1007/978-3-642-15760-8_38},
	abstract = {In this work, we deal with advanced context-dependent automatic speech recognition (ASR) of Czech spontaneous talk using hidden Markov models (HMM). Context-dependent units (e.g. triphones, diphones) in ASR systems provide significant improvement against simple non-context-dependent units. However, for spontaneous speech recognition we had to overcome some very challenging tasks. For one, the number of syllables compared to the size of spontaneous speech corpus makes the usage of context-dependent units very difficult. The main part of this article shows problems and procedures to effectively build and use a syllable-based ASR with the LASER (ASR system developed at Department of Computer Science and Engineering, Faculty of Applied Sciences). The procedures are usable with virtual any modern ASR.},
	language = {en},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Hejtmánek, Jan},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2010},
	keywords = {Acoustic Model, Automatic Speech Recognition, Hide Markov Model, Speech Recognition, Spontaneous Speech},
	pages = {299--305},
}

@article{fujimura_syllable_1975,
	title = {Syllable as a {Unit} of {Speech} {Recognition}},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162631},
	abstract = {Basic problems involved in automatic recognition of continuous speech are discussed with reference to the recently developed template matching technique using dynamic programming. Irregularities in phonetic manifestations of phonemes are discussed and it is argued that the syllable, phonologically redefined, will serve as the effective minimal unit in the time domain. English syllable structures are discussed from this point of view using the notions of "syllable features" and "vowel affinity."},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Fujimura, O.},
	month = feb,
	year = {1975},
	keywords = {Acoustics, Automatic speech recognition, Dentistry, Detectors, Dynamic programming, Isolation technology, Liquids, Speech analysis, Speech recognition, Tail},
	pages = {82--87},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2GFTFRKM\\authors.html:text/html},
}

@article{pratap_mls_2020,
	title = {{MLS}: {A} {Large}-{Scale} {Multilingual} {Dataset} for {Speech} {Research}},
	shorttitle = {{MLS}},
	url = {http://arxiv.org/abs/2012.03411},
	doi = {10.21437/Interspeech.2020-2826},
	abstract = {This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.},
	urldate = {2021-06-29},
	journal = {Interspeech 2020},
	author = {Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
	month = oct,
	year = {2020},
	note = {arXiv: 2012.03411},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {2757--2761},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\9JMDBDWR\\Pratap et al. - 2020 - MLS A Large-Scale Multilingual Dataset for Speech.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\QR4LRLNP\\2012.html:text/html},
}

@inproceedings{punjabi_joint_2021,
	title = {Joint {ASR} and {Language} {Identification} {Using} {RNN}-{T}: {An} {Efficient} {Approach} to {Dynamic} {Language} {Switching}},
	shorttitle = {Joint {ASR} and {Language} {Identification} {Using} {RNN}-{T}},
	doi = {10.1109/ICASSP39728.2021.9413734},
	abstract = {Conventional dynamic language switching enables seamless multilingual interactions by running several monolingual ASR systems in parallel and triggering the appropriate downstream components using a standalone language identification (LID) service. Since this solution is neither scalable nor cost- and memory-efficient, especially for on-device applications, we propose end-to-end, streaming, joint ASR-LID architectures based on the recurrent neural network transducer framework. Two key formulations are explored: (1) joint training using a unified output space for ASR and LID vocabularies, and (2) joint training viewed as multi-task optimization. We also evaluate the benefit of using auxiliary language information obtained on-the-fly from an acoustic LID classifier. Experiments with the English-Hindi language pair show that: (a) multi-task architectures perform better overall, and (b) the best joint architecture surpasses monolingual ASR (6.4–9.2\% word error rate reduction) and acoustic LID (53.9–56.1\% error rate reduction) baselines while reducing the overall memory footprint by up to 46\%.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Punjabi, Surabhi and Arsikere, Harish and Raeesy, Zeynab and Chandak, Chander and Bhave, Nikhil and Bansal, Ankish and Müller, Markus and Murillo, Sergio and Rastrow, Ariya and Stolcke, Andreas and Droppo, Jasha and Garimella, Sri and Maas, Roland and Hans, Mat and Mouchtaris, Athanasios and Kunzmann, Siegfried},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {code switching, Computer architecture, Error analysis, joint modeling, language identification, multilingual, recurrent neural network transducer, Recurrent neural networks, Switches, Training, Transducers, Vocabulary},
	pages = {7218--7222},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JRLSUL8J\\9413734.html:text/html},
}

@inproceedings{doshi_extending_2021,
	title = {Extending {Parrotron}: {An} {End}-to-{End}, {Speech} {Conversion} and {Speech} {Recognition} {Model} for {Atypical} {Speech}},
	shorttitle = {Extending {Parrotron}},
	doi = {10.1109/ICASSP39728.2021.9414644},
	abstract = {We present an extended Parrotron model: a single, end-to-end network that enables voice conversion and recognition simultaneously. Input spectrograms are transformed to output spectrograms in the voice of a predetermined target speaker while also generating hypotheses in a target vocabulary. We study the performance of this novel architecture, which jointly predicts speech and text, on atypical (e.g. dysarthric) speech. We show that with as little as an hour of atypical speech, speaker adaptation can yield a 77\% relative reduction in Word Error Rate (WER), measured by ASR performance on the converted speech. We also show that data augmentation using a customized synthesizer built on atypical speech can provide an additional 10\% relative improvement over the best speaker-adapted model. Finally, we show how these methods generalize across 8 types of atypical speech for a range of speech impairment severities.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Doshi, Rohan and Chen, Youzheng and Jiang, Liyang and Zhang, Xia and Biadsy, Fadi and Ramabhadran, Bhuvana and Chu, Fang and Rosenberg, Andrew and Moreno, Pedro J.},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Speech recognition, Error analysis, Vocabulary, Adaptation models, Conferences, Measurement uncertainty, sequence-to-sequence model, speech impairments, speech normalization, speech recognition, Synthesizers, voice conversion},
	pages = {6988--6992},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JLNLC79V\\9414644.html:text/html},
}

@inproceedings{li_towards_2019,
	title = {Towards {Code}-switching {ASR} for {End}-to-end {CTC} {Models}},
	doi = {10.1109/ICASSP.2019.8683223},
	abstract = {Although great progress has been made on end-to-end (E2E) models for monolingual and multilingual automatic speech recognition (ASR), there is no successful study for E2E models on the challenging intra-sentential code-switching (CS) ASR task to our best knowledge. In this paper, we propose an approach for CS ASR using E2E connectionist temporal classification (CTC) models. We use a frame-level language identification model to linearly adjust the posteriors of an E2E CTC model. We evaluate the proposed method on Microsoft live Chinese Cortana data with 7000 hours Chinese and English monolingual data and 300 hours CS data as the training data. Trained with only monolingual data without observing any CS data, the proposed method can obtain up to 6.3\% relative word error rate (WER) reduction. In the scenario of training with both monolingual and CS data, the proposed method can get up to 4.2\% relative WER improvement. This approach can also maintain comparable performance on a Chinese test set compared with baseline models.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Ke and Li, Jinyu and Ye, Guoli and Zhao, Rui and Gong, Yifan},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Speech recognition, language identification, Recurrent neural networks, ASR, code-switching, CTC, Data models, Decoding, end-to-end, Predictive models},
	pages = {6076--6080},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\ATTLXWWT\\8683223.html:text/html},
}

@article{naowarat_reducing_2021,
	title = {Reducing {Spelling} {Inconsistencies} in {Code}-{Switching} {ASR} using {Contextualized} {CTC} {Loss}},
	url = {http://arxiv.org/abs/2005.07920},
	abstract = {Code-Switching (CS) remains a challenge for Automatic Speech Recognition (ASR), especially character-based models. With the combined choice of characters from multiple languages, the outcome from character-based models suffers from phoneme duplication, resulting in language-inconsistent spellings. We propose Contextualized Connectionist Temporal Classification (CCTC) loss to encourage spelling consistencies of a character-based non-autoregressive ASR which allows for faster inference. The CCTC loss conditions the main prediction on the predicted contexts to ensure language consistency in the spellings. In contrast to existing CTC-based approaches, CCTC loss does not require frame-level alignments, since the context ground truth is obtained from the model's estimated path. Compared to the same model trained with regular CTC loss, our method consistently improved the ASR performance on both CS and monolingual corpora.},
	urldate = {2021-06-29},
	journal = {arXiv:2005.07920 [cs, eess]},
	author = {Naowarat, Burin and Kongthaworn, Thananchai and Karunratanakul, Korrawe and Wu, Sheng Hui and Chuangsuwanich, Ekapol},
	month = jun,
	year = {2021},
	note = {arXiv: 2005.07920},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: ICASSP 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\WCLBKTE4\\Naowarat et al. - 2021 - Reducing Spelling Inconsistencies in Code-Switchin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\D3PBEJCQ\\2005.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How many syllables exist in the {American} {English} language? - {Quora}},
	url = {https://www.quora.com/How-many-syllables-exist-in-the-American-English-language},
	urldate = {2021-06-29},
	annote = {I'm going to give you a very specific number, 15,831. That is how many syllables there are in the English language.A syllable is a vowel sound with or without consonants.Girl can be pronounced as one or two syllables grl or grr-ell. Same with boy, boy-yee. People from Australia get two syllables out of the word no, somehow.Let's count them all, shall we?No, that's been done already, here's a link: Page on nyu.eduIt provides a very thorough look at what a syllable is and how many there are (and maybe how many there should be).Hope this helps.},
	file = {How many syllables exist in the American English language? - Quora:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\8HCXBKRZ\\How-many-syllables-exist-in-the-American-English-language.html:text/html},
}

@misc{noauthor_wayback_2013,
	title = {Wayback {Machine}},
	url = {http://web.archive.org/web/20130914173723/http://semarch.linguistics.fas.nyu.edu/barker/Syllables/index.txt},
	urldate = {2021-06-29},
	month = sep,
	year = {2013},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\C5WUSTPG\\index.html:text/html},
}

@article{gao_visualvoice_2021,
	title = {{VisualVoice}: {Audio}-{Visual} {Speech} {Separation} with {Cross}-{Modal} {Consistency}},
	shorttitle = {{VisualVoice}},
	url = {http://arxiv.org/abs/2101.03149},
	abstract = {We introduce a new approach for audio-visual speech separation. Given a video, the goal is to extract the speech associated with a face in spite of simultaneous background sounds and/or other human speakers. Whereas existing methods focus on learning the alignment between the speaker's lip movements and the sounds they generate, we propose to leverage the speaker's face appearance as an additional prior to isolate the corresponding vocal qualities they are likely to produce. Our approach jointly learns audio-visual speech separation and cross-modal speaker embeddings from unlabeled video. It yields state-of-the-art results on five benchmark datasets for audio-visual speech separation and enhancement, and generalizes well to challenging real-world videos of diverse scenarios. Our video results and code: http://vision.cs.utexas.edu/projects/VisualVoice/.},
	urldate = {2021-06-29},
	journal = {arXiv:2101.03149 [cs, eess]},
	author = {Gao, Ruohan and Grauman, Kristen},
	month = apr,
	year = {2021},
	note = {arXiv: 2101.03149},
	keywords = {Computer Science - Sound, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: In CVPR 2021. Project page: http://vision.cs.utexas.edu/projects/VisualVoice/},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\XE6RDDT6\\Gao and Grauman - 2021 - VisualVoice Audio-Visual Speech Separation with C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\CVI3R4E5\\2101.html:text/html},
}

@inproceedings{rieger_idiosyncratic_2001,
	title = {Idiosyncratic fillers in the speech of bilinguals},
	volume = {DISS'01},
	url = {https://www.isca-speech.org/archive_open/diss_01/dis1_081.html},
	urldate = {2021-06-29},
	author = {Rieger, Caroline L.},
	year = {2001},
	pages = {81--84},
	file = {DISS'01 Abstract\: Rieger, Caroline L.:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7WUDW5FF\\dis1_081.html:text/html},
}

@article{daghaghi_accelerating_2021,
	title = {Accelerating {SLIDE} {Deep} {Learning} on {Modern} {CPUs}: {Vectorization}, {Quantizations}, {Memory} {Optimizations}, and {More}},
	shorttitle = {Accelerating {SLIDE} {Deep} {Learning} on {Modern} {CPUs}},
	url = {http://arxiv.org/abs/2103.10891},
	abstract = {Deep learning implementations on CPUs (Central Processing Units) are gaining more traction. Enhanced AI capabilities on commodity x86 architectures are commercially appealing due to the reuse of existing hardware and virtualization ease. A notable work in this direction is the SLIDE system. SLIDE is a C++ implementation of a sparse hash table based back-propagation, which was shown to be significantly faster than GPUs in training hundreds of million parameter neural models. In this paper, we argue that SLIDE's current implementation is sub-optimal and does not exploit several opportunities available in modern CPUs. In particular, we show how SLIDE's computations allow for a unique possibility of vectorization via AVX (Advanced Vector Extensions)-512. Furthermore, we highlight opportunities for different kinds of memory optimization and quantizations. Combining all of them, we obtain up to 7x speedup in the computations on the same hardware. Our experiments are focused on large (hundreds of millions of parameters) recommendation and NLP models. Our work highlights several novel perspectives and opportunities for implementing randomized algorithms for deep learning on modern CPUs. We provide the code and benchmark scripts at https://github.com/RUSH-LAB/SLIDE},
	urldate = {2021-06-29},
	journal = {arXiv:2103.10891 [cs]},
	author = {Daghaghi, Shabnam and Meisburger, Nicholas and Zhao, Mengnan and Wu, Yong and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.10891},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\BSYXYDSD\\Daghaghi et al. - 2021 - Accelerating SLIDE Deep Learning on Modern CPUs V.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HDNRPP74\\2103.html:text/html},
}

@article{musgrave_metric_2020,
	title = {A {Metric} {Learning} {Reality} {Check}},
	url = {http://arxiv.org/abs/2003.08505},
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best.},
	urldate = {2021-06-29},
	journal = {arXiv:2003.08505 [cs]},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	month = sep,
	year = {2020},
	note = {arXiv: 2003.08505},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Visit https://www.github.com/KevinMusgrave/powerful-benchmarker for supplementary material, including the source code, configuration files, log files, and interactive bayesian optimization plots},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\4DDYLHPA\\Musgrave et al. - 2020 - A Metric Learning Reality Check.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\MUCR9S4H\\2003.html:text/html},
}

@article{jang_univnet_2021,
	title = {{UnivNet}: {A} {Neural} {Vocoder} with {Multi}-{Resolution} {Spectrogram} {Discriminators} for {High}-{Fidelity} {Waveform} {Generation}},
	shorttitle = {{UnivNet}},
	url = {http://arxiv.org/abs/2106.07889},
	abstract = {Most neural vocoders employ band-limited mel-spectrograms to generate waveforms. If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible. However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated. To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multi-resolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.},
	urldate = {2021-06-29},
	journal = {arXiv:2106.07889 [cs, eess]},
	author = {Jang, Won and Lim, Dan and Yoon, Jaesam and Kim, Bongwan and Kim, Juntae},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.07889},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted to INTERSPEECH 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\HW6DH2C7\\Jang et al. - 2021 - UnivNet A Neural Vocoder with Multi-Resolution Sp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\D5XSACFF\\2106.html:text/html},
}

@article{marchand_automatic_2009,
	title = {Automatic {Syllabification} in {English}: {A} {Comparison} of {Different} {Algorithms}},
	volume = {52},
	issn = {0023-8309},
	shorttitle = {Automatic {Syllabification} in {English}},
	doi = {10.1177/0023830908099881},
	abstract = {Automatic syllabification of words is challenging, not least because the syllable is not easy to define precisely. Consequently, no accepted standard algorithm for automatic syllabification exists. There are two broad approaches: rule-based and data-driven. The rule-based method effectively embodies some theoretical position regarding the syllable, whereas the data-driven paradigm tries to infer “new” syllabifications from examples assumed to be correctly syllabified already. This article compares the performance of several variants of the two basic approaches. Given the problems of definition, it is difficult to determine a correct syllabification in all cases and so to establish the quality of the “gold standard” corpus used either to evaluate quantitatively the output of an automatic algorithm or as the example-set on which data-driven methods crucially depend. Thus, we look for consensus in the entries in multiple lexical databases of pre-syllabified words. In this work, we have used two independent lexicons, and extracted from them the same 18,016 words with their corresponding (possibly different) syllabifications. We have also created a third lexicon corresponding to the 13,594 words that share the same syllabifications in these two sources. As well as two rule-based approaches (Hammond's and Fisher's implementation of Kahn's), three data-driven techniques are evaluated: a look-up procedure, an exemplar-based generalization technique, and syllabification by analogy (SbA). The results on the three databases show consistent and robust patterns. First, the data-driven techniques outperform the rule-based systems in word and juncture accuracies by a very significant margin but require training data and are slower. Second, syllabification in the pronunciation domain is easier than in the spelling domain. Finally, best results are consistently obtained with SbA.},
	language = {en},
	number = {1},
	urldate = {2021-07-02},
	journal = {Language and Speech},
	author = {Marchand, Yannick and Adsett, Connie R. and Damper, Robert I.},
	month = mar,
	year = {2009},
	keywords = {analogy, computational linguistics, corpus linguistics, rule-based systems, speech technology, syllabification},
	pages = {1--27},
	file = {Accepted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\KIUS6D3Q\\Marchand et al. - 2009 - Automatic Syllabification in English A Comparison.pdf:application/pdf},
}

@inproceedings{zhang_learning_1997,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {English} syllabification for words},
	isbn = {978-3-540-69612-4},
	doi = {10.1007/3-540-63614-5_17},
	abstract = {This paper presents the LE-SR (Learning English Syllabification Rules learning system, which learns English syllabification rules using a symbolic pattern recognition approach. LE-SR was tested on NTC2, a 20,000 English word pronouncing dictionary. The ten-fold accuracy ranged from 94.55\% to 96.05\% for words and 96.81\% to 97.71\% for syllables.The frequency of the rule usage indicates that most of the syllabification (86.21\%) in NTC2 is covered by only 0.64\% of the syllabification rules produced by LE-SR, while exceptions (2.45\%) require 76.77\% of the syllabification rules. This experimental result is consistent with the linguistics literature. Relatively few rules cover the vast majority of cases, but a considerable number of exceptions must be handled individually.The learned rules can be used to divide any English word into syllables. Based on syllables, the English stress rules can be learned. Syllabification and stress greatly influence the naturalness of the output of a text-to-speech system. Existing text-to-speech systems have either obtained syllabification from a dictionary or used a few hand coded rules. Using machine learning approach to obtain these rules is a step towards producing natural speech sounding for text-to-speech systems.},
	language = {en},
	booktitle = {Foundations of {Intelligent} {Systems}},
	publisher = {Springer},
	author = {Zhang, Jian and Hamilton, Howard J.},
	editor = {Raś, Zbigniew W. and Skowron, Andrzej},
	year = {1997},
	keywords = {English Word, Natural Speech, Performance Element, Rule Usage, Vowel Sound},
	pages = {177--186},
}

@article{goslin_comparing_2007,
	title = {Comparing {French} syllabification in preliterate children and adults},
	volume = {28},
	doi = {10.1017/S0142716407070178},
	abstract = {The influence of development and literacy upon syllabification in French was evaluated by comparing the segmental behavior of 4- to 5-year-old preliterate children and adults using a pause insertion task. Participants were required to repeat bisyllabic words such as “fourmi” (ant) by inserting a pause between its two syllabic components (/fur/-/mi/). In the first experiment we tested segmentation over a range of 49 double intervocalic consonant clusters. A similar general segmentation behavior was observed in both age groups, with a pattern that fit the predictions from a legality principle-based model of syllabification. Experiment 2 revealed that opacity between phonological and orthographic representations lead to increased ambisyllabic responses and a reduction in segmentation consistency in adults. In total, these findings indicate that syllabic forms are consistently represented from an early age, but that segmentation in metalinguistic tasks is susceptible to contamination from spelling and etymological knowledge.},
	language = {en},
	number = {2},
	urldate = {2021-07-03},
	journal = {Applied Psycholinguistics},
	author = {Goslin, Jeremy and Floccia, Caroline},
	month = apr,
	year = {2007},
	pages = {341--367},
	file = {Full Text:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\BRW3JKHF\\Goslin and Floccia - 2007 - Comparing French syllabification in preliterate ch.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\C4GN9FZT\\C3CD62BF8020CBF33E607D1ECBEC7591.html:text/html},
}

@article{fallows_experimental_1981,
	title = {Experimental evidence for {English} syllabification and syllable structure},
	volume = {17},
	issn = {1469-7742, 0022-2267},
	url = {https://www.cambridge.org/core/journals/journal-of-linguistics/article/abs/experimental-evidence-for-english-syllabification-and-syllable-structure/E9E39A4D30B053C19700402CD7A28DA6},
	doi = {10.1017/S0022226700007027},
	abstract = {Most phonologists regard the syllable as a unit of language. In recent years, partly in reaction to Chomsky's and Halle's neglect of it in The sound pattern of English, much work has been done to incorporate the syllable into phonological theory (Anderson \& Jones, 1974; Bailey, 1978; Hoard, 1971; Hooper, 1972, 1974, 1976, 1978; Kahn, 1976; Pulgram, 1970; Rudes, 1977; Vennemann, 1972).Syllable theories have been based on evidence from phonetics, phonological processes, prosody, language change, child language acquisition, and language universals. The purpose of this experimental study is (a) to contribute empirical evidence about the nature of the syllable from native speakers' actual syllabifications of words and (b) to determine how this evidence reflects on the syllable theories already proposed.Syllable studies have focused on two major questions: (1) what is the structure of the syllable and (2) how are words divided into syllables (syllabified). These questions are obviously related, and the answers to each have implications for the other. This study was designed to elicit data directly addressing both questions.},
	language = {en},
	number = {2},
	urldate = {2021-07-03},
	journal = {Journal of Linguistics},
	author = {Fallows, Deborah},
	month = sep,
	year = {1981},
	note = {Publisher: Cambridge University Press},
	pages = {309--317},
	file = {Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\2RZVF5RQ\\E9E39A4D30B053C19700402CD7A28DA6.html:text/html},
}

@article{baker_trainable_1979,
	title = {Trainable grammars for speech recognition},
	volume = {65},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.2017061},
	doi = {10.1121/1.2017061},
	number = {S1},
	urldate = {2021-07-04},
	journal = {The Journal of the Acoustical Society of America},
	author = {Baker, J. K.},
	month = jun,
	year = {1979},
	note = {Publisher: Acoustical Society of America},
	pages = {S132--S132},
	file = {Full Text PDF:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\I75QYALW\\Baker - 1979 - Trainable grammars for speech recognition.pdf:application/pdf;Snapshot:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\Z5DLTTC8\\1.html:text/html},
}

@article{hadian_flat-start_2018,
	title = {Flat-{Start} {Single}-{Stage} {Discriminatively} {Trained} {HMM}-{Based} {Models} for {ASR}},
	volume = {26},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2018.2848701},
	abstract = {In recent years, end-to-end approaches to automatic speech recognition have received considerable attention as they are much faster in terms of preparing resources. However, conventional multistage approaches, which rely on a pipeline of training hidden Markov models (HMM)-GMM models and tree-building steps still give the state-of-the-art results on most databases. In this study, we investigate flat-start one-stage training of neural networks using lattice-free maximum mutual information (LF-MMI) objective function with HMM for large vocabulary continuous speech recognition. We thoroughly look into different issues that arise in such a setup and propose a standalone system, which achieves word error rates (WER) comparable with that of the state-of-the-art multi-stage systems while being much faster to prepare. We propose to use full biphones to enable flat-start context-dependent (CD) modeling and show through experiments that our CD modeling approach can be almost as effective as regular tree-based CD modeling. We show that our flat-start LF-MMI setup together with this tree-free CD modeling technique achieves 10 to 25 \% relative WER reduction compared to other end-to-end methods on well-known databases. The improvements are larger for smaller databases.},
	number = {11},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hadian, Hossein and Sameti, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {automatic speech recognition, Context modeling, Databases, Decoding, flat-start, hidden Markov models, Hidden Markov models, Lattice-free, Linear programming, maximum mutual information, Neural networks, single-stage, Training},
	pages = {1949--1961},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\LJVW5XRV\\8387866.html:text/html},
}

@article{baker_dragon_1975,
	title = {The {DRAGON} system–{An} overview},
	volume = {23},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1975.1162650},
	abstract = {This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model–that of a probabilistic function of a Markov process–is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities.},
	number = {1},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Baker, J.},
	month = feb,
	year = {1975},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Acoustic measurements, Automatic speech recognition, Data mining, Error analysis, Markov processes, Power system modeling, Power system reliability, Random variables, Speech recognition, Vocabulary},
	pages = {24--29},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\7ZUX56CQ\\1162650.html:text/html},
}

@inproceedings{krantz_language-agnostic_2019,
	title = {Language-{Agnostic} {Syllabification} with {Neural} {Sequence} {Labeling}},
	doi = {10.1109/ICMLA.2019.00141},
	abstract = {The identification of syllables within phonetic sequences is known as syllabification. This task is thought to play an important role in natural language understanding, speech production, and the development of speech recognition systems. The concept of the syllable is cross-linguistic, though formal definitions are rarely agreed upon, even within a language. In response, data-driven syllabification methods have been developed to learn from syllabified examples. These methods often employ classical machine learning sequence labeling models. In recent years, recurrence-based neural networks have been shown to perform increasingly well for sequence labeling tasks such as named entity recognition (NER), part of speech (POS) tagging, and chunking. We present a novel approach to the syllabification problem which leverages modern neural network techniques. Our network is constructed with long short-term memory (LSTM) cells, a convolutional component, and a conditional random field (CRF) output layer. Existing syllabification approaches are rarely evaluated across multiple language families. To demonstrate cross-linguistic generalizability, we show that the network is competitive with state of the art systems in syllabifying English, Dutch, Italian, French, Manipuri, and Basque datasets.},
	booktitle = {2019 18th {IEEE} {International} {Conference} {On} {Machine} {Learning} {And} {Applications} ({ICMLA})},
	author = {Krantz, Jacob and Dulin, Maxwell and De Palma, Paul},
	month = dec,
	year = {2019},
	keywords = {Hidden Markov models, Labeling, Natural language processing, Neural networks, Phonetics, Speech recognition, Supervised learning, Tagging, Task analysis},
	pages = {804--810},
	file = {Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\JBFIJQPI\\Krantz et al. - 2019 - Language-Agnostic Syllabification with Neural Sequ.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\VCHKMGCW\\8999232.html:text/html},
}

@inproceedings{sak_learning_2015,
	title = {Learning acoustic frame labeling for speech recognition with recurrent neural networks},
	doi = {10.1109/ICASSP.2015.7178778},
	abstract = {We explore alternative acoustic modeling techniques for large vocabulary speech recognition using Long Short-Term Memory recurrent neural networks. For an acoustic frame labeling task, we compare the conventional approach of cross-entropy (CE) training using fixed forced-alignments of frames and labels, with the Connectionist Temporal Classification (CTC) method proposed for labeling unsegmented sequence data. We demonstrate that the latter can be implemented with finite state transducers. We experiment with phones and context dependent HMM states as acoustic modeling units. We also investigate the effect of context in acoustic input by training unidirectional and bidirectional LSTM RNN models. We show that a bidirectional LSTM RNN CTC model using phone units can perform as well as an LSTM RNN model trained with CE using HMM state alignments. Finally, we also show the effect of sequence discriminative training on these models and show the first results for sMBR training of CTC models.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Sak, Haşim and Senior, Andrew and Rao, Kanishka and İrsoy, Ozan and Graves, Alex and Beaufays, Françoise and Schalkwyk, Johan},
	month = apr,
	year = {2015},
	note = {ISSN: 2379-190X},
	keywords = {acoustic modeling, Acoustics, Context modeling, CTC, Gold, Hidden Markov models, LSTM, Neural networks, RNN, Speech recognition, Training},
	pages = {4280--4284},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\AU75FQED\\7178778.html:text/html},
}

@inproceedings{soltau_neural_2017,
	title = {Neural {Speech} {Recognizer}: {Acoustic}-to-{Word} {LSTM} {Model} for {Large} {Vocabulary} {Speech} {Recognition}},
	shorttitle = {Neural {Speech} {Recognizer}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/1566.html},
	doi = {10.21437/Interspeech.2017-1566},
	language = {en},
	urldate = {2021-07-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Soltau, Hagen and Liao, Hank and Sak, Haşim},
	month = aug,
	year = {2017},
	pages = {3707--3711},
	file = {Submitted Version:C\:\\Users\\Jim O'Regan\\Zotero\\storage\\PPI3J34N\\Soltau et al. - 2017 - Neural Speech Recognizer Acoustic-to-Word LSTM Mo.pdf:application/pdf},
}

@inproceedings{prabhavalkar_comparison_2017,
	title = {A {Comparison} of {Sequence}-to-{Sequence} {Models} for {Speech} {Recognition}},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0233.html},
	doi = {10.21437/Interspeech.2017-233},
	language = {en},
	urldate = {2021-07-05},
	booktitle = {Interspeech 2017},
	publisher = {ISCA},
	author = {Prabhavalkar, Rohit and Rao, Kanishka and Sainath, Tara N. and Li, Bo and Johnson, Leif and Jaitly, Navdeep},
	month = aug,
	year = {2017},
	pages = {939--943},
}

@book{saussure_course_1959,
	address = {New York},
	title = {Course in general linguistics.},
	language = {eng},
	publisher = {Philosophical Library},
	author = {Saussure, Ferdinand de},
	translator = {Baskin, Wade},
	year = {1959},
	keywords = {Comparative linguistics, Language and languages},
	annote = {Open Library ID: OL23291521M},
}
