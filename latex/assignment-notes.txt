https://groups.google.com/g/kaldi-help/c/i17cx9SEVVo/m/w0uxwLE0BgAJ

The TIMIT testing protocol pretty much dictates that you are supposed to use the bigram phone LM, which is an extremely poor LM for that task.  That's one of the reasons why almost no results using TIMIT are meaningful-- because the use of a good LM is "not allowed", certain methods, like recurrent acoustic models and CTC, which *implicitly* do language modeling, appear to be doing well.

10.1109/TASLP.2018.2848701
hadian_flat-start_2018
The output labels in CTC are usually phonemes or
graphemes (i.e. characters), however, longer/shorter linguistic
units such as sub-words or CD phones can also be used.
Recently, [16] used whole word labels for CTC training on
large amount of data, where they achieved state-of-the-art
results without using a language model. The main advantage
of word-based CTC is that no pronunciation lexicon or LM
is needed, however, a potential drawback is that the set of
words that the model can predict are fixed and limited to the
words that have been used in training. A very recent paper
[19] addresses this issue by using a mixed-unit CTC model
which decomposes the OOV words into sequences of frequent
words and multi-letter units, outperforming the CD-phone-based 
conventional baseline

3) We propose to use full biphones to enable flat-start
CD modeling without requiring any previously trained
models or creating state-tying trees. We show empirically that our simple CD modeling scheme can perform
almost as well as a regular state-tying tree built using
HMM-GMM alignments.

goslin_comparing_2007

\citet{yi_efficiently_2021}
In this work, we fuse a pre-trained acoustic encoder (wav2vec2.0) and a pre-trained linguistic encoder (BERT) into an end-to-end ASR model

\citet{marchand_automatic_2009} list two major problems to the creation of a gold standard syllabification corpus: disagreement, and the impracticality of collecting a sufficiently large corpus.


